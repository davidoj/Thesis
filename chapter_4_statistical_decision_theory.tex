%!TEX root = main.tex

\chapter{Statistical Decision Theory}

\todo[inline]{I think I've got a good idea for a result that is relevant to the discussion below}

\emph{Proposition:} If you have a decision problem of the following form:

\begin{itemize}
    \item You get an observation $x$ and may return some mixtures of decisions in $\Delta(\sigalg{D})$; that is, you can choose some function $x\to \Delta(\sigalg{D})$
    \item For any function $Q:x\to \Delta(\sigalg{D})$ we have a forecast of the observations, decisions and consequences, and it is appropriate to model the forecast with some probability over observations decisions and consequences $\prob{P}_Q\in \Delta(\sigalg{X}\otimes\sigalg{D}\otimes\sigalg{Y})$
    \begin{itemize}
        \item You can probably make this a collection of probabilities/Markov kernel, it's just simpler to consider one probability for this outline
    \end{itemize}
    \item For all $Q,R:x\to \Delta(\sigalg{D})$, $\prob{P}_Q^{\RV{X}}=\prob{P}_R^{\RV{X}}$, $\prob{P}_Q^{\RV{Y}|\RV{X}\RV{D}}=\prob{P}_R^{\RV{Y}|\RV{X}\RV{D}}$ and $\prob{P}_Q^{\RV{D}|\RV{X}}=Q$
    \begin{itemize}
        \item That is: I should expect the same observations whatever function I end up choosing and I expect the same consequences holding the observations and decision fixed (even more informally: it doesn't matter how I choose decisions provided I end up choosing the same one)
        \item Finally, the decision function we choose is the relation between $\RV{X}$ and $\RV{D}$
    \end{itemize}
    \item There exists some $Q$ such that $\prob{P}_Q$ has full support
\end{itemize}

Then there is a unique see-do forecast $\kernel{T}$ such that for every $Q:x\to \Delta(\sigalg{D})$, $\kernel{T}$ ``intertwined'' with $Q$ is equal to $\prob{P}_Q$ (intertwining being a well-defined operation I haven't written down).

\emph{Proof sketch:} \citet{jacobs_causal_2019} introduces the notion of a ``2-comb'' which is a Markov kernel with two inputs, two outputs and one of the outputs is independent of one of the inputs. This is essentially equivalent to a see-do model (Definition \ref{def:seedo}) - ``essentially'' because we need to give wires names to get a see-do model, but the independence condition means that there is always a unique way to do this.

Jacobs et. al. prove a \emph{comb disintegration} theorem: given any probability with 3 wires and full support, there is a unique 2-comb and a Markov kernel that can be ``intertwined'' to give the original probability. Thus we can get a unique 2-comb $\RV{V}$ for some $Q$ where $\prob{P}_Q$.

Further, we can get a collection of 2-combs $\RV{U},\RV{W}$,... for $R, S:x\to \Delta(\sigalg{D})$ where $\prob{P}_R$ may or may not have full support.

If we have two 2-combs $\kernel{U}$ and $\kernel{V}$ such that $\kernel{U}^{\RV{X}|\RV{H}}=\kernel{V}^{\RV{X}|\RV{H}}$ and $\kernel{U}^{\RV{Y}|\RV{X}\RV{H}\RV{D}}=\kernel{V}^{\RV{Y}|\RV{X}\RV{H}\RV{D}}$ then $\kernel{U}=\kernel{V}$ (applying Lemma \ref{lem:representation_of_kernels}). Thus $\kernel{U}$ is a unique 2-comb that every probability $\prob{P}_Q$, $\prob{P}_R$ etc. can be disintegrated to.


\emph{Comment:} This is a ``soft representation theorem''. While the theorems discussed below try to establish the need for expected utility and probability from constraints on rational preferences or beliefs, this one says if you're already happy to use probability then you can represent your knowledge with a see-do forecast/see-do model

Also, it ties in with von Neumann-Morgenstern: we have assumed at the outset that we are choosing between $\prob{P}_Q$, $\prob{P}_R$ etc., which are lotteries (albeit non-standard ones) in the language of vNM.

It might also tie in with Walley - if we have a \emph{coherent prevision} for each decision function, then this can be uniquely represented with a set of probailities.

\todo[inline]{End of the sketch of the result}

\section{Representing prior knowledge in decision problems}

We introduced see-do models in Chapter \ref{ch:2p_statmodels} and gave the random variables suggestive names: $\RV{D}$ is the ``choices'', $\RV{X}$ is the ``observations'', $\RV{H}$ is the ``hypothesis'' and $\RV{Y}$ is the ``consequences''. However, as we discussed, the actual interpretations of these variables depend on what exactly the model is being used for. One use case is making decisions. 

Consider an idealised decision problem where a set of observations will be given which may take values in $X$, a utility function $u:Y\to \mathbb{R}$ is given and, after viewing the observations, one must select one decision from a known set $D$ and anticipates some element of $Y$ to occur as a consequence. The utility function measures which elements of $Y$ are preferred, where $u(y)>u(y')$ means $y$ is preferred to $y'$, and one aims to select a decision that results in an element of $Y$ that is preferred to other elements of $Y$ according to $u$.

In order to solve this kind of problem, we need some additional knowledge: we cannot choose a decision in $D$ on the basis of preferences over $Y$ unless we know decisions have some effect on which elements of $Y$ are likely to occur. In many, it is unlikely that we could ever know that each particular decision will force a particular element of $Y$ to occur, so we need some means of representing the consequences of decisions with uncertainty. We might use probability to represent all relevant uncertainty, and so each decision is associated with a particular probability distribution in $\Delta(\sigalg{Y})$. Alternatively, we could entertain two types of uncertainty: we continue to suppose each decision is associated with a probability distribution in $\Delta(\sigalg{Y})$ but we have some uncertainty about which map $D\to\Delta(\sigalg{Y})$ is appropriate and we are not willing to represent this uncertainty probabilistically, and choose a \emph{nonempty set} of maps $D\to \Delta(\sigalg{Y})$ to represent our full uncertainty. 

If our uncertainty is represented by a single map $D\to \Delta(\sigalg{Y})$ then we have a do-forecast (Definition \ref{def:do_forecast}. If our uncertainty is represented by a set of maps, then we can assign each map an index $h\in H$ for some set of hypotheses $H$ such that $h(d)\in \Delta(\sigalg{Y})$. Assuming measurability, then $\kernel{T}:H\times D\to \Delta(\sigalg{Y})$ given by $\kernel{T}:(h,d)\mapsto h(d)$ is a two-player statistical model (Definition \ref{def:2p_stat}).

We will often also wish to make use of the observations $X$ to improve our decision. If we are fortunate, the observations can reduce our uncertainty. We may wish to represent joint uncertainty as a single map to joint distributions $\kernel{T}:D\to \Delta(\sigalg{X}\otimes \sigalg{Y})$ and ``make use of'' an observation $x\in X$ by conditioning $\kernel{T}$ on the event $\mathds{1}_{\RV{X}=x}$. Alternatively, we could choose a set of such maps and ``make use of'' observations by conditioning each map on them. If we add the assumption $\RV{X}\CI \RV{D}$ (which will be explained in more detail in this chapter) for all such maps, in the case of a single map we have a see-do forecast (Definition \ref{def:do_forecast}) and in the case of sets of maps we have a see-do model (Definition \ref{def:seedo}).

\todo[inline]{Define conditioning chapter 2}

\subsection{How should knowledge be represented?}

Decision problems require certain types of additional knowledge, and see-do models or forecasts can represent knowledge of this type. We can ask if and when it is a sound choice to use these kinds of models for this purpose. Our answer, for the time being, is to lean on prior work: we will show in in Chapter \ref{ch:ints_counterfactuals} that graphical causal models are a subset of see-do models and that two-player statistical models in conjunction with a particular set of assumptions defining ``counterfactual random variables'' yield standard potential outcomes models. Finally, in this chapter we will see how see-do models combined with utility yield statistical decision problems. As all of these are types of see-do model, or can be derived from see-do models along with additional provisions, it follows that if any of them are sometimes sound approaches for representing knowledge in decision problems then see-do models must be sound choices.

An alternative approach to answering this question is to define a set of axioms that we want our knowledge representation to satisfy and then show that these axioms are compatible with see-do models or even that these axioms imply see-do models. There is a body of literature of this genre and we will briefly survey results and comment on their relationship to see-do models. Most such ``representation theorems'' aim to show that a particular means of representing knowledge is \emph{necessary} given a number set of assumptions that purport to define ``rational preferences'' or ``rational beliefs''. The question of whether a given set of axioms are in fact required to call an agents preferences or beliefs rational is a difficult one, and we will not have anything to add to that discussion here -- neither by commenting on axioms already proposed or by proposing our own. However, we are still interested in comparing see-do models with model types used in existing representation theorems.

The following discussion will often make reference to \emph{preference relations}. A preference relation is a relation $\succ,\prec,\sim$ on a set $A$ such that for any $a,a'$ in $A$ we have:
\begin{itemize}
    \item Exactly one of $a\succ a'$, $a\prec a'$, $a\sim a'$ holds
    \item $(a\succ a')\iff(a'\prec a)$
    \item $a\succ a'$ and $a'\succ a''$ implies $a\succ a''$
\end{itemize}

This defintion is meant to correspond to the common sense idea of having preferences over some large set of things, where $\succ$ can be read as ``strictly better than'', $\prec$ read as ``strictly worse than'' and $\sim$ read as ``as good as''. Given any two things from the set, I can say which one I prefer, or if I prefer neither (and all of these are mutually exclusive). If I prefer $a$ to $a'$ then I think $a'$ is worse than $a$. Furthermore, if I prefer $a$ to $a'$ and $a'$ to $a''$ then I prefer $a$ to $a''$.

\subsubsection{von Neumann-Morgenstern utility}

\citet{von_neumann_theory_1944} proved that when the \emph{vNM axioms} hold (not defined here; see the original reference or \citet{steele_decision_2020}), an agent's preferences between ``lotteries'' (which for our purposes are probability distributions in $\Delta(\sigalg{Y})$) can be represented with a utility function $u:Y\to \mathbb{R}$ unique up to affine transformation along with the principle of expected utility, which is, for lotteries $\prob{P},\prob{P}'\in\Delta(\sigalg{Y})$ the rule that $\mathbb{E}_{\prob{P}}[u]> \mathbb{E}_{\prob{P}'}[u]$ is equivalent to the statement $\prob{P} \succ \prob{P}'$.

If we consider a set of decisions $D$ and a do-forecast $\kernel{T}^{\RV{Y}|\RV{{D}}}$, then we can identify the set of lotteries with the evaluations of each decision under $\kernel{T}$ -- that is, the lotteries are $\{\kernel{T}^{\RV{Y}|\RV{D}}_d|d\in D\}$. We can then define preferences over decisions: $d$ is strictly preferred to $d'$ if and only if $\kernel{T}_{d}^{\RV{Y}|\RV{D}}$ is strictly preferred to $\kernel{T}_{d'}^{\RV{Y}|\RV{D}}$. Then, if preferences over the set of lotteries satisfies the \emph{vNM axioms}, there exists a utility function $u:Y\to \mathbb{R}$ such that
\begin{itemize}
    \item $d\succ d'$ if and only if $\kernel{T}_d^{\RV{Y}|\RV{D}}u>\kernel{T}_{d'}^{\RV{Y}|\RV{D}}u$
\end{itemize}

\todo[inline]{Is this worth stating as a theorem?}

\subsubsection{Savage's decision theory}

The von Neumann-Morgenstern theorem shows that, if we take do-forecasts for granted and assuming the vNM axioms then we can represent preferences between decisions using expected utility. We are more interested in when we should use do-forecasts or see-do forecasts or see-do models. 

\citet{savage_foundations_1954} proved a representation theorem for decision problems, which has more overlap with our original question - namely, what should we use to represent the knowledge we bring to a kind of idealised decision problem. Savage's decision problems featured \emph{states} $H$ (which we can be identified with hypotheses in Definition \ref{def:2p_stat}), \emph{acts} $D$ (which can be identified with decisions in \ref{def:2p_stat}) and \emph{outcomes} $O$ (which can be identified with outcomes in Definition \ref{def:2p_stat}). Savage takes it as a given that we have a deterministic two player statistical model $(\kernel{T},\RV{H},\RV{D},\RV{O})$, and furthermore for \emph{any} function $f:H\to O$ there exists some $d\in D$ such that $\kernel{T}_{h,d} = \delta_{f(h)}$, and furthermore we have a preference relation on $D$. He then shows that if the \emph{Savage axioms} hold for the preference relation on $D$ then there exists a unique probability measure $\prob{P}\in \Delta(\sigalg{H})$ and a utility $u:O\to \mathbb{R}$ unique up to affine transformation such that

\begin{align}
(d\succ d')\iff (\prob{P}\otimes \delta_d)\kernel{T} u > (\prob{P}\otimes \delta_{d'})\kernel{T} u\label{eq:savage_utility}
\end{align}

The use of two player statistical models is baked into Savage's representation theorem as a basic assumption. However, he also shows that if the Savage axioms hold for preferences among outcomes, then it is possible to define a unique do-forecast

\begin{align}
    \kernel{F}:=(\prob{P}\otimes \mathrm{Id}_D)\kernel{T}
\end{align}

(using the existing definitions for $\RV{D}$ and $\RV{O}$) such that

\begin{align}
    (d\succ d')\iff \kernel{F}_d\kernel{T} u > \kernel{F}_d u\label{eq:savage_utility2}
\end{align}

\todo[inline]{Is this worth stating as a theorem?}

\subsubsection{Jeffrey's decision theory}

The approach to decision making set out by \citet{jeffrey_logic_1990} and \citet{bolker_functions_1966} differs from Savage's approach in a number of ways. Firstly, the representation theorem proved by Bolker does not distinguish between hypotheses, outcomes and decisions. It assumes only an algebra of outcomes $(O,\sigalg{O})$ and a preference relation over these outcomes. If the preference relation satisfies the \emph{Jeffrey axioms} then there exists a utility $u:O\to\mathbb{R}$ and a probability distribution $\prob{P}\in \Delta(\sigalg{O})$ which are non-unique such that for $A,B\in \sigalg{O}$ and finite partition $C_1,...,C_n\in \sigalg{O}$ (the proof is given by \citet{bolker_functions_1966}):

\begin{align}
    (A \succ B) \iff (\prob{P}^{\RV{O}|\mathds{1}_A}_1u>\prob{P}^{\RV{O}|\mathds{1}_B}_1u) \label{eq:ev_dec_theory}
\end{align}

A key feature to note here is that instead of comparing two decisions by evaluating one Markov kernel at two different points (as in our theory and Savage's), Jeffrey compares two events by comparing two different Markov kernels $\prob{P}^{\RV{O}|\mathds{1}_A}$ and $\prob{P}^{\RV{O}|\mathds{1}_B}$.  In order to use such a model in a decision problem, Jeffrey suggests we can identify a subset of the events with the choices we can make: $D:=\{D_i\in \sigalg{O}|i\in A\}$ for some set $A$, so we get one Markov kernel for each decision we might make. 

Whether we should model decisions as a set $D$ and consequence maps as Markov kernels with $D$ as the domain or as a collection of events and consequence maps as a corresponding collection of Markov kernels, or something else is a somewhat subtle issue that I don't fully understand.

I think Jeffrey goes too far in saying decisions are events. Consider the problem of deciding whether or not to order a drink, and suppose that I choose to consider a set of options $D=\{d_1=\text{``order a drink and make sure that drink is water''}, d_2=\text{``order a drink''}, d_3=\text{``don't order a drink''}\}$. If I choose $d_1$, then it will certainly be true that I will order a drink and also true that I will order a drink of water, and in general an event ``A and B'' implies an event ``A''. However, $d_1$ and $d_2$ are distinct -- if I particularly want to drink water then I will choose $d_1$ and not $d_2$. This feature of decision making is reflected in our idealised decision problem at the start where we may make a single decision from our set of available choices.

At the same time, it does seem that in the same way that events leave many features of the outcome underspecified, when we describe choices we very often leave features of what we precisely intend to do underspecified. Choices aren't necessarily underspecified: imagine a reinforcement learner in a simulated discrete time environment. At each timestep this learner takes its history so far and outputs some action and, if we examine the code governing how it interacts with its environment we can probably deduce \emph{all} the actions available to it at each timestep. In less controlled contexts, which is really most contexts we are familiar with, if we want to come up with a list of decisions we could make then all the decisions in this list are likely to leave a large number of things that we could in principle decide on underspecified. In the example above, it seems I could decide to:
\begin{itemize}
    \item Order a drink
    \item Order a glass of water
    \item Order a glass of water and say please and thankyou when I do
    \item Order a glass of water, say please and thankyou, carefully avoid scratching my itchy ear
\end{itemize}

And so forth. I could go on adding details for a very long time without exhausting the set of things I could in principle decide to do. I usually would not want to consider all these ``in principle'' choices I have available. Many of the details are more or less irrelevant and not worth spending the time to think about. Furthermore, even if I do consider all choices I have available in principle, it seems plausible that a less specific decision could be preferable. For example I might expect the ``automatic execution'' of some things things I didn't fully specify to be closer to optimal than the best guess I have of how to specify them (consider managing a competent employee; one might get better results by being less specific in requests).

I think the question of how one could structure the decisions set $D$ is an interesting one. I have so far treated it as ``just a set'' with no particular additional structure. Jeffrey suggests that decisions are a collection of events, and decisions do seem to have some event-like features, like the fact that they specify incompletely what I will do next and the fact that different specifications can apparently be joined with ``and''. However, the proposition that decisions are identical to events does not seem quite right because, as in the example above, deciding to do ``A and B'' does not imply deciding to do ``A''.

I think this is also potentially an important question. In Chapter \ref{ch:inferring_causes} I discuss the assumption of \emph{imitability}, that it is within a decision maker's power to reproduce the observed data. Such an assumption, if it holds, licences a number of inferences from data to consequences. It may often be a plausible assumption considering the full set of decisions $D^*$ that a decision maker could make \emph{in principle}, but may be much less often plausible when considering the restricted set of decisions $D$ a decision maker is actually considering. Understanding how these may relate to one another may help to better understand assumptions like imitability.

\todo[inline]{Which has only just occurred to me}

\subsubsection{Causal decision theory}



Statistical decision problems were introduced by \citet{wald_statistical_1950}. A statistical decision problem posits a set of hypotheses $H$ and observations $X$ and the two are related by a Markov kernel $\kernel{M}:H\to \Delta(\sigalg{X})$ (which we can recognise as a \emph{statistical model} from the previous chapter). In addition, a statistical decision problem involves a set of decisions $D$ and a loss $l:H\times D\to [0,\infty)$. Defining a Markov kernel $\kernel{T}:=(\kernel{M}\otimes\mathrm{Id}_D)\utimes \kernel{F}_l$ by ``joining'' the statistical model and the loss and defining the consequences of a decision under a particular hypothesis to be, with probability 1, the loss induced by that decision in that hypothesis, we actually have a see-do model $(\kernel{T}, \RV{H}, \RV{X}, \RV{Y})$. This model has the special properties that the consequences $\RV{Y}$ take values in $[0,\infty)$ and $\kernel{T}^{\RV{Y}|\RV{D}\RV{H}}$ is deterministic. The relationship between statistical decision problems and see-do models will be discussed further below -- here, it is enough that the representation of a statistical decision problem is a special case of a see-do model.



\subsection{Decision functions}

\begin{itemize}
    \item Define decision functions
    \item Choosing a particular mixed decision vs choosing a mixture of decisions; disintegrations exist both ways
\end{itemize}

\subsection{Risk}

\begin{itemize}
    \item Define risk
    \item Define risk set
\end{itemize}

\subsection{Reachable consequences}

\begin{itemize}
    \item Reachable consequences $\sim$ risk set without a utility
\end{itemize}

\subsection{Decision rules}

\begin{itemize}
    \item Need a rule for selecting a decision function
    \item Admissibility
    \item Maximise EU w/a Bayes forecast
    \item Minimax
    \item Complete class theorem
\end{itemize}

\subsection{Comparison of experiments and actuators}

\begin{itemize}
    \item Comparison of experiments
    \item Comparison of actuators
    \item Limiting cases: no information and no influence (thus: some information and some influence is necessary for nontrivial problem)
\end{itemize}

\subsection{Equivalence of see-do models}

\begin{itemize}
    \item Definition of equivalence via reachable set
    \item Defintion + examples: decomposability
    \item Thm: an indecomposable see-do model has an equivalent decomposable model
\end{itemize}

\section{Scraps to be moved into skeleton above}
\todo[inline]{Currently a disorganised cut and paste}

\subsection{Decomposability}

Decomposability is a property of see-do models that is relevant to the distinction between counterfacutal and regular models. As we will show, many causal problems allow the use of decomposable see-do models. However, certain types of counterfactual problem do not.

\begin{definition}[decomposability]\label{def:decomposability}
A see-do model $(\kernel{T},\RV{H},\RV{D},\RV{X},\RV{Y})$ is \emph{decomposable} iff $\RV{Y}\CI_\kernel{T} \RV{X}|\RV{D}\RV{H}$. That is, if the consequence is independent of the observations given the hypothesis and the choice.
\end{definition}

Decomposable see-do models can be represented as a pair $(\kernel{B},\kernel{C})$ where $\kernel{B}$ is a one-player statistical model we call the \emph{observation model} and $\kernel{C}$ is a two-player statistical model we call the \emph{consequence model} (Corollary \ref{corr:decomp_representation}. Most models in the causal inference literature are decomposable -- if the observed data can tell us nothing useful beyond the distribution of observations, then we have a decomposable model.

\begin{theorem}[Observation and Consequence models]\label{th:obs_cmaps}
Any see-do model $(\kernel{T},\RV{H},\RV{O},\RV{D},\RV{X},\RV{Y})$ can be uniquely represented by the following pair of Markov kernels:
\begin{itemize}
    \item The \emph{observation model} $\kernel{T}^{\RV{X}|\RV{H}}$
    \item The \emph{context-sensitive consequence model} $\kernel{T}^{\RV{Y}|\RV{X}\RV{H}\RV{D}}$
\end{itemize}

Furthermore
\begin{align}
\kernel{T} = \begin{tikzpicture} \path (0,0) node (T) {$\RV{H}$}
        + (0,-1.15) node (D) {$\RV{D}$}
        ++ (0.5,0) node[copymap] (copy0) {}
        + (0.,-1.15) node[copymap] (copy2) {}
        ++ (0.7,0) node[kernel] (O) {$\kernel{T}^{\RV{X}|\RV{H}}$}
        ++ (0.7,0) node[copymap] (copy1) {}
        +  (0.9,-1) node[kernel] (C) {$\kernel{T}^{\RV{Y}|\RV{X}\RV{H}\RV{D}}$}
        ++ (1.9,0) node (X) {$\RV{X}$}
        +  (0,-1) node (Y) {$\RV{Y}$}
        + (0,0.5) node (H) {$\RV{H}$}
        + (0,-1.5) node (D2) {$\RV{D}$};
        \draw (T) -- (O) -- (X);
        \draw (copy0) to [out=-90,in=180] ($(C.west) + (0,0)$);
        \draw (D) to [out=0,in=180] ($(C.west) + (0,-0.15)$);
        \draw (copy1) to [out=-60,in=180] ($(C.west)+ (0,0.15)$);
        \draw (C) -- (Y);
        \draw (copy0) to [out = 65, in = 180] (H);
        \draw (copy2) to [out = -65, in = 180] (D2);
    \end{tikzpicture}
\end{align}
\end{theorem}

\todo[inline]{Maybe moves proofs out of main text}

\begin{proof}
By Lemma \ref{lem:representation_of_kernels}, 

\begin{align}
\kernel{T} = \begin{tikzpicture} \path (0,0) node (T) {$\RV{H}$}
        + (0,-1.15) node (D) {$\RV{D}$}
        ++ (0.5,0) node[copymap] (copy0) {}
        + (0.,-1.15) node[copymap] (copy2) {}
        ++ (0.7,0) node[kernel] (O) {$\kernel{T}^{\RV{X}|\RV{H}\RV{D}}$}
        ++ (0.7,0) node[copymap] (copy1) {}
        +  (0.9,-1) node[kernel] (C) {$\kernel{T}^{\RV{Y}|\RV{X}\RV{H}\RV{D}}$}
        ++ (1.9,0) node (X) {$\RV{X}$}
        +  (0,-1) node (Y) {$\RV{Y}$}
        + (0,0.5) node (H) {$\RV{H}$}
        + (0,-1.5) node (D2) {$\RV{D}$};
        \draw (T) -- (O) -- (X);
        \draw[name path=P1] (copy0) to [out=-90,in=180] ($(C.west) + (0,0)$);
        \draw (D) to [out=0,in=180] ($(C.west) + (0,-0.15)$);
        \draw (copy1) to [out=-60,in=180] ($(C.west)+ (0,0.15)$);
        \draw (C) -- (Y);
        \draw (copy0) to [out = 65, in = 180] (H);
        \draw (copy2) to [out = -65, in = 180] (D2);
        \draw[name path=P2] (copy2) to [out = 65, in = 180] ($(O.west)+(0,-0.15)$);
    \end{tikzpicture}
\end{align}


By the assumption $\RV{X}\CI_{\kernel{T}} \RV{D}|\RV{H}$ and version 2 of conditional independence from Theorem \ref{th:ci_equivalence},

\begin{align}
\kernel{T} &= \begin{tikzpicture} \path (0,0) node (T) {$\RV{H}$}
        + (0,-1.15) node (D) {$\RV{D}$}
        ++ (0.5,0) node[copymap] (copy0) {}
        + (0.,-1.15) node[copymap] (copy2) {}
        ++ (0.7,0) node[kernel] (O) {$\kernel{T}^{\RV{X}|\RV{H}}$}
        ++ (0.7,0) node[copymap] (copy1) {}
        +  (0.9,-1) node[kernel] (C) {$\kernel{T}^{\RV{Y}|\RV{X}\RV{H}\RV{D}}$}
        ++ (1.9,0) node (X) {$\RV{X}$}
        +  (0,-1) node (Y) {$\RV{Y}$}
        + (0,0.5) node (H) {$\RV{H}$}
        + (0,-1.5) node (D2) {$\RV{D}$};
        \draw (T) -- (O) -- (X);
        \draw (copy0) to [out=-90,in=180] ($(C.west) + (0,0)$);
        \draw (D) to [out=0,in=180] ($(C.west) + (0,-0.15)$);
        \draw (copy1) to [out=-60,in=180] ($(C.west)+ (0,0.15)$);
        \draw (C) -- (Y);
        \draw (copy0) to [out = 65, in = 180] (H);
        \draw (copy2) to [out = -65, in = 180] (D2);
        \draw[-{Rays[n=8]}] (copy2) to [out = 65, in = 180] ($(O.west)+(-0.2,-0.5)$);
    \end{tikzpicture}\\
    &= \begin{tikzpicture} \path (0,0) node (T) {$\RV{H}$}
        + (0,-1.15) node (D) {$\RV{D}$}
        ++ (0.5,0) node[copymap] (copy0) {}
        + (0.,-1.15) node[copymap] (copy2) {}
        ++ (0.7,0) node[kernel] (O) {$\kernel{T}^{\RV{X}|\RV{H}}$}
        ++ (0.7,0) node[copymap] (copy1) {}
        +  (0.9,-1) node[kernel] (C) {$\kernel{T}^{\RV{Y}|\RV{X}\RV{H}\RV{D}}$}
        ++ (1.9,0) node (X) {$\RV{X}$}
        +  (0,-1) node (Y) {$\RV{Y}$}
        + (0,0.5) node (H) {$\RV{H}$}
        + (0,-1.5) node (D2) {$\RV{D}$};
        \draw (T) -- (O) -- (X);
        \draw (copy0) to [out=-90,in=180] ($(C.west) + (0,0)$);
        \draw (D) to [out=0,in=180] ($(C.west) + (0,-0.15)$);
        \draw (copy1) to [out=-60,in=180] ($(C.west)+ (0,0.15)$);
        \draw (C) -- (Y);
        \draw (copy0) to [out = 65, in = 180] (H);
        \draw (copy2) to [out = -65, in = 180] (D2);
    \end{tikzpicture}
\end{align}

\end{proof}

\begin{corollary}\label{corr:decomp_representation}
A decomposable see-do model $\kernel{T}:H\times D\to \Delta(\sigalg{X}\otimes\sigalg{Y})$ can be uniquely represented by
\begin{itemize}
    \item The \emph{observation model} $\kernel{T}^{\RV{X}|\RV{H}}$
    \item The \emph{consequence model} $\kernel{T}^{\RV{Y}|\RV{H}\RV{D}}$
\end{itemize}
\end{corollary}

\begin{proof}
Because $\kernel{T}$ is decomposable, $\kernel{T}^{\RV{Y}|\RV{X}\RV{H}\RV{D}}= \stopper{0.2}_X\otimes \kernel{T}^{\RV{Y}|\RV{H}\RV{D}}$. Then by Lemma \ref{lem:representation_of_kernels} we have a unique representation of $\kernel{T}$.
\end{proof}

\subsubsection{Examples of decomposable and indecomposable see-do models}

Recall the previous example: suppose we are betting on the outcome of the flip of a possibly biased coin with payout 1 for a correct guess and 0 for an incorrect guess, and we are given $N$ previous flips of the coin to inspect. This situation can be modeled by a decomposable see-do model. Define $\kernel{B}:(0,1)\to \Delta(\{0,1\})$ by $\kernel{B}:\RV{H}\mapsto \mathrm{Bernoulli}(\RV{H})$. Then define $\prescript{1}{}{\kernel{T}}$ by:

\begin{itemize}
    \item $D=\{0,1\}$
    \item $X=\{0,1\}^N$
    \item $Y=\{0,1\}$
    \item $H=(0,1)$
    \item $\prescript{1}{}{\kernel{B}}:\splitter{0.1}^N\kernel{B}$
    \item $\prescript{1}{}{\kernel{C}}:(h,d)\mapsto \mathrm{Bernoulli}(1-|d-h|)$
\end{itemize}

In this model, the chance $\RV{H}$ of the coin landing on heads is as much as we can hope to know about how our bet will work out.

Suppose instead that in addition to the $N$ prior flips, we manage to look at the outcome of the flip on which we will bet. In this case, the situation can be modeled by the following indecomposable see-do model $\prescript{2}{}{\kernel{T}}$:

\begin{itemize}
    \item $D=\{0,1\}$
    \item $X=\{0,1\}^{N+1}$
    \item $Y=\{0,1\}$
    \item $H=(0,1)$
    \item $\prescript{2}{}{\kernel{T}}^{\RV{X}|\RV{H}}:\splitter{0.1}^{N+1}\kernel{B}$
    \item $\prescript{2}{}{\kernel{T}}^{\RV{Y}|\RV{X}\RV{H}\RV{D}}:(h,\mathbf{x},d)\mapsto \delta_{1-|d-x_{N+1}|}$
\end{itemize}

In this case, even if we are told the value of $\RV{H}$, we still benefit from using the observed data when making our decision.

It is possible to model the second situation with a decomposable model by including the result of the $N+1$th flip in the hypothesis. Define the new hypothesis space $H'=H\times\{0,1\}$ and let $\RV{H}_0$ be the projection to the old hypothesis space $H$. Define $\prescript{3}{}{\kernel{T}}$ by:

\begin{itemize}
    \item $D=\{0,1\}$
    \item $X=\{0,1\}^{N+1}$
    \item $Y=\{0,1\}$
    \item $H'=(0,1)\times\{0,1\}$
    \item $\prescript{3}{}{\kernel{B}}:(\splitter{0.1}^N\kernel{B}\otimes \delta_{x_{N+1}}$
    \item $\prescript{3}{}{\kernel{C}}:(h,x_{N+1},d)\mapsto \delta_{1-|d-x_{N+1}|}$
\end{itemize}

However, $\prescript{2}{}{\kernel{T}}^{\RV{X}_{N+1}|\RV{H}} = \kernel{B}$ while $\prescript{3}{}{\kernel{T}}^{\RV{X}_{N+1}|\RV{H}_0}$ is undefined, so $\prescript{3}{}{\kernel{T}}$ is a substantially different model to $\prescript{2}{}{\kernel{T}}$.

If an indecomposable see-do model is employed in a \emph{decision problem} it is possible to create an equivalent decision problem with a decomposable model as I will show later. Some counterfactual problems cannot be formulated as decision problems, and indecomposability is a property of the types of counterfacutal model proposed by \citet{pearl_causality:_2009}, but not to my knowledge of any causal models used in a ``decision like context''.

\subsection{Causal questions and decision functions}

\citet{pearl_book_2018} has proposed three types of causal question:
\begin{enumerate}
    \item Association: How are $\RV{W}$ and $\RV{Z}$ related? How would observing $\RV{W}$ change my beliefs about $\RV{Z}$?
    \item Intervention: What would happen if I do ... ? How can I make ... happen?
    \item Counterfactual: What if I had done ... instead of what I actually did?
\end{enumerate}

\emph{Causal decision problems} are, roughly speaking, ``interventional'' problems. In English, a causal decision problem roughly asks

\begin{quote}
    Given that I have data $\RV{X}$ and I know which values of $\RV{Y}$ I would like to see and some knowledge about how the world works, which of my available choices $D$ should I select?
\end{quote}

This type of question presupposes somewhat more than Pearl's prototypical interventional questions. First, it supposes that we have \emph{preferences} over the values that $\RV{Y}$ might take, which we need not have to answer the question ``What would happen if I do ...?''. Secondly, and crucially to our theory, causal decision problem suppose that we are given data and a set of choices. 

We will return to the question of preferences. For now, we will focus on the idea that a causal decision problem is about selecting a choice given data. That is, however the selection is made, the answer to a causal decision problem is always a \emph{decision function} $\kernel{D}:X\to \Delta(\sigalg{D})$.

A property that will be of interest when considering counterfactual models is \emph{decomposability}. A see-do model 


% Decisions are similar to the ``regime indicators'' found in \citet{dawid_decision-theoretic_2020}. They coincide precisely if we suppose that the observation and consequence spaces coincide ($X=Y$) and there exists an ``idle'' decision $d^*\in D$ such that $\kernel{C}_{(\cdot,d^*)} = \kernel{O}_{\cdot}$. However, in general we don't require that $\kernel{O}$ and $\kernel{C}$ are related in this manner. This assumption will be revisited in \todo[inline]{A section I haven't written yet}.

\subsubsection{Example}

Suppose we are betting on the outcome of the flip of a possibly biased coin with payout 1 for a correct guess and 0 for an incorrect guess, and we are given $N$ previous flips of the coin to inspect. This situation can be modeled by a decomposable see-do model. Define $\kernel{B}:(0,1)\to \Delta(\{0,1\})$ by $\kernel{B}:\RV{H}\mapsto \mathrm{Bernoulli}(\RV{H})$. Then define ${\kernel{T}}$ by:

\begin{itemize}
    \item Choice set: $D=\{0,1\}$
    \item Observation set: $X=\{0,1\}^N$
    \item Consequence set: $Y=\{0,1\}$
    \item Hypothesis set: $H=(0,1)$
    \item Observation map: ${\kernel{T}}^{\RV{X}|\RV{H}}:\splitter{0.1}^N\kernel{B}$
    \item Consequence model: ${\kernel{T}}^{\RV{Y}|\RV{D}\RV{H}}:(h,d)\mapsto \mathrm{Bernoulli}(1-|d-h|)$
\end{itemize}

In this model, the chance $\RV{H}$ of the coin landing on heads is as much as we can hope to know about the success of our bet. $\RV{H}$ may be inferred from observation by some standard method, and 



\subsubsection{Avoiding indecomposability with decision functions}

\todo[inline]{Show that a decision problem with a indecomposable model induces an equivalent decision problem with a decomposable model with an expanded set of choices, subject to some conditions.}

\subsubsection{Decision rules}

See-do models encode the relationship between observed data and consequences of decisions. In order to actually make decisions, we also require preferences over consequences. We suppose that a \emph{utility function} is given, and evaluate the desirability of consequences using \emph{expected utility}. A see-do model along with a utility allows us to evaluate the desirability of \emph{decisions rules} according to each hypothesis.

\begin{definition}[Utility function]
Given a See-Do Model $\kernel{T}:\RV{H}\times D\to \Delta(\sigalg{X}\otimes\sigalg{Y})$, a \emph{utility function} $u$ is a measurable function $Y\to \mathbb{R}$. 
\end{definition}

\begin{definition}[Expected utility]
Given a utility function $u:Y\to \mathbb{R}$ and probability measures $\mu,\nu\in \Delta(\sigalg{Y})$, the \emph{expected utility} of $\mu$ is $\mathbb{E}_{\mu}[u]$.

$\mu$ is \emph{preferred} to $\nu$ if $\mathbb{E}_{\mu}[u]\geq \mathbb{E}_{\nu}[u]$, and \emph{strictly preferred} if $\mathbb{E}_{\mu}[u]>\mathbb{E}_{\nu}[u]$.
\end{definition}

\begin{definition}[Decision rule]
Given a see-to map $\kernel{T}:\RV{H}\times D\to \Delta(\sigalg{X}\otimes\sigalg{Y})$, a \emph{decision rule} is a Markov kernel $X\to \Delta(\sigalg{D})$. A \emph{deterministic decision rule} is a decision rule that is deterministic.

\todo[inline]{Define deterministic Markov kernels}
\end{definition}

Expected utility together with a decision rule gives rise to the definition of \emph{risk}, which connects CSDT to classical statistical decision theory (SDT). For historical reasons, risks are minimised while utilities are maximised.

\begin{definition}[Risk]
Given a see-to map $\kernel{T}:\RV{H}\times D\to \Delta(\sigalg{X}\otimes\sigalg{Y})$, a utility $u:Y\to \mathbb{R}$ and the set of decision rules $\mathscr{U}$, the \emph{risk} is a function $l:\RV{H}\times \mathscr{U}\to \mathbb{R}$ given by

\begin{align}
    R(\RV{H},\kernel{U}) := - \int_X  \kernel{U}_x \kernel{T}^{\RV{Y}|\RV{D}\RV{X}\RV{H}}_{\cdot,x,\RV{H}} u d\kernel{T}^{\RV{X}|\RV{H}}_\RV{H}(x)
\end{align}

for $\RV{H}\in \RV{H}$, $\kernel{U}\in \mathscr{U}$. Here $\kernel{U}_x \kernel{T}^{\RV{Y}|\RV{D}\RV{X}\RV{H}}_{\cdot,x,\RV{H}} u$ is the product of the measure $\kernel{U}_x$, the kernel $\kernel{T}^{\RV{Y}|\RV{D}\RV{X}\RV{H}}_{\cdot,x,\RV{H}}:D\to \Delta(\sigalg{Y})$ and the function $u$.
\end{definition}

The loss induces a partial order on decision rules. If for all $\RV{H}$, $l(\RV{H},\kernel{U})\leq l(\RV{H},\kernel{U}')$ then $\kernel{U}$ is at least as good as $\kernel{U}'$. If, furthermore, there is some $\RV{H}_0$ such that $l(\RV{H}_0,\kernel{U})<l(\RV{H}_0,\kernel{U}')$ then $\kernel{U}$ is preferred to $\kernel{U}'$.

\begin{definition}[Induced statistical decision problem]
A see-do model $\kernel{T}:\RV{H}\times D\to \Delta(\sigalg{X}\otimes\sigalg{Y})$ along with a utility $u$ induces the \emph{statistical decision problem} $(\RV{H},\mathscr{U},R)$ with states $\RV{H}$, decisions $\mathscr{U}$ and risks $R$.

\todo[inline]{Statistical decision problems usually define the risk via the loss, but it is only possible to define a loss with a decomposable model. We don't actually need a loss, though: the complete class theorem still holds via the induced risk and Bayes risk}

\end{definition}


We develop causal statistical decision problems (CSDPs) inspired by statistical decision problems (SDPs) of \citet{wald_statistical_1950}. CSDPs differ from SDPs in that our preferences (i.e. utility or loss) are known less directly in former case. We show that every SDP can be represented by a CSDP and that the converse is sometimes but not always possible. We show that an analogoue of the fundamental \emph{complete class theorem} of SDPs applies to the class of CSDPs that can be represented by SDPs, but whether such a theorem applies more generally is an open question.

Following \citep{toutenburg_ferguson_1967}, we consider SDPs and CSDPs to represent normal form two person games. At the most abstract level the games represent the options and possible payoffs available to the decision maker, and this representation allows us to compare the two types of problem. In their more detailed versions,  CSDPs and SDPs differ in their representation of the state of the world and in the type of function that represents preferences. These differences are summarised in Table \ref{tab:sdp_cdp_comparison}.

\begin{definition}[Normal form two person game]
A normal form game is a triple $\langle \mathscr{S}, A, L\rangle$ where $\mathscr{S}$ and $A$ are arbitrary sets and $L:\mathscr{S}\times A\to [0,\infty)$ is a loss function.

\end{definition}
The set $\mathscr{S}$ is a set of possible states that the environment may occupy and $A$ is a set of actions the decision maker may take. The decision maker seeks an action in $A$ that minimises the loss $L$. Generally there is no action that minimises the loss for all environment states. A minimax solution is an action that minimises the worst case loss: $a^*_{mm} = \argmin_{a\in A} [\sup_{s\in \mathscr{S}} L(s,a)]$.

If the set $\mathscr{S}$ is equipped with a $\sigma$-algebra $\mathcal{S}$ and a probability measure $\xi\in \Delta(\mathcal{S})$ which we will call a ``prior'', a Bayes solution minimizes the expected risk with respect to $\xi$: $a^*_{ba} = \argmin_{a\in A} \int_{\mathcal{S}} L(s,a) \xi(ds)$.

\begin{definition}[Admissible Action]
Given a normal form two person game $\langle \mathscr{S}, A, L\rangle$, an action $a\in A$ is \emph{strictly better} than $a'\in A$ iff $L(s,a)\leq L(s,a')$ for all $s\in\mathscr{S}$ and $L(s_0,a)<L(s_0,a')$ for some $s_0\in \mathscr{S}$. If only the first holds, then $a$ is as good as $a'$. An \emph{admissible action} is an action $a\in A$ such that there is no action strictly better than $A$.
\end{definition}

\begin{definition}[Complete Class]
A class $C$ of decisions is a \emph{complete class} if for every $a\not\in C$ there is some $a'\in C$ that is strictly better than $a$.

$C$ is an \emph{essentially complete} class if for every $a\not\in C$ there is some $a'\in C$ that is as good as $a$.
\end{definition}

A statistical decision problem represents a normal form two-person game where the available actions are \emph{decision functions} that output a decision given data, the states of the environment are associated with probability measures on some measurable space and we assume a loss expressing preferences over decisions and states is known.

\begin{definition}[Statistical Experiment]\label{def:stat_expt}
A \emph{statistical experiment} relative to a set $\Theta$, a measurable space $(E,\mathcal{E})$ and a map $m:\Theta\to \Delta(\mathcal{E})$ is a multiset $\mathscr{H}=\{\mu_\theta|\theta\in \Theta\}$ where $\mu_\theta:=m(\theta)$. The set $\Theta$ indexes the ``state of nature''.
\end{definition}


\begin{definition}[Statistical Decision Problem]
A statistical decision problem (SDP) is a tuple $\langle\Theta, (\mathscr{H},m), D, \ell\rangle$. $\mathscr{H}\subset\Delta(\mathcal{E})$ is a statistical experiment relative to states $\Theta$, space $(E,\mathcal{E})$ and map $m:\Theta\to \Delta(\mathcal{E})$, $D$ is the set of available decisions with some $\sigma$-algebra $\mathcal{D}$ and $\ell:\Theta\times D\to \mathbb{R}$ is a loss function where $\ell(\theta,\cdot)$ is measurable with respect to $\mathcal{D}$ and $\mathcal{B}(\mathbb{R})$.

Denote by $\mathscr{J}$ the set of stochastic decision functions $E\to \Delta(\mathcal{D})$. For $J\in \mathscr{J}$ and $\mu_\theta\in \mathcal{H}$, the risk $R:\Theta\times\mathscr{J}\to [0,\infty)$ is defined as $R(J,\theta) = \int_D \ell(\theta,y) \mu_\theta J(dy)$. The triple $\langle \Theta, \mathscr{J}, R\rangle$ forms a two player normal form game.
\end{definition}

The loss function $\ell$ expresses preferences over general (state, decision) pairs. It may be the case that our preferences are most directly known over future states of the world - we know which results of our decisions are desirable and which are undesirable, which we represent with a \emph{utility function}. In this case, if we are to induce preferences over the possible decisions, that we have a model that is more informative than a statistical experiment. In particular, we require each state of nature to be associated with both a distribution over the given information and a map from decisions to distributions over results - we call this map a \emph{consequence}, and the object that pairs a distribution and a consequence with each state of the world a \emph{causal theory}.

\begin{definition}[Consequences]
Given a measurable result space $(F,\mathcal{F})$ and a measurable decision space $(D,\mathcal{D})$, a Markov kernel $\kappa:D \to \Delta(\mathcal{F})$ is a \emph{consequence mapping}, or just a \emph{consequence}.
\end{definition}

\begin{definition}[Causal state]
Given a consequence $\kappa:D\to \Delta(\mathcal{F})$, a measurable observation space $(E,\mathcal{E})$ and some distribution $\mu\in \Delta(\mathcal{E})$, the pair $(\kappa,\mu)$ is a \emph{causal state} on $E, D$ and $F$. We refer to $\kappa$ as the consequence and $\mu$ as the observed distribution.
\end{definition}

In many cases the observation space $E$ and the results space $F$ might coincide. However, these spaces are defined by different aspects of the given information: the former is fixed by what observations are available and the latter by which parts of the world are relevant to the investigator's preferences (see Theorems \ref{th:CSDP_u_red} and \ref{th:CSDP_ob_red}), and there is not a clear reason to insist that these spaces should always be the same.

\begin{definition}[Causal Theory]\label{def:causal_theory}
A causal theory $\mathscr{T}$ is a set of causal states sharing the same decision, observation and outcome spaces. We abuse notation to assign the ``type signature'' $\mathscr{T}:E\times D\rightarrowtriangle F$ for a causal theory with observed distributions in $\Delta(\mathcal{E})$ and consequences of type $D\to \Delta(\mathcal{F})$. The causal states of a theory $\mathscr{T}$ may be associated with a master set of states $\Theta$, but in contrast to a statistical experiment this is not necessary to define the basic associated decision problem.
\end{definition}

\begin{definition}[Causal Statistical Decision Problem]\label{def:CSDP}
A causal statistical decision problem (CSDP) is a triple $\langle \mathscr{T}, D, u \rangle$. $\mathscr{T}$ is a causal theory on $D\times E\rightarrowtriangle F$, $D$ is the decision set with $\sigma$-algebra $\mathcal{D}$ and $u:F\to \mathbb{R}$ is a measurable utility function expressing preference over the results of decisions.

Define the canonical loss $L:\mathscr{T}\times D\to \mathbb{R}$ by $L:(\kappa,\mu),y\mapsto -\mathbb{E}_{\gamma\kappa}[u]$. This change conforms with the conventions that utilities are maximised while losses are minimised.

Given a decision function $J\in\mathscr{J}$ and $(\kappa,\mu)\in \mathscr{T}$, we define the risk $R:\mathscr{T}\times \mathscr{J} \to [0,\infty)$ by $R(\kappa,\mu,J) := L((\kappa,\mu),\mu J)$. The triple $\langle \mathscr{T}, \mathscr{J}, R\rangle$ is a normal form two person game.
\end{definition}

The loss and the utility differ in that the loss expresses per-state preferences while the utility expresses state independent preferences. While we choose the loss to be a particular function of the utility here, it is possible to allow losses to be a more general class of functions of the utility and state without altering the preference ordering of a CSDP under minimax or Bayes decision rules. Given arbitrary $f:\mathscr{T}\to\mathbb{R}$, define $l:\mathscr{T}\times D\to \mathbb{R}$ by $l:(\kappa,\mu,y)\mapsto a f(\kappa,\mu) + b \mathbb{E}_{\delta_y\kappa}[u]$. We can define a loss (relative to $f$) $L:\mathscr{T}\times\Delta(\mathcal{D})\to [0,\infty]$ by
\begin{align}
    L((\kappa,\mu),\gamma) &:= \mathbb{E}_\gamma[l(\kappa,\mu,\cdot)]\\
                           &= a f(\kappa,\mu) - b \mathbb{E}_{\gamma\kappa}[u]\label{eq:canonical_loss}\\
\end{align}
For $(\kappa,\mu)\in \mathscr{T}$, $\gamma\in \Delta(\mathcal{D})$ and $a\in \mathbb{R}$, $b\in \mathbb{R}^{+}$. 

A common example of a loss of the type above is the \emph{regret}, which takes $a=b=1$ and $f(\kappa,\mu) = \sup_{\gamma'\in \Delta(\mathcal{D})} \mathbb{E}_{\gamma'\kappa}[u]$. Because expected utility preserves preference orderings under positive affine transformations, the ordering of preferences given a particular state is not affected by the choices of $a,b$ and $f$, nor is the Bayes ordering of preferences given some prior $\xi$ over $\mathscr{T}$. While it may be possible to formulate decision rules for which the choices of $a,b$ and $f$ do matter, we will take these properties as sufficient to allow us to choose $a=0$ and $b=1$. More general classes of loss are of interest. \emph{Regret theory}, for example, is a straightforward generalisation of the losses discussed here and is a prominent alternative to expected utility theory \citep{loomes_regret_1982}.

There are obvious similarities between SDPs and CSDPs: both have the same high level representation as a two person game which is arrived at by taking the expectation of a loss with respect to a decision function. In fact, if we consider two decision problems to be the same if they have the same representation as a two player game, we find that CSDPs are a special case of SDPs.

\begin{theorem}[CSDPs are a special case of SDPs]\label{th:csdps_are_sdps}
Given any CSDP $\alpha=\langle \mathscr{T}, D, u \rangle$ with two player game representation $\langle \mathscr{T}, \mathscr{J}, R\rangle$, there exists an SDP $\langle \mathscr{T}, (\mathscr{H},m), D,\ell \rangle$ with the same representation as a two player game.
\end{theorem}

\begin{proof}
Let $m:\mathscr{T}\to \mathscr{H}$ be defined such that $m:(\kappa,\mu)\mapsto \mu$ for $(\kappa,\mu)\in \mathscr{H}$. Define $\ell:\mathscr{T}\times D\to \mathbb{R}$ by $\ell:((\kappa,\mu),y)\mapsto -\mathbb{E}_{\delta_y \kappa}[u]$. Let $R'((\kappa,\mu),J) = \mathbb{E}_{\mu J}[\ell(\theta,\cdot)]$. Then
\begin{align}
    R'((\kappa,\mu),J) &= -\int_D \mathbb{E}_{\delta_y \kappa}[u] \mu J(dy)\\ 
                       &= -\int_D \int_F u(x) \kappa(y;dx) \mu J(dy)\\
                       &= -\int_F u(x) \mu J \kappa(dx)\\
                       &= R((\kappa,\mu),J)
\end{align}
\end{proof}

The converse is not true, as the set $\Theta$ in an SDP is of an arbitrary type and may not be a causal theory. However, it is possible for any SDP with environmental states $\Theta$ to find a CSDP with causal theory $\mathscr{T}$ such that the games represented by each decision problem are related by a surjective map $f:\Theta\to \mathscr{T}$ which associates each state of nature with a causal state. We call such a map a \emph{reduction} from an SDP to a CSDP.

\begin{definition}[Reduction]\label{def:red_sdp_CSDP}
Given normal form two person games $\alpha = \langle \mathscr{S}^\alpha, A, L^\alpha\rangle$ and $\beta = \langle \mathscr{S}^\beta, A, L^\beta \rangle$, $f:\mathscr{S}^\alpha\to \mathscr{S}^\beta$ is a \emph{reduction} from $\alpha$ to $\beta$ if, defining the image $f(\mathscr{S}^\alpha)=\{f(\theta)|\theta\in \mathscr{S}^\alpha\}$, we have $\langle \mathscr{S}^\beta, A, L^\beta \rangle = \langle f(\mathscr{S}^\alpha), A, L^\alpha\circ(f\otimes I_A)\rangle$.
\end{definition}

\begin{theorem}[SDP can be reduced to a CSDP]\label{th:csdps_represent_sdps}
Given any SDP $\langle \Theta, (\mathscr{H},m), D, \ell\rangle$ represented as the game $\alpha = \langle \Theta, \mathscr{J},R\rangle$, there exists a CSDP $\langle \mathscr{T},D,u\rangle$ represented as the game $\beta=\langle\mathscr{T},\mathscr{J},R' \rangle$ such that there is some reduction $f:\Theta\to \mathscr{T}$ from $\alpha$ to $\beta$.
\end{theorem} 

\begin{proof}
Take $\mathscr{H}\subset\Delta(\mathcal{E})$ and define $f:\Theta\to \Delta(\mathcal{E})\times \Delta(\mathcal{B}(\mathbb{R}))^D$ by $f:\theta\mapsto (y\mapsto \delta_{l(\theta,y)},\mu_\theta)$. Noting that $y\mapsto \delta_{l(\theta,y)}$ is a Markov kernel $D\to \Delta(\mathcal{B}(\mathbb{R}))$, the image $f(\Theta)$ is a causal theory $E\times D\rightarrowtriangle \mathbb{R}$. Consider the CSDP $\langle f(\Theta),D,-I_{(\mathbb{R})}\rangle$. Then, letting $R'$ denote the risk associated with this theory
\begin{align}
 R'((\kappa,\mu),J) &= -\int_\mathbb{R} \int_D (-x) \delta_{l(\theta,y)}(dx) \mu_\theta J(dy)\\
                    &= \int_D l(\theta,y) \mu_\theta J(dy)\\
                    &= R(\Theta,J)
\end{align}
\end{proof}

The fundamental \emph{complete class theorem} of SDPs establishes that there are no decision rules that dominate the set of all Bayes rules under some regularity assumptions. By theorem \ref{th:csdps_are_sdps}, this must also be true of CSDPs.

\begin{theorem}[Complete class theorem (CSDP)]\label{th:complete_class}
Given any CSDP $\alpha:=\langle \mathscr{T},D,u\rangle$ with two player game representation $\langle \mathscr{T},\mathscr{J},R\rangle$, if $|\mathscr{T}|<\infty$ and $\inf_{J\in\mathscr{J},(\kappa,\mu)\in\mathscr{H}} R((\kappa,\mu),J)>-\infty$, then the set of all Bayes decision functions is a complete class for $\alpha$ and the set of all admissible Bayes decision functions is a minimal complete class for $\alpha$.
\end{theorem}

\begin{proof}
By theorem \ref{th:csdps_are_sdps}, there exists an SDP $\beta$ such that $\alpha$ and $\beta$ have the same representation as a two player game. By assumption, $\beta$ has a finite set of states and a risk function that is bounded below. Therefore the Bayes rules on $\alpha$ are a complete class and admissible Bayes rules are a minimal complete class for the problem $\langle \mathscr{T},\mathscr{J},R\rangle$ \citep{toutenburg_ferguson_1967}.
\end{proof}


