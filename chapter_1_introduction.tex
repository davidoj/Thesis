
%!TEX root = main.tex


\chapter{Introduction}\label{ch:introduction}

\section{Making decisions with data}

Beginning in the 1930s, a number of associations between cigarette smoking and lung cancer were established: on a population level, lung cancer rates rose rapidly alongside the prevalence of cigarette smoking. Lung cancer patients were far more likely to have a smoking history than demographically similar individuals without cancer and smokers were around 40 times as likely as demographically similar non-smokers to go on to develop lung cancer. In laborotory experiments, cells which were introduced to tobacco smoke developed \emph{ciliastasis}, and mice exposed to cigarette smoke tars developed tumors\citep{proctor_history_2012}. Nevertheless, until the late 1950s, substantial controversy persisted over the question of whether the available data was sufficient to establish that smoking cigarettes \emph{caused} lung cancer. Cigarette manufacturers famously argued against any possible connection \citep{oreskes_merchants_2011} and Roland Fisher in particular argued that the available data was not enough to establish that smoking actually caused lung cancer \citep{fisher_cancer_1958}. Today, it is widely accepted that cigarettes do cause lung cancer, along with other serious conditions such as vascular disease and chronic respiratory disease \citep{world_health_organisation_tobacco_nodate,wiblin_why_2016}.

The question of a causal link between smoking and cancer is a very important one to many different people. Individuals who enjoy smoking (or think they might) may wish to avoid smoking if cigarettes pose a severe health risk, so they are interested in knowing whether or not it is so. Additionally, some may desire reassurance that their habit is not too risky, whether or not this is true. Potential and actual investors in cigarette manufacturers may see health concerns as a barrier to adoption, and also may personally want to avoid supporting products that harm many people. Like smokers, such people might have some interest in knowing the truth of this question, and a separate interest in hearing that cigarettes are not too risky, whether or not this is true. Governments and organisations with a responsibility for public health may see themselves as having responsibility to discourage smoking as much as possible if smoking is severely detrimental to health. The costs and benefits of poor decisions about smoking are large: 8 million annual deaths are attributed to cigarette-caused cancer and vascular disease in 2018\citep{world_health_organisation_tobacco_nodate} while  global cigarette sales were estimated at US\$711 billion in 2020 \citep{noauthor_cigarettes_nodate} (a figure which might be substantially larger if cigarettes were not widely believed to be harmful).

The question of whether or not cigarette smoking causes cancer illustrates two key facts about causal questions: First, having the right answers to causal questions can underpin decisions of tremendous important to large numbers of people. Second, confusion over causal questions can persist even when a great deal of data and facts relevant to the question are agreed upon.

Causal conclusions are often justified on the basis of ad-hoc reasoning. For an arbitrarily chosen example, \citet{krittanawong_association_2020} state:

\begin{quote}
[...] the potential benefit of increased chocolate consumption, reducing coronary artery disease (CAD) risk is not known. We aimed to explore the association between chocolate consumption and CAD.
\end{quote}

It is not clear whether Krittanawong et. al. mean that a negative association between chocolate consumption and CAD implies that deciding to eat more chocolate is likely to reduce coronary artery disease (which is suggested by the word ``benefit''), or that an association may be relevant to the question and the reader should draw their own conclusions. Whether the implication is being suggested by Krittanawong et. al. or merely imputed by na\"ive readers, it is being drawn on an ad-hoc basis -- no argument for the implication can be found in this paper. As \citet{pearl_causality:_2009} has forcefully argued, additional assumptions are always required to answer causal questions from associational facts, and stating these assumptions explicitly allows those assumptions to be productively scrutinised.

\section{Assumptions in predictive and decision-making algorithms}\label{sec:assumptions}

We can contrast the type of problem outlined above -- where one is called on to make choices on the basis of given data -- with prediction problems, where one is simply asked to provide a prediction of what is likely to happen in the future. We can also abuse terminology somewhat to include classification tasks under ``prediction'' -- the relevant similarity being the fact that there is a single true answer (either a class or a future event) unknown to the predictor (or classifier), and the success of the prediction (or class assignment) is judged on the basis of its fit to the true answer.

Data driven prediction problems and data driven decision making problems have a lot in common. The outcomes some people are interested in predicting are often outcomes other people want to influence. A forecaster might want to predict the winner of the next election, while a party strategist is interested in maximising their party's chance of victory. A product manager may be simultaneously interested in accurately inferring the sentiment expressed in reviews of their product, and in making product changes that increase the frequency that this sentiment is positive. Furthermore, data relevant to prediction is often relevant to decision making and vise-versa. Political parties often reason that electorates in which their predicted chance of victory is very low are not worth investing campaign resources in, and if a forecaster learns of evidence that one party had adopted a particularly effective election strategy they might want to revisit their prediction of the eventual election winner. The overlap is not perfect: comprehensive electorate level polls are probably more useful to the forecaster while small-scale controlled experiments are probably more useful to the strategist.

A key difference between prediction and influence problems is the ``multiplicity of futures'' that each problem asks us to consider. A forecaster wants to identify -- loosely speaking -- the single most likely outcome, while a strategist must consider multiple options and identify the likely outcomes associated with each of these. As a consequence of this difference, the forecaster receives more complete feedback about the quality of their forecast than the strategist. Unlike the forecaster, all but one of the options that the strategist considers are never realised, and the world never offers feedback on these alternative options.

This difference suggests that it might be easier to assess the reliability of a predictive algorithm than the reliability of a decision-making algorithm, and this is be borne out in practice. Validating a predictive algorithm using data split into training and holdout sets is a ubiquitous in machine learning. For many data generating processes, appropriately conducted validation is widely considered to be a reliable indicator of an algorithm's performance for sufficiently similar data generating processes. In contrast, the most well-known condition that is widely accepted to yield reliable decision making algorithms is that the data used to draw inferences comes from a well-conducted controlled experiment. Data that satisfies this is much rarer than data that standard machine learning validation approaches can be applied to. There are approaches to causal inference that don't depend on experimental data, but they depend on other assumptions which are similarly applicable to a limited fraction of datasets. Alternatives to controlled experiments often come with the additional headache of being difficult to assess for a given dataset.

Some of the most far-reaching recent development in algorithmic decision making have involved only the elementary theory of randomised experiments. Operational advances that enable controlled experiments to be conducted at large scales have driven substantial changes in the operations of many online businesses \citep{kohavi_surprising_2017}, and Abhijit Banerjee and Esther Duflo were recently awarded a Nobel prize in part for their pioneering role in the use of large numbers of randomised controlled trials (RCTs) to assess the effectiveness of different kinds of development interventions \citep{zhang_abdul_2014}. Some fields of science have also been significantly affected by ``negative progress'' in the science of assessing experimental results. For example, in psychology, strong evidence has emerged that experimental findings from this field provide weaker evidence to a reader of the findings about the consequences of the reader's actions than many had believed \citet{open_science_collaboration_estimating_2015,stroebe_what_2019}. In a similar time frame, standards for what constitutes a ``well-conducted'' experiment have risen across many fields \citep{nosek_preregistration_2018,liberati_prisma_2009}.

An individual who wants to use data to make better decisions can consider running a controlled experiment of their own. This may not be possible, and even if it is, there may be large amounts of apparently relevant data available that seems wasteful to ignore on the basis of its non-experimental provenance. This individual might therefore be motivated to make some additional assumptions which allow them to draw conclusions about how to act from non-experimental data.

Some examples of assumptions this person could consider are (a * indicates that causal conclusions may or may not follow, depending on the actual data at hand, and italics indicate technical terms that will be explained in more detail further into this thesis):
\begin{itemize}
    \item \textbf{Conditional ignorability:} There is an input variable independent of the \emph{potential outcomes} conditional on some covariates \citep[Chap. ~12]{imbens_causal_2015}, \citep[Chaps ~2, 3, 5]{angrist_mastering_2014}
    \item[*] \textbf{Potential outcomes proxy:} There is a variable closely correlated with the \emph{potential outcomes} for each observation \citep[Chap. ~21]{imbens_causal_2015}
    \item \textbf{Regression discontinuity:} \emph{Potential outcomes} are continuous about some covariate cutoff, above which one potential outcome is always observed and below which the other is always observed \citep{hahn_identification_2001}
    \item[*] \textbf{Known causal structure:} The set of observed and unobserved variables have a known \emph{causal structure} \citep{shpitser_complete_2008,richardson_nested_2017}
    \item[*] \textbf{Faithfulness:} The set of observed variables is \emph{causally sufficient} and the \emph{causal structure} is \emph{faithful} to the conditional independence structure of the observed variables \citep[Chap. ~5]{spirtes_causation_1993}
\end{itemize}

These assumptions all invoke either ``potential outcomes'' or ``causal structure''. Potential outcomes are, by definition, the union of a set of observed variables and a set of hypothetical claims. If one is inclined to accept that causal structures are ultimately underwritten by certain experimental procedures, then causal structures may be observed under some conditions (for example, if an experimental procedure is given for each $do()$-intervention, one can apply the method of \citep{eberhardt_almost_2008}) However, causal structures are often held to represent relationships that cannot be reasonably probed by an experiment (\citep[Chap. ~11]{pearl_causality:_2009},\citet{pearl_does_2018}), and in our view the general question of whether causal structures are observable or a mixture of observable properties and hypothetical claims is unresolved.

Decision making must entertain some hypothetical statements. As has been pointed out above, a decision maker must consider multiple options and their likely consequences, and select from among the options. The proposition ``the consequences of $\alpha$ are better than the consequences of $\alpha'$'' is a hypothetical one: the decision maker ultimately chooses only one of $\alpha$ or $\alpha'$, and usually cannot ever verify that the chosen option really is better than the alternative. However, the hypotheticals that the assumptions above ask us to entertain do not on the face of it seem necessary to compare different options like this. The hypothetical part of potential outcomes doesn't refer to possible consequences of choices but to hypotheticals that have (in some sense) ``already happened'' by the time the data is reviewed. An identifiable causal structure may have many implications besides those that are necessary to make a choice in the decision problem at hand.

Assumptions with mixtures of hypothetical and real implications are difficult to evaluate. On the one hand, the fact that they have real implications means that one cannot simply accept anything, while on the other hand the fact that some implications are hypothetical means that it can be difficult to form a clear picture of how the world actually differs depending on whether the assumption is true or false.

% Without going into detail about any of these assumptions, we propose that none of them are presently suitable for ``mass production'' in the way that controlled experiments are. To take one example, even though \citet{lee_randomized_2008} writes of the regression discontinuity assumption ``it is shown below that causal inferences from RD designs can sometimes be as credible as those drawn from a randomized experiment.'', there is a lively debate about the applicability of this method just to the specific case of US House of Representatives elections \citep{snyder_detecting_2005,grimmer_are_nodate,caughey_elections_2017,cuesta_misunderstandings_2016}, which is the example \citet{lee_randomized_2008} used to illustrate his argument in the first place. The amount of careful case-by case attention required to assess these assumptions seems to make it harder to use them in large-scale data-driven decision making in a manner analogous to the results of controlled experiments or predictive models. 

In brief:
\begin{itemize}
    \item Decision making algorithms require stronger assumptions than predictive algorithms so
    \item These assumptions are often particularly hard to evaluate
\end{itemize} 

\section{Exploring alternative foundations}

The development of decision making algorithms is constrained by the assumptions we can make. Such problems are often addressed using theories of causal inference. Theories of causal inference allow us to state assumptions precisely, to understand them in various different ways so that we can better apply our informal means of evaluating them and, hopefully, to make sound choices about whether to accept an assumption or not. A number of such theories exist, and we can distinguish three traditions with wide adoption in different fields: causal Bayesian networks, potential outcomes and structural equation models. These are described in more detail in Chapter \ref{ch:other_causal_frameworks}.

This thesis explores an alternative theory of algorithmic decision making. Our starting point is the observation in Section \ref{sec:assumptions} above that decision making problems call on a decision maker to compare a number of different options on the basis of their consequences. The conclusion is that, to construct a decision making model, where in classical statistics we might consider a probability distribution $\prob{P}$ defined on a sample space $(\Omega,\sigalg{F})$, we instead consider a function from the set of options $C$ to probability distributions on a sample space: $\prob{P}_\cdot:C\kto \Omega$. This extension of the basic statistical model is what underpins this entire thesis.

We call our approach a ``decision theoretic approach'', and a number of authors have previously explored a similar approach to causal inference \citep{heckerman_decision-theoretic_1995,dawid_causal_2000,dawid_influence_2002,dawid_decision-theoretic_2012,dawid_decision-theoretic_2020}. One might ask us all: if we already have three ways to theorise about this problem, why do we need a fourth? To a large extent, for questions like these, the proof is in the pudding -- do the alternative foundations offer insights that are hard to see from other points of view? However, we think that even before learning what alternatives can deliver, there are reasons to believe they are worth exploring.

A broad reason for paying special attention to theoretical foundations in causal inference is outlined in Section \ref{sec:assumptions}: causal inference typically requires assumptions that are strong and particularly hard to evaluate. Alternative theoretical foundations can yield alternative perspectives on old assumptions and suggest new assumptions that accomplish desired goals. For these reasons, causal inference is a field that merits particular attention to foundations. In fact, the reason we have several different causal inference traditions is because different researchers have, at different times, attacked the problem of how to formulate causal models in different ways. This project has been fruitful -- it has facilitated the entire field of research into causal inference. In addition to the handful of identification results suggested above, some other insights enabled by these projects are:
\begin{itemize}
    \item With potential outcomes, it is possible to offer a precise formal statement of what it is that controlled experiments should achieve -- \emph{strong ignorability} \citep{rubin_causal_2005}
    \item With potential outcomes we can precisely express the notion of ``the effect of a choice on the people whose behaviour changed on account of that choice'', formalised as the \emph{local average treatment effect} \citep{imbens_identification_1994}
    \item[-] There is a close correspondence between the intuitive idea of directed causal relationships between variables and the factorisation of joint probability distributions, formalised as the \emph{causal Markov condition} (\citet[Chap. ~1]{pearl_causality:_2009},\citet{wright_method_1934})
    \begin{itemize}
        \item Particular credit is due Pearl for pointing out how common it is for researchers to smuggle these ``intuitive causal ideas'' into discussions with no accompanying theory of causation - see for example \citet[pg. 96]{pearl_causality:_2009}
    \end{itemize}
    \item[-] The field of conditional-independence based \emph{causal discovery} \citet{spirtes_causation_1993}
    \item[-] The field of causal discovery based on the \emph{independence of cause and mechanism} \citet{scholkopf_causality_2022}
    \item[-] Causal identification in semi-Markovian models \citet{shpitser_complete_2008}
\end{itemize}

Here, items marked with a dot are (in our view) difficult to formulate using causal graphical models alone, while items marked with a dash are (again, in our view) difficult to understand from the potential outcomes perspective.

We have some specific reasons for wanting to go beyond these two frameworks. For the potential outcomes framework, the reason is quite simple: in order to make a decision, we must consider a map from some set of options $C$ to distributions over consequences, and the potential outcomes framework does not provide such a function at the base level. Once we incorporate such a function, it is not clear that there is any longer a need for potential outcomes at the axiomatic level (though it may still be possible to define variables in particular problems that play a similar role).

The situation with causal graphical models -- that typically do have a notion of intervention -- requires some more explanation. Causal graphical models are characterised by sets of variables with directed relationships between them, and these directed relationships are underwritten by the idea of \emph{interventions}. Our suspicion is that this kind of model describes a special case of the kind of model we investigate -- which, recall, is defined by a function from a set of options $C$ to a set of probability distributions over a fixed sample space -- but is not appropriate to describe the general case. The issue centres on variables and interventions.

First, it is important to point out that while interventions are suggestively named, they are not defined as ``things an experimenter actually goes and does''. Any identification of actions that can be taken with interventions is up to the analyst's discretion for a particular problem. 

\todo[inline]{the notion of intervention is incompatible with many sets of variables}

\subsection[Other theories]{Other theories of algorithmic decision making}

Causal inference theories aren't the only theories that address decision making algorithms. These questions are also addressed by the fields of reinforcement learning, optimal control and statistical decision theory (and, no doubt, others besides). One distinction we can draw between these fields and the field of causal inference is that a key difficulty in causal inference problems is just how to relate consequences of actions to observations. In reinforcement learning, an \emph{environment} is typically assumed that represents the ``ground truth'' of consequences of actions, and the history of consequences can be used to infer which environment an agent is operating in \citep{barto_reinforcement_1998}. While optimal control is such a large field it's inappropriate to make any sweeping generalisations, basic versions of control theory assume a \emph{system model} is available that maps states and inputs to updated states and outputs \citep{ogata_discrete-time_1995}. Finally, in statistical decision theory the relevant notion of consequences of actions is given by the \emph{state} and the \emph{loss}, which like the environment in reinforcement learning, are basic elements of the problem \citep{wald_statistical_1950}.

In the causal graphical models tradition, the graphical model plays the role of relating observations to consequences, and potential outcomes (arguably) play this role in the potential outcomes tradition (as is explained in more detail in Chapter \ref{ch:other_causal_frameworks}). In contrast to reinforcement learning and optimal control, causal inference often deals with ``one-shot'' problems, where the given data is assumed to be fixed. In contrast to statistical decision theory, the space of consequences is not restricted to a scalar loss value, and in fact is typically assumed to be identical to the space of observations.

There is substantial overlap between these different methods for relating observations to consequences. For example, \citet[Chap.~4]{lattimore_learning_2017} shows how the environment model in a reinforcement learning problem can be specified using a causal graphical model. In Chapter \ref{ch:evaluating_decisions}, we will discuss \emph{hypotheses} in decision making models which are very similar to \emph{states} in statistical decision theory, and in Chapter \ref{ch:2p_statmodels} we will show how a decision making model in conjunction with a utility function induces a statistical decision problem.



% \section{Theories of causal inference}



% For causal questions that are controversial or difficult, it is tremendously advantageous to be able to address them transparently. Theories of causation enable this; given a theory of causation and a set of assumptions, if anyone claims that some conclusion follows it is publicly verifiable whether or not it actually does so. If the deduction is correct, then any remaining disagreement must be in the assumptions or in the theory. For people who are interested in understanding what is true, pinpointing disagreement can be enlightening. Someone could learn, for example, that there are assumptions that they find plausible that permit conclusions they did not initially believe. Alternatively, if a motivated conclusion follows only from implausible assumptions, hearing these assumptions explicitly might make the conclusion less attractive. 

% Theories of causation help us to answer causal questions, which means that before we have any theory, we already have causal questions we want to answer. If potential outcomes notation and causal graphical models had never been invented there would still be just as many people who want to the answer to questions something like ``does smoking causes cancer?'', even if on-one could say what exactly they meant by ``causes'' and even if many people actually want answers to slightly different questions. Theories exist to serve our need for transparent answers to causal questions.

% Potential outcomes and causal graphical models are prominent examples of ``practical theories'' of causation. I call them ``practical theories'' because most of the time we encounter them they are being used to answer ``practical'' questions like ``Does smoking cause cancer?'', or ``In general, when does data allow us to conclude that $X$ causes $Y$?'' It is less common to see the ``fundamental questions'' addressed, like ``Does the theory of causal graphical models offer an adequate account of what `cause' means?'', which is more often found in the field of philosophy. \citet{spirtes_causation_1993} explain their motivation to study what I call ``practical theories of causation'' as follows:

% \begin{quote}
% One approach to clarifying the notion of causation -- the philosophers’ approach ever since Plato -- is to try to define ``causation'' in other terms, to provide necessary and sufficient and noncircular conditions for one thing, or feature or event or circumstance, to cause another, the way one can define ``bachelor'' as ``unmarried adult male human.'' Another approach to the same problem -- the mathematician’s approach ever since Euclid -- is to provide axioms that use the notion of causation without defining it, and to investigate the necessary consequences of those assumptions. We have few fruitful examples of the first sort of clarification, but many of the second [...]
% \end{quote}

% I think what Spirtes, Glymour and Scheines (henceforth: SGS) mean here is that they \emph{define} a notion of causation -- because causal graphical models do define a notion of causation -- without interrogating whether it means the same thing as the word ``causation''. Incidentally, since publication of this paragraph, the notion of causation defined by causal graphical models has been subject to substantial interrogation by philosophers \citep{woodward_causation_2016}.

% I am sympathetic to the argument that it does not matter a great deal whether ``causal-graphical-models-causation'' and ``causation'' mean the same thing in everyday language. It is common for words to have somewhat different meanings when used by specialists to when they are used by laypeople, and this isn't because the specialists are ignorant or confused about their subject. However, I think it matters a lot which causal questions can be transparently answered by ``causal-graphical-models-causation'', and so I believe that the notions of causation adopted by practical theories do warrant scrutiny.

% I think one reason that SGS are keen to avoid dwelling on the definition of causation is that satisfactory definitions of causation are difficult. For example, causal graphical models depend on the notion of \emph{causal relationships} between variables. These may be defined as follows:

% \begin{quote}
% $\RV{X}_i$ is a \emph{cause} of $\RV{X}_j$ if there is an \emph{ideal intervention} on $\RV{X}_i$ that changes the value $\RV{X}_j$
% \end{quote}

% This definition is incomplete without a definition of ``ideal interventions''. Ideal interventions may be defined by their action in ``causally sufficient models'':
% \begin{itemize}
%     \item An $[\RV{X}_i,\RV{X}_j]$-ideal intervention is an operation whose result is determined by applying the \emph{do-calculus} to a \emph{causally sufficient} model $((\Omega,\mathcal{F},\prob{P}),\diagram{G},\vecRV{U})$
%     \item A model $((\Omega,\mathcal{F},\prob{P}),\diagram{G},\vecRV{U})$ is $[\RV{X}_i,\RV{X}_j]$-causally sufficient if $\RV{U}$ contains $\RV{X}_i$, $\RV{X}_j$ and ``all intervenable variables that \emph{cause}'' both $\RV{X}_i$ and $\RV{X}_j$ \footnote{Weaker conditions for causal sufficiency are possible, but they don't avoid circularity \citep{shpitser_complete_2008}}
% \end{itemize}

% While I don't offer a definition of the \emph{do-calculus} in this introduction, it can be rigorously defined, see for example \citet{pearl_causality:_2009}. The problem is that the definition of a \emph{causally sufficient} model itself invokes the word \emph{cause}, which is what the original definition was trying to address. Circularity is a recognised problem with interventional definitions of causation \citep{woodward_causation_2016}. In Section \ref{sec:cbns_without_d}, I further show models with ideal interventions generally have counterintuitive properties. The purpose of a theory of causation like causal graphical models is to support transparent reasoning about causal questions, and a circular definition that leads to counterintuitive conclusions undermines this purpose.

% As with Euclid's parallel postulate, I think it is reasonable to ask if the notion of ideal interventions and other causal definitions can be modified or avoided. Causal statistical decision theory (CSDT) is a theory of causation that is motivated by the problem of \emph{what is generally needed to answer causal questions} rather than \emph{what does ``causation'' mean?} Along similar lines to CSDT, \citet{dawid_decision-theoretic_2020} has observed that the problem of deciding how to act in light of data can be formalised without appeal to theories of causation. We develop this in substantial detail, showing how both \emph{interventional models} and \emph{counterfactual models} arise as special cases of CSDT.\todo{I want to revisit the claims about what I actually show, hopefully to add to it}

% A key feature of CSDT is what I call the \emph{option set}. This is the set of decisions, acts or counterfactual propositions under consideration in a given problem. A causal graphical model and a potential outcomes model will both implicitly define an option set as a result of their basic definitions of causation, but CSDT demands that this is done explicitly. I argue that this is a key strength of CSDT, on the basis of the following claims which I defend in the following chapters:

% \begin{itemize}
%     \item Causal questions are not well-posed without an option set in the same way a function is not well-defined without its domain
%     \item The option set need not correspond in any fixed manner to the set of observed variables
%     \item The nature of the option set can affect the difficulty of causal inference questions
% \end{itemize}


% \todo[inline]{I commented out an additional section about potential outcomes and closest world counterfactuals, which is a second example of ``opaque causal definitions''. I'm interested if any readers think it would be good to have a second example}


% Potential outcomes basic assumptions

% \begin{itemize}
%     \item Potential outcomes defines ``the treatment effect of $\RV{X}_i$ on $\RV{X}_j$'' in terms of the value of $\RV{X}_j$ under the \emph{counterfacutal supposition} that $\RV{X}_i$ had taken a different value
% \end{itemize}

% In fact, the notion of ``ideal intervention'' often seems to underpin potential outcomes models as well. Work in the potential outcomes theory often uses the idea of the value of $\RV{X}_j$ under a counterfactual supposition concerning $\RV{X}_i$ interchangeably with the idea the response of $\RV{X}_j$ to an idealised intervention on $\RV{X}_i$ \citep{morgan_counterfactuals_2014,rubin_causal_2005,richardson2013single}. \cite{lewis_causation_1986} offered a definition of the value $\RV{X}_j$ under counterfactual suppositions in terms of the value it would take in the world that was ``closest'' to the real world but in which the value of $\RV{X}_i$ was altered. There are many ways that we could use to measure how close one world is to another, many of which need not invoke any notion of ``ideal intervention'', but I have never encountered practical work on causal inference that was based on considerations of such similarity measures.


\section{Causally compatible variables}\label{sec:cc_vars}

