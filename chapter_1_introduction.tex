
%!TEX root = main.tex

\begin{itemize}
    \item Causal inference is difference because it's about changing the world
    \item Causal inference demands hypotheticals
    \item Progression from ad-hoc theory -> formal theory
    \item Here are some formal theories:
    \begin{itemize}
        \item CBN
        \item PO
    \end{itemize}
    \item Formal theories help us think about difficult questions
    \item Here are some difficult questions not easily addressed using
    \begin{itemize}
        \item CBN: causal variables
        \item PO: causation with directions
    \end{itemize}
    \item DT approach: well, we've got to make decisions anyway
    \item What does it bring?
    \begin{itemize}
        \item Unrolling causal models: a general ``type signature''
        \item Identification as a symmetry, more negative view
        \item Alternative perspective on causal direction: it supports inference from conditional independence
    \end{itemize}
\end{itemize}

\chapter{Introduction}\label{ch:introduction}

\todo[inline]{Introduction stands alone}

\section{Making decisions with data}

Beginning in the 1930s, a number of associations between cigarette smoking and lung cancer were established: on a population level, lung cancer rates rose rapidly alongside the prevalence of cigarette smoking. Lung cancer patients were far more likely to have a smoking history than demographically similar individuals without cancer and smokers were around 40 times as likely as demographically similar non-smokers to go on to develop lung cancer. In laborotory experiments, cells which were introduced to tobacco smoke developed \emph{ciliastasis}, and mice exposed to cigarette smoke tars developed tumors\citep{proctor_history_2012}. Nevertheless, until the late 1950s, substantial controversy persisted over the question of whether the available data was sufficient to establish that smoking cigarettes \emph{caused} lung cancer. Cigarette manufacturers famously argued against any possible connection \citep{oreskes_merchants_2011} and Roland Fisher in particular argued that the available data was not enough to establish that smoking actually caused lung cancer \citep{fisher_cancer_1958}. Today, it is widely accepted that cigarettes do cause lung cancer, along with other serious conditions such as vascular disease and chronic respiratory disease \citep{world_health_organisation_tobacco_nodate,wiblin_why_2016}.

The question of a causal link between smoking and cancer is a very important one to many different people. Individuals who enjoy smoking (or think they might) may wish to avoid smoking if cigarettes pose a severe health risk, so they are interested in knowing whether or not it is so. Additionally, some may desire reassurance that their habit is not too risky, whether or not this is true. Potential and actual investors in cigarette manufacturers may see health concerns as a barrier to adoption, and also may personally want to avoid supporting products that harm many people. Like smokers, such people might have some interest in knowing the truth of this question, and a separate interest in hearing that cigarettes are not too risky, whether or not this is true. Governments and organisations with a responsibility for public health may see themselves as having responsibility to discourage smoking as much as possible if smoking is severely detrimental to health. The costs and benefits of poor decisions about smoking are large: 8 million annual deaths are attributed to cigarette-caused cancer and vascular disease in 2018\citep{world_health_organisation_tobacco_nodate} while  global cigarette sales were estimated at US\$711 billion in 2020 \citep{noauthor_cigarettes_nodate} (a figure which might be substantially larger if cigarettes were not widely believed to be harmful).

The question of whether or not cigarette smoking causes cancer illustrates two key facts about causal questions: First, having the right answers to causal questions can underpin decisions of tremendous importance to large numbers of people. Second, confusion over causal questions can persist even when a great deal of data and and a great many facts relevant to the question are agreed upon. Understanding how the world might be influenced is often both valuable and difficult.

We can think about a number of different ways one could go about learning how to influence the world from data:

One option is to try to obtain data that we're confident tells us directly about the consequences of the different options under consideration. For some purposes, data produced from well-conducted experiments is widely agreed to provide reliable information about the effectiveness and safety of treatments tested in the experiment. 

Alternatively, we could use the data to solve an intermediate problem, and make use of pre-existing knowledge about how to influence the world given a solution to this problem. For example, if I am on a long car trip, my tank is three quarters empty and I'm at the last fuel station for 200km, then given an answer to the question of how far my car will travel on one quarter a tank of fuel it is easy to decide whether or not I should fill up right now, and if I've logged my mileage previously I might use the data I collected to answer this question. In this example, the data don't tell me directly whether or not I should fill up. 

However, we might be in a position where, we aren't so confident that the data we can acquire can provide us with reliable guidance directly about our choice, and we don't know of any surrogate problems that make the causal question straightforward. In this case, we may want to make use of a formal theory of causal inference. When we can't see the solution immediately, a theory of causal inference can help us see more clearly the consequences of things we already know. It can also provide a language that we can use to discuss assumptions and conclusions with other people.

A lot can be said for the first two options. ``Collecting the right data'' has driven some of the most significant recent developments in algorithmic decision making. Operational advances that enable controlled experiments to be conducted at large scales have driven substantial changes in of many online businesses \citep{kohavi_surprising_2017}, and Abhijit Banerjee and Esther Duflo were recently awarded a Nobel prize in part for their pioneering role in the use of large numbers of randomised controlled trials (RCTs) to assess the effectiveness of many different development interventions in many different contexts\citep{zhang_abdul_2014}. Some fields of science have also been significantly affected by ``negative progress'' in the science of assessing experimental results. For example, in psychology in particular, replication attempts have shown that causal conclusions from experimental psychological data are less robust than many had hoped or (perhaps) believed \citet{open_science_collaboration_estimating_2015,stroebe_what_2019}. At the same time, standards for what constitutes a ``well-conducted'' experiment have risen across many fields \citep{nosek_preregistration_2018,liberati_prisma_2009}.

``Solving intermediate problems'' has also been behind tremendous technological advances. A machine that recognises your face has no useful impact in the world by itself, but there are a lot of people who know how to use such a machine for commercial or other purposes.

While a lot of progress can be made by getting around the problem that it's hard to make data-driven decisions that directly aim to influence the world, we think it is also interesting to attack the problem directly. One question we might want to ask is: \emph{why} is this problem hard? While we don't claim to know for sure, a major source of difficulty seems to come from the fact that decision making requires us to consider hypotheticals.

\subsection[Hypotheticals]{Decision making requires thinking about hypotheticals}\label{sec:assumptions}

Data driven prediction problems and data driven decision making problems can have a lot in common. The outcomes some people are interested in predicting are often outcomes other people want to influence. A forecaster might want to predict the winner of the next election, while a party strategist is interested in maximising their party's chance of victory. A product manager may be simultaneously interested in accurately inferring the sentiment expressed in reviews of their product, and in making product changes that increase the frequency that this sentiment is positive. Furthermore, data relevant to prediction is often relevant to decision making and vise-versa. Political parties often reason that electorates in which their predicted chance of victory is very low are not worth investing campaign resources in, and if a forecaster learns of evidence that one party had adopted a particularly effective election strategy they might want to revisit their prediction of the eventual election winner. The overlap is not perfect: comprehensive electorate level polls are probably more useful to the forecaster while small-scale controlled experiments are probably more useful to the strategist, but there's a lot of overlap nonetheless.

A distinguishing feature of decision making problems is that they demand the decision maker consider a collection of hypotheticals, most of which are never realised. The strategist must consider a number of different strategies to pursue, and ultimately only learns of the outcome under the strategy their party \emph{did} pursue. The election forecaster, on the other hand, can consider a number of different forecasts, but after the fact they learn exactly how accurate \emph{every} forecast was and, in particular, whether the forecast they made was better than others they could have made.

Statistical probability is well-established and widely used as a formal theory of data-driven prediction. We can speculate, on the basis of the observations above, that a formal theory of data-driven decision making may be obtained by augementing statistical probability with the right kind of hypotheticals. In fact, even though causal inference is not quite synonymous with data-driven decision making, the most widely used theories of causal inference are theories of statistical probability augmented with a particular notion of hypothetical.

To understand the role of hypotheticals in theories of causal inference and decision making, it helps to think about the role of random variables in statistical probability. Random variables have two ``faces''; on one hand, they are defined as measurable functions whose domain is the sample space, and this allows us to reason about them using the tools of mathematics. However, they \emph{also} refer to the results of measurements conducted in the real world. This feature of random variables is not a consequence of any mathematical definition, but it is this connection to the real world that allows us to use insights derived from mathematical reasoning to inform predictions we can make about real-world events.

Hypotheticals are similarly two-faced. On the one hand, particular kinds of hypotheticals have formal definitions in theories of causal inference and decision making, and on the other hand the fact that they point to something outside the mathematical model is what allows us to use the model to help us make decisions (or to do whatever else we might want to do with a causal model). Just what it is that hypotheticals in causal models refer to is a trickier question than what random variables refer to.

\subsection{Structural interventions}

One of widely-used theory of causal inference uses the term \emph{interventions} for the relevant class of hypotheticals. Interventions are defined with respect to a particular set of variables that we will call \emph{causal variables}. A graphical causal model (for the purposes of this introduction) assigns to each causal variable a possibly empty set of \emph{parents} (or causes) selected from the rest of the causal variables. Usually this assignment of parents has no cycles, but there are versions of interventional models that allow cycles \citep{bongers_theoretical_2016,forre_markov_2017}.

In this manner, the assignment of parents can be represented with a directed acyclic graph -- each causal variable is associated with a node of the graph, and an arrow $\RV{X}\rightarrowtriangle \RV{Y}$ appears in the graph just when $\RV{X}$ is a parent of $\RV{Y}$. For example, the following graph
\begin{align}
\tikzfig{simple_dag}
\end{align}
identifies three causal variables $\RV{X}$, $\RV{Y}$ and $\RV{Z}$, and identifies $\RV{X}$ and $\RV{Z}$ as parents of $\RV{Y}$, $\RV{Z}$ as a parent of $\RV{X}$ and $\RV{Y}$ as a parent of $\RV{X}$.

Given a graphical causal model and a joint probability distribution over the causal variables, an intervention on a causal variable $\RV{X}$ is formally an operation that alters the distribution of $\RV{X}$ conditional on its parents in a known way, while not affecting the distribution of any other causal variable conditional on \emph{its} parents.

Beyond causal models, interventions on $\RV{X}$ refer to actions that can be taken that alter the real thing represented by $\RV{X}$ in a predictable way while also avoiding influencing any of the other real things represented by any of the other causal variables. It can be reasonably clear what it means for an action to avoid influencing other causal variables; for example, take $\RV{Z}$ to be the last month's average rainfall, $\RV{X}$ to be the average number of flowers I see when I walk for the past month and $\RV{Y}$ to be how often I walk in the past month. In a common sense way, deciding to go for a walk today changes $\RV{Y}$ but has no effect on rainfall or flower growth. Also in a common sense way, someone else planting flowers might induce me to walk more often, but does not affect the rainfall or my inclination to walk holding weather and flowers constant. Finally, seeding clouds might cause more rainfall, which could cause more flowers and might affect my walking in an unpredictable way, but it probably doesn't alter the dependence of my walking on the weather and the scenery. 

However, it's not always clear how to interpret interventions. While cloud seeding seems to fit the general notion of an intervention, it could alter the relationship between rainfall and flower growth by inducing non-seasonal rain. One could understand this difficulty as a problem with the selection of causal variables; by omitting a variable that represents the season, we make it so that an action that is ``actually'' an intervention does not correspond to the intervention operation in our causal model.

\subsection{Potential outcomes}

The other widely used theory of causal inference uses the term \emph{potential outcomes} for its class of hypotheticals. Formally, potential outcomes are statistical random variables associated with a pair of ``ordinary'' statistical random variables. For example, if we have $\RV{X}$ again representing the average number of flowers I see and $\RV{Y}$ again representing my frequency of walks, we can define a vector of potential outcomes as a copy of $\RV{Y}$ for every possible value of $\RV{X}$: $(\RV{Y}^x)_{x\in [0,100]}$. 

Beyond the model, a potential outcome $\RV{Y}^{50}$ represents ``how often I would've taken walks last month if I had seen 50 flowers each time on average''. In cases like this, it's not clear that there is a common-sense interpretation of potential outcomes. Counterfactual statements themselves are often difficult enough to grasp that some additional theory seems needed to make sense of them.

There are some cases where potential outcomes have a somewhat easier interpretation. A version of a classic example is when $\RV{X}$ represents whether or not a patient took antibiotics and $\RV{Y}$ represents the presence of an ear infection at a follow-up appointment. In this case $\RV{Y}^0$ represents the presence of an ear infection ``had the patient not taken antibiotics'' and $\RV{Y}^1$ represents the presence of an ear infection ``had the patient taken antibiotics''. If we adopt the patients point of view, we can view these as the consequences that the patient should consider when deciding whether or not to take the medicine.

In fact, some authors have argued that potential outcomes are underpinned by choices -- that is, we have pootential outcomes $\RV{Y}^X$ precisely when $X$ is a set of options that somebody could, in principle, have chosen \citep[~pg. 4]{imbens_causal_2015}. 

In the philosophical investigation of the interpretation of counterfactuals, accounts based on structural interventions are one of the most prominent theories \citep[Section 3.3]{starr_counterfactuals_2021}, though there are several versions of this account and like all of the other theories of counterfactuals they are controversial.

\subsection{Successes of theories of causal inference}

Formal theories of causal inference exist to help us to draw reliable conclusions from data when informal reasoning isn't good enough to do so. A full account of the successes of intervention and potential outcomes based theories is beyond the scope of this introduction, but a brief overview will help to situate the work in this thesis in the broader context of causal inference theories.

\subsubsection{Successes of potential outcomes}

Potential outcomes models are characterised by the inclusion of potential outcomes variables, typically notated with superscripts $\RV{Y}^0$, $\RV{Y}^1$. These variables represent counterfactual notions -- $\RV{Y}^0$ can be read ``the value that $\RV{Y}$ would have taken, had $\RV{X}$ been 0''. The potential outcomes framework has been particularly influential in econometrics, with use of potential outcomes in that field predating the actual term ``potential outcomes''. In econometrics, and in other areas that potential outcomes have been widely used, models often involve people acting deliberately (or, sometimes, rationally) and associate potential outcomes with prospective consequences of peoples' choices.

\textbf{Models involving rational agents}: As we have noted, in some situations the potential outcomes $\RV{Y}^0$, $\RV{Y}^1$ and so forth look like a set of prospective consequences that a decision maker is choosing between. In some settings, we can let them be precisely that - a decision maker's expectation of the consequences of different actions they can choose. An early application of potential outcomes was to the problem of determining supply and demand curves given data on the quantity of goods exchanged and the price at which they were exchanged found in \citet{tinbergen1997determination} and \citet{haavelmo_statistical_1943} (in \citep[ch. ~20]{hendry_foundations_1997}). In this work, a ``supply curve'' is defined as a function that maps a hypothetical price to the quantity of goods that would be supplied, were that the actual price of goods, which is for all intents and purposes a vector of potential outcomes $(\RV{Y}^x)_{x\in A}$ for some set $A$ of prices under consideration. 

Note that, in this case, ``what potential outcomes mean'' might be able to be grounded in a theory of behaviour of buyers and sellers. We can suppose that sellers are actually asking questions like ``how much would I sell if I asked for a price $x$?'' and getting hints about the answer from buyers and other sellers. Under some idealisations -- for example, maybe we require the buyers and sellers to all agree on questions of this nature -- we might be able to consider the potential outcomes to represent the the traders' answers to these questions.

\textbf{Analysis of randomised experiments:} The potential outcomes framework offers an account of what it is that a randomised experiment achieves so that it enables causal conclusions to be drawn. Under this framework, the critical condition is the independence of the input $\RV{X}$ from the vector of potential outcomes $(\RV{Y}^x)_{x\in X}$ (in the potential outcomes framework, $\RV{X}$ is often called an \emph{assignment}). If the value of $\RV{X}$ is completely determined by some physical randomisation procedure then (so the argument goes) it must be independent of the potential outcomes. 

A more general kind of randomised experiment completely determines the values of inputs based on a collection of other variables $\RV{W}$ called \emph{covariates} and some physical randomisation procedure. In this kind of experiment, the inputs are independent of the potential outcomes conditional on the covariates. When this holds, the inputs $\RV{X}$ depend probabilistically on the covariates $\RV{W}$, and this dependence is usually called the \emph{assignment mechanism}. When this dependence is known or able to be estimated, it can facilite the calculation of many causal effects of interest (see Section \ref{sec:potential_outcomes} for more details). Using the potential outcomes framework, many techniques have been developed to estimate the assignment mechanism and to estimate causal quantities of interest given the assignment mechanism -- for example, many methods and worked examples can be found in \citet{imbens_causal_2015}.

Because the potential outcomes framework doesn't offer a theory of counterfactuals, it doesn't come with an explanation of why the inputs are indpendent of the potential outcomes in a randomised experiments -- this is a matter that stands outside the formal theory. One could consider an explanation that goes something like: we imagine the potential outcomes to be fixed at the time that the inputs are decided, so the inputs have no influence over them. Furthermore, if the inputs are physically randomised, the potential outcomes can have no influence over the inputs, and so the two are independent. We might also consider the converse of this claim: perhaps any reasonable theory of counterfactuals must yield the conclusion that potential outcomes are independent of any physically randomised inputs.

Unlike the case of supply and demand, the deliberate action in a randomised experiment is the assignment carried out by the experimenter rather than the choices made by subjects of the model.

\textbf{Subpopulations with different behaviour:} Rather than having an input $\RV{X}$ that is completely determined by a physical randomization mechanism, sometimes experiments of interest have some physically randomized $\RV{Z}$ that influences but does not fully determine $\RV{X}$. For example, if $\RV{X}$ records whether or not someone took a medicine, $\RV{Z}$ might record whether or not the medicine was prescribed. Experimenters might be able to have prescriptions randomized, but not the actual act of taking the medicine. The potential outcomes framework first offers us a way to understand that this is not analogous to an experiment where $\RV{X}$ is randomised: by application of probability theory, the fact that the potential outcomes are independent of $\RV{Z}$ does not mean that they are independent of $\RV{X}$.

A notable result proven using the potential outcomes framework is that, under an assumption that prescribing a medicing never induces anyone to avoid the medicine who would otherwise have taken it (``no defiers''), it is possible to determine the effect of taking the medicine on the subpopulation of ``compliers'' -- people who were induced by the prescription to switch from not taking the medicine to taking it. See \cite{imbens_identification_1994} for more details.

In this setting, there are often deliberate choices carried out by the experimenter -- namely, the assignment of $\RV{Z}$ -- and also deliberate choices carried out by experimental subjects -- the choice of $\RV{X}$.

\subsubsection{Successes of structural interventional models}

Strutural interventional models feature a collection of causal variables, and each variable is assigned a set of parents from the remaining causal variables. Each causal variable may be intervened on, which in general alters the distribution of the variable conditional on its parents while not changing the distribution of any other causal variable conditional on that variable's parents.

\textbf{A formal theory of directed causal relationships:} A point that is repeatedly made in \citet{pearl_causality:_2009} and \citet{pearl_book_2018} is that informal notions of causal relationships play a key role in the formulation of many statistical models. For example, in the account of randomised experiments above, we said that the treatment assignment was ``determined by'' a physically random procedure. This statement is not backed by a formal theory, but the relation invoked by the phrase ``determined by'' is something like a causal relationship. It implies that the treatment assignment and the output of the randomisation are deterministically related, but ``being determined by something'' is not a symmetric relationship.

Structural interventional models offer a theory of causal relationships that aims to clarify these intuitions. They have been used to analyse questions like ``what is the likelihood that the medicine caused the reaction?'' (\citet[ch. ~9]{pearl_causality:_2009}, \citet{pearl_causes_2015}), which differ from traditional causal inference questions that are more focussed on the consequences of actions than on attributing responsibility. 

The question of whether the structural interventional account explains causal intuitions has been taken on by philosophers (see, for example, \citet{woodward_causation_2016}), but whether it is successful in doing so is contested \citep{cartwright_modularity_2001}.

Interventions might not be the \emph{only} possible way to ground intuitions about directed causal relationships. An alternative proposition is that the distribution of a causes and the distribution of an effect conditional on the distribution of a cause should be \emph{algorithmically independent}, or that the relation between them should be \emph{generic} \citep{lemeire_replacing_2013}. Because this principle can induce directed relationships between pairs of variables, it can potentially offer an account of directed causal relationships without appealing to interventions, though it has been substantially less studied than the structural intervention account of directed relationships. However, unlike the interventional theory. this does not seem to tell us how causal relationships should inform our ideas of the consequences of taking an action\footnote{Throughout this thesis, we use the term ``intervention'' to refer to an operation defined in the structural interventional account of causal inference, or the interpretation of this operation. We use the word ``action'' to refer to something that may or may not be interpretable as an intervention.}.

One of the contributions of the present work is Theorem \ref{th:det_obs_to_cons} in Section \ref{sec:precedent}, which shows that under an assumption of generic relationships between the conditional distributions of causes and effects together with the assumption that a proposed plan of action has a precedent then conditional independences in observed data can imply certain relationships do not change under any action the decision maker might take. Invariant relationships under action, which are taken to be axiomatic in the theory of structural interventions, can be shown to follow from the assumption of generic relationships between conditional distributions and the previously mentioned assumption that actions are precedented. We think this suggests that it may be possible to forge a unified view of directed causal relationships that subsumes both the notion that causal relationships should be invariant under action and the notion that conditional distributions should be generically related in the causal direction, although precisely how to do this is an open question.

\textbf{Causal inference under generic assumptions:} Traditionally, analysis of causal inference problems involves certain non-generic assumptions like the assumption of independence of inputs and potential outcomes. These assumptions are non-generic because they do not apply to arbitrary causal inference problems, and so the analysis made under these assumptions can only be applied to data generated in particular contexts (for example, in controlled experiments).

The structural models tradition, however, has fostered the analysis of \emph{causal discovery}, which is the problem of learning causal relationships from data which may not be known to satisfy certain strong assumptions. There are two main approaches to causal discovery: conditional independence-based causal discovery infers a family of causal graphs from conditional independences inferred from a dataset, while the previously mentioned theory that conditionaly probabilities should be algorithmically independent in the causal direction has led to a wide variety of different approaches for discovering the direction of casuation. Early examples of conditional independence based inference are the PC algorithm and the Causal Inference Algorithm \citet[Ch. 5\& 6]{spirtes_causation_1993} and Greedy Equivalent Search \citet{chickering_optimal_2003,chickering_learning_2002}, while more recently it has been discovered that the problem of searching for a graph satisfying inferred conditional independences can be posed as a continuous optimization problem \citet{zheng_dags_2018,ng_graph_2019}. Examples of the application of the metaphor of algorithmic independence include -- \citet[ch. 4, 5, 6 \& 7]{peters_elements_2017} presents an overview of some of these techniques.

While the aim of this analysis is to discover causal relationships from generic data, in practice the key assumptions are not completley generic. \citet{uhler_geometry_2013} examines how frequently the key assumption of $\lambda$\emph{-faithfulness} underpinning the viability conditional independence based approach is violated, and finds that (under their assumptions) models with more than 10 variables and relativley dense causal connections almost always violate the condition. Owing to the fact that algorithmic independence is incomputable and suitable approximations have to be found for practical algorithms. the algorithmic independence based approach has typically involved special conditions like linear causal relationships with non-Gaussian additive noise.

\textbf{Causal identification in complex models:} Potential outcomes approaches have proposed a wide variety of sufficient assumptions for estimating causal effects, but there are models in which causal effects can be estimated that are typically ignored by the potential outcomes approach. The graphical models community usually separates the problem of \emph{identification} and \emph{estimation}; a causal effect is \emph{identified} if it can be computed from the joint probability of the observed variables. If this is possible, then the causal effect coudl in principle be calculated from an estimate of the joint probability (though estimating the entire joint probability is usually more than is needed). 

One of the simplest graphical models in which causal effects are identified that haven't received much attention in the potential outcomes literature is the ``front-door'' condition. Given a graphical model for which it is impossible to identify the effect of $\RV{X}$ on $\RV{Y}$, but the effect of $\RV{X}$ on $\RV{W}$ and the effect of $\RV{W}$ on $\RV{Y}$ can be separately identified, then it is possible to compose the two to identify the effect of $\RV{X}$ on $\RV{Y}$\citet[Section 3.3.2]{pearl_causality:_2009}. In fact, a complete characterisation of the identifiability of graphical models has been given by \citet{shpitser_complete_2008}, and a more recent alternative characterisation is presented in \citet{richardson_nested_2017}.   

\subsection{Challenges to popular theories of causal inference}

Despite the fact that both approaches have substantially advanced everyone's understanding of causal inference, we believe that both approaches face difficulties that make them hard to apply outside of special settings. The difficulty with potential outcomes is easy to state: the potential outcomes framework offers no clear theory of counterfactuals. Thus the appropriate interpretation of counterfactuals statements must either be obvious or established by convention or else communication using the framework risks confusion. 

As we noted earlier, some authors have suggested that potential outcomes should represent the potential consequences of choices or actions, and that this might perhaps form the basis of a theory of counterfacutals. The work in this thesis builds a theory of causal inference starting from the view that the fundamental problem to be addressed is a problem of making informed choices. Theory that that is developed looks quite different to a theory of potential outcomes, but we don't have a particularly strong view on whether our theory is fundamentally different to potential outcomes, or whether the two approaches are fundamentally similar but look different because we focus on modelling an abstract problem of making informed choices, while potential outcomes is often focused much more onvarious concrete problems.

Alternatively, some authors have argued that the theory of structural interventions is the appropriate theory of counterfactuals for potential outcomes \citep[chap. ~7]{pearl_causality:_2009}. In this case, the following remarks on structural interventional theories apply.

\subsubsection{Difficulties for structural models}

If a decision maker has a sound informal understanding of causal relationships relevant to their problem, structural models are often an excellent tool to formalise this understanding and derive conclusions from it. However, if a decision maker is dealing with a problem where he does \emph{not} have a sound informal understanding of causal relationships, what does the structural approach offer him? The structural model community might suggest that he performs \emph{causal discovery}; this is some procedure that takes his data and offers him a best guess of the structural model associated with this data.

What role should this learned structural model play in the decision maker's subsequent deliberation? We propose three answers to this question:
\begin{enumerate}
    \item The structural model tells the decision maker what his options are and what their consequences are
    \item The structural model can be combined with the decision maker's prior knowledge of what his options are to offer an assessment of their consequences
    \item The structural model and the decision maker's options coevolve; perhaps the decision maker has an initial idea of what his options are which motivates a particular avenue of causal discovery which, in turn, migh prompt the decision maker to reevaluate his options and so forth
\end{enumerate}

We consider the second and third answers reasonable, though the third answer is beyond the scope of this thesis. However, these two answers seem to be in tension with typical practice in causal discovery. Causal discovery algorithms typically take only the given data as input, and depend in no way on any specification of the decision maker's options. Despite this, whether or not a structural model can offer an assessment of the consequences of a set of options is sensitive to the options under consideration.

\begin{example}[Different sets of options require different models]\label{ex:prob_int_2}
Suppose on day $i$, at some point during the day, a volunteer Ella looks at the current outside temperature and logs whether it is ``cold'' or ``hot'' as $\RV{T}_i$, whether or not she's wearing a jumper as $\RV{X}_i$ and whether she feels cold, comfortable or hot as $\RV{Y}_i$. Under normal circumstances, in cold weather she usually wears a jumper and feels comfortable, and in hot weather she wears no jumper and feels comfortable. If she is uncharacteristically not wearing a jumper on cold days, she feels cold, and if she is wearing one on hot days she feels hot.

Suppose she's asked to wear her jumper no matter what. In this case, she will feel comfortable on cold days as before, and will feel hot on hot days also as before. Under this ``intervention'', the relationship between her perceived body temperature and the joint specification of her clothing and the weather is unchanged. If we assume an acyclic structural model, that the instruction to wear a jumper should indeed be modeled by an intervention in this model, and that the temperature may be a cause but not an effect of body temperature and jumper wearing, then we can conclude from the results of the intervention that jumper wearing causes body temperature perception and that this relationship is unconfounded given the daily temperature:
\begin{align}
    \tikzfig{simple_dag_jumper}\label{ex:simple_dag_jumper}
\end{align}

Suppose we \emph{also} want to consider the effect of actions affecting Ella's perceived body temperature. We could ask her to exercise intensely before filling out the survey on some days. What we find is, because exercise raises her body temperature, after exercising she feels comfortable with no jumper in cold weather and feels hot in hot weather with no jumper. However, we \emph{also} find that she now prefers not to wear a jumper in cold weather. This finding does \emph{not} correspond to the structural model \eqref{ex:simple_dag_jumper}.

The following modification to this structure also does not yield the desired result:
\begin{align}
    \tikzfig{simple_dag_jumper_bd}
\end{align}
Under normal circumstances, the condition $\RV{Y}_i=\text{``feel hot''}$ always happened when Ella was wearing a jumper, but under the exercise condition it corresponds to no jumper wearing. Thus the conditional distribution of $\RV{X}_i$ given $\RV{T}_i$ and $\RV{Y}_i$ is not the same in the exercise condition to the normal condition.

If we introduce a new unobserved variable $\RV{Y}^X$ that represents Ella's beliefs about how she would feel if she were or were not to wear a jumper, then the following structural model can yield the desired result for both interventions (note: there are also other possibilities):
\begin{align}
    \tikzfig{not_simple_dag_jumper}\label{ex:not_simple_dag}
\end{align}

Here, we say that Ella's beliefs about her perceived body temperature are influenced by the outside temperature, whether or not she's wearing a jumper and her actual perceived body temperature. Here, once again, her perceived temperature is caused only by $\RV{T}_i$ and $\RV{X}_i$, which reproduces the fact that her perceived temperature depends on these variables in exactly the same way after intervention on her jumper wearing. However, the only cause of her jumper wearing is her beliefs about how comfortable she will feel with a jumper on, which can be influenced by interventions on her body temperature. Thus this model can also reproduce the assumed fact that when her body temperature is intervened on, she acts to maintain a comfortable equilibrium.

Concretely, taking $-1$ to be ``cold'', $0$ to be ``comfortable'', $1$ to be ``hot'', $0$ also for ``no jumper'' and $1$ for ``jumper'', we specify the model as
\begin{align}
    \RV{T}_i &\sim U(\{-1,0\})\\
    \RV{Y}^X_i &\leftarrowtriangle \begin{cases}
                                x\mapsto x-1 & \text{ if } \RV{Y}_i = \RV{X}_i -1\\
                                x\mapsto x & \text{ if } \RV{Y}_i = \RV{X}_i\\
                                x\mapsto 1 & \text{ if } \RV{Y}_i = \RV{X}_i+1
                            \end{cases}\\
    \RV{X}_i &\leftarrowtriangle \begin{cases}
                    1 &\text{ if }\RV{Y}^X_i=x\mapsto x-1\\
                    0 &\text{ otherwise}
    \end{cases}\label{eq:struct_x}\\
    \RV{Y}_i &\leftarrowtriangle \RV{T}_i + \RV{X}_i\label{eq:struct_y}
\end{align}
Here, a left arrow indicates a causal assignment. This has a unique solution for each value of $\RV{T}_i$. Instructing Ella to wear a jumper is modelled by replacing the right hand side of Eq. \eqref{eq:struct_x} with 1 and instructing Ella to exerciese is modelled by replacing the right hand side of  Eq. \eqref{eq:struct_y} with $\text{max}(1,\RV{T}_i + \RV{X}_i + 1)$. See \citet{bongers_theoretical_2016,forre_causal_2020} for a much more in-depth treatment of structural models with cycles, and see \cite{eberhardt_combining_2010,ghassami_causal_2020} for algorithms for discovering linear cyclic causal structures

Suppose, finally, we are interested in the results of a) tampering with the device Ella uses to determine the temperature each day and b) putting up a large structure to shade Ella's house. Both of these actions are likely to affect the reading $\RV{T}_i$, but otherwise have different consequences. It does not seem possible, therefore, that any single structural model limited to the variables $\RV{X}_i$, $\RV{Y}_i$ and $\RV{T}_i$ and functions thereof can represent the consequences of both of these actions.
\end{example}

In this example, different kinds of models are suitable for assessing the consequences of different sets of options, and a model accommodating all of the options considered is substantially more complex than a model accommodating only the request to wear a jumper. Maybe it is in principle possible to come up with a structural model that covers every set of options anyone might want to consider -- though it's far from obvious that this really is possible -- but it would be very surprising to us if there was any practical way to do so.

Practically, there seems to be some tension between the views that structural models should prescribe exactly what can be done and the view that they should be flexible enough to accommodate a decision maker's needs. For example, we can find in the literature a wide variety of types of intervention that can be considered alongside structural models: beyond the standard ``perfect interventions'' \citep[ch. ~1]{pearl_causality:_2009,hauser_characterization_2012} we have soft interventions \citep{correa_calculus_2020,eberhardt_interventions_2007}, general or fat-hand interventions \citep{eberhardt_interventions_2007,yang_characterizing_2018,glymour_evaluating_2017} and general interventions with unknown targets \citep{brouillard_differentiable_2020}. Offering such a variety of different kinds of interventions seems to acknowledge that decision makers need some flexibility to specify structural models that will suit their needs.

On the other hand, evaulation of causal discovery research almost invariably employs a measure that does not depend on any set of options under consideration at all -- as in the structural hamming distance, which counts the number of edges that differ between an inferred structure and a putative ``true'' structure -- or that assumes that options are given by perfect interventions with respect to the true structure, as in the structural intervention distance \citep{peters_structural_2015}. To offer just a few examples, \citep{brouillard_differentiable_2020,scherrer_learning_2022,toth_active_2022,forre_constraint-based_2018,chickering_optimal_2003,ng_graph_2019,zheng_dags_2018,spirtes_causation_1993} all evaluate their methods according to one or both of these kinds of measures.

Why do we have on the one hand an acknowledgement of the need to allow ``interventions'' in structural models to be flexible enough to accommodate a decision maker's needs, while on the other hand causal discovery methods are evaluated only according to a rigid interpretation of perfect interventions? One of the reasons for this, we presume, is that if we consider a very broad class of interventions -- say, general interventions with unknown targets -- then a structural model places no constraints on what consequences can be achieved by an arbitrary intervention. However, we want causal discovery to tell us something useful, and restricting our attention to what it tells us about perfect interventions ensures that at least it tells us something nontrivial. Exactly when this is also something useful, we don't know.

In short, there is some conceptual difficulty for structural models in determining how they should interface with a given decision maker's options. In this thesis, we ask (loosely speaking) the reverse of this question; rather than starting with a structural model and asking how we can accommodate a decision maker, we start with a decision maker and ask what sorts of models they might want to use.

\section{Outlining our approach}

As we have noted, decision making requires the consideration of a collection of hypotheticals -- specifically, a decision maker must consider the options she has available, and specifically wants to consider the consequences of choosing each of these options. Our approach is to suppose that our job is to help a decision maker evaluate their options. This is an idealisation; a lot of causal analysis doesn't end up directly making a decision, but it might a third party's decisions in ways the analyst may or may not anticipate. However, it's a different idealisation to the frameworks discussed above. Potential outcomes depend on certain counterfactual statements, structural models depend on ideal interventions and our approach depends on a decision maker's set of options. However, decision making is often at least indirectly the aim of causal inference, and decision problems require the decision maker to consider a set of options, while they do not require a decision to consider counterfactuals or interventions. 

This approach, which can be called a decision theoretic approach to casual inference, has previously been explored by \citet{heckerman_decision-theoretic_1995} as well as \citet{dawid_causal_2000,dawid_influence_2002,dawid_decision-theoretic_2012,dawid_decision-theoretic_2020}. Our approach builds on this earlier work, with a particular focus on the way assumptions of symmetry or regularity in sequential decision problems can lead to models that support non-trivial inferences from data.

To a decision maker, our approach offers the possibility of analysing their problem with fewer assumptions. The two approaches surveyed above require a decision maker who wishes to formally pose their problem to accept a set of options to consider, probability theory, an account of causal effects (whether structural or some other kind of counterfactual) and some ``bridging'' assumption that links their options to the causal effects. In contrast, we only require them to accept a set of options and probability theory. In either case, the decision maker will also have to make additional assumptions that reflect their best guesses about how to use their available data to make a good choice. A key question is whether this approach also allows for a reduction in the number of required assumptions to construct models that facilitate nontrivial inferences.

In answer to this question, we make the following key contributions:

A basic condition that corresponds approximately to unconfoundedness in standard causal analysis is the assumption of \emph{conditionally independent and identical responses}. In the spirit of De Finetti's analysis of conditionally independent and identically distributed sequences, we examine in Chapter \ref{ch:evaluating_decisions} the relationship between conditionally independent and identical responses corresponds to symmetries of a decision making model. This offers an alternative means of analysing assumptions of unconfoundedness in terms of the interchangeability of different datasets. For example, models of observational and experimental data with conditionally independent and identical responses must treat both datasets as interchangeable for the purpose of predicting the consequences of inputs in either dataset. We regard this as a mostly negative result; for most decision problems, conditionally independent and identical responses can usually be rejected (though it may be tenable in some approximate form).

The established approaches to causal inference have received a great deal cumulative development effort, and many specific applications have already been studied. In Chapter \ref{ch:other_causal_frameworks}, we show how to represent Causal Bayesian Networks and Potential Outcomes models as decision making models, and explain the additional assumptions needed to make this happen. Because we can perform this translation, this means that it's possible in principle to translate established application specific reasoning to our framework.

In Chapter \ref{ch:other_causal_frameworks} we prove Theorem \ref{th:det_obs_to_cons}. This theorem establisehes that, subject to the assumptions of \emph{precedent} (``whatever I can do, it's been done before'') and \emph{generic probabilstic relations between conditionals}, one can conclude from a conditional independence in observed data that there is a corresponding relationship that is invariant over the consequences of every option available to the decision maker. In the structural models literature generic probabilistic relations between conditionals have been used as an assumption to justify the inference of directed causal relationships. Given a directed causal relationship along with sufficient additional structural assumptions to support a means of causal identification and an identification between some class of interventions in the structural model with a decision maker's options, one can also deduce invariant relationships over the consequences of available options. Thus, while we don't show that it is a formal subset, the assumptions needed by Theorem \ref{th:det_obs_to_cons} do seem to be a subset of the assumptions needed to come to the same conclusion in the structural framework. We do argue in Section \ref{sec:precedent} that the assumption of precedent, at least, is built into the structural framework.

In Chapter \ref{ch:other_causal_frameworks} we also examine symmetries of experiments that support a judgement of independent and identical response functions. We begin with an assumption of \emph{individual-level response functions}, reminiscent of the potential outcomes model construction, except each ``potential outcomes'' is associated with a unique identifier. We then show that under the assumptions of permutability of identifiers and full control over the inputs, the input-output relations are given by conditionally independent and identical response functions. This result differs from standard accounts of causal identification in controlled experiments in that we connect the required assumptions to symmetries of experiments that might be performed.

Causal inference theories aren't the only theories that address decision making algorithms. These questions are also addressed by the fields of reinforcement learning, optimal control and statistical decision theory (and, no doubt, others besides). One distinction we can draw between these fields and the field of causal inference is that a key difficulty in causal inference problems is just how to relate consequences of actions to observations. In reinforcement learning, an \emph{environment} is typically assumed that represents the ``ground truth'' of consequences of actions, and the history of consequences can be used to infer which environment an agent is operating in \citep{barto_reinforcement_1998}. While optimal control is such a large field it's inappropriate to make any sweeping generalisations, basic versions of control theory assume a \emph{system model} is available that maps states and inputs to updated states and outputs \citep{ogata_discrete-time_1995}. Finally, in statistical decision theory the relevant notion of consequences of actions is given by the \emph{state} and the \emph{loss}, which like the environment in reinforcement learning, are basic elements of the problem \citep{wald_statistical_1950}.

There is substantial overlap between these different methods for relating observations to consequences. For example, \citet[Chap.~4]{lattimore_learning_2017} shows how the environment model in a reinforcement learning problem can be specified using a causal graphical model. In Chapter \ref{ch:2p_statmodels} we survey the literature on modelling decision problems, and show that many different decision theories posit that decision models, including \emph{evidential decision theory} and \emph{causal decision theory} share the same basic type. We also show how a particular class of decision making models -- a class that contains the models investigated in all subsequent chapters -- a classical statistical decision problem.

The mathematical basis for essentially all of the work in this thesis is probability theory, though we make use of nonstandard constructions within the theory to facilitate the representation of sets of options in the models we consider. We introduce the relevant theory in Chapter \ref{ch:tech_prereq}. As is common in causal inference, we often use a graphical language to represent probabilistic decision models. The language we use is somewhat different to the directed graphs that are standard in the area. We use a string diagram notation that can be related to ordinary directed graphs as in \citet{fong_causal_2013}, but also supports equality statements and a collection of transformations that can be applied to a diagram to yield an equivalent diagram.

% These assumptions all invoke either ``potential outcomes'' or ``causal structure''. Potential outcomes are, by definition, the union of a set of observed variables and a set of hypothetical claims. If one is inclined to accept that causal structures are ultimately underwritten by certain experimental procedures, then causal structures may be observed under some conditions (for example, if an experimental procedure is given for each $do()$-intervention, one can apply the method of \citep{eberhardt_almost_2008}) However, causal structures are often held to represent relationships that cannot be reasonably probed by an experiment (\citep[Chap. ~11]{pearl_causality:_2009},\citet{pearl_does_2018}), and in our view the general question of whether causal structures are observable or a mixture of observable properties and hypothetical claims is unresolved.

% Decision making must entertain some hypothetical statements. As has been pointed out above, a decision maker must consider multiple options and their likely consequences, and select from among the options. The proposition ``the consequences of $\alpha$ are better than the consequences of $\alpha'$'' is a hypothetical one: the decision maker ultimately chooses only one of $\alpha$ or $\alpha'$, and usually cannot ever verify that the chosen option really is better than the alternative. However, the hypotheticals that the assumptions above ask us to entertain do not on the face of it seem necessary to compare different options like this. The hypothetical part of potential outcomes doesn't refer to possible consequences of choices but to hypotheticals that have (in some sense) ``already happened'' by the time the data is reviewed. An identifiable causal structure may have many implications besides those that are necessary to make a choice in the decision problem at hand.

% Assumptions with mixtures of hypothetical and real implications are difficult to evaluate. On the one hand, the fact that they have real implications means that one cannot simply accept anything, while on the other hand the fact that some implications are hypothetical means that it can be difficult to form a clear picture of how the world actually differs depending on whether the assumption is true or false.

% % Without going into detail about any of these assumptions, we propose that none of them are presently suitable for ``mass production'' in the way that controlled experiments are. To take one example, even though \citet{lee_randomized_2008} writes of the regression discontinuity assumption ``it is shown below that causal inferences from RD designs can sometimes be as credible as those drawn from a randomized experiment.'', there is a lively debate about the applicability of this method just to the specific case of US House of Representatives elections \citep{snyder_detecting_2005,grimmer_are_nodate,caughey_elections_2017,cuesta_misunderstandings_2016}, which is the example \citet{lee_randomized_2008} used to illustrate his argument in the first place. The amount of careful case-by case attention required to assess these assumptions seems to make it harder to use them in large-scale data-driven decision making in a manner analogous to the results of controlled experiments or predictive models. 

% In brief:
% \begin{itemize}
%     \item Decision making algorithms require stronger assumptions than predictive algorithms
%     \item These assumptions are often particularly hard to evaluate
% \end{itemize} 

% \section{Exploring alternative foundations}

% The development of decision making algorithms is constrained by the assumptions we can make. Such problems are often addressed using theories of causal inference. Theories of causal inference allow us to state assumptions precisely, to understand them in various different ways so that we can better apply our informal means of evaluating them and, hopefully, to make sound choices about whether to accept an assumption or not. A number of such theories exist, and we can distinguish three traditions with wide adoption in different fields: causal Bayesian networks, potential outcomes and structural equation models. These are described in more detail in Chapter \ref{ch:other_causal_frameworks}.

% This thesis explores an alternative theory of algorithmic decision making. Our starting point is the observation in Section \ref{sec:assumptions} above that decision making problems call on a decision maker to compare a number of different options on the basis of their consequences. The conclusion is that, to construct a decision making model, where in classical statistics we might consider a probability distribution $\prob{P}$ defined on a sample space $(\Omega,\sigalg{F})$, we instead consider a function from the set of options $C$ to probability distributions on a sample space: $\prob{P}_\cdot:C\kto \Omega$. This extension of the basic statistical model is what underpins this entire thesis.

%  One might ask us all: if we already have three ways to theorise about this problem, why do we need a fourth? To a large extent, for questions like these, the proof is in the pudding -- do the alternative foundations offer insights that are hard to see from other points of view? However, we think that even before learning what alternatives can deliver, there are reasons to believe they are worth exploring.

% A broad reason for paying special attention to theoretical foundations in causal inference is outlined in Section \ref{sec:assumptions}: causal inference typically requires assumptions that are strong and particularly hard to evaluate. Alternative theoretical foundations can yield alternative perspectives on old assumptions and suggest new assumptions that accomplish desired goals. For these reasons, causal inference is a field that merits particular attention to foundations. In fact, the reason we have several different causal inference traditions is because different researchers have, at different times, attacked the problem of how to formulate causal models in different ways. This project has been fruitful -- it has facilitated the entire field of research into causal inference. In addition to the handful of identification results suggested above, some other insights enabled by these projects are:
% \begin{itemize}
%     \item With potential outcomes, it is possible to offer a precise formal statement of what it is that controlled experiments should achieve -- \emph{strong ignorability} \citep{rubin_causal_2005}
%     \item With potential outcomes we can precisely express the notion of ``the effect of a choice on the people whose behaviour changed on account of that choice'', formalised as the \emph{local average treatment effect} \citep{imbens_identification_1994}
%     \item[-] There is a close correspondence between the intuitive idea of directed causal relationships between variables and the factorisation of joint probability distributions, formalised as the \emph{causal Markov condition} (\citet[Chap. ~1]{pearl_causality:_2009},\citet{wright_method_1934})
%     \begin{itemize}
%         \item Particular credit is due Pearl for pointing out how common it is for researchers to smuggle these ``intuitive causal ideas'' into discussions with no accompanying theory of causation - see for example \citet[pg. 96]{pearl_causality:_2009}
%     \end{itemize}
%     \item[-] The field of conditional-independence based \emph{causal discovery} \citet{spirtes_causation_1993}
%     \item[-] The field of causal discovery based on the \emph{independence of cause and mechanism} \citet{scholkopf_causality_2022}
%     \item[-] Causal identification in semi-Markovian models \citet{shpitser_complete_2008}
% \end{itemize}

% Here, items marked with a dot are (in our view) difficult to formulate using causal graphical models alone, while items marked with a dash are (again, in our view) difficult to understand from the potential outcomes perspective.

% We have some specific reasons for wanting to go beyond these two frameworks. For the potential outcomes framework, the reason is quite simple: in order to make a decision, we must consider a map from some set of options $C$ to distributions over consequences, and the potential outcomes framework does not provide such a function at the base level. Once we incorporate such a function, it is not clear that there is any longer a need for potential outcomes at the axiomatic level (though it may still be possible to define variables in particular problems that play a similar role).

% The situation with causal graphical models -- that typically do have a notion of intervention -- requires some more explanation. Causal graphical models are characterised by sets of variables with directed relationships between them, and these directed relationships are underwritten by the idea of \emph{interventions}. Our suspicion is that this kind of model describes a special case of the kind of model we investigate -- which, recall, is defined by a function from a set of options $C$ to a set of probability distributions over a fixed sample space -- but is not appropriate to describe the general case. The issue centres on variables and interventions.

% First, it is important to point out that while interventions are suggestively named, they are not defined as ``things an experimenter actually goes and does''. Any identification of actions that can be taken with interventions is up to the analyst's discretion for a particular problem. 





% \section{Theories of causal inference}



% For causal questions that are controversial or difficult, it is tremendously advantageous to be able to address them transparently. Theories of causation enable this; given a theory of causation and a set of assumptions, if anyone claims that some conclusion follows it is publicly verifiable whether or not it actually does so. If the deduction is correct, then any remaining disagreement must be in the assumptions or in the theory. For people who are interested in understanding what is true, pinpointing disagreement can be enlightening. Someone could learn, for example, that there are assumptions that they find plausible that permit conclusions they did not initially believe. Alternatively, if a motivated conclusion follows only from implausible assumptions, hearing these assumptions explicitly might make the conclusion less attractive. 

% Theories of causation help us to answer causal questions, which means that before we have any theory, we already have causal questions we want to answer. If potential outcomes notation and causal graphical models had never been invented there would still be just as many people who want to the answer to questions something like ``does smoking causes cancer?'', even if on-one could say what exactly they meant by ``causes'' and even if many people actually want answers to slightly different questions. Theories exist to serve our need for transparent answers to causal questions.

% Potential outcomes and causal graphical models are prominent examples of ``practical theories'' of causation. I call them ``practical theories'' because most of the time we encounter them they are being used to answer ``practical'' questions like ``Does smoking cause cancer?'', or ``In general, when does data allow us to conclude that $X$ causes $Y$?'' It is less common to see the ``fundamental questions'' addressed, like ``Does the theory of causal graphical models offer an adequate account of what `cause' means?'', which is more often found in the field of philosophy. \citet{spirtes_causation_1993} explain their motivation to study what I call ``practical theories of causation'' as follows:

% \begin{quote}
% One approach to clarifying the notion of causation -- the philosophers’ approach ever since Plato -- is to try to define ``causation'' in other terms, to provide necessary and sufficient and noncircular conditions for one thing, or feature or event or circumstance, to cause another, the way one can define ``bachelor'' as ``unmarried adult male human.'' Another approach to the same problem -- the mathematician’s approach ever since Euclid -- is to provide axioms that use the notion of causation without defining it, and to investigate the necessary consequences of those assumptions. We have few fruitful examples of the first sort of clarification, but many of the second [...]
% \end{quote}

% I think what Spirtes, Glymour and Scheines (henceforth: SGS) mean here is that they \emph{define} a notion of causation -- because causal graphical models do define a notion of causation -- without interrogating whether it means the same thing as the word ``causation''. Incidentally, since publication of this paragraph, the notion of causation defined by causal graphical models has been subject to substantial interrogation by philosophers \citep{woodward_causation_2016}.

% I am sympathetic to the argument that it does not matter a great deal whether ``causal-graphical-models-causation'' and ``causation'' mean the same thing in everyday language. It is common for words to have somewhat different meanings when used by specialists to when they are used by laypeople, and this isn't because the specialists are ignorant or confused about their subject. However, I think it matters a lot which causal questions can be transparently answered by ``causal-graphical-models-causation'', and so I believe that the notions of causation adopted by practical theories do warrant scrutiny.

% I think one reason that SGS are keen to avoid dwelling on the definition of causation is that satisfactory definitions of causation are difficult. For example, causal graphical models depend on the notion of \emph{causal relationships} between variables. These may be defined as follows:

% \begin{quote}
% $\RV{X}_i$ is a \emph{cause} of $\RV{X}_j$ if there is an \emph{ideal intervention} on $\RV{X}_i$ that changes the value $\RV{X}_j$
% \end{quote}

% This definition is incomplete without a definition of ``ideal interventions''. Ideal interventions may be defined by their action in ``causally sufficient models'':
% \begin{itemize}
%     \item An $[\RV{X}_i,\RV{X}_j]$-ideal intervention is an operation whose result is determined by applying the \emph{do-calculus} to a \emph{causally sufficient} model $((\Omega,\mathcal{F},\prob{P}),\diagram{G},\vecRV{U})$
%     \item A model $((\Omega,\mathcal{F},\prob{P}),\diagram{G},\vecRV{U})$ is $[\RV{X}_i,\RV{X}_j]$-causally sufficient if $\RV{U}$ contains $\RV{X}_i$, $\RV{X}_j$ and ``all intervenable variables that \emph{cause}'' both $\RV{X}_i$ and $\RV{X}_j$ \footnote{Weaker conditions for causal sufficiency are possible, but they don't avoid circularity \citep{shpitser_complete_2008}}
% \end{itemize}

% While I don't offer a definition of the \emph{do-calculus} in this introduction, it can be rigorously defined, see for example \citet{pearl_causality:_2009}. The problem is that the definition of a \emph{causally sufficient} model itself invokes the word \emph{cause}, which is what the original definition was trying to address. Circularity is a recognised problem with interventional definitions of causation \citep{woodward_causation_2016}. In Section \ref{sec:cbns_without_d}, I further show models with ideal interventions generally have counterintuitive properties. The purpose of a theory of causation like causal graphical models is to support transparent reasoning about causal questions, and a circular definition that leads to counterintuitive conclusions undermines this purpose.

% As with Euclid's parallel postulate, I think it is reasonable to ask if the notion of ideal interventions and other causal definitions can be modified or avoided. Causal statistical decision theory (CSDT) is a theory of causation that is motivated by the problem of \emph{what is generally needed to answer causal questions} rather than \emph{what does ``causation'' mean?} Along similar lines to CSDT, \citet{dawid_decision-theoretic_2020} has observed that the problem of deciding how to act in light of data can be formalised without appeal to theories of causation. We develop this in substantial detail, showing how both \emph{interventional models} and \emph{counterfactual models} arise as special cases of CSDT.\todo{I want to revisit the claims about what I actually show, hopefully to add to it}

% A key feature of CSDT is what I call the \emph{option set}. This is the set of decisions, acts or counterfactual propositions under consideration in a given problem. A causal graphical model and a potential outcomes model will both implicitly define an option set as a result of their basic definitions of causation, but CSDT demands that this is done explicitly. I argue that this is a key strength of CSDT, on the basis of the following claims which I defend in the following chapters:

% \begin{itemize}
%     \item Causal questions are not well-posed without an option set in the same way a function is not well-defined without its domain
%     \item The option set need not correspond in any fixed manner to the set of observed variables
%     \item The nature of the option set can affect the difficulty of causal inference questions
% \end{itemize}


% \todo[inline]{I commented out an additional section about potential outcomes and closest world counterfactuals, which is a second example of ``opaque causal definitions''. I'm interested if any readers think it would be good to have a second example}


% Potential outcomes basic assumptions

% \begin{itemize}
%     \item Potential outcomes defines ``the treatment effect of $\RV{X}_i$ on $\RV{X}_j$'' in terms of the value of $\RV{X}_j$ under the \emph{counterfacutal supposition} that $\RV{X}_i$ had taken a different value
% \end{itemize}

% In fact, the notion of ``ideal intervention'' often seems to underpin potential outcomes models as well. Work in the potential outcomes theory often uses the idea of the value of $\RV{X}_j$ under a counterfactual supposition concerning $\RV{X}_i$ interchangeably with the idea the response of $\RV{X}_j$ to an idealised intervention on $\RV{X}_i$ \citep{morgan_counterfactuals_2014,rubin_causal_2005,richardson2013single}. \cite{lewis_causation_1986} offered a definition of the value $\RV{X}_j$ under counterfactual suppositions in terms of the value it would take in the world that was ``closest'' to the real world but in which the value of $\RV{X}_i$ was altered. There are many ways that we could use to measure how close one world is to another, many of which need not invoke any notion of ``ideal intervention'', but I have never encountered practical work on causal inference that was based on considerations of such similarity measures.


\section{Causally compatible variables}\label{sec:cc_vars}

