
%!TEX root = main.tex


\chapter{Introduction}\label{ch:introduction}

\todo[inline]{I'm thinking of classification problems as types of prediction problems, though it's not really what ``prediction'' means. The key feature is that there is a ground truth that is not known at the time the prediction or class is offered, but will become fully known at some later point.}

Data driven prediction problems and data driven decision making problems have a lot in common. The outcomes some people are interested in predicting are often outcomes other people want to influence. A forecaster might want to predict the winner of the next election, while a party strategist is interested in maximising their party's chance of victory. A product manager may be simultaneously interested in accurately inferring the sentiment expressed in reviews of their product, and in making product changes that increase the frequency that this sentiment is positive. Furthermore, data relevant to prediction is often relevant to decision making and vise-versa. Political parties often reason that electorates in which their predicted chance of victory is very low are not worth investing campaign resources in, and if a forecaster learns of evidence that one party had adopted a particularly effective election strategy they might want to revisit their prediction of the eventual election winner. The overlap is not perfect: comprehensive electorate level polls are probably more useful to the forecaster while small-scale controlled experiments are probably more useful to the strategist.

A key difference between prediction and influence problems is the ``multiplicity of futures'' that each problem asks us to consider. A forecaster wants to identify -- loosely speaking -- the single most likely outcome, while a strategist must consider multiple options and identify the likely outcomes associated with each of these. As a consequence of this difference, the forecaster receives more complete feedback about the quality of their forecast than the strategist. Unlike the forecaster, all but one of the options that the strategist considers are never realised, and the world never offers feedback on these alternative options.

This difference suggests that it might be easier to assess the reliability of a predictive algorithm than the reliability of a decision-making algorithm, and this is be borne out in practice. Validating a predictive algorithm using data split into training and holdout sets is a ubiquitous in machine learning. For many data generating processes, appropriately conducted validation is widely considered to be a reliable indicator of an algorithm's performance for sufficiently similar data generating processes. In contrast, the most well-known condition that is widely accepted to yield reliable decision making algorithms is that the data used to draw inferences comes from a well-conducted controlled experiment. Data that satisfies this is much rarer than data that standard machine learning validation approaches can be applied to. There are approaches to causal inference that don't depend on experimental data, but they depend on other assumptions which are similarly applicable to a limited fraction of datasets. Alternatives to controlled experiments often come with the additional headache of being difficult to assess for a given dataset.

Some of the most far-reaching recent development in algorithmic decision making have involved only the elementary theory of randomised experiments. Operational advances that enable controlled experiments to be conducted at large scales have driven substantial changes in the operations of many online businesses \citep{kohavi_surprising_2017}, and Abhijit Banerjee and Esther Duflo were recently awarded a Nobel prize in part for their pioneering role in the use of large numbers of randomised controlled trials (RCTs) to assess the effectiveness of different kinds of development interventions \citep{zhang_abdul_2014}. Some fields of science have also been significantly affected by ``negative progress'' in the science of assessing experimental results. For example, in psychology, strong evidence has emerged that experimental findings from this field provide weaker evidence to a reader of the findings about the consequences of the reader's actions than many had believed \citet{open_science_collaboration_estimating_2015,stroebe_what_2019}. In a similar time frame, standards for what constitutes a ``well-conducted'' experiment have risen across many fields \citep{nosek_preregistration_2018,liberati_prisma_2009}.

An individual who wants to use data to make better decisions can consider running a controlled experiment of their own. This may not be possible, and even if it is, there may be large amounts of apparently relevant data available that seems wasteful to ignore on the basis of its non-experimental provenance. This individual might therefore be motivated to make some additional assumptions which allow them to draw conclusions about how to act from non-experimental data.

Some examples of assumptions this person could consider are (where * indicates that causal conclusions follow only when the data displays some key features):
\begin{itemize}
    \item There is an input variable independent of the \emph{potential outcomes} conditional on some covariates \citep[Chap. ~12]{imbens_causal_2015}, \citep[Chaps ~2, 3, 5]{angrist_mastering_2014}
    \item [*] There is a variable closely correlated with the \emph{potential outcomes} for each observation \citep[Chap. ~21]{imbens_causal_2015}
    \item \emph{Potential outcomes} are partially observed and vary in a simple way with some variable
    \item [*] The set of observed and unobserved variables have a known \emph{causal structure}
    \item The set of observed variables is \emph{causally sufficient} and the \emph{causal structure} is \emph{faithful} to the conditional independence structure of the observed variables

    \item DID (assumption: initial level accounts for confounders)
    \item Causal sufficiency + faithfulness
    \item Detailed causal structure w/identifiable substructure
\end{itemize}

A common feature of all of these: they're kinda confusing



 - Predictive machine learning has advanced tremendously w/combination of experimentation + theory
 - Causal inference has stuck close to established theory
 - Deriving assumptions is an exact science, making assumptions is not
 - Assumptions are so important in causal inference, and coming at it from a slightly different (decision theoretic) direction can a) expand the space of available assumptions and b) situating common assumptions in a different context


 Further reasons for alternative foundations
 - CGM theory has been undeniably productive -- mediation, confounding, ``m-structures'', causal discovery
 - Some outstanding questions:
 - Not always obvious how to map pre-formal knowledge to interventions
 - Not all variables are ``causally compatible''



\section{Theories of causal inference}

Beginning in the 1930s, a number of associations between cigarette smoking and lung cancer were established: on a population level, lung cancer rates rose rapidly alongside the prevalence of cigarette smoking. Lung cancer patients were far more likely to have a smoking history than demographically similar individuals without cancer and smokers were around 40 times as likely as demographically similar non-smokers to go on to develop lung cancer. In laborotory experiments, cells which were introduced to tobacco smoke developed \emph{ciliastasis}, and mice exposed to cigarette smoke tars developed tumors\citep{proctor_history_2012}. Nevertheless, until the late 1950s, substantial controversy persisted over the question of whether the available data was sufficient to establish that smoking cigarettes \emph{caused} lung cancer. Cigarette manufacturers famously argued against any possible connection \citep{oreskes_merchants_2011} and Roland Fisher in particular argued that the available data was not enough to establish that smoking actually caused lung cancer \citep{fisher_cancer_1958}. Today, it is widely accepted that cigarettes do cause lung cancer, along with other serious conditions such as vascular disease and chronic respiratory disease \citep{world_health_organisation_tobacco_nodate,wiblin_why_2016}.

The question of a causal link between smoking and cancer is a very important one to many different people. Individuals who enjoy smoking (or think they might) may wish to avoid smoking if cigarettes pose a severe health risk, so they are interested in knowing whether or not it is so. Additionally, some may desire reassurance that their habit is not too risky, whether or not this is true. Potential and actual investors in cigarette manufacturers may see health concerns as a barrier to adoption, and also may personally want to avoid supporting products that harm many people. Like smokers, such people might have some interest in knowing the truth of this question, and a separate interest in hearing that cigarettes are not too risky, whether or not this is true. Governments and organisations with a responsibility for public health may see themselves as having responsibility to discourage smoking as much as possible if smoking is severely detrimental to health. The costs and benefits of poor decisions about smoking are large: 8 million annual deaths are attributed to cigarette-caused cancer and vascular disease in 2018\citep{world_health_organisation_tobacco_nodate} while  global cigarette sales were estimated at US\$711 billion in 2020 \citep{noauthor_cigarettes_nodate} (a figure which might be substantially larger if cigarettes were not widely believed to be harmful).

The question of whether or not cigarette smoking causes cancer illustrates two key facts about causal questions: First, having the right answers to causal questions is of tremendous importance to huge numbers of people. Second, confusion over causal questions can persist even when a great deal of data and facts relevant to the question are agreed upon.

Causal conclusions are often justified on the basis of ad-hoc reasoning. For example \citet{krittanawong_association_2020} state:

\begin{quote}
[...] the potential benefit of increased chocolate consumption, reducing coronary artery disease (CAD) risk is not known. We aimed to explore the association between chocolate consumption and CAD.
\end{quote}

It is not clear whether Krittanawong et. al. mean that a negative association between chocolate consumption and CAD implies that increased chocolate consumption is likely to reduce coronary artery disease (which is suggested by the word ``benefit''), or that an association may be relevant to the question and the reader should draw their own conclusions. Whether the implication is being suggested by Krittanawong et. al. or merely imputed by na\"ive readers, it is being drawn on an ad-hoc basis -- no argument for the implication can be found in this paper. As \citet{pearl_causality:_2009} has forcefully argued, additional assumptions are always required to answer causal questions from associational facts, and stating these assumptions explicitly allows those assumptions to be productively scrutinised.

For causal questions that are controversial or difficult, it is tremendously advantageous to be able to address them transparently. Theories of causation enable this; given a theory of causation and a set of assumptions, if anyone claims that some conclusion follows it is publicly verifiable whether or not it actually does so. If the deduction is correct, then any remaining disagreement must be in the assumptions or in the theory. For people who are interested in understanding what is true, pinpointing disagreement can be enlightening. Someone could learn, for example, that there are assumptions that they find plausible that permit conclusions they did not initially believe. Alternatively, if a motivated conclusion follows only from implausible assumptions, hearing these assumptions explicitly might make the conclusion less attractive. 

Theories of causation help us to answer causal questions, which means that before we have any theory, we already have causal questions we want to answer. If potential outcomes notation and causal graphical models had never been invented there would still be just as many people who want to the answer to questions something like ``does smoking causes cancer?'', even if on-one could say what exactly they meant by ``causes'' and even if many people actually want answers to slightly different questions. Theories exist to serve our need for transparent answers to causal questions.

Potential outcomes and causal graphical models are prominent examples of ``practical theories'' of causation. I call them ``practical theories'' because most of the time we encounter them they are being used to answer ``practical'' questions like ``Does smoking cause cancer?'', or ``In general, when does data allow us to conclude that $X$ causes $Y$?'' It is less common to see the ``fundamental questions'' addressed, like ``Does the theory of causal graphical models offer an adequate account of what `cause' means?'', which is more often found in the field of philosophy. \citet{spirtes_causation_1993} explain their motivation to study what I call ``practical theories of causation'' as follows:

\begin{quote}
One approach to clarifying the notion of causation -- the philosophers’ approach ever since Plato -- is to try to define ``causation'' in other terms, to provide necessary and sufficient and noncircular conditions for one thing, or feature or event or circumstance, to cause another, the way one can define ``bachelor'' as ``unmarried adult male human.'' Another approach to the same problem -- the mathematician’s approach ever since Euclid -- is to provide axioms that use the notion of causation without defining it, and to investigate the necessary consequences of those assumptions. We have few fruitful examples of the first sort of clarification, but many of the second [...]
\end{quote}

I think what Spirtes, Glymour and Scheines (henceforth: SGS) mean here is that they \emph{define} a notion of causation -- because causal graphical models do define a notion of causation -- without interrogating whether it means the same thing as the word ``causation''. Incidentally, since publication of this paragraph, the notion of causation defined by causal graphical models has been subject to substantial interrogation by philosophers \citep{woodward_causation_2016}.

I am sympathetic to the argument that it does not matter a great deal whether ``causal-graphical-models-causation'' and ``causation'' mean the same thing in everyday language. It is common for words to have somewhat different meanings when used by specialists to when they are used by laypeople, and this isn't because the specialists are ignorant or confused about their subject. However, I think it matters a lot which causal questions can be transparently answered by ``causal-graphical-models-causation'', and so I believe that the notions of causation adopted by practical theories do warrant scrutiny.

I think one reason that SGS are keen to avoid dwelling on the definition of causation is that satisfactory definitions of causation are difficult. For example, causal graphical models depend on the notion of \emph{causal relationships} between variables. These may be defined as follows:

\begin{quote}
$\RV{X}_i$ is a \emph{cause} of $\RV{X}_j$ if there is an \emph{ideal intervention} on $\RV{X}_i$ that changes the value $\RV{X}_j$
\end{quote}

This definition is incomplete without a definition of ``ideal interventions''. Ideal interventions may be defined by their action in ``causally sufficient models'':
\begin{itemize}
    \item An $[\RV{X}_i,\RV{X}_j]$-ideal intervention is an operation whose result is determined by applying the \emph{do-calculus} to a \emph{causally sufficient} model $((\Omega,\mathcal{F},\prob{P}),\diagram{G},\vecRV{U})$
    \item A model $((\Omega,\mathcal{F},\prob{P}),\diagram{G},\vecRV{U})$ is $[\RV{X}_i,\RV{X}_j]$-causally sufficient if $\RV{U}$ contains $\RV{X}_i$, $\RV{X}_j$ and ``all intervenable variables that \emph{cause}'' both $\RV{X}_i$ and $\RV{X}_j$ \footnote{Weaker conditions for causal sufficiency are possible, but they don't avoid circularity \citep{shpitser_complete_2008}}
\end{itemize}

While I don't offer a definition of the \emph{do-calculus} in this introduction, it can be rigorously defined, see for example \citet{pearl_causality:_2009}. The problem is that the definition of a \emph{causally sufficient} model itself invokes the word \emph{cause}, which is what the original definition was trying to address. Circularity is a recognised problem with interventional definitions of causation \citep{woodward_causation_2016}. In Section \ref{sec:cbns_without_d}, I further show models with ideal interventions generally have counterintuitive properties. The purpose of a theory of causation like causal graphical models is to support transparent reasoning about causal questions, and a circular definition that leads to counterintuitive conclusions undermines this purpose.

As with Euclid's parallel postulate, I think it is reasonable to ask if the notion of ideal interventions and other causal definitions can be modified or avoided. Causal statistical decision theory (CSDT) is a theory of causation that is motivated by the problem of \emph{what is generally needed to answer causal questions} rather than \emph{what does ``causation'' mean?} Along similar lines to CSDT, \citet{dawid_decision-theoretic_2020} has observed that the problem of deciding how to act in light of data can be formalised without appeal to theories of causation. We develop this in substantial detail, showing how both \emph{interventional models} and \emph{counterfactual models} arise as special cases of CSDT.\todo{I want to revisit the claims about what I actually show, hopefully to add to it}

A key feature of CSDT is what I call the \emph{option set}. This is the set of decisions, acts or counterfactual propositions under consideration in a given problem. A causal graphical model and a potential outcomes model will both implicitly define an option set as a result of their basic definitions of causation, but CSDT demands that this is done explicitly. I argue that this is a key strength of CSDT, on the basis of the following claims which I defend in the following chapters:

\begin{itemize}
    \item Causal questions are not well-posed without an option set in the same way a function is not well-defined without its domain
    \item The option set need not correspond in any fixed manner to the set of observed variables
    \item The nature of the option set can affect the difficulty of causal inference questions
\end{itemize}


\todo[inline]{I commented out an additional section about potential outcomes and closest world counterfactuals, which is a second example of ``opaque causal definitions''. I'm interested if any readers think it would be good to have a second example}


% Potential outcomes basic assumptions

% \begin{itemize}
%     \item Potential outcomes defines ``the treatment effect of $\RV{X}_i$ on $\RV{X}_j$'' in terms of the value of $\RV{X}_j$ under the \emph{counterfacutal supposition} that $\RV{X}_i$ had taken a different value
% \end{itemize}

% In fact, the notion of ``ideal intervention'' often seems to underpin potential outcomes models as well. Work in the potential outcomes theory often uses the idea of the value of $\RV{X}_j$ under a counterfactual supposition concerning $\RV{X}_i$ interchangeably with the idea the response of $\RV{X}_j$ to an idealised intervention on $\RV{X}_i$ \citep{morgan_counterfactuals_2014,rubin_causal_2005,richardson2013single}. \cite{lewis_causation_1986} offered a definition of the value $\RV{X}_j$ under counterfactual suppositions in terms of the value it would take in the world that was ``closest'' to the real world but in which the value of $\RV{X}_i$ was altered. There are many ways that we could use to measure how close one world is to another, many of which need not invoke any notion of ``ideal intervention'', but I have never encountered practical work on causal inference that was based on considerations of such similarity measures.


\section{Causally compatible variables}\label{sec:cc_vars}

