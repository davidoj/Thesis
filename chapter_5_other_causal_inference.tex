%!TEX root = main.tex

\chapter{Other causal modelling frameworks}\label{ch:other_causal_frameworks}

In this chapter, we examine the types of decision models that can be constructed from causal Bayesian networks and potential outcomes models. Neither of these popular approaches to causal inference yields a fully specified decision making model. Causal Bayesian networks are usually specified in a ``rolled up'' form, and certain judgements must be made about how this should be unrolled to a sequential model. Potential outcomes models, on the other hand, do not feature a native notion of ``choices'', and a judgement must be made about what the relevant collection of choices in a potential outcomes model is.

\section{What is a Causal Bayesian Network?}

\subsection{Definition of a Causal Bayesian Network}

We follow the definition of a Causal Bayesian Network on \citet[page ~23-24]{pearl_causality:_2009}. There are a couple of technical differences: we require that interventional models are a measurable map from interventions to probability distributions, and we assume that there is a common sample space for every interventional distribution. There are also some non-technical differences: the notation is adapted for compatibility with the rest of the work in this thesis, and we separate the definition into two parts for clarity (Definitions \ref{def:interventional} and \ref{def:CBN}).

An interventional model is a \emph{Causal Bayesian Network} with respect to a directed acyclic graph if it satisfies a number of compatibility requirements. The following definitions are standard, and reproduced here for convenience. The definitions here are terse, readers should refer to \citet[chap. ~1]{pearl_causality:_2009} for a more intuitive explanation.

\begin{definition}[Directed graph]
A directed graph $\graph{G}=(\graph{V},\graph{E})$ is a set of nodes $\graph{V}$ and edges, which are ordered pairs of nodes $\graph{E}\subset \graph{V}\times\graph{V}$. Nodes are written using the font $\node{V}$.
\end{definition}

The parents of a target node are all nodes with an edge ending at the target node.

\begin{definition}[Parents]
Given a directed graph $\graph{G}=(\graph{V},\graph{E})$ and $\node{V}_i\subset \graph{V}$, the parents of $\node{V}_i$ are $\PA(\node{V}_i):=\{\node{V}_j|(\node{V}_j,\node{V}_i)\in\graph{E}\}$.
\end{definition}

A path is a sequence of edges such that the $i$th edge and the $i+1$th edge share exactly one node.

\begin{definition}[Path]
Given a directed graph $\graph{G}=(\graph{V},\graph{E})$, a path is a sequence of edges $(E_i)_{i\in A}$ (where $A$ is either $[n]$ or $\mathbb{N}$) such that for any $i$, $E_i$ and $E_{i+1}$ share exactly one node.
\end{definition}

A directed path is a sequence of edges such that the end of the $i$th edge is the beginning of the $i+1$th edge.

\begin{definition}[Directed path]
Given a directed graph $\graph{G}=(\graph{V},\graph{E})$, a directed path is a sequence of edges $(E_i)_{i\in A}$ (where $A$ is either $[n]$ or $\mathbb{N}$) such that for any $i$, $E_i=(\node{V}_k,\node{V}_l)$ implies $E_{i+1}=(\node{V}_l,\node{V}_m)$ for some $\node{V}_m\in \graph{V}$.
\end{definition}

In an acyclic graph, directed paths never reach to the same node more than once.

\begin{definition}[Directed acyclic graph]
A directed graph $\graph{G}=(\graph{V},\graph{E})$ is acyclic if, for every path, each node appears at most once. Directed acyclic graph is abbreviated to ``DAG''.
\end{definition}

D-separation is a key property of directed acyclic graphs for defining causal Bayesian networks. It is defined with respect to undirected paths.

\begin{definition}[Blocked path]
Given a DAG $\graph{G}=(\graph{V},\graph{E})$, a path $p$ is blocked by $\node{V}_A\subset\graph{V}$ iff
\begin{enumerate}
    \item $(\node{V}_i,\node{V}_j)\in p$ and $(\node{V}_j,\node{V}_k)\in p$ while $\node{V}_j\in \node{V}_A$
    \item $(\node{V}_j,\node{V}_i)\in p$ and $(\node{V}_j,\node{V}_k)\in p$ while $\node{V}_j\in \node{V}_A$
    \item $(\node{V}_i,\node{V}_j)\in p$ and $(\node{V}_k,\node{V}_j)\in p$ while $\node{V}_j\cup \DE(\node{V}_j)\cap \node{V}_A=\emptyset$
\end{enumerate}
\end{definition}

\begin{definition}[d-separation]
Given a DAG $\graph{G}=(\graph{V},\graph{E})$, $\node{V}_A$ is $d$-separated from $\node{V}_B$ by $\node{V}_C$ (all subsets of \graph{V}) if $\node{V}_C$ blocks every path starting at $\node{V}_A$ and ending at $\node{V}_B$. This is written $\node{V}_A\perp_{\mathcal{G}} \node{V}_B | \mathbf{V}_C$.
\end{definition}

\begin{definition}[Variable-node association]
Given a graph $\graph{G}=(\graph{V},\graph{E})$ and a sequence of variables $(\RV{V}_i)_{i\in A}$, if $|A|=|\graph{V}|$ we can associate a variable with each node of the graph with an invertible map $m:\{\RV{V}_i|i\in A\}\to \graph{V}$. By convention, we give associated variables and nodes corresponding indices, and graphical operations are defined on variables through $m$, i.e. $\mathrm{Pa}(\RV{V}_i):=m(\mathrm{Pa}(m^{-1}(\RV{V}_i)))$.
\end{definition}

\begin{definition}[Compatibility]\label{def:compat}
Given a measurable space $(\Omega,\sigalg{F})$, a Markov kernel $\prob{P}_\cdot:C\kto \Omega$ and a sequence of variables $(\RV{V}_i)_{i\in A}$ with $\RV{V}_i:\Omega\to V_i$ and a DAG $\mathcal{G}$ with nodes $\{\node{V}_i\}_{i\in A}$ and the variable-node association $m:\RV{V}_i\mapsto \node{V}_i$, $\prob{P}_\cdot$ is compatible with $\mathcal{G}$ relative to $m$ if for all $I,J,K\subset A$, $\node{V}_I\perp_{\mathcal{G}} \node{V}_J | \mathbf{V}_K$ implies $\RV{V}_I\CI^e_{\prob{P}_\cdot} \RV{V}_J | (\RV{V}_K,\RV{C})$.
\end{definition}

The following definition is reproduced fron \citet{pearl_causality:_2009} with the differences mentioned: notation has been matched to ours, the interventional model is assumed to be measurable and the interventional distributions defined on a common sample space.

\begin{definition}[Interventional model]\label{def:interventional}
An interventional model is a tuple $(\prob{P}_\cdot,C,\Omega,(\RV{V}_i)_{i\in A})$ where $(\Omega,\sigalg{F})$ is a measurable space,  $\RV{V}:=(\RV{V}_i)_{i\in A}$ a sequence of variables with $\RV{V}_i:\Omega\to V_i$, $V_i$ denumerable, where $C$ the choice set
\begin{align}
    C:=\{\mathrm{do}_{\emptyset}\}\cup \{(\mathrm{do}_B,v_B)|B\subset A,v_B\in \mathrm{Range}(V_B)\}
\end{align}
That is, we take every subsequence $\RV{V}_B$ of $\RV{V}$ and add to $C$ every element of the range of $\RV{V}_B$, each labeled with the symbol $\mathrm{do}_B$.
\end{definition}

\begin{definition}[Causal Bayesian network]\label{def:CBN}
Given an interventional model $(\prob{P}_\cdot,C,\Omega,(\RV{V}_i)_{i\in A})$ and a directed acyclic graph $\graph{G}$ with nodes $\graph{V}$, $(\prob{P}_\cdot,C,\Omega,(\RV{V}_i)_{i\in A},\graph{G})$  is a \emph{causal Bayesian network} with respect the node-variable association $m:\node{V}_i\mapsto \RV{V}_i$ if:
\begin{enumerate}
    \item $\prob{P}_\cdot$ is compatible with $\graph{G}$ with respect to $m$
    \item $B\neq \emptyset \implies \prob{P}_{(\mathrm{do}_{B},v_B)}^{\RV{V}_B} = \delta_{v_B}$
    \item $\prob{P}_{(\mathrm{do}_{B},v_B)}^{\RV{V}_i|\mathrm{Pa}(\RV{V}_i)}\overset{\prob{P}_{(\mathrm{do}_{B},v_B)}}{\cong}\prob{P}_{\mathrm{do}_{\emptyset}}^{\RV{V}_i|\mathrm{Pa}(\RV{V}_i)}$ for all $i\not\in B$
\end{enumerate}
\end{definition}

We have a two comments to make about this definition. First, the sequence of variables $(\RV{V}_i)_{i\in A}$ cannot be arbitrary -- they must be ``causally compatible'' (see Section \ref{sec:cc_vars}. For example, the sequence $\RV{V}:=(\RV{X},\RV{X})$ for some $\RV{X}:\Omega\to X$ is a perfectly legitimate variable, but by condition (2) the intervention $\prob{P}_{\mathrm{do}_{\{1,2\}},(0,1)}$ is asked to assign probability 1 to the impossible event $(\RV{X}\yields 0)\cap (\RV{X}\yields 1)$. Second, condition (3) is subtly under-specified: $\prob{P}_{\mathrm{do}_{\emptyset}}^{\RV{V}_i|\mathrm{Pa}(\RV{V}_i)}$ is not necessariliy $\prob{P}_{(\mathrm{do}_{B},v_B)}$-almost surely unique. We could therefore either require condition (3) to hold for some version of $\prob{P}_{\mathrm{do}_{\emptyset}}^{\RV{V}_i|\mathrm{Pa}(\RV{V}_i)}$ for each intervention in $C$, or we could require it for a single version of $\prob{P}_{\mathrm{do}_{\emptyset}}^{\RV{V}_i|\mathrm{Pa}(\RV{V}_i)}$ uniformly over all interventions in $C$.

For continuously valued variables, the ability to pick a version of the conditional probability for each intervention leads to undesirable results. Suppose $\node{V}_i$ is a parent of $\node{V}_j$, and the associated variable $\RV{V}_i$ is continuously valued and $\prob{P}_{\mathrm{do}_{\emptyset}}^{\RV{V}_i}(\{v_i\})=0$ for all singletons $v_i\in V_i$. Then for every intervention $\mathrm{do}_{\{i\}}(v_i)$, we can choose a version of $\prob{P}_{\mathrm{do}_{\emptyset}}^{\RV{V}_j|\RV{V}_i}$ that takes an arbitrary value at the point $v_i$ (because this point has measure 0), so property (3) is satisfied trivially. Because of this, we will henceforth suppose that variables are discrete.

\subsection{Unrolling a causal Bayesian network}\label{sec:unrolling}

Given a probability space $(\prob{P},\Omega,\sigalg{F})$ and an independent and identically distributed (IID) sequence $\RV{X}:=(\RV{X}_i)_{i\in [n]}$, it is common to ``roll up'' the joint distribution $\prob{P}^{\RV{X}}\in\Delta(X^n)$ to a single representative distribution $\prob{P}^{\RV{X}_0}\in \Delta(X)$ and say something like ``the $\RV{X}_i$ are IID according to $\prob{P}^{\RV{X}_0}$''. Because of the IID assumption, the full joint distribution $\prob{P}^{\RV{X}}\in\Delta(X^n)$ can be unambiguously reconstructed from a statement like this.

A causal Bayesian network is similarly a rolled-up representation of a model of some sequence of variables. Unlike an IID sequence, it isn't completely unambiguous how to unroll it. We propose the following method: first, posit a sequence of variables $\RV{V}:= (\RV{V}_ij)_{i\in A,j\in [n]}$, and extend the set $C$ to be the set of sequences of interventions 
\begin{align}
\{(\mathrm{do}_{B_j j}(v_B))_{j\in [n]}|\forall j: B_j \subset A,v_{B_j}\in \mathrm{Range}(\RV{V}_{B_j})\}
\end{align}
i.e. $C$ now consists of all sequences of separate interventions to each subsequence of variables $\RV{V}_{Aj}:=(\RV{V}_{ij})_{i\in A}$, understood to refer to variables arising from a particular iteration of the decision procedure. 

Given a graph $\graph{G}=(\graph{V},\graph{E})$, we now have a collection of variable-node association maps $m_j:\{\RV{V}_{ij}|i\in A\}\to \graph{V}$ such that $m_j(\RV{V}_{ij})=\node{V}_i$.

We now need to specify how variables in an unrolled causal Bayesian network are distributed, given some sequence of interventions. By analogy with the original case of IID variables, we conclude that the $\RV{V}_{Aj}:=(\RV{V}_{ij})_{i\in A}$ are mutually independent given any particular sequence of interventions. Furthermore, Definition \ref{def:CBN} constrains the distribution of each variable given a particular sequence of interventions from $C$. For a sequence of interventions $\alpha\in C$, let $\pi_j(\alpha)$ be the $j$th intervention in the sequence. We might posit the following analogue of condition (3): 
\begin{itemize}
    \item [3'] $\pi_j(\alpha)=(\mathrm{do}_{B_j},v_{B_j})$ implies $\prob{P}_{\alpha}^{\RV{V}_{ij}|\mathrm{Pa}(\RV{V}_{ij})}\overset{\prob{P}_{\alpha}}{\cong}\prob{P}_{\mathrm{do}_{\emptyset}^n}^{\RV{V}_{i1}|\mathrm{Pa}(\RV{V}_{i1})}$ for all $i\not\in B$
\end{itemize}
Where $\mathrm{do}_{\emptyset}^n$ is a sequence of $n$ $\mathrm{do}_{\emptyset}$ interventions. This is a combination of an assumption that variables in the sequence are conditionally identically distributed given appropriate interventions and condition (3) from Definition \ref{def:CBN}. However, it's not quite satisfactory. Take $B:=\mathrm{Pa}(\RV{V}_{i1})$, and suppose $\prob{P}_{\mathrm{do}_{\emptyset}^n}^{\RV{V}_{B1}}(\{x\})=0$. Then (3') would be satisfied by a model for which
\begin{align}
    \prob{P}_{(\mathrm{do}_{B_1},x,\mathrm{do}_{B_2},x)}^{\RV{V}_{i1}|\RV{B}}(U|x) &= \delta_0(U)\\
    \prob{P}_{(\mathrm{do}_{B_1},x,\mathrm{do}_{B_2},x)}^{\RV{V}_{i2}|\RV{B}}(U|x) &= \delta_1(U)
\end{align}
that is, if the empty intervention is unsupported over some element of the range of a variable, then (3') allows models that assign different consequences to repetitions of the same intervention on this variable, if those intervention forces the variable into the region that originally had no support.

We propose instead the restricted assumption of identical response functions: for any pair $\RV{V}_{ij}$ and $\RV{V}_{ik}$, unless $i$ is intervened on by $\pi_j(\alpha)$ and not intervened on by $\pi_{k}(\alpha)$, then then the conditional probability of $\RV{V}_{ij}$ given its parents is equal to the conditional probability of $\RV{V}_{ik}$ given its parents. This is condition [3*].

In order to be able to ``roll up'' a sequence of interventions, we also require that the response to the $j$th intervention does not depend on any of the interventions other than the $j$th. If this were not the case, then even if the restricted assumption of identical response functions were satisfied, different sequences of interventions would ``roll up'' to different interventional models. Condition 4* is the formalisation of this requirement. In Chapter \ref{ch:evaluating_decisions}, we showed that conditionally independent and identical response functions allow for the estimation of conditional probabilities from previous data, but noted that this did not necessarily imply that estimating conditional probabilities under a particular fixed choice was sufficient for decision making. Condition 4* is the assumption that, once we have the interventional conditional probabilities for any sequence of interventions, nothing else is needed.

Condition 5* is the requirement that observations are mutually independent.

\begin{definition}[Unrolled causal Bayesian network]\label{def:unr_CBN}
Given an interventional model $(\prob{P}_\cdot,C,\Omega,(\RV{V}_{ij})_{i\in A,j\in [n]})$ and a directed acyclic graph $\graph{G}$ with nodes $\graph{V}$, $(\prob{P}_\cdot,C,\Omega,(\RV{V}_i)_{i\in A},\graph{G})$  is an \emph{unrolled causal Bayesian network} with respect the node-variable association maps $m_j:\node{V}_{ij}\mapsto \RV{V}_i$ if, for all $j,k\in [n]$:
\begin{itemize}
    \item [1*] $\prob{P}_\cdot^{\RV{V}_{Aj}}$ is compatible with $\graph{G}$ with respect to $m_j$ for all $j\in [n]$
    \item [2*] $\pi_j(\alpha) = (\mathrm{do}_{B_j},v_{B_j})$ and $B_j\neq \emptyset$ implies $\prob{P}_{(\mathrm{do}_{B_j},v_{B_j})}^{\RV{V}_{Bj}} = \delta_{v_{B_j}}$
    \item [3*] If $\pi_j(\alpha) = (\mathrm{do}_{B_j},v_{B_j})$, $\pi_k(\alpha) = (\mathrm{do}_{B_k},v_{B_k})$ and $i\not \in B_j\cup B_k$ then $\prob{P}_{\alpha}^{\RV{V}_{ij}|\mathrm{Pa}(\RV{V}_{ij})}\overset{\prob{P}_{\alpha}}{\cong}\prob{P}_{\mathrm{do}_{\emptyset}}^{\RV{V}_{ik}|\mathrm{Pa}(\RV{V}_{ik})}$
    \item [4*] $\pi_j(\alpha)=\pi_j(\alpha')$ implies $\prob{P}_\alpha^{\RV{V}_{Aj}}= \prob{P}_{\alpha'}^{\RV{V_{Aj}}}$
    \item [5*] $\RV{V}_{Aj}\CI^e_{\prob{P}_{C}} \RV{V}_{A [n]\setminus\{j\}} |\RV{C}$
\end{itemize}
\end{definition}

\subsection{Uncertainty in an unrolled causal Bayesian network}

Condition 3* of Definition \ref{def:unr_CBN} establishes that, depending on the precise sequence of interventions chosen, certain conditionals are identical. In Chapter \ref{ch:evaluating_decisions}, we considered conditionals (or ``response functions'') that were identical \emph{conditional on some hypothesis} $\RV{H}$. Problems addressed with causal Bayesian networks are also usually problems where these conditional distributions are initially unknown (and, in some cases, they may remain unknown even after examining an arbitrarily large amount of data). We propose to use the same method to represent uncertainty over conditional distributions in a causal Bayesian network. In particular, an \emph{uncertain} unrolled causal Bayesian network is an interventional model $(\prob{P}_\cdot,C,\Omega,(\RV{V}_{ij})_{i\in A,j\in [n]})$ with some variable $\RV{H}$ such that, conditional on any $h\in H$, the result is an unrolled causal Bayesian network.

\begin{definition}[Uncertain unrolled causal Bayesian network]\label{def:unc_unr_cbn}
Given an interventional model $(\prob{P}_\cdot,C,\Omega,(\RV{V}_{ij})_{i\in A,j\in [n]})$ and a directed acyclic graph $\graph{G}$, $(\prob{P}_\cdot,C,\Omega,(\RV{V}_{ij})_{i\in A,j\in [n]},\RV{H},\graph{G})$ is an \emph{uncertain unrolled causal Bayesian network} with respect to some variable $\RV{H}:\Omega\to H$ if for each $h\in H$, defining $\prob{P}_{\cdot,h}:= \alpha\mapsto \prob{P}_\alpha^{\mathrm{id}_{\Omega}|\RV{H}}(\cdot|h)$, $(\prob{P}_{\cdot,h},C,\Omega,(\RV{V}_i)_{i\in A},\graph{G})$ is an unrolled causal Bayesian network.
\end{definition}

Recalling the discussion in Section \ref{sec:probability_set_models}, Definition \ref{def:unc_unr_cbn} associates each intervention with a unique probability distribution. One could imagine therefore callling uncertain unrolled causal Bayesian networks ``Bayesian causal Bayesian networks'', although this is obviously a bit of a confusing name.

An uncertain unrolled causal Bayesian network is \emph{almost} a conditionally independent and identical response function model. Due to 3*, such a model features conditionally independent and identical response functions wherever $\alpha$ consists of a sequence of interventions none of which target $i$. This leads us to the key result of this section: considering a subset of the interventions in $C$, an uncertain unrolled causal Bayesian network is IO contractible (with respect to some parameters) by application of Theorem \ref{th:ciid_rep_kernel}.

\begin{theorem}[IO contractibility of CBNs]\label{th:causal_contractibility_cbn}
Given an uncertain unrolled causal Bayesian network $(\prob{P}_\cdot,C,\Omega,(\RV{V}_{ij})_{i\in A,j\in [n]},\RV{H},\graph{G})$, take $C'\subset C$ to be sequences of interventions that, for some $j\in [n]$, do not target a particular $\RV{V}_{ij}$ for any $i\in A$. Then $\RV{V}_{i[n]}\CI^e_{\prob{P}_{C'}}\RV{C}|(\RV{H},\mathrm{Pa}(\RV{V}_{i [n]}))$ and $\prob{P}_C^{\RV{V}_{i[n]}|\RV{H}\mathrm{Pa}(\RV{V}_{i [n]})}$ is IO contractible over $\RV{H}$. 
\end{theorem}

\begin{proof}
First we will prove $\RV{V}_{i[n]}\CI^e_{\prob{P}_{C'}}\RV{C}|(\RV{H},\mathrm{Pa}(\RV{V}_{i [n]}))$. This is equivalent to the claim that $\prob{P}_\alpha^{\RV{V}_{i[n]}|\RV{H}\mathrm{Pa}(\RV{V}_{i[n]})}$ is the same as $\prob{P}_{\alpha'}^{\RV{V}_{i[n]}|\RV{H}\mathrm{Pa}(\RV{V}_{i[n]})}$ for any $\alpha,\alpha'$. By assumption 5* of Definition \ref{def:unr_CBN}, for each $h\in H$
\begin{align}
    \RV{V}_{Aj}\CI^e_{\prob{P}_{\alpha,h}} \RV{V}_{A [n]\setminus\{j\}} |\RV{C}
\end{align}
which implies
\begin{align}
    &\RV{V}_{Aj}\CI^e_{\prob{P}_{\alpha}} \RV{V}_{A [n]\setminus\{j\}} |(\RV{C},\RV{H})\\
    &\implies \RV{V}_{ij}\CI^e_{\prob{P}_{\alpha}} \RV{V}_{A [n]\setminus\{j\}} |(\RV{H},\mathrm{Pa}(\RV{V}_{i[n]}),\RV{C})\label{eq:mutual_conditional_independence}
\end{align}
thus it is sufficient to show that, for any $\alpha,\alpha'\in C'$ and $j\in[n]$
\begin{align}
    \prob{P}_\alpha^{\RV{V}_{ij}|\RV{H}\mathrm{Pa}(\RV{V}_{ij})} &= \prob{P}_{\alpha'}^{\RV{V}_{ij}|\RV{H}\mathrm{Pa}(\RV{V}_{ij})}
\end{align}
By assumption, if $\pi_j(\alpha) =: (\mathrm{do}_{B_j},v_{B_j})$ and $\pi_j(\alpha') =: (\mathrm{do}_{B_j'},v_{B_j'}')$, $i\not\in B_j\cup B_j'$, and similarly replacing the $j$s with $k$s for any $k\in [n]$. Define $\alpha''$ such that, for some $k$, $\pi_k(\alpha'')=\pi_k(\alpha')$ and $\pi_j(\alpha'')=\pi_j(\alpha)$. Then by 4*, for all $h\in H$
\begin{align}
    \prob{P}_{\alpha}^{\RV{V}_{ij}|\RV{H}\mathrm{Pa}(\RV{V}_{ij})}(A|h,y) &= \prob{P}_{\alpha''}^{\RV{V}_{ij}|\RV{H}\mathrm{Pa}(\RV{V}_{ij})}(A|h,y)\\
    &= \prob{P}_{\alpha''}^{\RV{V}_{ik}|\RV{H}\mathrm{Pa}(\RV{V}_{ik})}(A|h,y)&\text{by 3*}\\
    &= \prob{P}_{\alpha'}^{\RV{V}_{ik}|\RV{H}\mathrm{Pa}(\RV{V}_{ik})}(A|h,y)&\text{by 4*}\\
    &= \prob{P}_{\alpha'}^{\RV{V}_{ij}|\RV{H}\mathrm{Pa}(\RV{V}_{ij})}(A|h,y)&\text{by 3*}\\
    \implies \prob{P}_\alpha^{\RV{V}_{ij}|\RV{H}\mathrm{Pa}(\RV{V}_{ij})} &= \prob{P}_{\alpha'}^{\RV{V}_{ij}|\RV{H}\mathrm{Pa}(\RV{V}_{ij})}
\end{align}

Next, IO contractibility of $\prob{P}_C^{\RV{V}_{i[n]}|\RV{H}\mathrm{Pa}(\RV{V}_{i [n]})}$ over $\RV{H}$. By Eq. \eqref{eq:mutual_conditional_independence}
\begin{align}
    \RV{V}_{ij}\CI^e_{\prob{P}_{\alpha}} (\RV{V}_{i [1,j)},\mathrm{Pa}(\RV{V}_{i [1,j)}) )|(\RV{H},\mathrm{Pa}(\RV{V}_{i[n]}),\RV{C})
\end{align}
furthermore, by 3* and the assumption that no intervention $\alpha\in C'$ targets $\RV{V}_{ij}$ for any $j$, for any $\alpha\in C'$
\begin{align}
    \prob{P}_{\alpha}^{\RV{V}_{ij}|\RV{H}\mathrm{Pa}(\RV{V}_{ij})}(A|h,y) &= \prob{P}_{\alpha}^{\RV{V}_{ik}|\RV{H}\mathrm{Pa}(\RV{V}_{ik})}(A|h,y) 
\end{align}
thus $\prob{P}_C$ has independent and identical response functions conditional on $\RV{H}$ and by Theorem \ref{th:ciid_rep_kernel}, $\prob{P}_C^{\RV{V}_{i[n]}|\RV{H}\mathrm{Pa}(\RV{V}_{i [n]})}$ is IO contractible over $\RV{H}$.
\end{proof}

\subsection[Probabilistic Graphical Models]{Probabilistic Graphical Models}

\citet{lattimore_replacing_2019,lattimore_causal_2019} have previously published work in which they demonstrated how to ``unroll'' causal Bayesian networks into what they call ``Probabilistic Graphical Models''. Their work goes into more detail than this thesis on how identifiability results transfer from causal Bayesian networks to their unrolled forms.

Illustrating the fact that some choices must be made in order to work out what kind of rolled-up model corresponds to a given causal Bayesian network, Rohde and Lattimore consider an unrolling where the empty intervention is always made in conjunction with a particular intervention on a particular node in the DAG. Where we explicitly write down a model of an entire sequence of observations, Probabilistic Graphical Models can be assumed to represent a sequence of an arbitrary number of empty interventions in conjunction with an arbitrary number of particular interventions on particular nodes in the DAG. Such compact representations are of course very useful when the extra details are redundant. The difference underscores the approach taken to causal modelling in this thesis -- we proceed cautiously, aiming to explicitly represent all relevant assumptions that go into building a particular type of causal model, and approach that can lead to relatively verbose model definitions and representations.

Precisely, a probabilistic graphical model is a map $\prob{P}_\cdot$ from the set of single-node interventions $C$ to probability distributions $\prob{P}_\alpha$ defined on $(\Omega,\sigalg{F})$. A probabilistic graphical model is typically associated with a causal Bayesian network $(\prob{Q}_\cdot,C,\Omega',(\RV{V}_i)_{i\in A},\graph{G})$ where, for each $\RV{V}_i:\Omega\to V_i$ in the original causal Bayesian network, two variables $\RV{V}_i$ and $\RV{V}_i^*$ are defined on $(\Omega,\sigalg{F})$. The probabilistic graphical model also adds a ``parameter'' $\RV{W}_i$ for each variable pair $(\RV{V}_i,\RV{V}_i^*)$ such that, taking $C'$ to be interventions not targeting $\RV{V}_i^*$, for any $\alpha\in C'$, $\prob{P}_\alpha^{\RV{V}_i|\RV{W}_i\mathrm{Pa}(\RV{V}_i)}=\prob{P}_\alpha^{\RV{V}_i^*|\RV{W}_i\mathrm{Pa}(\RV{V}^*_i)}$ and $\RV{V}_i\CI^e_{\prob{P}_{C'}} (\RV{V}^*_A,\RV{C})|(\RV{W}_i)$ (where parents are assessed relative to the graph $\graph{G}$). This should look familiar - it is specifying, in a very similar manner to Theorem \ref{th:causal_contractibility_cbn}, that a Probabilistic Graphical Model constructed from a causal Bayesian network $(\prob{Q}_\cdot,C,\Omega',(\RV{V}_i)_{i\in A},\graph{G})$ features independent and identical response functions for each node given its parents conditional on the parameter $\RV{W}_i$.

A depiction of probabilistic graphical models and uncertain unrolled causal Bayesian networks using string diagrams gives some intuition regarding the structure of these different types of models, as well as some of the ``off-page'' assumptions of ordinary causal Bayesian networks.

Here is the original graph $\graph{G}$ associated with $(\prob{Q}_\cdot,C,\Omega',(\RV{V}_i)_{i\in A},\graph{G})$:

\begin{align}
    \tikzfig{cbn_example_cgm}
\end{align}

Here is the probabilistic graphical model associated with the intervention $(\mathrm{do}_2,v_2)$

\begin{align}
    \prob{P}_\alpha^{\RV{V}\RV{V}^*} &=\\
    \tikzfig{cgm}
\end{align}

and here is the uncertain unrolled CBN associated with the restricted set of interventions $C'$ that consists of, for each element of the sequence, either the empty intervention or some intervention targeting $\RV{V}_2$

\begin{align}
    \prob{P}_\alpha^{\RV{V}_{[1,3][n]}} &= 
    \tikzfig{cbn_unrolled_example}
\end{align}

where 

\begin{align}
    \prob{P}_\alpha^{\RV{V}_{2i}|\RV{V}_{1i}\RV{H}} &= \begin{cases}
        \delta_{v} & \pi_i(\alpha)=(\mathrm{do}_2,v)\\
        \prob{P}_{mathrm{do}_{\emptyset}}^{\RV{V}_{2i}|\RV{V}_{1i}\RV{H}} & \text{otherwise}
    \end{cases}
\end{align}

\subsection{Hidden confounders and precedents}\label{sec:precedent}

One of the particularly interesting questions in causal inference is how to infer consequences of actions from observational data. A particular question of interest for problems of this type is the question of what kinds of inductive assumptions are applicable to this problem.

A common assumption in the causal Bayesian network tradition applicable to this kind of problem is the assumption of \emph{hidden confounders}. Suppose we have an uncertain unrolled causal Bayesian network $(\prob{P}_\cdot,C,\Omega,(\RV{V}_{ij})_{i\in [3],j\in [n]},\RV{H},\graph{G})$ where the graph $\RV{G}$ is as follows:
\begin{align}
    \tikzfig{cbn_example_cgm}
\end{align}
and we consider the subset $C'\subset C$ of interventions that are either empty or target $\RV{V}_2$ only. We note that Theorem \ref{th:causal_contractibility_cbn} implies that $\prob{P}_C^{\RV{V}_{3[n]}|\RV{H}\RV{V}_{1[n]}\RV{V}_{2[n]}}$ is IO contractible, but not $\prob{P}_C^{\RV{V}_{3[n]}|\RV{H}\RV{V}_{2[n]}}$. We can specify somewhat informally that $\RV{V}_{1[n]}$ is not observed -- that is, it is not associated with a measurement procedure. 

The assumption of a hidden confounder often implies that, for any choice, the consequences of ``interventions'' have been anticipated by \emph{some} fraction of the observations. Specifically, the IO contractibility of $\prob{P}_C^{\RV{V}_{3[n]}|\RV{H}\RV{V}_{1[n]}\RV{V}_{2[n]}}$ implies that $\prob{P}_C^{\RV{V}_{3[n]}|\RV{H}\RV{V}_{2[n]}}$ is unchanged by swaps that leave $\RV{V}_{1[n]}$ unchanged.

\begin{theorem}\label{th:condit_exchange}
Given $(\prob{P}_\cdot,C,\Omega,(\RV{V}_{ij})_{i\in [3],j\in [n]},\RV{H},\graph{G})$ with $\prob{P}_C^{\RV{V}_{3[n]}|\RV{H}\RV{V}_{1[n]}\RV{V}_{2[n]}}$ IO contractible over $\RV{H}$, $V_i$ discrete for all $i\in [3]$ and $\prob{P}_C^{\RV{V}_{[2]i}|\RV{H}}(v_1,v_2|h)>0$ for all $v_1,v_2,h$, let $\RV{Q}:\Omega\to [n]^n$ be a permutation of $[n]$ such that $\RV{V}_{1[n]}=\RV{V}_{1\RV{Q}([n])}$. Then
\begin{align}
    \prob{P}_C^{\RV{V}_{3[n]}|\RV{H}\RV{V}_{2[n]}} &= \prob{P}_C^{\RV{V}_{3\RV{Q}([n])}|\RV{H}\RV{V}_{2\RV{Q}([n])}}
\end{align}
\end{theorem}

\begin{proof}
By IO contractibility of $\prob{P}_C^{\RV{V}_{3[n]}|\RV{H}\RV{V}_{1[n]}\RV{V}_{2[n]}}$ over $\RV{H}$
\begin{align}
    \prob{P}_C^{\RV{V}_{3[n]}|\RV{H}\RV{V}_{1[n]}\RV{V}_{2[n]}} &= \prob{P}_C^{\RV{V}_{3\RV{Q}([n])}|\RV{H}\RV{V}_{1\RV{Q}([n])}\RV{V}_{2\RV{Q}([n])}}\\
    &= \prob{P}_C^{\RV{V}_{3\RV{Q}([n])}|\RV{H}\RV{V}_{1[n]}\RV{V}_{2\RV{Q}([n])}}
\end{align}
Thus
\begin{align}
    &\prob{P}_C^{\RV{V}_{3[n]}|\RV{H}\RV{V}_{2[n]}}\\
     &= \tikzfig{contractible_over_hidden}\\
     &= \tikzfig{contractible_over_hidden_2}\\
     &= \tikzfig{contractible_over_hidden_3}\\
     &= \prob{P}_C^{\RV{V}_{3\RV{Q}([n])}|\RV{H}\RV{V}_{2\RV{Q}([n])}}
\end{align}
\end{proof}

Theorem \ref{th:condit_exchange} says that IO contractibility of $\prob{P}_C^{\RV{V}_{3[n]}|\RV{H}\RV{V}_{1[n]}\RV{V}_{2[n]}}$ implies the ``exchange commutativity'' of $\prob{P}_C^{\RV{V}_{3[n]}|\RV{H}\RV{V}_{2[n]}}$ with respect to the unknown permutation $\RV{Q}$ of the indices of $[n]$ -- in fact, for all such unknown permutations that preserve $\RV{V}_{1[n]}$. That is, $\RV{V}_{1[n]}$ determines some subset of indices that are, in some sense, interchangeable. If this subset includes indices that correspond to passive observations and indices that correspond to consequences of actions (indices associated with $\mathrm{do}_\emptyset$ and $\mathrm{do}_A$ for nonempty $A$ respectively), then we can see this as the assumption that the consequences of actions were preempted by some known set of the passive observations.

One could consider making \emph{only} a ``preemption'' assumption. Such an assumption could be stated informally in terms like ``however I ultimately act, my actions will have been preempted by some of the observations that are already available''. Such an assumption -- by Theorem \ref{th:condit_exchange} -- seems to be implicit in many causal Bayesian network based causal models. In an informal form, it long predates causal modelling; Ecclesiastes 1:9 reads
\begin{quote}
Everything that happens has happened before; nothing is new, nothing under the sun.\citep{noauthor_holy_1995}
\end{quote}

It's not obvious how to formalise this general idea of preemption. One possibility, slightly different to the idea of ``unknown exchangeable subsets'' from Theorem \ref{th:condit_exchange}, is to model the data as if the observations were generated by someone much like the decision maker except with a larger set of options available and whose actions are unknown to the decision maker. Specifically, we suppose that our hypothetical ``decision maker analogue'' has a IO contractible model comprising the full sequence of their own choices plus the real decision maker's choices, and they take some unknown sequence of mixed choices. We furthermore assume that they do this in a manner that puts positive probability on taking any of the decision maker's actual choices at each point in time (that is, they didn't just have the \emph{option} to preempt the decision make, but they actually did preempt them). For convenience, we index the variables under the control of hypothetical decision maker analogue with  $i=...,-2,-1$ and the variables under the control of the decision maker with the single index $i=0$. This reflects the intuitive fact that the passive observations/hypothetical decision maker's choices usually come before the consequences of the real decision maker's actions, though in the present analysis the reversed indexing does not play any important role.

To formalise this, we say there is a variable $\RV{D}$ that is like an ``action'' at the $i=0$ index in the sense that a choice $\alpha$ that leads to a distribution of actions $\RV{D}_0$ identical to some mixture of other choices $\alpha'$ and $\alpha''$ induces consequences equal to the same mixture of $\alpha'$ and $\alpha''$. We require that the model is IO contractible over the sequence of actions $(\RV{D}_i)_{-i\in \{0\}\cup\mathbb{N}}$, and that each action has positive probability in the ``historical'' indices $i<0$. The reason that this assumption is weaker than IO contractibility is that $\RV{D}_i$ for $i<0$ is unobserved -- that is, rules for choosing the action $\RV{D}_0$ cannot depend on $\RV{D}_i$ for $i<0$.

\begin{definition}[Preemption]\label{def:preemption}
Given a probability set $\prob{P}_C$ on $(\Omega,\sigalg{F})$ and variables $\RV{Y}:=(\RV{Y}_i)_{-i\in \{0\}\cup\mathbb{N}}$ and $\RV{D}:=(\RV{D}_i)_{-i\in \{0\}\cup\mathbb{N}}$, $D$ discrete, we say $(\prob{P}_C,\RV{D},\RV{Y})$ is preempted if for directing random measure $\RV{H}$, $\prob{P}_\alpha^{\RV{Y}_i|\RV{HD}_i}$ are independent and identical responses for all $i$, 
\begin{align}
    \prob{P}_\alpha^{\RV{D}_0|\RV{Y}_{-\mathbb{N}}} &= a\prob{P}_{\alpha'}^{\RV{D}_0|\RV{Y}_{-\mathbb{N}}}+b\prob{P}_{\alpha''}^{\RV{D}_0|\RV{Y}_{-\mathbb{N}}}\\
    \implies \prob{P}_\alpha &= a\prob{P}_{\alpha'}+b\prob{P}_{\alpha''}
\end{align}
as well as $\prob{P}_\alpha^{\RV{D}_{-\mathbb{N}}}$ is exchangeable and, defining $\RV{G}$ to be the directing random measure of $(\RV{P}_C,*,\RV{D})$, $\prob{P}_\alpha^{\RV{D}_i|\RV{G}}(\alpha|g)>0$ for all $\alpha\in C$, almost all $g\in G$, $i<0$.
\end{definition}

The assumption of preemption as given in Definition \ref{def:preemption} can, under some side conditions, yield nontrivial conclusions. Theorem \ref{th:det_obs_to_cons} comes with a lot of complicated conditions, so it it worth explaining with an example first.

Suppose we have a collection of doctors $\RV{Z}_i$ who each see a series of patients and offer a treatment $\RV{X}_i$ and report their results $\RV{Y}_i$. Each doctor may decide whether or not to prescribe based on any number of unobserved factors, and may offer additional unrecorded treatments, vary in their bedside manner and so forth, and there may be stochastic variation in any of these. The decision maker is also a doctor, and is reviewing the data contained in the sequences $(\RV{X}_i,\RV{Y}_i,\RV{Z}_i)_{i\in [n]}$. The decision maker supposes that whatever overall treatment plan they adopt (which could also depend on features not listed in this set of variables), the same thing has probably been done at least sometimes by some of these prior doctors -- that is, their treatment protocol is preempted. They also assume that the doctor's identity has no bearing on outcomes over and above the treatment protocol, they assume that doctors don't all select the same mixture of treatment protocols. It then stands to reason that the doctors who choose different treatment plans will see slightly different results \emph{if the different treatment plans actually lead to different results}. Conversely, if there is \emph{no} variation in results after conditioning on whether patients were treated this suggests that whether or not treatment occurred is the \emph{only} important feature of any treatment plan.

One way that this story could fail is if the doctors all knew exactly the long-run probabilistic outcomes of different treatment plans and coordinated with one another, they could (in principle) each pick different mixtures of treatment plans just so that the variation in outcomes is masked -- that is, for example, doctor 1 picks a medium effectiveness plan 100\% of the time, while doctor 2 picks a highly effective plan 50\% of the time and a low effectiveness plan 50\% of the time leading to the same distribution over outcomes. The conditions in Theorem \ref{th:det_obs_to_cons} requiring domination by the uniform measure on $[0,1]$ are assumptions that this kind of thing does not happen, either because the doctors don't coordinate or because, even if they did coordinate, they would not know the long-run averages of outcomes associated with each plan precisely enough to completely mask the variation.

\begin{notation}[Matrix notation]
Given a sequential input-output model $(\prob{P}_C,\RV{D},\RV{Y})$ with $D,Y$ discrete, the directing random measure $\RV{H}$ takes values in the set of Markov kernels $D\kto Y$, which can be identified with a subset of matrices in $\mathbb{R}^{|D|\times |Y|}$. We can therefore refer to elements of $H$ as matrices $(h_d^y)_{d\in D,y\in Y}$ with $\sum_{y\in Y} h_d^y = 1$ for all $d$, and $h_d^y\overset{\prob{P}_\alpha}{\cong}\prob{P}_\alpha^{\RV{Y}|\RV{H}\RV{D}}(y|h,d)$. We can also define $\RV{H}_d^y$ as the $d,y$-th projection of $\RV{H}$.
\end{notation}

\begin{theorem}\label{th:det_obs_to_cons}
Suppose we have a probability set $\prob{P}_C$ on $(\Omega,\sigalg{F})$ with variables $\RV{Y}:=(\RV{Y}_i)_{-i\in \{0\}\cup\mathbb{N}}$, $\RV{D}:=(\RV{D}_i)_{-i\in \{0\}\cup\mathbb{N}}$, $\RV{X}:=(\RV{X}_i)_{-i\in \{0\}\cup\mathbb{N}}$ and $\RV{Z}_i:= (\RV{Z}_i)_{-i\in \mathbb{N}}$, with $D,X,Y,Z$ discrete. Suppose further that $(\prob{P}_C,(\RV{D},\RV{Z}),(\RV{X},\RV{Y}))$ is preempted. Let $\RV{G}$ be the directing random measure of $(\RV{P}_C,\RV{Z},\RV{D})$ and $\RV{H}$ the directing random measure of $(\prob{P}_C,(\RV{D},\RV{Z}),(\RV{X},\RV{Y}))$. Suppose for all $i$, $\RV{Y}_i\CI^e_{\prob{P}_C} \RV{Z}_i|(\RV{H},\RV{X}_i,\RV{C})$. Finally suppose for each $h\in H$ and $d\in D$ and $z,z'\in Z$,
\begin{align}
    \prob{P}_\alpha^{\RV{G}^d_{z}|\RV{H}\RV{G}^d_{z'}}(\cdot|h,g^d_{z'}) \ll U_{[0,1]}\label{eq:lebesgue_dom}
\end{align}
where $U_{[0,1]}$ is the uniform probability measure on $([0,1],\mathcal{B}([0,1]))$ -- that is, the Lebesgue measure on $[0,1]$ restricted to the Borel sets.

Then $\prob{P}_\alpha^{\RV{Y}_i|\RV{H}\RV{X}_i}$ are independent and identical responses for all $-i\in\{0\}\cup\mathbb{N}$.
\end{theorem}

\begin{proof}
First, define matrices $k$ and $l$ by
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_i|\RV{H}\RV{D}_i\RV{Z}_i\RV{X}_i}(y|h,d,z,x)&\overset{\prob{P}_\alpha}{\cong}: k^y_{dzx}\\
    \prob{P}_\alpha^{\RV{X}_i|\RV{H}\RV{D}_i\RV{Z}_i}(x|h,d,z)&{\prob{P}_\alpha}{\cong}: l^x_{dz}
\end{align}
noting that both $k$ and $l$ are almost surely deterministic functions of $h$ by \todo[inline]{independence lemma}.

The assumption $\RV{Y}_i\CI^e_{\prob{P}_C} \RV{Z}_i|(\RV{H},\RV{X}_i,\RV{C})$ implies, for $\prob{P}_\alpha$-almost all $k,l,\alpha,z,z',x,y$
\begin{align}
    \sum_{d\in D} k^y_{dzx}\frac{l^x_{dz}g^d_z}{\sum_{d'\in D}l^x_{d'z}g^{d'}_z}&= \sum_{d\in D} k^y_{dz'x}\frac{l^x_{dz'}g^d_{z'}}{\sum_{d'\in D}l^x_{d'z'}g^{d'}_{z'}}\label{eq:polynomial}
\end{align}
Fixing $k$, $l$ and $g^d_z$, Eq. \eqref{eq:polynomial} defines a polynomial constraints on $g^d_{z'}$. We will show that, unless $k^y_{dzx}=k^y_{d'zx}$ for all $d,d'$ and $z$ then this constraint is nontrivial for some $z'$. For arbitrary $d$, without loss of generality, assume $k^y_{dz'x} > k^y_{d^<z'x}$ for some $d^<$. 

Then either $l^x_{dz'}=l^x_{d^<z'}$, $l^x_{dz'}< l^x_{d^<z'}$ or $l^x_{dz'}=l^x_{d^>z'}$. Consider the first case, and take $g'$ such that $g{\prime d}_{z'}=g^{d}_{z'}-\epsilon$ and $g{\prime d^<}_{z'}=g^{d^<}_{z'}+\epsilon$ and equal to $g$ otherwise. There is almost surely some $\epsilon$ such that $g'$ is a Markov kernel, as $g^{d}_{z'}>0$ almost surely. Then
\begin{align}
    \frac{l^x_{dz}g^d_z}{\sum_{d'\in D}l^x_{d'z}g^{d'}_z} &> \frac{l^x_{dz}g^{\prime d}_z}{\sum_{d'\in D}l^x_{d'z}g^{\prime d'}_z}\\
    \frac{l^x_{d^<z}g^{d^<}_z}{\sum_{d'\in D}l^x_{d'z}g^{d'}_z} &< \frac{l^x_{d^<z}g^{\prime d^<}_z}{\sum_{d'\in D}l^x_{d'z}g^{\prime d'}_z}
\end{align}
because by assumption the denominator remains the same. But then
\begin{align}
    \sum_{d\in D} k^y_{dzx}\frac{l^x_{dz}g^d_z}{\sum_{d'\in D}l^x_{d'z}g^{d'}_z}&> \sum_{d\in D} k^y_{dz'x}\frac{l^x_{dz'}g^{\prime d}_{z'}}{\sum_{d'\in D}l^x_{d'z'}g^{\prime d'}_{z'}}\label{eq:inequality}
\end{align}
because on the left side a larger term in the sum receives more weight, a smaller term receives less weight and all other terms are weighted equally.

Consider $l^x_{dz'}> l^x_{d^<z'}$. Then we still have
\begin{align}
    \frac{l^x_{dz}g^d_z}{\sum_{d'\in D}l^x_{d'z}g^{d'}_z} &> \frac{l^x_{dz}g^{\prime d}_z}{\sum_{d'\in D}l^x_{d'z}g^{\prime d'}_z}\\
    \frac{l^x_{d^<z}g^{d^<}_z}{\sum_{d'\in D}l^x_{d'z}g^{d'}_z} &< \frac{l^x_{d^<z}g^{\prime d^<}_z}{\sum_{d'\in D}l^x_{d'z}g^{\prime d'}_z}
\end{align}
For the first inequality, both the numerator and the denominator shrink. For the second, note that
\begin{align}
    \frac{l^x_{d^<z}g^{d^<}_z}{\sum_{d'\in D}l^x_{d'z}g^{d'}_z} &= \frac{l^x_{d^<z}(g^{d^<}_z+\epsilon)}{(1+\frac{\epsilon}{g^{d'}_z})\sum_{d'\in D}l^x_{d'z}g^{d'}_z}\\
    < \frac{l^x_{d^<z}(g^{d^<}_z+\epsilon)}{\sum_{d'\in D}l^x_{d'z}g^{d'}_z}
\end{align}
and so the conclusion in Eq. \eqref{eq:inequality} follows for the same reasons. Finally considering $l^x_{dz'}< l^x_{d^<z'}$, analogous reasoning implies Eq. \eqref{eq:inequality} once again.

Thus, unless $k^y_{dzx}=k^y_{d'zx}$ for all $d,d'$ and $z$, Eq. \eqref{eq:polynomial} implies a nontrivial constraint on  $g^d_{z'}$ for some $z'$. Thus, for some $z'$, $x$, $d,d'$, $y$ the set of solutions $A:=\{g^d_{z'}|\text{Eq. \eqref{eq:polynomial} is satisfied}\land k^y_{dzx}\neq k^y_{d'zx}\}$ has Lebesgue measure 0 in the set $[0,1]^{D}$ \citep{okamoto_distinctness_1973}, and so finally
\begin{align}
    \prob{P}_\alpha^{\RV{G}^d_{z'}|\RV{H}\RV{G}^d_{z'}}(A|h,g^d_{z}) = 0
\end{align}
by the assumption that this probability is dominated by the Lebesgue measure. On the other hand, by assumption the set $B:=\{g^d_{z'}|\text{Eq. \eqref{eq:polynomial} is satisfied}\}$ has measure 1. Thus we conclude that $k^y_{dzx}=k^y_{d'zx}$ with probability 1. That is, $\RV{Y}_i\CI_{\prob{P}_C}^e \RV{D}_i|(\RV{H},\RV{Z}_i,\RV{X}_i,\RV{C})$. Thus, by contraction with $\RV{Y}_i\CI^e_{\prob{P}_C} \RV{Z}_i|(\RV{H},\RV{X}_i,\RV{C})$, we have $\RV{Y}_i\CI^e_{\prob{P}_C} (\RV{Z}_i,\RV{D}_i)|(\RV{H},\RV{X}_i,\RV{C})$, which implies $\prob{P}_\alpha^{\RV{Y}_i|\RV{H}\RV{X}_i}$ are independent and identical response functions.
\end{proof}

We will now specify the medical example in more detail
\begin{example}\label{ex:doctor_detailed}
Let $\RV{Y}:=(\RV{Y}_i)_{-i\in \{0\}\cup\mathbb{N}}$ be associated with patient outcomes, $\RV{D}:=(\RV{D}_i)_{-i\in \{0\}\cup\mathbb{N}}$ with treatment plans (including, for example, what assessments are made, what other treatments are used and so forth), some of which the decision maker can consider, $\RV{X}:=(\RV{X}_i)_{-i\in \{0\}\cup\mathbb{N}}$ to be the recommendation of a particular treatment and $\RV{Z}_i:= (\RV{Z}_i)_{-i\in \mathbb{N}}$ to be other doctor's identifiers in the dataset.

Assume $\RV{D}_i$ screens off $\RV{Z}_i$ from $\RV{X}_i$ and $\RV{Y}_i$ -- that is, the latent treatment plan is sufficiently detailed to screen off the relevance of the doctor's identity (this is a causal assumption), and the stochastic response to treatment plans is identical for each patient with positive support in the past data for each plan the decision maker is considering. That is, assume $(\prob{P}_C,\RV{D},(\RV{X},\RV{Y}))$ is preempted. By the assumption that $\RV{D}_i$ screens off $\RV{Z}_i$, we can also conclude that $(\prob{P}_C,(\RV{D},\RV{Z}),(\RV{X},\RV{Y}))$ is preempted.

Suppose that each doctor makes choices $\RV{G}^d_z$ by some deterministic function of their beliefs of what every other doctor does $\RV{G}^e_{z'}$ and of the effect of treatment plans $\RV{H}$, but they estimate both $\RV{G}^e_{z'}$ and $\RV{H}$ with continuously distributed noise. Then their beliefs, and hence (by supposition) their choices end up dominated by the uniform measure on $[0,1]$.

If the decision maker is then told by an oracle that for all $i$, $\RV{Y}_i\CI^e_{\prob{P}_C} \RV{Z}_i|(\RV{H},\RV{X}_i,\RV{C})$, she may then conclude that $\prob{P}_\alpha^{\RV{Y}_i|\RV{H}\RV{X}_i}$ are independent and identical responses for all $-i\in\{0\}\cup\mathbb{N}$.
\end{example}

This story makes a number of assumptions with a causal character. First, the assumption of preemption, which we've acknowledged isn't perfectly worked out, is taken here quite literally -- we are not just supposing that the problem can be posed ``as if'' the consequences of our choices had been previously realised, but we are actually taking the $\RV{D}_i$s to literally stand for unobserved choices that actual doctors make. Secondly, the assumption that the unobserved $\RV{D}_i$s screen off the doctor's identities is reminiscent of the idea from the causal graphical models tradition that variables have complete sets of causal parents, some of which may be unobserved, but all of which together screen that variable off from other nondescendant variables. Note that this assumption isn't required by Theorem \ref{th:det_obs_to_cons}, but it supports the particular story being told here.

Finally, the idea that the long run frequencies of each doctor's choices are generic conditioned on the other doctor's choices and the long run input-output relationship seems closely related to the idea of ``independent causal mechanisms''. This comes up in two forms in the literature. First, it is used to justify the assumption of \emph{causal faithfulness}: here, it is shown that \emph{if} one makes an assumption that conditional probabilities in a causal model are generic with respect to one another in a manner similar to Equation \eqref{eq:lebesgue_dom}, then causal faithfulness holds with probability 1 \citep{meek_strong_1995}. However, it's been noted that conditional probabilities routinely do line up in ``non-generic'' ways in an anti-causal direction.

Interestingly, Theorem \ref{th:det_obs_to_cons} itself doesn't depend on a notion of causal direction, and merely shows that a conclusion of independent and identical response functions follows from an assumption of preemption and an assumption of ``generic mechanism association''. Example \ref{ex:doctor_detailed} shows one way that this generic association could be argued for. 

Note that in that example, there is no reason not to expect that each doctor doesn't select a mixture of treatment plans that is \emph{close} to having support at a special singleton -- in fact, it is assumed that doctors try to take into account the response of patient outcomes to treatment plans and the actions of other doctors, and that they simply fail to do this perfectly. We also cheat by having an oracle tell the decision maker that the key conditional independence holds. In particular, we ask the decision maker to conclude something precise about $\RV{H}$ (namely, the key conditional independence) while also assuming that none of the other doctors are able to do this. 

% Theorem \ref{th:preempt_diverse} is a more plausibly useful theorem than \ref{th:det_obs_to_cons}. It shows that, if we believe that $\prob{P}_\alpha^{\RV{Y}_0}$ is preempted by $\prob{P}_\alpha^{\RV{Y}_{-\mathbb{N}}}$, and after conditioning on some $\RV{V}$ the set of distributions of preempting actions is suitably diverse, then we can conclude conditional independence in the consequences of the decision maker's actions from conditional independence in the given observations.

% \begin{theorem}\label{th:preempt_diverse}
% Suppose, given a probability set $\prob{P}_C$ on $(\Omega,\sigalg{F})$ with variables $\RV{Y}:=(\RV{Y}_i)_{-i\in \{0\}\cup\mathbb{N}}$, $\RV{D}:=(\RV{D}_i)_{-i\in \{0\}\cup\mathbb{N}}$ and for each $i$, $(\RV{X}_i,\RV{V}_i,\RV{Z}_i) = f\circ \RV{Y}_i$ for some $f$ with both $D$ and $X$ denumerable, $\prob{P}_\alpha^{\RV{Y}_0}$ is preempted by $\prob{P}_\alpha^{\RV{Y}_{-\mathbb{N}}}$ with respect to $\RV{D}$ and and some $\RV{W}$. Let $\RV{H}$ be the directing random measure of the input-output model $(\prob{P}_C,\RV{D},\RV{Y})$ (Definition \ref{def:dir_rand_meas}). If for each $h\in H$ and each $x\in X$, $i<0$ there is some set of real-valued weights $\{a_i|i\in V\}$ such that
% \begin{align}
%     \prob{P}_\alpha^{\RV{Z}_i|\RV{H}\RV{D}_i\RV{X}_i}(\cdot|h,d,\cdot) &= \sum_{j\in V} a_j \prob{P}_\alpha^{\RV{Z}_i|\RV{H}\RV{V}_i\RV{X}_i}(\cdot|h,j,\cdot)
% \end{align}
% and if for all $i$, $\RV{Z}_i\CI^e_{\prob{P}_C} \RV{V}_i|(\RV{X}_i,\RV{H},\RV{C})$ then $\prob{P}_\alpha^{\RV{Z}|\RV{HX}}$ is IO contractible over $\RV{H}$.
% \end{theorem}

% \begin{proof}
% By assumption
% \begin{align}
%     \prob{P}_\alpha^{\RV{Z}_0|\RV{HD}_0\RV{X}_0}(A|h,d,x) &= \prob{P}_\alpha^{\RV{Z}_{-1}|\RV{HD}_{-1}\RV{X}_{-1}}(A|h,d,x)
% \end{align}
% and also by assumption, for all $A,h,d,x$:
% \begin{align}
%     \prob{P}_\alpha^{\RV{Z}_{-1}|\RV{H}\RV{D}_i\RV{X}_{-1}}(A|h,d,x) &= \sum_{j\in V} a_j \prob{P}_\alpha^{\RV{Z}_{-1}|\RV{H}\RV{V}_{-1}\RV{X}_{-1}}(A|h,j,x)\\
%         &= \sum_{j\in V} a_j \prob{P}_\alpha^{\RV{X}_{-1}|\RV{H}\RV{X}_{-1}}(A|h,x)&\text{by }\RV{Z}_{-1}\CI^e_{\prob{P}_C} \RV{V}_{-1}|(\RV{X}_{-1},\RV{H},\RV{C})\\
%     &= \prob{P}_\alpha^{\RV{Z}_{-1}|\RV{H}\RV{X}_{-1}}(A|h,x)\sum_{j\in V} a_j\\
%     &= \prob{P}_\alpha^{\RV{Z}_{-1}|\RV{H}\RV{X}_{-1}}(A|h,x)&\text{ by taking }A=Z
% \end{align}
% thus
% \begin{align}
%     \prob{P}_\alpha^{\RV{Z}_0|\RV{HD}_0\RV{X}_0}(A|h,d,x) &= \prob{P}_\alpha^{\RV{Z}_{-1}|\RV{H}\RV{D}_{-1}\RV{X}_{-1}}(A|h,d,x)\\
%     &= \prob{P}_\alpha^{\RV{Z}_{-1}|\RV{H}\RV{X}_{-1}}(A|h,x)\\
%     &= \prob{P}_\alpha^{\RV{Z}_0|\RV{H}\RV{X}_0}(A|h,x)\label{eq:indep_of_d}
% \end{align}
% Furthermore, by assumption $\RV{Z}_0\CI^e_{\prob{P}_C} (\RV{Z}_{\{0\}^C},\RV{D}_{\{0\}^C}|(\RV{H},\RV{D}_0,\RV{X}_0,\RV{C})$ and $\RV{Z}_0\CI^e_{\prob{P}_C} \RV{D}_{0}|(\RV{H},\RV{D}_0,\RV{C})$ by Equation \eqref{eq:indep_of_d}, thus $\prob{P}_\alpha^{\RV{Z}|\RV{HX}}$ is IO contractible over $\RV{H}$.
% \end{proof}

% Note that this is a result similar to \citet{peters_causal_2016}, where our variable $\RV{V}$ is understood to be analogous to the environment $e$ in that work.

There is a substantial literature that aims to draw causal conclusions from observational data by first applying a graph learning algorithm to a sequence of observational data, and then using the graph obtained as a DAG for a causal Bayesian network. Earlier examples treat the graph learning problem as a discrete optimization problem and include the PC algorithm and the Causal Inference Algorithm \citet[Ch. 5\& 6]{spirtes_causation_1993} and Greedy Equivalent Search \citet{chickering_optimal_2003,chickering_learning_2002}. More recent examples pose graph learning as a continuous optimization problem \citet{zheng_dags_2018,ng_graph_2019}. Underpinning all of these approaches are a number of key assumptions, which include the assumption of \emph{faithfulness} -- that missing edges in the learned graph correspond to missing edges in the appropriate causal DAG -- and often also the assumption of \emph{causal sufficiency}, which is the assumption that there are ``no relevant unobserved variables''. Together, these assumptions imply that certain conditional independences in the observational data sequence imply the same conditional independences in the data produced under intervention. One open question we raise is: can this implication also be understood as a special case of the interventional data being preempted by the observational data?

\subsubsection{On unobservability}\label{sec:on_unobservability}

The fact that we are offering the assumption that covariates are unobservable as an informal assumption is due to the fact that we are limiting our attention to data-independent models (recall Definition \ref{def:weak_di}). In these models, actions never depend on the available data, and choosing some action based on observations must happen outside the model. If we were considering some data-dependent variation of a causal Bayesian network, the fact that $\RV{V}_{1[n]}$ is unobserved would have formal implications for our model. For example, if $\RV{V}_{1i}$ is unobserved for all $i$ while $\RV{V}_{2i}$ is directly controlled for all $i$, then we should require that, under every choice $\alpha$, $\RV{V}_{2i}$ is independent of $\RV{V}_{1[1,i)}$ given $\RV{V}_{\{2,3\}[1,i)}$ -- that is, there is no choice that induces the controlled variable $\RV{V}_{2i}$ to be dependent on the history of unobserved variables $\RV{V}_{1[1,i)}$, given the history of the observed variables.

\section{What is a Potential Outcomes model?}

Potential outcomes is another popular framework for modelling causal problems. There are two key differences between the potential outcomes approach and the causal Bayesian network approach: potential outcomes models are ``unrolled by default'' and they feature no notion of ``intervention''. A third difference relates to the possibility of expressing ``counterfactual'' statements, although this difference seems to be contingent on the particular manner we use to unroll a causal Bayesian network - see Section \ref{sec:counterfactual_statements}, and recall from Section \ref{sec:unrolling} that we had to make some choices in our construction of unrolled causal Bayesian networks, Definition \ref{def:unr_CBN}.

Thus, to formulate a decision making model from a potential outcomes model, we do have to make a judgement about what the ``choices'' are (while CBNs provide the notion of ``intervention'' for this role), while we do not need to make any judgements about how to unroll a potential outcomes model, because this is already given. For the following, we rely on \citet{rubin_causal_2005} for the definition of a potential outcomes model.

Our definition of potential outcomes has a lot in common with the tabulated conditional distribution (Definition \ref{def:tab_cd}). However, it is different: in particular, $\prob{P}_\alpha^{\RV{Y}_i|\RV{Y}^D_i\RV{D}_i}(y^d|y^D,d)=1$, which is usually false for Definition \ref{def:tab_cd}.

\begin{definition}[Potential outcomes]\label{def:potential_outcomes}
Given $(\prob{P}_C,\Omega,\sigalg{F})$ and, for some $i$, variables $\RV{D}_i:\Omega\to D$ ($D$ denumerable), $\RV{Y}_i:\Omega\to Y$ and $\RV{Y}^D_i:\Omega\to Y^D$, $\RV{Y}^D_i$ is a vector of \emph{potential outcomes} with respect to $\RV{D}_i$ for all $\alpha$
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_i|\RV{Y}^D_i\RV{D}_i} &= \tikzfig{lookup_representation_single}
\end{align}
Where $\prob{F}_{\text{lus}}$ is the Markov kernel associated with the single-shot lookup map
\begin{align}
    \text{lus}:Y^D\times D &\to Y\\
    (d,(y_{i})_{i\in D})&\mapsto y_{d}
\end{align}
\end{definition}

Note that $|D|$ copies of $\RV{Y}_i$ $(\RV{Y}_i,\RV{Y}_i,\RV{Y}_i,...\RV{Y}_i)$ always satisfies Definition \ref{def:potential_outcomes}. This definition is not the sole constraint on potential outcomes, but the additional constraints come from what we want them to model, and are therefore not able to be formally stated.

A ``potential outcomes model'' is simply a probability map with potential outcomes. Traditionally, potential outcomes models did not feature any choices. That is, a ``traditional'' potential outcomes model is a probability space $(\prob{P},\Omega,\sigalg{F})$ rather than a probability function. We extend this to probability functions in what we think is the obvious way - to replace the probability space with a probability function space.

\begin{definition}[Potential outcomes model]
$(\prob{P}_C,\Omega,\sigalg{F})$ is a potential outcomes model with respect to $\RV{Y}^D:=(\RV{Y}^D_i)_{i\in A}$, $\RV{Y}:=(\RV{Y}_i)_{i\in A}$ and $(\RV{D}_i)_{i\in A}$ if $\RV{Y}^D_i$ is a vector of potential outcomes with respect to $\RV{D}_i$ and $\RV{Y}_i$ for all $i\in A$.
\end{definition}

\begin{theorem}
A potential outcomes model $(\prob{P}_C,\Omega,\sigalg{F})$ with respect to $\RV{D}_i:\Omega\to D$, $\RV{Y}_i:\Omega\to Y$ and $\RV{Y}^D_i:\Omega\to Y^D$, $\RV{Y}^D_i$ has $\RV{Y}\CI_{\prob{P}_C}^e\RV{C}|(\RV{D},\RV{Y}^D)$ and $\prob{P}_C^{\RV{Y}|\RV{Y}^D\RV{D}}$ is IO contractible (with respect to $*$).
\end{theorem}

\begin{proof}
IO contractibility of $\prob{P}_C^{\RV{Y}|\RV{Y}^D\RV{D}}$ follows from the fact that $\RV{Y}_i$ is deterministic given $\RV{Y}^D_i$ and $\RV{D}_i$, and thus $\RV{Y}_i\CI^e_{\prob{P}_C} (\RV{D}_{\{i\}^C},\RV{Y}_{\{i\}^C},\RV{C})|(\RV{Y}^D_{i},\RV{D}_i)$. Furthermore, for all $i,j$
\begin{align}
    \prob{P}_C^{\RV{Y}_i|\RV{Y}^D_i\RV{D}_i} &= \prob{P}_C^{\RV{Y}_j|\RV{Y}^D_j\RV{D}_j}
\end{align}
hence the $\prob{P}_C^{\RV{Y}_i|\RV{Y}^D_i\RV{D}_i}$ are independent and identical response functions conditional on $*$.

From Definition \ref{def:potential_outcomes}, $\prob{P}_\alpha^{\RV{Y}_i|\RV{Y}^D_i\RV{D}_i}$ is the same for all $\alpha\in C$, and by the argument above,
\begin{align}
    \prob{P}_C^{\RV{Y}_i|\RV{Y}^D_i\RV{D}_i\RV{Y}^D_{\{i\}^C}\RV{D}_{\{i\}^C}} &= \prob{P}_C^{\RV{Y}_i|\RV{Y}^D_i\RV{D}_i}\otimes \mathrm{del}_{Y^{|D\times A\setminus\{i\}|}\times D^{|A|}}
\end{align}
hence
\begin{align}
    \prob{P}_C^{\RV{Y}|\RV{Y}^D\RV{D}} &= \bigotimes_{i\in A} \prob{P}_C^{\RV{Y}_1|\RV{Y}^D_1\RV{D}_1}
\end{align}
hence $\RV{Y}\CI_{\prob{P}_C}^e\RV{C}|(\RV{D},\RV{Y}^D)$.
\end{proof}

A key theorem of potential outcomes is that, if $\RV{D}$ is ``strongly ignorable'' with respect to $\RV{Y}^D$, then the average treatment effect is identified. ``Strong ignorability'' here means that the probability $\prob{P}_\alpha^{\RV{D}_i}(d)>0$ for each $d$ and for each choice $\alpha$ the inputs $\RV{D}$ are independent of the potential outcomes $\RV{Y}^D$ given the covariates and the choice. We reproduce this theorem in terms of IO contractibility. Note that Theorem \ref{th:potential_outcomes_identifiability} applies to potential outcomes models with sets of choices, rathter than simply to single probability distributions.

\begin{theorem}[Potential outcomes identifiability]\label{th:potential_outcomes_identifiability}
If $(\prob{P}_C,\Omega,\sigalg{F})$ is a potential outcomes model with respect to $\RV{Y}^D:=(\RV{Y}^D_i)_{i\in \mathbb{N}}$, $\RV{Y}:=(\RV{Y}_i)_{i\in \mathbb{N}}$ and $(\RV{D}_i)_{i\in \mathbb{N}}$, each value of $D$ occurs infinitely often with probability 1, there is some $\RV{X}:= (\RV{X}_i)_{i\in \mathbb{N}}$ such that $\prob{P}_\alpha^{\RV{Y}^D\RV{X}}$ is exchangeable for all $\alpha$ and $\RV{D} \CI^e_{\prob{P}_C} \RV{Y}^D | (\RV{X},\RV{Y},\RV{C})$ and for each $\alpha$ $\prob{P}_\alpha^{\RV{D}}$ is absolutely continuous with respect to some exchangeable distribution in $\Delta(D^{\mathbb{N}})$, then there is some $\RV{W}$ such that for all $\alpha$ $\prob{P}_\alpha^{\RV{Y}|\RV{WXD}}$ is IO contractible over $\RV{W}$.
\end{theorem}

\begin{proof}
By exchangeability of $\prob{P}_\alpha^{\RV{Y}^D\RV{X}}$, $\prob{P}_\alpha^{\RV{Y}^D|\RV{X}}$ commutes with exchange. Because $\RV{Y}$ is deterministic given $\RV{D}$ and $\RV{Y}^D$, $\RV{Y}\CI^e_{\prob{P}_C} (\RV{X},\RV{C})|(\RV{Y}^D,\RV{D})$ Thus, for some finite permutation $\rho$, by IO contractibility of $\prob{P}_C^{\RV{Y}|\RV{Y}^D\RV{D}}$
\begin{align}
    \prob{P}_\alpha^{\RV{Y}|\RV{XD}} &= \tikzfig{potential_outcomes_ccontract1}\\
    &= \tikzfig{potential_outcomes_ccontract2}\\
    &= \tikzfig{potential_outcomes_ccontract3}
\end{align}
IO contractibility of $\prob{P}_\alpha^{\RV{Y}|\RV{WXD}}$ over some $\RV{W}$ follows from Theorem \ref{lem:exch_prod_ciid}.
\end{proof}

% \subsection{Counterfactual statements}\label{sec:counterfactual_statements}

% \section{Weaker assumptions than IO contractibility}\label{sec:weaker_assumptions}

% The results so far apply to purely observational models or to models where every ``input'' in the sequence is fixed at the point of choosing $\alpha$ (or a fixed random function is chosen at this point). Most of the interest in causal inference is how to use observational data -- which is plentiful -- to deduce consequences of choices. Suppose in the following that superscript ``$o$'' refers to observational variables (obtained by some measurement procedure not responsive to choices) and ``$v$'' refers to interventional variables (obtained by some measurement procedure responsive to choices). That is $\RV{Y}^o:=(\RV{Y}_i^o)_{i\in \mathbb{N}}$ is a sequence of observational variables, $\RV{Y}^v$ a sequence of interventional variables and $\RV{Y}^{o,v}:=(\RV{Y}^o_i,\RV{Y}^v_i)_{i\in\mathbb{N}}$ is a mixed sequence of both observational and interventional variables. $\RV{Y}_i^o$ and $\RV{Y}_i^v$ are assumed to take values in the same set $Y$.

% One approach to bridging the gap between observations and interventions is to assume ``causal sufficiency'', which is tantamount (in the data-independent case) to assuming IO contractibility of $\prob{P}_C^{\RV{Y}^{o,v}|\RV{X}^{o,v}\RV{D}^{o,v}}$ with $\RV{D}^v$ responsive to choices and $\RV{X}^v$ unresponsive (see Example \ref{pgph:backdoor}). As discussed, this is rarely a reasonable assumption -- it implies interchangeability between observational and interventional samples.

% A weaker assumption that is often adopted is to consider models satisfying IO contractibility with respect to $\prob{P}_C^{\RV{Y}^{o,v}|\RV{U}^{o,v}\RV{D}^{o,v}}$, where $\RV{U}^{o,v}$ is unobserved. That is, while $\RV{U}^{o,v}$ appears in the model, it is not associated with any measurement procedure. This model still asserts that $(\RV{U}^o_i,\RV{X}^o_i,\RV{Y}^o_i)$ triples are interchangeable with $(\RV{U}^v_i,\RV{X}^v_i,\RV{Y}^v_i)$ triples, but neither of these are measurement outcomes. On the other hand, $(\RV{D}_i^o,\RV{Y}_i^o)$ pairs are not generally interchangeable with $(\RV{D}_i^v,\RV{Y}_i^v)$.

% Consider models that satisfy IO contractibility with respect to $\prob{P}_C^{\RV{Y}^{o,v}|\RV{W}^{o,v}}$, where no comment is made about whether $\RV{W}^{o,v}$ is observed, unobserved or some function of observed and unobserved variables. This is a generalisation of the class of models discussed in the previous paragraph.  In isolation, this assumption is not especially interesting -- for example, the support of $\RV{W}^{o}_i$ and $\RV{W}^v_i$ might be disjoint. Suppose also, then, that $W$ is finite and $\RV{W}^o_i$ has full support. This assumption amounts to the assumption that, no matter what choice is made, ``nothing truly new can be done'' (which we call ``Ecclesiastes' assumption''\footnote{Ecclesiastes 1:9 reads ``Everything that happens has happened before; nothing is new, nothing under the sun.''\citep{noauthor_holy_1995}}). More precisely, for any choice $\alpha\in C$ and any consequence $\RV{Y}_i^v$, there is a random subsequence $\RV{Q}$ of indices $(1,2,3,....)$ such that the distribution $\prob{P}_\alpha^{\RV{Y}^{o,v}}$ is unchanged by permutations that only swap elements in the sequence $(RV{Y}^o_\RV{Q},\RV{Y}^v_i)$.

% \begin{theorem}\label{th:condit_exchange}
% Given just-do model $\prob{P}_C$ with $\prob{P}_C^{\RV{Y}^{o,v}|\RV{W}^{o,v}}$ IO contractible, $W$ finite and $\prob{P}_C^{\RV{W}^o|\RV{H}}(w|h)>0$ for all $w,h$, define $q:W^{\mathbb{N}}\times W\to (\{*\}\cup \mathscr{P}(\mathbb{N})$ by 
% \begin{align}
%     q:((w^o_j)_{\mathbb{N}},w^v_i)&\mapsto \{j|w^o_j=w^v_i\}
% \end{align}
% and take $\RV{Q}:=q\circ(\RV{W}^o,\RV{W}^v_i)$ for arbitrary $i\in \mathbb{N}$. For an index set $U\in\mathbb{N}$ take $\text{swap}_{\cdot}:Y^{\mathbb{N}}\times Y^{\mathbb{N}}\to Y^{\mathbb{N}}\times Y^{\mathbb{N}}$ to be an arbitrary finite swap that acts as the identity on all indices $(j,x)\not\in \RV{Q}\times \{o\}\cup\{(i,v)\}$. Then $\prob{P}^{\RV{Y}^{o}\RV{Y}^v_i}\text{swap}_{\RV{Q}} = \prob{P}^{\RV{Y}^{o}\RV{Y}^v_i}$.
% \end{theorem}

% \begin{proof}
% Note that for $B_j\in \sigalg{W}$, where $\rho_q:\mathbb{N}\times\{i,v\}\to \mathbb{N}\times\{i,v\}$ is the permutation function associated with $\text{swap}_{q}$
% \begin{align}
%     \prob{P}_\alpha^{\RV{W}^o\RV{W}^v_i}\text{swap}_{\RV{Q}} (\bigtimes_{j\in\mathbb{N}} B_j) &= \int_{W^{\mathbb{N}}}\int_{\mathscr{P}(\mathbb{N})} \prod_{k\not\in q\times\{o\}\cup\{(i,v)\}} \delta_{w_k}(B_k) \prod_{l\in q\times\{o\}\cup\{(i,v)\}} \delta_{\rho_q(w_l)} (B_l) \prob{P}_\alpha^{\RV{Q}|\RV{W}^o\RV{W}^v_i}(\mathrm{d}q|w)\prob{P}_\alpha^{}(\mathrm{d}w)\\
%     &= \int_{W^{\mathbb{N}}}\int_{\mathscr{P}(\mathbb{N})} \prod_{k\not\in q\times\{o\}\cup\{(i,v)\}} \delta_{w_k}(B_k) \prod_{l\in q\times\{o\}\cup\{(i,v)\}} \delta_{w_l} (B_l) \prob{P}_\alpha^{\RV{Q}|\RV{W}^o\RV{W}^v_i}(\mathrm{d}q|w)\prob{P}_\alpha^{}(\mathrm{d}w)\label{eq:all_the_same}\\
%     &= \prob{P}_\alpha^{\RV{W}^o\RV{W}^v_i}
% \end{align}
% where Eq. \eqref{eq:all_the_same} follows from the fact that for every $k,l\in q\times\{o\}\cup\{(i,v)\}$, $w_k=w_l$.

% Thus for $A\in \sigalg{Y}^{\mathbb{N}}$
% \begin{align}
%     \prob{P}_\alpha^{\RV{Y}^{o}\RV{Y}^v_i}\text{swap}_{\RV{Q}}(A) &= [\prob{P}_\alpha^{\RV{W}^o\RV{W}^v_i} \prob{P}_\alpha^{\RV{Y}^{o}\RV{Y}^v_i|\RV{Q}\RV{W}^o\RV{W}^v_i}\text{swap}_{\RV{Q}}](A)\\
%     &= [\prob{P}_\alpha^{\RV{W}^o\RV{W}^v_i} \text{swap}_{\RV{Q}^{-1}} \prob{P}_\alpha^{\RV{Y}^{o}\RV{Y}^v_i|\RV{W}^o\RV{W}^v_i}\text{swap}_{\RV{Q}}](A)\\
%     &= \prob{P}_\alpha^{\RV{Y}^{o}\RV{Y}^v_i}\label{eq:by_cc1}
% \end{align}
% Where Eq. \eqref{eq:by_cc1} follows from IO contractibility of $\prob{P}_\alpha^{\RV{Y}^{o}\RV{Y}^v_i|\RV{W}^o\RV{W}^v_i}$.
% \end{proof}

% It also follows from Ecclesiastes' assumption and finite $W$ that if some $\RV{X}_i^o$, $\RV{Z}_i^o$ are \emph{deterministically} related given $\RV{W}$, then $\prob{P}_C^{\RV{Z}|\RV{X}}$ is IO contractible.

% \begin{theorem}\label{th:det_obs_to_cons}
% Given just-do model $\prob{P}_C$ with $\prob{P}_C^{\RV{X}^{o,v}\RV{Z}^{o,v}|\RV{W}^{o,v}}$ IO contractible, $W$ finite and $\prob{P}_C^{\RV{W}^o_i|\RV{H}}(w|h)>0$ for all $w,h$, if $\prob{P}_C^{\RV{Z}^o_0|\RV{X}^o_0\RV{H}}$ is deterministic then $\prob{P}_C^{\RV{Z}^{o,v}|\RV{X}^{o,v}}$ is IO contractible.
% \end{theorem}

% \begin{proof}
% Because $\prob{P}_\alpha^{\RV{W}_0^o}\prob{P}_C^{\RV{Z}^o_0|\RV{X}^o_0\RV{W}^o_0\RV{H}}$ is deterministic, so is $\prob{P}_C^{\RV{Z}_0|\RV{X}_0\RV{W}_0\RV{H}}$.

% Fix $h\in H$.  Suppose there is some $w,w'\in W$ such that
% \begin{align}
%     \prob{P}_C^{\RV{Z}_0|\RV{X}_0\RV{W}_0\RV{H}}(A|x,w,h) &\neq \prob{P}_C^{\RV{Z}_0|\RV{X}_0\RV{W}_0\RV{H}}(A|x,w',h)
% \end{align}
% then, by determinism, we can assume without loss of generality
% \begin{align}
%     \prob{P}_C^{\RV{Z}_0|\RV{X}_0\RV{W}_0\RV{H}}(A|x,w,h) = 1\\
%     \prob{P}_C^{\RV{Z}_0|\RV{X}_0\RV{W}_0\RV{H}}(A|x,w',h) = 0
% \end{align}
% but $W$ is finite and $\prob{P}_C^{\RV{W}^o_i|\RV{H}}(w|h)>0$ for all $w$, so there is some $a>0$ such that $\prob{P}_C^{\RV{W}^o_i|\RV{H}}(w|h)\geq a$ for all $w$, and so
% \begin{align}
%     a \leq \sum_{w\in W} \prob{P}_C^{\RV{W}^o_0|\RV{X}^o_0,\RV{H}}(w|x,h)\prob{P}_C^{\RV{Z}_0|\RV{X}_0\RV{W}_0\RV{H}}(A|x,w,h)\leq 1-a
% \end{align}
% contradicting determinism of $\prob{P}_C^{\RV{Z}^o_0|\RV{X}^o_0\RV{H}}$.

% Thus for all $w,w'$
% \begin{align}
%     \prob{P}_C^{\RV{Z}_0|\RV{X}_0\RV{W}_0\RV{H}}(A|x,w,h) &= \prob{P}_C^{\RV{Z}_0|\RV{X}_0\RV{W}_0\RV{H}}(A|x,w',h)
% \end{align}
% i.e. $\RV{Z}_0\CI^e_{\prob{P}_C} (\RV{W}_0,\RV{C})|(\RV{X}_0,\RV{H})$. But then there is some $\prob{P}_C^{\RV{Z}_0|\RV{X}_0\RV{H}}$ such that
% \begin{align}
%     \prob{P}_C^{\RV{Z}_0|\RV{X}_0\RV{W}_0\RV{H}} &= \prob{P}_C^{\RV{Z}_0|\RV{X}_0\RV{H}}\otimes \text{erase}_W\\
%     \implies \prob{P}_\alpha^{\RV{Z}^v_i|\RV{X}^v_i\RV{H}} &= \prob{P}_C^{\RV{Z}_0|\RV{X}_0\RV{H}}
% \end{align}
% \end{proof}

% Theorem \ref{th:det_obs_to_cons} doesn't hold in the case of approximate determinism, however. Intuitively, approximate determinism can hold if there is some value of $\RV{W}$ for which $\RV{Z}$ is not conditionally independent given $\RV{H}$ and $\RV{X}$, but it only ocurrs very rarely in observations. On the other hand, values of $\RV{W}$ rare in observations might, under some choices, become common. 

% \begin{example}
% Say $\prob{P}_C^{\RV{Z}^o_i|\RV{X}^o_i\RV{H}}$ is \emph{approximately deterministic} if $\prob{P}_C^{\RV{Z}^o_i|\RV{X}^o_i\RV{H}}(A|x,h)\in [0,\epsilon]\cup[1-\epsilon,1]$ for all $A\in\sigalg{Z}$, $x,h\in X\times H$.

% Take $Z=X=W=H=\{0,1\}$. Set
% \begin{align}
%     \prob{P}_C^{\RV{Z}_0|\RV{X}_0\RV{W}_0\RV{H}}(1|1,1,1) = 1\\
%     \prob{P}_C^{\RV{Z}_0|\RV{X}_0\RV{W}_0\RV{H}}(1|1,0,1) = 0
% \end{align}
% and
% \begin{align}
%     \prob{P}_C^{\RV{W}^o_0|\RV{H}}(1|1)=1-\epsilon
% \end{align}
% then
% \begin{align}
%     \prob{P}_C^{\RV{Z}_0|\RV{X}_0\RV{H}}(1|1,1) = 1-\epsilon
% \end{align}
% however, suppose there is some $\alpha$ such that
% \begin{align}
%     \prob{P}_\alpha^{\RV{W}^v_i|\RV{H}}(1|1)=0
% \end{align}
% then
% \begin{align}
%     \prob{P}_\alpha^{\RV{Z}_0|\RV{X}_0\RV{H}}(1|1,1) = 0\\
%     &\neq \prob{P}_C^{\RV{Z}_0|\RV{X}_0\RV{H}}(1|1,1)
% \end{align}
% \end{example}


\section{Individual-level response functions}\label{sec:ilevel_ccontract}

Exchangeability of potential outcomes, a key assumption in Theorem \ref{th:potential_outcomes_identifiability}, is hard to explain in terms of symmetries we actually expect to see in the world. Given some experiment producing a sequence of pairs $(\RV{D}_i,\RV{Y}_i)_{i\in\mathbb{N}}$, say where $\RV{D}_i$s are treatment administrations and $\RV{Y}_i$s are health outcomes, there's no obvious generic way to design a related experiment whose model is the same as the original except with potential outcomes $\RV{Y}^D_i$ interchanged. This is in sharp contrast to the assumption of exchangeability of observed outcomes - say, instead of the potential outcomes being exchangeable, we hold that the pairs $(\RV{D}_i,\RV{Y}_i)$ are exchangeable in the original experiment. Then we commit ourselves to the proposition that an alternative experiment which proceeds exactly as the first except, before being ``committed to memory'', the experimental results are interchanged should be modeled exactly as the first.

One could propose that exchangeability of potential outcomes in our example experiment corresponds to an \emph{exchangeability of patients}; perhaps, if we believe the model should be unchanged after we swap the order in which patients are seen, then we should accept that the model has exchangeable potential outcomes. First, note that this isn't a generic transformation like swapping labels -- it depends on the experiment featuring a sequence of patients who can be swapped. Secondly, this proposition depends on some assumption that ties patients to potential outcomes. For example, if each patient were assumed to have a fixed but unknown vector of potential outcomes that is unchanged by the swapping operation, then swapping patients does indeed correspond to swapping potential outcomes.

We formalise the idea of ``potential outcomes attached to individuals'' as \emph{individual-level response functions}. We offer a formal definition of the assumption of individual-level response functions, but like exchangeability of potential outcomes it is difficult to understand fundamentally what this assumption entails, or what it might be motivated by. Nevertheless, it does allow us to separate the assumption of ``exchangeable potential outcomes'' into the assumption of individual level response functions and the assumption of exchangeability of individuals. We also use this notion to prove Theorem \ref{th:cc_ind_treat}. At a high level, it plays a similar role to Theorem \ref{th:potential_outcomes_identifiability}: it seems to justify causal identifiability in certain kinds of controlled experiments. However, the content of the two theorems is very different. While Theorem \ref{th:potential_outcomes_identifiability} concerns independence of the inputs and potential outcomes along with exchangeability of the potential outcomes, Theorem \ref{th:cc_ind_treat} says (informally) if:
\begin{itemize}
    \item There are individual-level response functions
    \item Individuals are exchangeable
    \item Inputs are deterministically controlled by the choice
    \item There is only one choice for each value of the inputs
\end{itemize}
then the model is also IO contractible with respect to the inputs and the outputs only (ignoring the individual identifiers). In our view, this comes closer to a set of assumptions that are directly appliccable to a controlled experiment than those in Theorem \ref{th:potential_outcomes_identifiability}, and reflects \citet{kasy_why_2016}'s dictum that, for the identifiability of causal effects, a ``controlled experiment'' is sufficient.

\subsection{References to individual-level IO contractibility}

The role of individuals has often been mentioned in literature on causal inference. For example, \citet{greenland_identifiability_1986} explain
\begin{quote}
    Equivalence of response type may be thought of in terms of exchangeability of individuals: if the exposure states of the two individuals had been exchanged, the same data distribution would have resulted.
\end{quote}
Here, the idea of ``exchangeable individuals'' plays a role in the author's reasoning about model construction, but ``individuals'' are not actually referenced by the resulting model, and ``exchanging individuals'' does not correspond to a model transformation.

\citet{dawid_decision-theoretic_2020} suggests (with some qualifications) that ``post-treatment exchangeability'' for a decision problem regarding taking aspirin to treat a headache may be acceptable if the data are from
\begin{quote}
    A group of individuals whom I can regard, in an intuitive sense, as similar to myself, with headaches similar to my own.
\end{quote}
As in the previous work, the similarity of individuals involved in an experiment is raised when justifying particular model constructions, but the individuals are not referenced by the model.

\citet[pg. 98]{pearl_causality:_2009} writes
\begin{quote}
    Although the term unit in the potential-outcome literature normally stands for the identity of a specific individual in a population, a unit may also be thought of as the set of attributes that characterize that individual, the experimental conditions under study, the time of day, and so on – all of which are represented as components of the vector $u$ in structural modeling.
\end{quote}
Once again, the idea of an individual (or a particular set of conditions) is raised in the context of explaining modelling choices. Unlike the previous authors, Pearl introduces a vector $u$ to stand for the ``unit''. However, he subsequently assumes that $u$ is a sequence of \emph{independent samples} from some distribution. This seems to contradict an important feature of ``individuals'' or ``units'': individuals are typically supposed to be unique, a property that will usually not be satisfied by independently sampling from some distribution (at least, as long as the distribution is discrete).

Finally, \citet{rubin_causal_2005} writes:
\begin{quote}
    Here there are $N$ units, which are physical objects at particular points in time (e.g., plots of land, individual people, one person at repeated points in time).
\end{quote}
Note that Rubin's explanation of \emph{units} guarantees that they are unique: they are particular things at particular times. These units are associated with input-output functions (the \emph{potential outcomes}), which are later assumed to be exchangeable:
\begin{quote}
    the indexing of the units is, by definition, a random permutation of $1,..., N$, and thus any distribution on the science must be row-exchangeable
\end{quote}

Our proposition is: can the intuition that unique individuals are an important for the motivation for causal models, be captured by considering models that feature ``unique identifier'' variables referencing these unique individuals?

\subsection{Unique identifiers}

A sequence of \emph{unique identifiers} is a vector of finite or infinite length such that no two coordinates are equal. We are interested in models that assign positive probability to any particular coordinate having any particular value. This is straightforward in the finite case. In the infinite case, note that a vector of $|\mathbb{N}|$ unique values with an arbitrary entry $k$ in the $j$th coordinate can be obtained by starting with $(i)_{i\in \mathbb{N}}$ and then transposing $j$ with $k$. More generally, we consider infinite length sequences of unique identifiers to be elements of the set of finite permutations $\mathbb{N}\to\mathbb{N}$.

\begin{definition}[Measurable space of unique identifiers]
The measurable space of unique identifiers $(I,\sigalg{I})$ is the set $I$ of finite permutations $\mathbb{N}\to \mathbb{N}$ with the discrete $\sigma$-algebra $\sigalg{I}$.
\end{definition}

The set $I$ is countable, as it is the countable union of finite subsets (i.e. the permutations that leave all but the first $n$ numbers unchanged for all $n$).

\begin{definition}[Unique identifier]
Given a sample space $(\Omega,\sigalg{F})$, a \emph{sequence of unique identifiers} $\sigalg{I}:\Omega\to I$ is a variable taking values in $I$.
\end{definition}

The values of each coordinate of sequence of unique identifiers is just called an identifier (for obvious reasons, we don't call it an identity).

\begin{definition}[Identification]
Given $\RV{I}$, define the $i$-th \emph{identifier} $\RV{I}_i:=\mathrm{ev}(i,\RV{I})$, where $\mathrm{ev}:\mathbb{N}\times I\to \mathbb{N}$ is the evaluation map $(i,f)\mapsto f(i)$.
\end{definition}

For \emph{any} sample space $(\Omega,\sigalg{F})$ we can define a trivial $\sigalg{I}$ that maps every $\omega\in\Omega$ to $(1,2,3,....)=:(\mathbb{N})$. In this case, the identifiers are all known by the modeller at the outset. Using this sequence of identifiers renders exchange commutativity trivial.

\begin{example}\label{eq:il_exchc}
Given a sequential input-output model $(\prob{P}_C, (\RV{D},\RV{I}),\RV{Y})$ where $\RV{I}$ is the identifier variable $\omega\mapsto (\mathbb{N})$, $\prob{P}_\alpha^{\RV{Y}|\RV{DI}}$ commutes with exchange.

This is because for any permutation $\rho:\mathbb{N}\to\mathbb{N}$ except the identity, $\prob{P}_\alpha^{\RV{Y}|\RV{DI}}$ and $\text{swap}_{\rho}\prob{P}_\alpha^{\RV{Y}|\RV{DI}}$ will have no common support; the first will be supported on $\RV{I}\yields (\mathbb{N})$ only, and the second only on $\RV{I}\yields \rho(\mathbb{N})$.
\end{example}

We are particularly interested in models where exchange commutativity is not trivial, so we focus on the case where each identifier $\RV{I}_i$ has some non-zero probability of taking any value in $\mathbb{N}$. 

% \subsection{Individual-level IO contractibility and unobserved confounding}

% Our first result is that some models with individual-level IO contractibility can be seen as models with unobserved confounding. A model $\prob{P}_C$ with individual-level IO contractibility features a IO contractible $\prob{P}_C^{\RV{Y}|\RV{ID}}$ for a sequence of outputs $\RV{Y}$, inputs $\RV{D}$ and individual identifiers $\RV{I}$. A model with unobserved confounding features IO contractible $\prob{P}_C^{\RV{Y}|\RV{UD}}$ where $\RV{Y}$ and $\RV{D}$ are as before and $\RV{U}$ is an ``unobserved confounder''. They key difference between $\RV{I}$ and $\RV{U}$ is that the individual identifier for each observation is unique, while unobserved variables (typically) have $|U|<N$ where $N$ is the number of observations.

\subsection[Identification]{Identification with individual-level response functions}

The key result of this section is Theorem \ref{th:cc_ind_treat}. A key assumption for this theorem is the assumption of ``individual-level response functions''. That is, the assumption that, given a sequential input-output model $(\prob{P}_C, (\RV{D},\RV{I}),\RV{Y})$, $\prob{P}_C$ features independent and identical response functions conditional on some variable $\RV{J}$ (unlike $\RV{H}$ from Definition \ref{def:dir_rand_meas}, $\RV{J}$ is not necessarily a function of the inputs and outputs). We also assume that each individiual identifier $\RV{I}_i$ has positive probability of taking on any particular value.

This assumption is somewhat difficult to understand. If we imagine that the identifiers $\RV{I}_i$ are, for example, patient names in some medical experiment, it rules out certain possibilities. For example, this assumption is incompatible with the idea that, if Tina is first in line, then she will be deterministically cured by the treatment while if she is last in line she will be deterministically not-cured by it. In fact, it says precisely that, conditional on $\RV{J}$, according to the model Tina will respond in the same stochastic way no matter where in line she is, no matter which other patients have been seen and no matter what their treatments or outcomes were. Unlike the ``directing random measure'' from Definition \ref{def:dir_rand_meas}, $\RV{J}$ is not some long-term limit of observations. Rather, it seems to be just a choice we could make for how we parametrize our model.

Before proving Theorem \ref{th:cc_ind_treat}, we prove a number of lemmas and a preliminary theorem. Lemma \ref{lem:ind_to_cc} and Theorem \ref{lem:ind} do \emph{not} require that $\RV{I}$ be a sequence of unique identifiers, they hold just as well if it is a sequence of non-unique labels; that is, if $\RV{I}_i\yields \RV{I}_j$ had positive measure for some $i\neq j$. The reason why we are interested mainly in the case where $\RV{I}$ is a sequence of unique identifiers is that the assumption $\RV{Y}\CI^e_{\prob{P}_C} \RV{I}|\RV{C}$ is substantially more limiting in the case that $\RV{I}$ is a non-unique sequence of labels. In particular, it implies that the conditional probability of $(\RV{Y}_1,\RV{Y}_2)$ given $(\RV{I}_1=1,\RV{I}_2=1)$ is exactly the same as the conditional probability of $(\RV{Y}_1,\RV{Y}_2)$ given $(\RV{I}_i=1,\RV{I}_2=2)$; observations associated with equal labels are no more relevant that observations associated with different labels.

In the following, it is helpful to assume that each sub-experiment has a ``unique identifier'' $\RV{I}_i$, with the sequence of all sub-experiment labels given by $\RV{I}$. With this, if $\prob{P}_C^{\RV{Y}|\RV{DI}}$ is assumed IO contractible, then it's possible to talk about the individual response functions $\prob{P}_C^{\RV{Y}_i|\RV{I}_i\RV{H}\RV{D}_i}$. These plays a role very similar to the $i$th vector of potential outcomes $\RV{Y}^D_i$. Because $\RV{I}_i$ is unique (i.e. never equal to $\RV{I}_j$ for $j\neq i$), only one observation of any individual is ever given, just like only one potential outcome is ever observed.

Theorem \ref{th:cc_ind_treat} can also be extended to the case where $\RV{D}$ is a function of the choice $\alpha$ and a ``random signal'' $\RV{R}$, as in Theorem \ref{cor:extend_to_randomised}.

\begin{lemma}\label{lem:ind_to_cc}
Given sequential input-output model $(\prob{P}_C,(\RV{D},\RV{I}),\RV{Y})$ with $\prob{P}_\alpha^{\RV{Y}|\RV{WDI}}$ IO contractible over $\RV{W}$, if $\RV{Y}\CI_{\prob{P}_C}^e (\RV{I},\RV{C})|(\RV{W},\RV{D})$ and for any $j\in I$, $\sum_{\alpha\in C} \prob{P}_\alpha^{\RV{I}_i}(j)>0$, then there is some $\RV{W}'$ such that $\prob{P}_\alpha^{\RV{Y}|\RV{W}'\RV{D}}$ is also IO contractible over $\RV{W}$.
\end{lemma}

\begin{proof}
Fix arbitrary $\nu\in \Delta(I^{\mathbb{N}})$ such that $\sum_{\alpha\in C} \prob{P}_\alpha^{\RV{I}_i} \gg \nu$. By assumption of IO contractibility of $\prob{P}_C^{\RV{Y}|\RV{WDI}}$ and Theorem \ref{th:ciid_rep_kernel}
\begin{align}
    \prob{P}_C^{\RV{Y}|\RV{WDI}} &\overset{\prob{P}_C}{\cong} \tikzfig{index_independence_1}\\
    &\overset{\prob{P}_C}{\cong} \tikzfig{index_independence_2}
\end{align}
Where $\Pi_{D,i}:D^{\mathbb{N}}\kto D$ projects the $i$th coordinate, and similarly for $\Pi_{Y,i}$.

In particular, for any $i\in \mathbb{N}$, $j\in I$, this holds for some $\nu$ such that $\nu(\Pi_{Y,i}^{-1} (j))=1$ and by extension for any finite $A\subset \mathbb{N}$ we can find $\nu$ such that $\nu(\Pi_{Y,i}^{-1} (j))=1$ for all $i\in A$, $j\in I$. Thus, for any $n\in \mathbb{N}$
\begin{align}
    \prob{P}_C^{\RV{Y}_{[n]}|\RV{W}\RV{D}_{[n]}\RV{I}_{[n]}} &\overset{\prob{P}_C}{\cong} \tikzfig{index_independence_3}\label{eq:follows_from_determinism}\\
    &\overset{\prob{P}_C}{\cong} \tikzfig{index_independence_4}\label{eq:follows_from_equality}
\end{align}

where Equation \eqref{eq:follows_from_determinism} follows from Theorem \ref{th:fong_det_kerns} and Equation \eqref{eq:follows_from_equality} follows from the fact that Equation \eqref{eq:follows_from_determinism} holds for arbitrary $j\in I$.

Thus by Lemma \ref{lem:infinitely_extended_kernels}
\begin{align}
    \prob{P}_C^{\RV{Y}|\RV{WD}} &= \tikzfig{index_independence_5}
\end{align}
Applying Theorem \ref{th:ciid_rep_kernel}, $\prob{P}_C^{\RV{Y}|\RV{WD}}$ is IO contractible over $\RV{W}$.
\end{proof}

\begin{theorem}\label{th:ind}
Given a sequential input-output model $(\prob{P}_C,(\RV{D},\RV{I}),\RV{Y})$ on $(\Omega,\sigalg{F})$ with $Y$ standard measurable and $C$ countable, if there is some $\RV{J}$ such that for each $\alpha$
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_i|\RV{J}\RV{I}_i\RV{D}_i} &= \prob{P}_\alpha^{\RV{Y}_i|\RV{J}\RV{I}_i\RV{D}_i} &\forall i,j\in \mathbb{N}\\
    &\RV{Y}_i\CI^e_{\prob{P}_C} (\RV{I}_{\{i\}^C},\RV{D}_{\{i\}^C})|(\RV{J},\RV{I}_i,\RV{D}_i)
\end{align}
and
\begin{align}
    &\RV{Y}\CI^e_{\prob{P}_C} \RV{I} | \RV{C}\\
    &\RV{YIJ}\CI^e_{\prob{P}_C} \RV{D}|\RV{C}\\
    &\RV{YIJ}\CI^e_{\prob{P}_C} \RV{C}|\RV{D}\\
    &\forall i,j\in \mathbb{N}: \sum_{\alpha\in C} \prob{P}_\alpha^{\RV{I}_i}(j)>0
\end{align}
then $\prob{P}_C^{\RV{Y}|\RV{JD}}$ is IO contractible over $\RV{J}$.
\end{theorem}

\begin{proof}
For any $\alpha\in C$
\begin{align}
    \prob{P}_\alpha^{\RV{YJ}|\RV{I}} &= \tikzfig{kernel_fac_with_idents}\\
    &= \tikzfig{kernel_fac_with_idents_indepped}
\end{align}

Define $\kernel{Q}$ by $\alpha\mapsto \prob{P}_\alpha$ and $\kernel{Q}^{\cdot|\cdot\RV{C}}$ by $\alpha\mapsto \prob{P}_\alpha^{*}$ and $\kernel{Q}^{\RV{C}}$ is an arbitrary distribution in $\Delta(C)$ with full support. Note that the support of $\kernel{Q}^{\RV{IDYJ}}$ is the union of the support of $\prob{P}^{\RV{IDYJ}}_\alpha$ for all $\alpha$. Then
\begin{align}
    \kernel{Q}^{\RV{YJ}|\RV{IC}} &\overset{\prob{Q}}{\cong} \tikzfig{kernel_fac_with_idents_kernelised}
\end{align}

By assumption $\RV{YI}\CI^e_{\prob{P}_C} \RV{D}|\RV{C}$, it is also the case that
\begin{align}
    \kernel{Q}^{\RV{Y}|\RV{ID}} &\overset{\prob{Q}}{\cong} \tikzfig{kernel_Q_fac_with_idents}\\
    &\overset{\prob{Q}}{\cong} \tikzfig{kernel_Q_fac_with_idents_indepped}\\
    &\overset{\prob{Q}}{\cong} \tikzfig{kernel_Q_fac_with_idents_subbed}
\end{align}
But
\begin{align}
    \kernel{Q}^{\RV{Y}|\RV{ID}}=\sum_{\alpha\in C} \prob{P}_\alpha^{\RV{Y}|\RV{ID}}\kernel{Q}^{\RV{C}}(\alpha)\\
    &= \prob{P}_C^{\RV{Y}|\RV{ID}}\\
    \implies \tikzfig{kernel_Q_fac_with_idents_subbed} &= \prob{P}_C^{\RV{Y}|\RV{ID}}
\end{align}

Furthermore, by assumption $\RV{Y}\CI^e_{\prob{P}_C} \RV{I} | \RV{C}$, so there is some $\kernel{K}:C\kto Y\times W$ such that
\begin{align}
    \kernel{Q}^{\RV{YJ}|\RV{IC}} &\overset{\prob{Q}}{\cong} \tikzfig{kernel_Q_indepped}\\
    \implies \prob{P}_C^{\RV{YJ}|\RV{ID}} &= \tikzfig{kernel_Q_fac_with_idents_swapped}\\
    &= \tikzfig{kernel_P_indep}
\end{align}
Then by Lemma \ref{lem:ind_to_cc}, $\prob{P}_C^{\RV{Y}|\RV{JD}}$ is IO contractible over $\RV{J}$.
\end{proof}

Lemma \ref{lem:exch_to_ind} is used to apply Theorem \ref{th:ind} to models where $\RV{I}$ is a sequence of unique identifiers. Only in this case, exchangeability of the unique identifiers implies the identifiers are independent of the outcomes $\RV{Y}$.

\begin{lemma}\label{lem:exch_to_ind}
Given any probability set $\prob{P}_C$ where $\RV{Y}\CI_{\prob{P}_C}^e \RV{C}|(\RV{D},\RV{I})$ and $\RV{I}:\Omega\to I$ is an infinite sequence of unique identifiers, if for each finite permutation $\rho:\mathbb{N}\to \mathbb{N}$
\begin{align}
    \prob{P}_\alpha^{\RV{Y}|\RV{I}} &= (\text{swap}_{\rho(I)}\otimes \text{Id}_X )\prob{P}_\alpha^{\RV{Y}|\RV{I}}
\end{align}
then $\RV{Y}\CI_{\prob{P}_C}^e \RV{I}|\RV{C}$.
\end{lemma}

\begin{proof}
By definition of the set $I$ of finite permutations, for every $\rho\in I$, $B\in\sigalg{Y}^{\mathbb{N}}$, $d\in D^{\mathbb{N}}$ there is a finite permutation $\rho^{-1}\in I$ such that $\rho\circ\rho^{-1}=\text{id}_{\mathbb{N}}$. Then
\begin{align}
    \prob{P}_\alpha^{\RV{Y}|\RV{I}}(B|\rho) &= (\kernel{F}_{\rho^{-1}}\otimes \text{Id}_X )\prob{P}_\alpha^{\RV{Y}|\RV{I}}(B|\rho)\\
    &= \prob{P}_\alpha^{\RV{Y}|\RV{I}}(B|\text{id}_{\mathbb{N}})
\end{align}
Therefore
\begin{align}
    \prob{P}_\alpha^{\RV{Y}|\RV{I}} &\overset{\prob{P}_C}{\cong} \text{erase}_{I}\otimes \prob{P}_\alpha^{\RV{Y}}
\end{align}
\end{proof}

Theorem \ref{th:cc_ind_treat} presents a set of sufficient conditions for $\prob{P}_C^{\RV{Y}_i|\RV{H}\RV{D}_i}$ to be conditionally independent and identical response functions with respect to the standard directing random measure $\RV{H}$:
\begin{enumerate}
    \item There exist variables $\RV{I}$ representing ``unique identifiers'' which satisfy the assumption that $\prob{P}_C^{\RV{Y}_i|\RV{J}\RV{D}_i\RV{I}_i}$ are a sequence of independent and identical response functions for some $\RV{J}$
    \item The identifiers $\RV{I}$ can be swapped without altering the model of the consequences $\RV{Y}$
    \item The inputs $\RV{D}$ and the choice $\RV{C}$ are substitutable with respect to $\RV{Y}$ and $\RV{I}$: $\RV{YI}\CI^e_{\prob{P}_C} \RV{C}|\RV{D}$ and $\RV{YI}\CI^e_{\prob{P}_C} \RV{D}|\RV{C}$
\end{enumerate}

\begin{theorem}\label{th:cc_ind_treat}
Given a sequential input-output model $(\prob{P}_C,(\RV{D},\RV{I}),\RV{Y})$, on $(\Omega,\sigalg{F})$ with $Y$ standard measurable and $C$ and $D$ countable, $\RV{I}$ an infinite sequence of unique identifiers, if there is some $\RV{J}$ such that
\begin{align}
    \prob{P}_\alpha^{\RV{Y}|\RV{I}} &= (\text{swap}_{\rho(I)}\otimes \text{Id}_X )\prob{P}_\alpha^{\RV{Y}|\RV{I}}&\forall \text{ finite permutations }\rho\\
    &\RV{YIJ}\CI^e_{\prob{P}_C} \RV{D}|\RV{C}\\
    &\RV{YIJ}\CI^e_{\prob{P}_C} \RV{C}|\RV{D}\\
    &\forall i,j\in \mathbb{N}: \sum_{\alpha\in C} \prob{P}_\alpha^{\RV{I}_i}(j)>0
\end{align}
and for each $\alpha$
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_i|\RV{J}\RV{I}_i\RV{D}_i} &= \prob{P}_\alpha^{\RV{Y}_i|\RV{J}\RV{I}_i\RV{D}_i} &\forall i,j\in \mathbb{N}\\
    &\RV{Y}_i\CI^e_{\prob{P}_C} (\RV{I}_{\{i\}^C},\RV{D}_{\{i\}^C})|(\RV{J},\RV{I}_i,\RV{D}_i)
\end{align}
then $\prob{P}_C^{\RV{Y}|\RV{HD}}$ is also IO contractible over the directing random measure $\RV{H}$.
\end{theorem}

\begin{proof}
Apply lemma \ref{lem:exch_to_ind} to get $\RV{Y}\CI^e_{\prob{P}_C} \RV{I} | \RV{C}$, then apply Theorem \ref{th:ind} for $\prob{P}_C^{\RV{Y}|\RV{J}\RV{D}}$ IO contractible. We need to show $\RV{Y}\CI^e_{\prob{P}_C} \RV{J}|(\RV{H},\RV{D},\RV{C})$.

Note that for each $d\in D$, the map $\omega\mapsto \prob{P}_C^{\RV{Y}|\RV{JD}}(\cdot|\RV{J}(\omega),d^{|\mathbb{N}|})$ is measurable with respect to the exchangeable $\sigma$-algebra $\sigalg{E}\subset\sigalg{F}$, and is hence $\RV{H}$-measurable. Furthermore, for any $\mathbf{d}\in D^{\mathbb{N}}$, the $\omega\mapsto \prob{P}_C^{\RV{Y}|\RV{JD}}(\cdot|\RV{J}(\omega),\mathbf{d})$ is a function of the vector valued map $(\omega\mapsto \prob{P}_C^{\RV{Y}|\RV{JD}}(\RV{J}(\omega),d^{|\mathbb{N}|}))_{d\in D}$ and is therefore also $\RV{H}$-measurable. Thus $\RV{Y}\CI^e_{\prob{P}_C} \RV{J}|(\RV{H},\RV{D},\RV{C})$ as desired.
\end{proof}

Theorem \ref{th:cc_ind_treat} can be extended to the case where decisions $\RV{D}$ are a one-to-one deterministic function of the choice, or a random mixtures of one-to-one deterministic functions of the choice. This extension is applicable a randomised controlled trial, where the treatments are deterministically controlled and randomly assigned.

\begin{theorem}\label{cor:extend_to_randomised}
Consider a sequential input-output model $(\prob{P}_{C'},\RV{D},\RV{Y})$ where $\prob{P}_{C'}^{\RV{Y}|\RV{WD}}$ is IO contractible over $\RV{W}$, and construct a second model $(\prob{P}_{C},\RV{D},\RV{Y})$ where $\prob{P}_C$ is the union of $\prob{P}_{C'}$ and its convex hull. Then $\prob{P}_{C}^{\RV{Y}|\RV{WD}}$ is also IO contractible.
\end{theorem}

\begin{proof}
For all $\alpha\in C$, there is some probability measure $\mu:C'\to [0,1]$ such that
\begin{align}
    \prob{P}_\alpha^{\RV{Y}|\RV{WD}} &= \sum_{\beta\in C'} \mu(\beta) \prob{P}_\beta^{\RV{Y}|\RV{WD}}\\
    &= \prob{P}_{C'}^{\RV{Y}|\RV{WD}}
\end{align}
thus
\begin{align}
    \prob{P}_C^{\RV{Y}|\RV{WD}} = \prob{P}_{C'}^{\RV{Y}|\RV{WD}}
\end{align}
and in particular, $\prob{P}_C^{\RV{Y}|\RV{WD}}$ is IO contractible.
\end{proof}

Theorem \ref{cor:extend_to_randomised} can be used to argue that, given a sequence of experiments IO contractible under deterministic choices, adding random mixtures of these choices also yields a IO contractible sequence. \citet{kasy_why_2016} argues that as long as the experimenter controls the treatment assignment, causal effects are identified (i.e. the randomisation step is not strictly necessary). Example \ref{ex:randomised_experiment} shows that this argument might be supported, but Example \ref{ex:bad_randomised_experiment} shows that there are subtle ways that might lead to this argument failing.

We assume an infinite sequence, which is clearly unreasonable. Extending the representation theorems to the case of finite sequences, using for example the result of \citet{diaconis_finite_1980} with establishes that finite exchangeable distributions are approximately mixtures of independent and identically distributed sequences, would allow some implausible assumptions in the following example to be removed.

Theorem \ref{th:cc_ind_treat} is used in the following example to argue that, under certain conditions, a controlled experiment supports a IO contractible model.

\begin{example}\label{ex:randomised_experiment}
A sequential experiment is modeled by a probability set $\prob{P}_C$ with binary treatments $\RV{D}:=(\RV{D}_i)_{i\in \mathbb{N}}$ and binary outcomes $\RV{Y}:=(\RV{Y}_i)_{i\in\mathbb{N}}$. The set of choices $C$ is the set of all probability distributions$\Delta(D^N)$ for some $N\subset\mathbb{N}$ (this is to ensure $C$ is countable).

Each treatment $\RV{D}_i$ is given to a patient, and each patient provides a unique identifier $\RV{I}_i$ which for simplicity we assume is a number in $\mathbb{N}$ (instead of, say, a driver's license number and state of issue), and that (implausibly) there is a positive probability for $\RV{I}_i$ to take any value in $\mathbb{N}$ for any choice $\alpha$.

The treatments are decided as follows: the analyst consults the model $\prob{P}_C$, and, according to $\prob{P}_C$ and some previously agreed upon decision rule, comes up with a (possibly stochastic) sequence  of treatment distributions $\alpha:=(\mu_i)_{i\in N}$ with each $\mu_i$ in $\Delta(\{0,1\})$. If $\mu_i$ is deterministic -- that is, it puts probability 1 on some treatment $d_i$, the experiment administrator will assign patient $i$ the treatment $d_i$. Otherwise, if $\mu_i$ is nondeterministic, the administrator will consult a random number generator that yields treatment assignments according to $\mu_i$, and treatment will then be assigned deterministically according to the result. Letting $C'\subset C$ be the deterministic elements of $C$, this scheme is assumed by the analyst to support the assumptions $\RV{YIJ}\CI^e_{\prob{P}_{C'}} \RV{D}|\RV{C}$ and $\RV{YIJ}\CI^e_{\prob{P}_{C'}} \RV{C}|\RV{D}$ for any $\RV{J}$, and the randomisation procedure is deemed sufficient to ensure that for any mixed $\alpha\in C$ where $\alpha = \sum_{\beta\in C'} \mu(\beta) \beta$, $\prob{P}_\alpha = \sum_{\beta\in C'} \prob{P}_\beta$.

Furthermore, assume $\prob{P}_C^{\RV{Y}|\RV{DI}}$ is IO contractible. Then by Theorem \ref{th:ciid_rep_kernel_k}, there is some $\RV{J}$ such that, conditional on $\RV{J}$, $\prob{P}_C^{\RV{Y}_i|\RV{J}\RV{D}_i\RV{I}_i}$ are conditionally independent and identical response functions. The analyst constructing the model has no particular knowledge about any identifier, and so for any choice the associated model is assumed invariant to permutations of identifiers - that is $\RV{Y}\CI^e_{\prob{P}_C} \RV{I}|\RV{C}$ (see Lemma \ref{lem:ind}). The assumption that this holds given any choice can be tricky -- not only must the identifiers appear symmetric to the analyst constructing the model, but nothing breaking this symmetry may be learned from the choice $\alpha$ (see the Example \ref{sec:dm_control}). One reason supporting this assumption is that the decision maker selects $\alpha$ according to a rule known in advance, so they do not ``learn'' anything upon picking a particular $\alpha$.

Then, for the deterministic subset $C'\subset C$, application of Theorem \ref{th:cc_ind_treat} yields $\prob{P}_{C'}^{\RV{Y}|\RV{HD}}$ is IO contractible over $\RV{H}$, and by application of Corollary \ref{cor:extend_to_randomised}, so is $\prob{P}_{C}^{\RV{Y}|\RV{D}}$.
\end{example}

Permutability of identifiers can fail when the rule for selecting $\alpha$ is not known in advance. The following example is extreme in order to illustrate the issue clearly. The distinction between the analyst and the administrator is also intended to make the example easier to parse. The key point is that, when the rule for selecting $\alpha$ is not known in advance, symmetries that are apparent at the time of model construction do not necessarily hold for every choice $\alpha$, and this remains true if e.g. the selection of choices leads to less extreme confounding or the analyst and the administrator are actually the same person.

The following example involves the choice $\alpha$ depending on some covariate $\RV{U}$. It is not straightforward to express the idea that ``$\alpha$ depends on $\RV{U}$'' in a probability set model $\prob{P}_C$, and they are intended to apply to situations where the choice doesn't depend on anything not already expressed in the model (as in Example \ref{ex:randomised_experiment}). However, the fact that probability sets don't work well in situations where the choice depends on something not expressed in the model doesn't mean that you can't use a probability set to model such a situation, it just means that you shouldn't do it. This is what the following example shows.

The condition $\RV{YIJ}\CI^e_{\prob{P}_C} \RV{C}|\RV{D}$ without also having $\RV{YIJ}\CI^e_{\prob{P}_C} \RV{D}|\RV{C}$ does \emph{not} imply the conclusion of Theorem \ref{th:cc_ind_treat}. Informally, if $\RV{D}$ gives some ``extra information'' over and above $\RV{C}$, then any symmetry that holds before we observe $\RV{D}$ might not hold after $\RV{D}$ has been observed. We have argued in Section \ref{sec:dm_control} somewhat informally that the choice $\RV{C}$ should be completely under the decision maker's control -- for Theorem \ref{th:cc_ind_treat}, this perfect control has to extend to the sequence of inputs $\RV{D}$. Constructing the following example requires the hypotheses that any given identifier $i\in\mathbb{N}$ could be associated with one of two input-output maps $D\kto Y$. Thus the space of hypotheses is a sequence of binary values $H=\{0,1\}^{\mathbb{N}}$. Equipped with the product topology, $H$ is a countable product of separable, completely metrizable spaces and is therefore also separable and completely metrizable \citep[Thm. 16.4,Thm. 24.11]{willard_general_1970}. Thus $(H,\mathcal{B}(H))$ is a standard measurable space and, because it is uncountable, it is isomorphic to $([0,1],\mathcal{B}([0,1]))$.

\begin{example}
Take $Y=C=D=\{0,1\}$ and take $(H,\sigalg{H})$ to be $\{0,1\}^{\mathbb{N}}$ equipped with the product topology. For any $i\neq 1$, $\RV{Y}_i\RV{I}_i\RV{D}_i\CI^e_{\prob{P}_C} \RV{C}$, while $\prob{P}_\alpha^{\RV{D}_1}=\delta_\alpha$ and $\RV{I}_i\CI^e_{\prob{P}_C} \RV{C}$.

$\RV{YI}\CI^e_{\prob{P}_C} \RV{C}|\RV{D}$ follows from the fact that $\RV{C}$ can be (almost surely) written as a function of $\RV{D}$.

For all $i,\in \mathbb{N}$, $y,d\in \{0,1\}$, $h\in H$ set
\begin{align}
    \prob{P}_C^{\RV{Y}_i|\RV{H}\RV{I}_i\RV{D}_i}(y|h,j,d) &= \delta_1(p(j,h))\delta_d(y) + \delta_0(p(j,h))\delta_{1-d}(y)
\end{align}
where $p(j,h)$ projects the $j$-th component of $h$. That is, if $h$ maps $j$ to 1, $\RV{Y}$ goes with $\RV{D}$ while if $h$ maps $j$ to $0$, $\RV{Y}$ goes opposite $\RV{D}$. Suppose also 
\begin{align}
    \RV{Y}_i\CI_{\prob{P}_C}^e (\RV{X}_{<i},\RV{Y}_{<i},\RV{I}_{<i},\RV{C})|(\RV{X}_i,\RV{Y}_i,\RV{H})
\end{align}
Then $\prob{P}_C^{\RV{Y}|\RV{DI}}$ is IO contractible. Set $\prob{P}_{C}^{\RV{H}}$ to be the uniform measure on $(H,\sigalg{H})$ and for $i>1$
\begin{align}
    \prob{P}_C^{\RV{D}_i|\RV{I}_i\RV{H}}(d|j,h) &= \delta_{p(j,h)}(d)
\end{align}
that is, if $h$ maps $j$ to 1, $\RV{D}$ is 1 while if $h$ maps $j$ to $0$, $\RV{D}$ is 0. This also implies
\begin{align}
    \prob{P}_C^{\RV{I}_i|\RV{D}_i\RV{H}}(p(\cdot,h)^{-1}(d)|d,h) &= 1\label{eq:all_eq_d}
\end{align}

Then, for $i>1$
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_i|\RV{H}\RV{D}_i}(y|h,d) &= \sum_{j\in \mathbb{N}} \delta_1(p(j,h))\delta_d(y)\prob{P}_C^{\RV{I}_i|\RV{D}_i\RV{H}}(j|d,h) + \delta_0(p(j,h))\delta_{1-d}(y)\prob{P}_C^{\RV{I}_i|\RV{D}_i\RV{H}}(j|d,h)\\
    &= \sum_{j\in \mathbb{N}} \delta_1(d)\delta_d(y)\prob{P}_C^{\RV{I}_i|\RV{D}_i\RV{H}}(j|d,h) + \delta_0(d)\delta_{1-d}(y)\prob{P}_C^{\RV{I}_i|\RV{D}_i\RV{H}}(j|d,h)&\text{by Eq \eqref{eq:all_eq_d}}\\
    &= \delta_1(y)\\
    \implies \prob{P}_\alpha^{\RV{Y}_i|\RV{D}_i}(y|d) &= \delta_1(y)
\end{align}

For $q\in I$, set
\begin{align}
    \prob{P}_C^{\RV{I}|\RV{H}}(q|h)&= \begin{cases}
        0.5 & q=(1,2,3,4,...) \text{ or } (1,3,2,4,...)\\
        0&\text{otherwise}
    \end{cases}
\end{align}
and set
\begin{align}
    \prob{P}_C^{\RV{H}|\RV{D}}(h) &= \begin{cases}
        0.5 & h=(0,1,0,1,1,...)\text{ or }h=(0,0,1,1,1,...)\\
        0 &\text{otherwise}
    \end{cases}
\end{align}
Let $\overline{H}$ be the support of $\prob{P}_C^{\RV{H}|\RV{D}}(h)$.

Then for $i=1$
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_1|\RV{D}_1}(y|h,d) &= \sum_{h\in H} \sum_{j\in \mathbb{N}} \prob{P}_\alpha^{\RV{I}_1|\RV{D}_1\RV{H}}(j|d,h)\prob{P}_C^{\RV{H}|\RV{D}_1}(h|d)\left(\delta_1(p(j,h))\delta_d(y) + \delta_0(p(j,h))\delta_{1-d}(y)\right)\\
    &= \sum_{h\in \overline{H}} 0.5( \delta_1(p(1,h))\delta_d(y) + \delta_0(p(1,h))\delta_{1-d}(y))\\
    &= \delta_{1-d}(y))\\
    &\neq  \prob{P}_\alpha^{\RV{Y}_i|\RV{D}_i}(y|h,d) & i\neq 1
\end{align}
Thus $\prob{P}_C^{\RV{Y}|\RV{D}}$ is not IO contractible by Theorem \ref{th:equal_of_condits}. 

However, given any finite permutation $\rho:\mathbb{N}\to\mathbb{N}$
\begin{align}
    \prob{P}_\alpha^{\RV{Y}|\RV{I}}(y|q) &= \sum_{h\in \overline{H}}\sum_{d\in\{0,1\}^{\mathbb{N}}} \prod_{i\in \mathbb{N}} \prob{P}_C^{\RV{Y}_i|\RV{I}_i\RV{D}_i\RV{H}}(y_i|q_i,d_i,h) \prob{P}_\alpha^{\RV{D}_i|\RV{I}_i\RV{H}}(d_i|q_i,h)\prob{P}_C^{\RV{H}}(h)\\
    &= \delta_{1-\alpha}(y_1)\delta_{(1)_{i\in\mathbb{N}}}(y_{>1})\\
    &= \prob{P}_\alpha^{\RV{Y}|\RV{I}}(y|\rho^{-1}(q))\\
    &= \kernel{F}_{\rho}\prob{P}_\alpha^{\RV{Y}|\RV{I}}(y|q)
\end{align}
\end{example}

\subsection{Other examples}

\paragraph{Example 5: Backdoor adjustment}

The ``backdoor adjustment'' formula is a fundamental tool for many kinds of causal inference. This is a short example to show the conditions under which it's applicable, stated in terms of IO contractibility. Suppose a sequential input-output model $(\prob{P}_C,(\RV{D},\RV{X}),\RV{Y})$ where $(\prob{P}_\cdot^{\RV{Y}|\RV{WDX}}$ is IO contractible, and:
\begin{itemize}
    \item $i>n\implies \RV{X}_{i}\CI^e_{\prob{P}_C}\RV{D}_{i}|(\RV{H},\RV{C})$
    \item $\prob{P}_\alpha^{\RV{X}_{i}|\RV{H}}\cong \prob{P}_\alpha^{\RV{X}_{1}|\RV{H}}$ for all $\alpha$
 \end{itemize}
Then the model exhibits a kind of ``backdoor adjustment''. Specifically, for $i>n$
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_{i}|\RV{D}_{i}\RV{H}}(A|d,h) &= \int_X \prob{P}_\alpha^{\RV{Y}_{i}|\RV{X}_{i}\RV{D}_{i}\RV{H}}(A|d,x,h)\prob{P}_\alpha^{\RV{X}_{i}|\RV{D}_{i}\RV{H}}(\mathrm{d}x|d,h)\\
    &= \int_X \prob{P}_\alpha^{\RV{Y}_{1}|\RV{X}_{1}\RV{D}_{1}\RV{H}}(A|d,x,h)\prob{P}_\alpha^{\RV{X}_{i}|\RV{H}}(\mathrm{d}x|h)\\
    &= \int_X \prob{P}_\alpha^{\RV{Y}_{1}|\RV{X}_{1}\RV{D}_{1}\RV{H}}(A|d,x,h)\prob{P}_\alpha^{\RV{X}_{1}|\RV{H}}(\mathrm{d}x|h)\label{eq:backdoor}
\end{align}

Equation \eqref{eq:backdoor} is identical to the backdoor adjustment formula \citep[Chap. 1]{pearl_causality:_2009} for an intervention on $\RV{D}_1$ targeting $\RV{Y}_1$ where $\RV{X}_1$ is a common cause of both.

\paragraph{Example 6: the provenance of the choice variable}\label{sec:dm_control}

\todo[inline]{Use individual-level ccont}

The point of this example is to clarify the idea of a ``choice'' variable. If we say that some value is the outcome of a choice, a straightforward interpretation of this term suggests that this value was chosen by someone, somewhere. However, for the purposes of decision making models, there are important differences between:
\begin{itemize}
    \item Values chosen by someone, somewhere
    \item Values chosen by the decision make, using the decision making model
\end{itemize}

We call this example the ``I choose vs you choose'' problem. Suppose we have a decision maker (``DM'') and an administrator (``admin'') cooperating to collect data to support DM to make a choice.

First, consider the ``I choose'' condition. Here, DM's choice $\alpha\in \{0,1\}^2$ deterministically sets the value of binary inputs $\RV{D}_1,\RV{D}_2$, and the decision maker is interested in evaluating the corresponding binary outputs $\RV{Y}_1,\RV{Y}_2$. The decision maker assesses that their knowledge of the real-world mechanisms that gives rise to each output $\RV{Y}_i$ in the context on an input $\RV{D}_i$ render these mechanisms indistinguishable. 's point of view, the input-output relations for each step are indistinguishable. In particular, they assess that the marginal probabilities of the outputs are the same given a corresponding input, and that the evidence that the first experiment brings to bear on the second is equivalent to the evidence that the second brings to bear on the first. Thus, they assess that exchange commutativity is appropriate; for all $\alpha$:
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_1\RV{Y}_2|\RV{D}_1\RV{D}_2} &= \prob{P}_\alpha^{\RV{Y}_2\RV{Y}_1|\RV{D}_2\RV{D}_1}
\end{align}







 but this example suggests another reason one might want to avoid deterministic treatment assignments. If the choices $\alpha$ are a deterministic sequence of assignments for each index $i$, this means that there is an enormous set of possible choices, and many degrees of freedom if the choices ``aren't actually chosen'' in the sense of the example above. In contrast, if the set of choices is a single parameter in $[0,1]$ which is then used to assign all treatments according to a random procedure depending only on this parameter, there are many fewer degrees of freedom to exploit if the choice ``isn't actually chosen''.

A particular concern arises when the choice variable $\RV{C}$ is not associated with the output of a decision procedure involving the model $\prob{P}_C$. In this situation, the value of $\RV{C}$ can affect the model in potentially unexpected ways. ``Potentially unexpected'' is a vague notion, and we can't say whether $\RV{C}$ being completely under the decision maker's control avoids ``unexpected'' dependence on $\RV{C}$, but it seems to be less problematic.

We set this up in terms of an ``analyst'' and an ``administrator'' who have responsibility for different parts of the procedure. They don't strictly need to be different people, but it helps make the issue clearer. The analyst's job is to construct a model $\prob{P}_C$, evaluate different options $\alpha\in C$ and offer advice regarding the choice. The administrator's job is to choose some $\alpha\in C$ satisfying the analyst's requirements and to carry out any procedure arising from this.

This separation of concerns gives the administrator a degree of freedom in their choice, and they can potentially use this to choose $\alpha$ with access to information that the analyst lacks.

In particular, suppose an experiment is modeled by a sequential input-output model $(\prob{P}_C,(\RV{D},\RV{U}),\RV{Y})$ and the set of choices $C=[0,1]^{\mathbb{N}}$ is a length $\mathbb{N}$ sequence of probability distributions in $\Delta(\{0,1\})$. The analyst, based on their knowledge of the experiment, constructs $\prob{P}_C$ such that $\prob{P}_C^{\RV{Y}_i|\RV{U}_i\RV{D}_i}(1|\cdot,\cdot)$ is given by:
\begin{center}
\begin{tabular}{ c | c | c }
  & $\RV{D}_i=0$ & $\RV{D}_i=1$ \\\hline 
 $\RV{U}_i=0$ & 0 & 0 \\ \hline 
 $\RV{U}_i=1$ & 1 & 1   
\end{tabular}
\end{center}
and the triples $(\RV{D}_i,\RV{U}_i,\RV{Y}_i)$ are mutually independent given $\RV{C}$. This makes $\prob{P}_C^{\RV{Y}|\RV{UD}}$ IO contractible over $*$. Suppose also 
\begin{align}
    \prob{P}_\alpha^{\RV{D}_i}(1) &= \alpha_i
\end{align}
where $\alpha=(\alpha_i)_{i\in\mathbb{N}}$. From the analyst's point of view, both before and after making their recommendations the $\RV{U}_i$ are also IID. This will be expressed with a probability distribution $\prob{Q}$ representing the analyst's prior knowledge:
\begin{align}
    \prob{Q}^{\RV{U}_i}(1) &= 0.5
\end{align}
one might be tempted to reason that, if $\prob{Q}$ is the analyst's state of knowledge after making any reccomendation, then we should take $\prob{P}_C^{\RV{U}}=\prob{Q}^{\RV{U}}$. Call the resulting model $\prob{P}_C'$. Together with the other assumptions above, this would imply
\begin{align}
    \prob{P}_C^{\prime \RV{Y}_i|\RV{D}_i}(1|d) &= 0.5 & \forall d\in \{0,1\}
\end{align}
Thus $\prob{P}_C^{\RV{Y}|\RV{D}}$ is also IO contractible.

However, the analyst's recommendation \emph{does not} fix the value of $\RV{C}$. Suppose analyst actually recommends any $\alpha$ such that $\lim_{n\to\infty} \sum_i^n \frac{\alpha_i}{n} = 0.5$ (acknowledging that, in this contrived example, there's no obvious reason to do so). Suppose that the administrator operates by the following rule: \emph{first} they observe the value of $\RV{U}_i$, then they choose $\alpha_i$ equal to whatever they saw with an $\epsilon$ sized step towards $0.5$. That is, if they see $\RV{U}_i\yields 1$, they choose $\alpha_i=1-\epsilon$, where $\epsilon < 0.5$.

Then the analyst should instead adopt the model
\begin{align}
    \prob{P}_\alpha^{\RV{U}_i}(1) &= \mathds{1}_{\alpha_i>0.5}
\end{align}
Take $\alpha$ such that $\alpha_i=1-\epsilon$ and $\alpha_j=\epsilon$. Then
\begin{align}
    \prob{P}_{\alpha}^{\RV{Y}_i|\RV{D}_i}(1|1) &= 1\\
    &\neq \prob{P}_\alpha^{\RV{Y}_j|\RV{D}_j}(1|1)\\
    &=0
\end{align}
everything has been assumed IID, so $\prob{P}_C^{\RV{Y}|\RV{HD}}$ is not IO contractible.

The original justification for having a set of choices $C$ is that $C$ is the set of things that, after deliberation aided by the model $\prob{P}_C$, the decision maker might select. The present example does not conform to this understanding of the meaning of the set $C$, and it suggests that one should be cautious when modelling ``decision problems'' with ``choices'' that are not actually the things that are being chosen.

This point is related to the question of why experimenters randomise. \citet{kasy_why_2016} argues that ``randomised controlled trials are not needed for causal identifiability, only controlled trials'', and suggests that experiments should sometimes be designed with deterministic assignments of patients to treatment and control groups, optimised according to the experiment designer's criteria. Following this, \citet{banerjee_theory_2020} suggested that deterministic rules might falter when one can't pick a function to balance covariates in a way that satisfies everyone in a panel of reviewers. 

Without solving the problem, we observe that the terms ``control'' and ``choice'' here subsume both different kinds of choice indicated above, each of which has different implications for the construction of decision making models. We offer a speculative alternative explanation for randomisation: perhaps that the same model may be appropriate for both notions of ``choice'' under randomised choices, but not under nonrandomised choices.


\citet{savje_randomization_2021} argues that random assignment (under his definition) does not imply unconfoundedness

\section{Conclusion}

We review the decision making models implied by causal Bayesian networks and potential outcomes models. We find that these kinds of models have complementary ``missing pieces'' needed to induce the relevant decision making model -- while causal Bayesian networks already have interventions that provide a kind of ``choice set'', they need to be unrolled to a sequential model. On the other hand potential outcomes models can already be specified in an unrolled form, but need some notion of ``choice set'' to induce a decision making model. Common formulations of both models feature conditionally independent and identical response functions. We explore individual-level response functions as a means of establishing the widely accepted result that randomised trials. We note that the assumption of individual-level response functions seems to be a missing step in the often cited idea that exchangeability of individuals implies exchangeability of potential outcomes, and we show that with individual-level response functions, exchangeable individuals and completely controllable inputs, causal relationships are identified. The need for completely controllable inputs is also widely accepted, but to our knowledge it only appears as a formal assumption in Theorem \ref{th:data_ind_CC}.