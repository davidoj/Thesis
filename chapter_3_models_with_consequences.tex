
%!TEX root = main.tex

\chapter{Models with consequences}\label{ch:2p_statmodels}

In chapter \ref{ch:tech_prereq} we introduced a collection of tools for constructing probabilistic models. Tools by themselves aren't enough to construct models: we also need a purpose. \emph{Why are we constructing probabilistic models}? Our provisional answer to this question is: we construct probabilistic models to help us compare different choices we could make on the basis of the outcomes we anticipate that they will bring about. We aren't claiming that this is the only reason to create a probabilistic model, but comparing different choices is an important job that lots of people regularly need to undertake, and it is a job for which causal models are often employed.

If we are going to buld models that facilitate the comparison of different choices on the basis of their consequences, then they must have a few properties:

\begin{itemize}
    \item We want a set of choices $C$ with at least a partial order on it, because this is how we mathematically represent ``comparison''
    \item By supposition, we don't have a partial order on $C$ at the outset, otherwise our problem is already solved
    \item However, we do have a partial order on \emph{consequences}, which are elements of some set $F$
    \item In order to use the partial order on $F$ to induce a partial order on $C$, we need to associate one or more elements of $F$ with each element of $C$
\end{itemize}

At a very general level, a model that helps us to compare choices is a set-valued function $C\to \mathscr{P}(F)$. In order to say more about ``models for mathematically-assisted decisions'', we need to make some more specific assumptions about how these models are constructed - for example, we might take the set of consequences $F$ to be a set of probability distributions on a common sample space. However, even before we do this, the requirement for a domain of choices $C$ arises purely because we want to compare things mathematically.

In practice, we will take the consequences $F$ to be a set of probability distributions on a common sample space. There is a large literature on whether this is always an appropriate way to represent consequences, and we will address a small slice of it in Section \ref{sec:how_represent_conseqeunces}, but arguing that this is always a reasonable choice is not our objective here. Rather, we observe that there is a long history of using probability theory for the purpose of representing uncertain knowledge, and probability theory in conjunction with the principle of expected utility is often used to compare the desirability of uncertain outcomes.

While it doesn't frequently enter into the reasoning we present in the upcoming chapters, we spend some time here considering how probabilistic models are related to the ``real world phenomena'' that they are supposed to be models of. In particular, \emph{variables} are ``points of attachment'' between the model and the real world -- they point to the parts of the real world that correspond to features of the model. There are many standard accounts of how variables accomplish this in regular statistical models, but we need to elaborate on this account slightly to account for the inclusion of a domain $C$ of choices that are each associated with their own consequences. In Section \ref{sec:variable}, we present both the standard account of variables and how we propose to elaborate this account to include a domain of choices.

\section{Variables}\label{sec:variable}

In probability theory, it is standard to assume the existence of a probability space $(\mu,\Omega,\sigalg{F})$ and to define \emph{random variables} as measurable functions from $(\Omega,\sigalg{F})$ to $(\mathbb{R},\mathcal{B}(\mathbb{R}))$. However, variables aren't \emph{just} functions -- they're also typically understood to correspond to some measured aspect of the real world. For example, \citet{pearl_causality:_2009} offers the following two, purportedly equivalent, definitions of variables:
\begin{quote}
By a \emph{variable} we will mean an attribute, measurement or inquiry that may take on one of several possible outcomes, or values, from a specified domain. If we have beliefs (i.e., probabilities) attached to the possible values that a variable may attain, we will call that variable a random variable.
\end{quote}

\begin{quote}
This is a minor generalization of the textbook definition, according to which a random variable is a mapping from the sample space (e.g., the set of elementary events) to the real line. In our definition, the mapping is from the sample space to any set of objects called ``values,'' which may or may not be ordered.
\end{quote}

However, these quotes are describing different things -- in fact, they're not even describing the same \emph{kind} of thing. The first is talking about a \emph{measurement}, which is something we can do in the real world that produces as a result an element of a mathematical set. The second is talking about a \emph{function}, which is a purely mathematical object with a domain and a codomain and a mapping from the former into the latter.

The way we address this distinction is: a procedure that takes place in the real world and yields as results elements of a mathematical set is called a \emph{measurement procedure}. We suppose for a given problem that there is a ``complete measurement procedure'' $\proc{S}$, and the result of every specific measurement procedure of interest can be reconstructed from the result of the complete measurement procedure by applying a function to the latter result. The function $\RV{X}$ that yields the result of a specific measurement procedure $\proc{X}$ given the result of the complete measurement procedure $\proc{S}$ is the \emph{variable} associated with the measurement procedure $\proc{X}$.

In this way, the variable $\RV{X}$ -- which is by itself just a mathematical function -- is made relevant to the real-world by combining it with a total measurement procedure $\proc{S}$.

We can even use this scheme to address situations where it is possible to make different choices. We can simply posit a sub-procedure $\proc{C}$ that yields the choice $\alpha$ that we eventually make. However, modelling this can be tricky. If we want to use the consequence model to help make the decision, then it seems that the model of the decision procedure $\proc{C}$ will need to be self-referential. Furthermore, even if we have a model of $\proc{C}$ that says we will certainly decide on a particular element $\alpha^*$, we still need to map every element of $C$ to a consequence because this is what enables the comparison of elements of C. Thus, modelling $\proc{C}$ makes the model more complicated, and it's not obvious that this is answering the question that we are interested in. This complication is not obviously intractable, but we do not address it here. Instead, we simply assume that we have a collection $\proc{S}_\alpha$ of total measurement procedures that all yield elements of the same set, and each of which are executed if the choice made is $\alpha$.

\subsection{Variables and measurement procedures}

We illustrate this approach with the example of Newton's second law in the form $\RV{F}=\RV{MA}$. This model relates ``variables'' $\RV{F}$, $\RV{M}$ and $\RV{A}$. As \citet{feynman_feynman_1979} noted, in order to understand this law, we must bring some pre-existing understanding of force, mass and acceleration independent of the law itself. Furthermore, we contend, this knowledge cannot be expressed in any purely mathematical statement. In order to say what the net force on a given object is, even a highly knowledgeable physicist will have to go and do some measurements, which is a procedure that they carry out involving interacting with the real world somehow and obtaining as a result a vector representing the net forces on that object.

That is, the variables $\RV{F}$, $\RV{M}$ and $\RV{A}$ are referring to the \emph{results of measurement procedures}. We will introduce a separate notation to refer to these measurement procedures -- $\proc{F}$ is the procedure for measuring force, $\proc{M}$ and $\proc{A}$ for mass and acceleration respectively. A measurement procedure $\proc{F}$ is akin to \citet{menger_random_2003}'s notion of variables as ``consistent classes of quantities'' that consist of pairing between real-world objects and quantities of some type. Force $\proc{F}$ itself is not a well-defined mathematical thing, as measurement procedures are not mathematically well-defined. At the same time, the set of values it may yield \emph{are} well-defined mathematical things. No actual procedure can be guaranteed to return elements of a mathematical set known in advance -- anything can fail -- but we assume that we can study procedures reliable enough that we don't lose much by making this assumption.

Note that, because $\proc{F}$ is not a purely mathematical thing, we cannot perform mathematical reasoning with $\proc{F}$ directly. Rather, we introduce a variable $\RV{F}$ which, as we will see, is a well-defined mathematical object, assert that it corresponds to $\proc{F}$ and conduct our reasoning using $\RV{F}$.

\subsection{Measurement procedures}\label{sec:mprocs}

\begin{definition}[Measurement procedure]
A \emph{measurement procedure} $\proc{B}$ is a procedure that involves interacting with the real world somehow and delivering an element of a mathematical set $X$ as a result. A procedure $\proc{B}$ is said to takes values in a set $B$.
\end{definition}

We adopt the convention that the procedure name $\proc{B}$ and the set of values $B$ share the same letter.

\begin{definition}[Values yielded by procedures]
$\proc{B}\yields x$ is the proposition that the the procedure $\proc{B}$ will yield the value $x\in X$. $\proc{B}\yields A$ for $A\subset X$ is the proposition $\lor_{x\in A} \proc{B}\yields x$.
\end{definition}

\begin{definition}[Equivalence of procedures]\label{def:equality}
Two procedures $\proc{B}$ and $\proc{C}$ are equal if they both take values in $X$ and $\proc{B}\yields x\iff \proc{C}\yields x$ for all $x\in X$.
\end{definition}

If two involve different measurement actions in the real world but necessarily yield the same result, we say they are equivalent.

It is worth noting that this notion of equivalence identifies procedures with different real-world actions. For example, ``measure the force'' and ``measure everything, then discard everything but the force'' are often different -- in particular, it might be possible to measure the force only before one has measured everything else. Thus the result yielded by the first procedure could be available before the result of the second. However, if the first is carried out in the course of carrying out the second, they both yield the same result in the end and so we treat them as equivalent. 

Measurement procedures are like functions without well-defined domains. Just like we can compose functions with other functions to create new functions, we can compose measurement procedures with functions to produce new measurement procedures.

\begin{definition}[Composition of functions with procedures]
Given a procedure $\proc{B}$ that takes values in some set $B$, and a function $f:B\to C$, define the ``composition'' $f\circ \proc{B}$ to be any procedure $\proc{C}$ that yields $f(x)$ whenever $\proc{B}$ yields $x$. We can construct such a procedure by describing the steps: first, do $\proc{B}$ and secondly, apply $f$ to the value yielded by $\proc{B}$.
\end{definition}

For example, $\proc{MA}$ is the composition of $h:(x,y)\mapsto xy$ with the procedure $(\proc{M},\proc{A})$ that yields the mass and acceleration of the same object. Measurement procedure composition is associative:

\begin{align}
    (g\circ f)\circ\proc{B}\text{ yields } x &\iff B\text{ yields } (g\circ f)^{-1}(x) \\
    &\iff B\text{ yields } f^{-1}(g^{-1}(x))\\
    &\iff f\circ B \text{ yields } g^{-1}(x)\\
    &\iff g\circ(f\circ B)\text{ yields } x
\end{align}


One might wonder whether there is also some kind of ``tensor product'' operation that takes a standalone $\proc{M}$ and a standalone $\proc{A}$ and returns a procedure $(\proc{M},\proc{A})$. Unlike function composition, this would be an operation that acts on two procedures rather than a procedure and a function. Thus this ``append'' combines real-world operations somehow, which might introduce additional requirements (we can't just measure mass and acceleration; we need to measure the mass and acceleration of the same object at the same time), and may be under-specified. For example, measuring a subatomic particle's position and momentum can be done separately, but if we wish to combine the two procedures then we can get different results depending on the order in which we combine them.

Our approach here is to suppose that there is some complete measurement procedure $\proc{S}$ to be modeled, which takes values in the observable sample space $(\Psi,\sigalg{E})$ and for all measurement procedures of interest there is some $f$ such that the procedure is equivalent to $f\circ \proc{S}$ for some $f$. In this manner, we assume that any problems that arise from a need to combine real world actions have already been solved in the course of defining $\proc{S}$.

Given that measurement processes are in practice finite precision and with finite range, $\Psi$ will generally be a finite set. We can therefore equip $\Psi$ with the collection of measurable sets given by the power set $\sigalg{E}:=\mathscr{P}(\Psi)$, and $(\Psi,\sigalg{E})$ is a standard measurable space. $\sigalg{E}$ stands for a complete collection of logical propositions we can generate that depend on the results yielded by the measurement procedure $\proc{S}$.

One could also consider measurement procedures to produce results in $(\mathbb{R},\mathcal{B}(\mathbb{R}))$ (i.e. the reals with the Borel sigma-algebra) or a set isomorphic to it. This choice is often made in practice, and following standard practice we also often consider variables to take values in sets isomorphic to $(\mathbb{R},\mathcal{B}(\mathbb{R}))$. However, for measurement in particular this seems to be a choice of convenience rather than necessity -- for any measurement with finite precision and range, it is possible to specify a finite set of possible results.

\subsection{Observable variables}

Our \emph{complete} procedure $\proc{S}$ represents a large collection of subprocedures of interest, each of which can be obtained by composition of some function with $\proc{S}$. We call the pair consisting of a subprocedure of interest $\proc{X}$ along with the variable $\RV{X}$ used to obtain it from $\proc{S}$ an \emph{observable variable}.

\begin{definition}[Observable variable]
Given a measurement procedure $\proc{S}$ taking values in $(\Psi,\sigalg{E})$, an observable variable is a pair $(\RV{X}\circ \proc{S},\RV{X})$ where $\RV{X}:(\Psi,\sigalg{E})\to (X,\sigalg{X})$ is a measurable function and $\proc{X}:=\RV{X}\circ \proc{S}$ is the measurement procedure induced by $\RV{X}$ and $\proc{S}$.
\end{definition}

For the model $\RV{F}=\RV{MA}$, for example, suppose we have a complete measurement procedure $\proc{S}$ that yields a triple (force, mass, acceleration) taking values in the sets $X$, $Y$, $Z$ respectively. Then we can define the ``force'' variable $(\proc{F},\RV{F})$ where $\proc{F}:=\RV{F}\circ \proc{S}$ and $\RV{F}:X\times Y\times Z\to X$ is the projection function onto $X$.

A measurement procedure yields a particular value when it is completed. We will call a proposition of the form ``$\proc{X}$ yields $x$'' an \emph{observation}. Note that $\proc{X}$ need not be a complete procedure here. Given the complete procedure $\proc{S}$, a variable $\RV{X}:\Psi\to X$ and the corresponding procedure $\proc{X}=\RV{X}\circ\proc{S}$, the proposition ``$\proc{X}$ yields $x$'' is equivalent to the proposition ``$\proc{S}$ yields a value in $\RV{X}^{-1}(x)$''. Because of this, we define the \emph{event} $\RV{X}\yields x$ to be the set $\RV{X}^{-1}(x)$.

\begin{definition}[Event]
Given the complete procedure $\proc{S}$ taking values in $\Psi$ and an observable variable $(\RV{X}\circ \proc{S},\RV{X})$ for $\RV{X}:\Psi\to X$, the \emph{event} $\RV{X}\yields x$ is the set $\RV{X}^{-1}(x)$ for any $x\in X$.
\end{definition}

If we are given an observation ``$\proc{X}$ yields $x$'', then the corresponding event $\RV{X}\yields x$ is \emph{compatible with this observation}.

It is common to use the symbol $=$ instead of $\bowtie$ to stand for ``yields'', but we want to avoid this because $\RV{Y}=y$ already has a meaning, namely that $\RV{Y}$ is a constant function everywhere equal to $y$.

An \emph{impossible event} is the empty set. If $\RV{X}\yields x=\emptyset$ this means that we have identified no possible outcomes of the measurement process $\proc{S}$ compatible with the observation ``$\proc{X}$ yields $x$''. 

\subsection{Model variables}

Observable variables are special in the sense that they are tied to a particular measurement procedure $\proc{S}$. However, the measurement procedure $\proc{S}$ does not enter into our mathematical reasoning; it guides our construction of a mathematical model, but once this is done mathematical reasoning proceeds entirely with mathematical objects like sets and functions, with no further reference to the measurement procedure.

A \emph{model variable} is simply a measurable function with domain $(\Psi,\sigalg{E})$.

Model variables do not have to be derived from observable variables. We may instead choose a sample space for our model $(\Omega,\sigalg{F})$ that does not correspond to the possible values that $\proc{S}$ might yield. In that case, we require a surjective model variable $\RV{S}:\Omega\to \Psi$ called the complete observable variable, and every observable variable $(\RV{X}'\circ \proc{S},\RV{X}')$ is associated with the model variable $\RV{X}:=\RV{X}'\circ \RV{S}$.

An \emph{unobserved variable} is a variable whose set of possible values is not constrained by the results of the measurement procedure.

\begin{definition}[Unobserved variable]\label{def:unobserved_variable}
Given a sample space $(\Omega,\sigalg{F})$ and a complete observable variable $\RV{S}:\Omega\to\Psi$, a model variable $\RV{Y}:\Omega\to Y$ is \emph{unobserved} if $\RV{Y}(\RV{S}\yields s)=Y$ for all $s\in \Psi$.
\end{definition}

\subsection{Variable sequences and partial order}

Given $\RV{Y}:\Omega\to X$, we can define a sequence of variables: $(\RV{X},\RV{Y}):=\omega\mapsto (\RV{X}(\omega),\RV{Y}(\omega))$. $(\RV{X},\RV{Y})$ has the property that $(\RV{X},\RV{Y})\yields (x,y)= \RV{X}\yields x\cap \RV{Y}\yields y$, which supports the interpretation of $(\RV{X},\RV{Y})$ as the values yielded by $\RV{X}$ and $\RV{Y}$ together.

Define the partial order on variables $\varlessthan$ where $\RV{X}\varlessthan \RV{Y}$ can be read ``$\RV{X}$ is completely determined by $\RV{Y}$''.

\begin{definition}[Variables determined by another variable]\label{def:variable_po}
Given a sample space $(\Omega,\sigalg{F})$ and variables $\RV{X}:\Omega\to X$, $\RV{Y}:\Omega\to Y$, $\RV{X}\varlessthan \RV{Y}$ if there is some $f:Y\to X$ such that $\RV{X}=f\circ \RV{Y}$.
\end{definition}

Clearly, $\RV{X}\varlessthan(\RV{X},\RV{Y})$ for any $\RV{X}$ and $\RV{Y}$.

\subsection{Decision procedures}\label{sec:actions}

The kind of problem we want to solve requires us to compare the consequences of different choices from a set of possibilities $C$. We take the \emph{consequences of} $\alpha\in C$ to refer to the values obtained by some measurement procedure $\proc{S}_\alpha$ associated with the choice $\alpha$.

As we have said, what exactly a ``measurement procedure'' is is a bit vague -- it's ``what we actually do to get the numbers we associate with variables''. It seems we could describe the above in terms of a single measurement procedure $\proc{S}$, which involves:

\begin{enumerate}
    \item Choose $\alpha$
    \item Proceed according to $\proc{S}_\alpha$
\end{enumerate}

However, $\proc{S}$ is problematic to model. The model is often part of the process of choosing $\alpha$, and so a model of $\proc{S}$ that involves the step ``choose $\alpha$'' will be self-referential. Because of this, we don't try to model $\proc{S}$, and whether this changes anything is an open question.

\begin{definition}[Decision procedure]
A decision procedure is a collection $\{\proc{S}_\alpha\}_{\alpha\in C}$ of measurement procedures.
\end{definition}

Like measurement procedures, a decision procedure $\{\proc{S}_\alpha\}_{\alpha\in A}$ isn't a well-defined mathematical object; it's not really a ``set'', because the contents are real-world actions.

\section{How should consequences be represented in decision problems?}\label{sec:how_represent_conseqeunces}

Decision problems require certain types of additional knowledge, and see-do models or forecasts can represent knowledge of this type. We can ask if and when it is a sound choice to use these kinds of models for this purpose, or even whether reasonable postulates of rationality demand the use of see-do models for certain types of problem.

In fact, we won't tackle the question of whether rational deliberation demands the use of see-do models here, though we will briefly survey some of the attempts at this question that have already been undertaken. Though we do not present an axiomatic justification of see-do models, we will argue that they build on well-established tools for inference from data and decision making under uncertainty:
\begin{itemize}
    \item In this chapter, we will show that see-do forecasts can be derived from representing the problem of choosing \emph{inference functions} as the problem of choosing between lotteries, a principle that has been widely used to formalize problems of choice under uncertainty since .... Pascal/Bernoulli? ... and continues to be a standard approach today \todo{sort this out, obviously}
    \item Also in this chapter, we will show that see-do models are closely related to \emph{statistical decision theory}, as introduced by \citet{wald_statistical_1950}, which laid the foundations of statistical learning theory \citep{vapnik_nature_2013} which is a standard approach to studying the theory of machine learning today
    \item In the Chapter \ref{ch:ints_counterfactuals}, we will show that there is a formal identity between interventional models and see-do models and, furthermore, there is also a formal identity between potential outcomes models and a subset of see-do models
\end{itemize} 

See-do models follow from the application of tried and true approaches to decision making to problems of causal inference, and subsume standard approaches to causal inference. This is a good reason to think that they are interesting to understand even without an axiomatic foundation.

Nevertheless, there is a substantial body of literature on the question of axiomatic foundations for knowledge representation in decision problems. Most ``representation theorems'' in this literature aim to show that a particular means of representing knowledge is \emph{necessary} given a number set of assumptions that purport to define ``rational preferences'' or ``rational beliefs''. The question of whether a given set of axioms are in fact required to call an agents preferences or beliefs rational is a difficult one, and we will not have anything to add to that discussion here -- neither by commenting on axioms already proposed or by proposing our own. However, we are still interested in comparing see-do models with model types used in existing representation theorems. Understanding the relationship between the various model types proposed in this literature and see-do models can be illuminating, even without critiquing the systems of axioms proposed (which are in many cases quite complicated).

The following discussion will often make reference to \emph{preference relations}. A preference relation is a relation $\succ,\prec,\sim$ on a set $A$ such that for any $a,a'$ in $A$ we have:
\begin{itemize}
    \item Exactly one of $a\succ a'$, $a\prec a'$, $a\sim a'$ holds
    \item $(a\succ a')\iff(a'\prec a)$
    \item $a\succ a'$ and $a'\succ a''$ implies $a\succ a''$
\end{itemize}

This definition is meant to correspond to the common sense idea of having preferences over some large set of things, where $\succ$ can be read as ``strictly better than'', $\prec$ read as ``strictly worse than'' and $\sim$ read as ``as good as''. Given any two things from the set, I can say which one I prefer, or if I prefer neither (and all of these are mutually exclusive). If I prefer $a$ to $a'$ then I think $a'$ is worse than $a$. Furthermore, if I prefer $a$ to $a'$ and $a'$ to $a''$ then I prefer $a$ to $a''$.

\subsection{von Neumann-Morgenstern utility}

\citet{von_neumann_theory_1944} proved that when the \emph{vNM axioms} hold (not defined here; see the original reference or \citet{steele_decision_2020}), an agent's preferences between ``lotteries'' (which for our purposes are probability distributions in $\Delta(\sigalg{Y})$) can be represented with a utility function $u:Y\to \mathbb{R}$ unique up to affine transformation along with the principle of expected utility, which is, for lotteries $\prob{P},\prob{P}'\in\Delta(\sigalg{Y})$ the rule that $\mathbb{E}_{\prob{P}}[u]> \mathbb{E}_{\prob{P}'}[u]$ is equivalent to the statement $\prob{P} \succ \prob{P}'$.

If we consider a set of decisions $D$ and a do-forecast $\kernel{T}^{\RV{Y}|\RV{{D}}}$, then we can identify the set of lotteries with the evaluations of each decision under $\kernel{T}$ -- that is, the lotteries are $\{\kernel{T}^{\RV{Y}|\RV{D}}_d|d\in D\}$. We can then define preferences over decisions: $d$ is strictly preferred to $d'$ if and only if $\kernel{T}_{d}^{\RV{Y}|\RV{D}}$ is strictly preferred to $\kernel{T}_{d'}^{\RV{Y}|\RV{D}}$. Then, if preferences over the set of lotteries satisfies the \emph{vNM axioms}, there exists a utility function $u:Y\to \mathbb{R}$ such that
\begin{itemize}
    \item $d\succ d'$ if and only if $\kernel{T}_d^{\RV{Y}|\RV{D}}u>\kernel{T}_{d'}^{\RV{Y}|\RV{D}}u$
\end{itemize}

\todo[inline]{Is this worth stating as a theorem?}

\subsection{Savage's decision theory}

The von Neumann-Morgenstern theorem shows that, if we take do-forecasts for granted and assuming the vNM axioms then we can represent preferences between decisions using expected utility. We are more interested in when we should use do-forecasts or see-do forecasts or see-do models. 

\citet{savage_foundations_1954} proved a representation theorem for decision problems, which has more overlap with our original question - namely, what should we use to represent the knowledge we bring to a kind of idealised decision problem. Savage's decision problems featured \emph{states} $H$ (which we can be identified with hypotheses in Definition \ref{def:2p_stat}), \emph{acts} $D$ (which can be identified with decisions in \ref{def:2p_stat}) and \emph{outcomes} $O$ (which can be identified with outcomes in Definition \ref{def:2p_stat}). Savage takes it as a given that we have a deterministic two player statistical model $(\kernel{T},\RV{H},\RV{D},\RV{O})$, and furthermore for \emph{any} function $f:H\to O$ there exists some $d\in D$ such that $\kernel{T}_{h,d} = \delta_{f(h)}$, and furthermore we have a preference relation on $D$. He then shows that if the \emph{Savage axioms} hold for the preference relation on $D$ then there exists a unique probability measure $\prob{P}\in \Delta(\sigalg{H})$ and a utility $u:O\to \mathbb{R}$ unique up to affine transformation such that

\begin{align}
(d\succ d')\iff (\prob{P}\otimes \delta_d)\kernel{T} u > (\prob{P}\otimes \delta_{d'})\kernel{T} u\label{eq:savage_utility}
\end{align}

The use of two player statistical models is baked into Savage's representation theorem as a basic assumption. However, he also shows that if the Savage axioms hold for preferences among outcomes, then it is possible to define a unique do-forecast

\begin{align}
    \kernel{F}:=(\prob{P}\otimes \mathrm{Id}_D)\kernel{T}
\end{align}

(using the existing definitions for $\RV{D}$ and $\RV{O}$) such that

\begin{align}
    (d\succ d')\iff \kernel{F}_d\kernel{T} u > \kernel{F}_d u\label{eq:savage_utility2}
\end{align}

\todo[inline]{Is this worth stating as a theorem?}

\subsection{Jeffrey's decision theory}

The approach to decision making set out by \citet{jeffrey_logic_1990} and \citet{bolker_functions_1966} differs from Savage's approach in a number of ways. Firstly, the representation theorem proved by Bolker does not distinguish between hypotheses, outcomes and decisions. It assumes only an algebra of outcomes $(O,\sigalg{O})$ and a preference relation over these outcomes. If the preference relation satisfies the \emph{Jeffrey axioms} then there exists a utility $u:O\to\mathbb{R}$ and a probability distribution $\prob{P}\in \Delta(\sigalg{O})$ which are non-unique such that for $A,B\in \sigalg{O}$ and finite partition $C_1,...,C_n\in \sigalg{O}$ (the proof is given by \citet{bolker_functions_1966}):

\begin{align}
    (A \succ B) \iff (\prob{P}^{\RV{O}|\mathds{1}_A}_1u>\prob{P}^{\RV{O}|\mathds{1}_B}_1u) \label{eq:ev_dec_theory}
\end{align}

A key feature to note here is that instead of comparing two decisions by evaluating one Markov kernel at two different points (as in our theory and Savage's), Jeffrey compares two events by comparing two different Markov kernels $\prob{P}^{\RV{O}|\mathds{1}_A}$ and $\prob{P}^{\RV{O}|\mathds{1}_B}$.  In order to use such a model in a decision problem, Jeffrey suggests we can identify a subset of the events with the choices we can make: $D:=\{D_i\in \sigalg{O}|i\in A\}$ for some set $A$, so we get one Markov kernel for each decision we might make. 

Whether we should model decisions as a set $D$ and consequence maps as Markov kernels with $D$ as the domain or as a collection of events and consequence maps as a corresponding collection of Markov kernels, or something else is a somewhat subtle issue that I don't fully understand.

I think Jeffrey goes too far in saying decisions are events. Consider the problem of deciding whether or not to order a drink, and suppose that I choose to consider a set of options $D=\{d_1=\text{``order a drink and make sure that drink is water''}, d_2=\text{``order a drink''}, d_3=\text{``don't order a drink''}\}$. If I choose $d_1$, then it will certainly be true that I will order a drink and also true that I will order a drink of water, and in general an event ``A and B'' implies an event ``A''. However, $d_1$ and $d_2$ are distinct -- if I particularly want to drink water then I will choose $d_1$ and not $d_2$. This feature of decision making is reflected in our idealised decision problem at the start where we may make a single decision from our set of available choices.

At the same time, it does seem that in the same way that events leave many features of the outcome underspecified, when we describe choices we very often leave features of what we precisely intend to do underspecified. Choices aren't necessarily underspecified: imagine a reinforcement learner in a simulated discrete time environment. At each timestep this learner takes its history so far and outputs some action and, if we examine the code governing how it interacts with its environment we can probably deduce \emph{all} the actions available to it at each timestep. In less controlled contexts, which is really most contexts we are familiar with, if we want to come up with a list of decisions we could make then all the decisions in this list are likely to leave a large number of things that we could in principle decide on underspecified. In the example above, it seems I could decide to:
\begin{itemize}
    \item Order a drink
    \item Order a glass of water
    \item Order a glass of water and say please and thankyou when I do
    \item Order a glass of water, say please and thankyou, carefully avoid scratching my itchy ear
\end{itemize}

And so forth. I could go on adding details for a very long time without exhausting the set of things I could in principle decide to do. I usually would not want to consider all these ``in principle'' choices I have available. Many of the details are more or less irrelevant and not worth spending the time to think about. Furthermore, even if I do consider all choices I have available in principle, it seems plausible that a less specific decision could be preferable. For example I might expect the ``automatic execution'' of some things things I didn't fully specify to be closer to optimal than the best guess I have of how to specify them (consider managing a competent employee; one might get better results by being less specific in requests).

I think the question of how one could structure the decisions set $D$ is an interesting one. I have so far treated it as ``just a set'' with no particular additional structure. Jeffrey suggests that decisions are a collection of events, and decisions do seem to have some event-like features, like the fact that they specify incompletely what I will do next and the fact that different specifications can apparently be joined with ``and''. However, the proposition that decisions are identical to events does not seem quite right because, as in the example above, deciding to do ``A and B'' does not imply deciding to do ``A''.

I think this is also potentially an important question. In Chapter \ref{ch:inferring_causes} I discuss the assumption of \emph{imitability}, that it is within a decision maker's power to reproduce the observed data. Such an assumption, if it holds, licences a number of inferences from data to consequences. It may often be a plausible assumption considering the full set of decisions $D^*$ that a decision maker could make \emph{in principle}, but may be much less often plausible when considering the restricted set of decisions $D$ a decision maker is actually considering. Understanding how these may relate to one another may help to better understand assumptions like imitability.