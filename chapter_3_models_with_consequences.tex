
%!TEX root = main.tex

\chapter{Models with consequences}\label{ch:2p_statmodels}

In chapter \ref{ch:tech_prereq} we introduced a collection of tools for constructing probabilistic models. Tools by themselves aren't enough to construct models: we also need a purpose. \emph{Why are we constructing probabilistic models}? Our provisional answer to this question is: we construct probabilistic models to help us compare different choices we could make on the basis of the outcomes we anticipate that they will bring about. We aren't claiming that this is the only reason to create a probabilistic model, but comparing different choices is an important job that lots of people regularly need to undertake, and it is a job for which causal models are often employed.

If we are going to buld models that facilitate the comparison of different choices on the basis of their consequences, then they must have a few properties:

\begin{itemize}
    \item We want a set of choices $C$ with at least a partial order on it, because this is how we mathematically represent ``comparison''
    \item By supposition, we don't have a partial order on $C$ at the outset, otherwise our problem is already solved
    \item However, we do have a partial order on \emph{consequences}, which are elements of some set $F$
    \item In order to use the partial order on $F$ to induce a partial order on $C$, we need to associate one or more elements of $F$ with each element of $C$
\end{itemize}

At a very general level, a model that helps us to compare choices is a set-valued function $C\to \mathscr{P}(F)$. In order to say more about ``models for mathematically-assisted decisions'', we need to make some more specific assumptions about how these models are constructed - for example, we might take the set of consequences $F$ to be a set of probability distributions on a common sample space. However, even before we do this, the requirement for a domain of choices $C$ arises purely because we want to compare things mathematically.

In practice, we will take the consequences $F$ to be a set of probability distributions on a common sample space. There is a large literature on whether this is always an appropriate way to represent consequences, and we will address a small slice of it in Section \ref{sec:how_represent_conseqeunces}, but arguing that this is always a reasonable choice is not our objective here. Rather, we observe that there is a long history of using probability theory for the purpose of representing uncertain knowledge, and probability theory in conjunction with the principle of expected utility is often used to compare the desirability of uncertain outcomes.

While it doesn't often enter into the reasoning we present in the upcoming chapters, we spend some time here considering how probabilistic models are related to the ``real world phenomena'' that they are supposed to be models of. In particular, \emph{variables} are ``points of attachment'' between the model and the real world -- they point to the parts of the real world that correspond to features of the model. There are many standard accounts of how variables accomplish this in regular statistical models, but we need to elaborate on this account slightly to account for the inclusion of a domain $C$ of choices that are each associated with their own consequences. In Section \ref{sec:variable}, we present both the standard account of variables and how we propose to elaborate this account to include a domain of choices.

\subsection{TL;DR}

We model the consequences of a set of choices $C$, and we do this with a map $\prob{P}_\cdot:C\kto \Omega$ for some sample space $\Omega$. The range of this map $\prob{P}_C:=\{\prob{P}_\alpha|\alpha\in C\}$ is a probability set.

In Chapter \ref{ch:evaluating_decisions}, we consider maps from the choice set $C$ to unique probability measures. A straightforward generalisation of this scheme is to map elements of $C$ to \emph{probability sets} rather than unique probability measures. We can still construct a probability set from the range of $\prob{P}_\cdot$, in this case it is the union $\prob{P}_C=\cup_{\alpha\in C} \prob{P}_\alpha$. In Chapter \ref{ch:sdt}, we consider this generalisation.


\section{Variables}\label{sec:variable}

In probability theory, it is standard to assume the existence of a probability space $(\mu,\Omega,\sigalg{F})$ and to define \emph{random variables} as measurable functions from $(\Omega,\sigalg{F})$ to $(\mathbb{R},\mathcal{B}(\mathbb{R}))$. However, variables aren't \emph{just} functions -- they're also typically understood to correspond to some measured aspect of the real world. For example, \citet{pearl_causality:_2009} offers the following two, purportedly equivalent, definitions of variables:
\begin{quote}
By a \emph{variable} we will mean an attribute, measurement or inquiry that may take on one of several possible outcomes, or values, from a specified domain. If we have beliefs (i.e., probabilities) attached to the possible values that a variable may attain, we will call that variable a random variable.
\end{quote}

\begin{quote}
This is a minor generalization of the textbook definition, according to which a random variable is a mapping from the sample space (e.g., the set of elementary events) to the real line. In our definition, the mapping is from the sample space to any set of objects called ``values,'' which may or may not be ordered.
\end{quote}

However, these quotes are describing different things -- in fact, they're not even describing the same \emph{kind} of thing. The first is talking about a \emph{measurement}, which is something we can do in the real world that produces as a result an element of a mathematical set. The second is talking about a \emph{function}, which is a purely mathematical object with a domain and a codomain and a mapping from the former into the latter.

The way we address this distinction is: a procedure that takes place in the real world and yields as results elements of a mathematical set is called a \emph{measurement procedure}. We suppose for a given problem that there is a ``complete measurement procedure'' $\proc{S}$, and the result of every specific measurement procedure of interest can be reconstructed from the result of the complete measurement procedure by applying a function to the latter result. The function $\RV{X}$ that yields the result of a specific measurement procedure $\proc{X}$ given the result of the complete measurement procedure $\proc{S}$ is the \emph{variable} associated with the measurement procedure $\proc{X}$.

In this way, the variable $\RV{X}$ -- which is by itself just a mathematical function -- is made relevant to the real-world by combining it with a total measurement procedure $\proc{S}$.

We can even use this scheme to address situations where it is possible to make different choices. We can simply posit a sub-procedure $\proc{C}$ that yields the choice $\alpha$ that we eventually make. However, modelling this can be tricky. If we want to use the consequence model to help make the decision, then it seems that the model of the decision procedure $\proc{C}$ will need to be self-referential. Furthermore, even if we have a model of $\proc{C}$ that says we will certainly decide on a particular element $\alpha^*$, we still need to map every element of $C$ to a consequence because this is what enables the comparison of elements of C. Thus, modelling $\proc{C}$ makes the model more complicated, and it's not obvious that this is answering the question that we are interested in. This complication is not obviously intractable, but we do not address it here. Instead, we simply assume that we have a collection $\proc{S}_\alpha$ of total measurement procedures that all yield elements of the same set, and each of which are executed if the choice made is $\alpha$.

\subsection{Variables and measurement procedures}

We illustrate this approach with the example of Newton's second law in the form $\RV{F}=\RV{MA}$. This model relates ``variables'' $\RV{F}$, $\RV{M}$ and $\RV{A}$. As \citet{feynman_feynman_1979} noted, in order to understand this law, we must bring some pre-existing understanding of force, mass and acceleration independent of the law itself. Furthermore, we contend, this knowledge cannot be expressed in any purely mathematical statement. In order to say what the net force on a given object is, even a highly knowledgeable physicist will have to go and do some measurements, which is a procedure that they carry out involving interacting with the real world somehow and obtaining as a result a vector representing the net forces on that object.

That is, the variables $\RV{F}$, $\RV{M}$ and $\RV{A}$ are referring to the \emph{results of measurement procedures}. We will introduce a separate notation to refer to these measurement procedures -- $\proc{F}$ is the procedure for measuring force, $\proc{M}$ and $\proc{A}$ for mass and acceleration respectively. A measurement procedure $\proc{F}$ is akin to \citet{menger_random_2003}'s notion of variables as ``consistent classes of quantities'' that consist of pairing between real-world objects and quantities of some type. Force $\proc{F}$ itself is not a well-defined mathematical thing, as measurement procedures are not mathematically well-defined. At the same time, the set of values it may yield \emph{are} well-defined mathematical things. No actual procedure can be guaranteed to return elements of a mathematical set known in advance -- anything can fail -- but we assume that we can study procedures reliable enough that we don't lose much by making this assumption.

Note that, because $\proc{F}$ is not a purely mathematical thing, we cannot perform mathematical reasoning with $\proc{F}$ directly. Rather, we introduce a variable $\RV{F}$ which, as we will see, is a well-defined mathematical object, assert that it corresponds to $\proc{F}$ and conduct our reasoning using $\RV{F}$.

\subsection{Measurement procedures}\label{sec:mprocs}

\begin{definition}[Measurement procedure]
A \emph{measurement procedure} $\proc{B}$ is a procedure that involves interacting with the real world somehow and delivering an element of a mathematical set $X$ as a result. A procedure $\proc{B}$ is said to takes values in a set $B$.
\end{definition}

We adopt the convention that the procedure name $\proc{B}$ and the set of values $B$ share the same letter.

\begin{definition}[Values yielded by procedures]
$\proc{B}\yields x$ is the proposition that the the procedure $\proc{B}$ will yield the value $x\in X$. $\proc{B}\yields A$ for $A\subset X$ is the proposition $\lor_{x\in A} \proc{B}\yields x$.
\end{definition}

\begin{definition}[Equivalence of procedures]\label{def:equality}
Two procedures $\proc{B}$ and $\proc{C}$ are equal if they both take values in $X$ and $\proc{B}\yields x\iff \proc{C}\yields x$ for all $x\in X$.
\end{definition}

If two involve different measurement actions in the real world but necessarily yield the same result, we say they are equivalent.

It is worth noting that this notion of equivalence identifies procedures with different real-world actions. For example, ``measure the force'' and ``measure everything, then discard everything but the force'' are often different -- in particular, it might be possible to measure the force only before one has measured everything else. Thus the result yielded by the first procedure could be available before the result of the second. However, if the first is carried out in the course of carrying out the second, they both yield the same result in the end and so we treat them as equivalent. 

Measurement procedures are like functions without well-defined domains. Just like we can compose functions with other functions to create new functions, we can compose measurement procedures with functions to produce new measurement procedures.

\begin{definition}[Composition of functions with procedures]
Given a procedure $\proc{B}$ that takes values in some set $B$, and a function $f:B\to C$, define the ``composition'' $f\circ \proc{B}$ to be any procedure $\proc{C}$ that yields $f(x)$ whenever $\proc{B}$ yields $x$. We can construct such a procedure by describing the steps: first, do $\proc{B}$ and secondly, apply $f$ to the value yielded by $\proc{B}$.
\end{definition}

For example, $\proc{MA}$ is the composition of $h:(x,y)\mapsto xy$ with the procedure $(\proc{M},\proc{A})$ that yields the mass and acceleration of the same object. Measurement procedure composition is associative:

\begin{align}
    (g\circ f)\circ\proc{B}\text{ yields } x &\iff B\text{ yields } (g\circ f)^{-1}(x) \\
    &\iff B\text{ yields } f^{-1}(g^{-1}(x))\\
    &\iff f\circ B \text{ yields } g^{-1}(x)\\
    &\iff g\circ(f\circ B)\text{ yields } x
\end{align}


One might wonder whether there is also some kind of ``tensor product'' operation that takes a standalone $\proc{M}$ and a standalone $\proc{A}$ and returns a procedure $(\proc{M},\proc{A})$. Unlike function composition, this would be an operation that acts on two procedures rather than a procedure and a function. Thus this ``append'' combines real-world operations somehow, which might introduce additional requirements (we can't just measure mass and acceleration; we need to measure the mass and acceleration of the same object at the same time), and may be under-specified. For example, measuring a subatomic particle's position and momentum can be done separately, but if we wish to combine the two procedures then we can get different results depending on the order in which we combine them.

Our approach here is to suppose that there is some complete measurement procedure $\proc{S}$ to be modeled, which takes values in the observable sample space $(\Psi,\sigalg{E})$ and for all measurement procedures of interest there is some $f$ such that the procedure is equivalent to $f\circ \proc{S}$ for some $f$. In this manner, we assume that any problems that arise from a need to combine real world actions have already been solved in the course of defining $\proc{S}$.

Given that measurement processes are in practice finite precision and with finite range, $\Psi$ will generally be a finite set. We can therefore equip $\Psi$ with the collection of measurable sets given by the power set $\sigalg{E}:=\mathscr{P}(\Psi)$, and $(\Psi,\sigalg{E})$ is a standard measurable space. $\sigalg{E}$ stands for a complete collection of logical propositions we can generate that depend on the results yielded by the measurement procedure $\proc{S}$.

One could also consider measurement procedures to produce results in $(\mathbb{R},\mathcal{B}(\mathbb{R}))$ (i.e. the reals with the Borel sigma-algebra) or a set isomorphic to it. This choice is often made in practice, and following standard practice we also often consider variables to take values in sets isomorphic to $(\mathbb{R},\mathcal{B}(\mathbb{R}))$. However, for measurement in particular this seems to be a choice of convenience rather than necessity -- for any measurement with finite precision and range, it is possible to specify a finite set of possible results.

\subsection{Observable variables}

Our \emph{complete} procedure $\proc{S}$ represents a large collection of subprocedures of interest, each of which can be obtained by composition of some function with $\proc{S}$. We call the pair consisting of a subprocedure of interest $\proc{X}$ along with the variable $\RV{X}$ used to obtain it from $\proc{S}$ an \emph{observable variable}.

\begin{definition}[Observable variable]
Given a measurement procedure $\proc{S}$ taking values in $(\Psi,\sigalg{E})$, an observable variable is a pair $(\RV{X}\circ \proc{S},\RV{X})$ where $\RV{X}:(\Psi,\sigalg{E})\to (X,\sigalg{X})$ is a measurable function and $\proc{X}:=\RV{X}\circ \proc{S}$ is the measurement procedure induced by $\RV{X}$ and $\proc{S}$.
\end{definition}

For the model $\RV{F}=\RV{MA}$, for example, suppose we have a complete measurement procedure $\proc{S}$ that yields a triple (force, mass, acceleration) taking values in the sets $X$, $Y$, $Z$ respectively. Then we can define the ``force'' variable $(\proc{F},\RV{F})$ where $\proc{F}:=\RV{F}\circ \proc{S}$ and $\RV{F}:X\times Y\times Z\to X$ is the projection function onto $X$.

A measurement procedure yields a particular value when it is completed. We will call a proposition of the form ``$\proc{X}$ yields $x$'' an \emph{observation}. Note that $\proc{X}$ need not be a complete procedure here. Given the complete procedure $\proc{S}$, a variable $\RV{X}:\Psi\to X$ and the corresponding procedure $\proc{X}=\RV{X}\circ\proc{S}$, the proposition ``$\proc{X}$ yields $x$'' is equivalent to the proposition ``$\proc{S}$ yields a value in $\RV{X}^{-1}(x)$''. Because of this, we define the \emph{event} $\RV{X}\yields x$ to be the set $\RV{X}^{-1}(x)$.

\begin{definition}[Event]
Given the complete procedure $\proc{S}$ taking values in $\Psi$ and an observable variable $(\RV{X}\circ \proc{S},\RV{X})$ for $\RV{X}:\Psi\to X$, the \emph{event} $\RV{X}\yields x$ is the set $\RV{X}^{-1}(x)$ for any $x\in X$.
\end{definition}

If we are given an observation ``$\proc{X}$ yields $x$'', then the corresponding event $\RV{X}\yields x$ is \emph{compatible with this observation}.

It is common to use the symbol $=$ instead of $\bowtie$ to stand for ``yields'', but we want to avoid this because $\RV{Y}=y$ already has a meaning, namely that $\RV{Y}$ is a constant function everywhere equal to $y$.

An \emph{impossible event} is the empty set. If $\RV{X}\yields x=\emptyset$ this means that we have identified no possible outcomes of the measurement process $\proc{S}$ compatible with the observation ``$\proc{X}$ yields $x$''. 

\subsection{Model variables}

Observable variables are special in the sense that they are tied to a particular measurement procedure $\proc{S}$. However, the measurement procedure $\proc{S}$ does not enter into our mathematical reasoning; it guides our construction of a mathematical model, but once this is done mathematical reasoning proceeds entirely with mathematical objects like sets and functions, with no further reference to the measurement procedure.

A \emph{model variable} is simply a measurable function with domain $(\Psi,\sigalg{E})$.

Model variables do not have to be derived from observable variables. We may instead choose a sample space for our model $(\Omega,\sigalg{F})$ that does not correspond to the possible values that $\proc{S}$ might yield. In that case, we require a surjective model variable $\RV{S}:\Omega\to \Psi$ called the complete observable variable, and every observable variable $(\RV{X}'\circ \proc{S},\RV{X}')$ is associated with the model variable $\RV{X}:=\RV{X}'\circ \RV{S}$.

An \emph{unobserved variable} is a variable whose set of possible values is not constrained by the results of the measurement procedure.

\begin{definition}[Unobserved variable]\label{def:unobserved_variable}
Given a sample space $(\Omega,\sigalg{F})$ and a complete observable variable $\RV{S}:\Omega\to\Psi$, a model variable $\RV{Y}:\Omega\to Y$ is \emph{unobserved} if $\RV{Y}(\RV{S}\yields s)=Y$ for all $s\in \Psi$.
\end{definition}

\subsection{Variable sequences and partial order}

Given $\RV{Y}:\Omega\to X$, we can define a sequence of variables: $(\RV{X},\RV{Y}):=\omega\mapsto (\RV{X}(\omega),\RV{Y}(\omega))$. $(\RV{X},\RV{Y})$ has the property that $(\RV{X},\RV{Y})\yields (x,y)= \RV{X}\yields x\cap \RV{Y}\yields y$, which supports the interpretation of $(\RV{X},\RV{Y})$ as the values yielded by $\RV{X}$ and $\RV{Y}$ together.

Define the partial order on variables $\varlessthan$ where $\RV{X}\varlessthan \RV{Y}$ can be read ``$\RV{X}$ is completely determined by $\RV{Y}$''.

\begin{definition}[Variables determined by another variable]\label{def:variable_po}
Given a sample space $(\Omega,\sigalg{F})$ and variables $\RV{X}:\Omega\to X$, $\RV{Y}:\Omega\to Y$, $\RV{X}\varlessthan \RV{Y}$ if there is some $f:Y\to X$ such that $\RV{X}=f\circ \RV{Y}$.
\end{definition}

Clearly, $\RV{X}\varlessthan(\RV{X},\RV{Y})$ for any $\RV{X}$ and $\RV{Y}$.

\subsection{Decision procedures}\label{sec:actions}

The kind of problem we want to solve requires us to compare the consequences of different choices from a set of possibilities $C$. We take the \emph{consequences of} $\alpha\in C$ to refer to the values obtained by some measurement procedure $\proc{S}_\alpha$ associated with the choice $\alpha$.

As we have said, what exactly a ``measurement procedure'' is is a bit vague -- it's ``what we actually do to get the numbers we associate with variables''. It seems we could describe the above in terms of a single measurement procedure $\proc{S}$, which involves:

\begin{enumerate}
    \item Choose $\alpha$
    \item Proceed according to $\proc{S}_\alpha$
\end{enumerate}

However, $\proc{S}$ is problematic to model. The model is often part of the process of choosing $\alpha$, and so a model of $\proc{S}$ that involves the step ``choose $\alpha$'' will be self-referential. Because of this, we don't try to model $\proc{S}$, and whether this changes anything is an open question.

\begin{definition}[Decision procedure]
A decision procedure is a collection $\{\proc{S}_\alpha\}_{\alpha\in C}$ of measurement procedures.
\end{definition}

Like measurement procedures, a decision procedure $\{\proc{S}_\alpha\}_{\alpha\in A}$ isn't a well-defined mathematical object; it's not really a ``set'', because the contents are real-world actions.

\section{Representation theorems}\label{sec:how_represent_conseqeunces}

We have chosen to represent decision problems with probability functions $C\kto \Omega$ for some sample space $(\Omega,\sigalg{F})$ of ``consequences''. We haven't offered a deep justification for this choice. One question we might be inclined to ask is: what are we missing out on by making this choice? Representation theorems can help us to answer this question. A representation theorem proposes a collection of axioms regulating the kinds of preferences or beliefs we want our models to admit, and demonstrates that certain families of models can represent all preferences or beliefs consistent with these axioms. In particular, the representation theorems of Savage and Jeffreys both show that preferences among choices that satisfy their respective axiomatisations can be modeled with probability sets and utilities. Thus the question of what we're missing out on can be given the answer: we're missing out on the representation of preferences that satisfy neither set of axioms.

The following discussion will often make reference to \emph{complete preference relations}. A preference relation is a relation $\succ,\prec,\sim$ on a set $A$ such that for any $a,a'$ in $A$ we have:
\begin{itemize}
    \item Exactly one of $a\succ a'$, $a\prec a'$, $a\sim a'$ holds
    \item $(a\succ a')\iff(a'\prec a)$
    \item $a\succ a'$ and $a'\succ a''$ implies $a\succ a''$
\end{itemize}

This definition is meant to correspond to the common sense idea of having preferences over some set of things, where $\succ$ can be read as ``strictly better than'', $\prec$ read as ``strictly worse than'' and $\sim$ read as ``as good as''. Given any two things from the set, I can say which one I prefer, or if I prefer neither (and all of these are mutually exclusive). If I prefer $a$ to $a'$ then I think $a'$ is worse than $a$. Furthermore, if I prefer $a$ to $a'$ and $a'$ to $a''$ then I prefer $a$ to $a''$.

\subsection{von Neumann-Morgenstern utility}

\citet{von_neumann_theory_1944} proved that when the \emph{vNM axioms} hold (not defined here; see the original reference or \citet{steele_decision_2020}), an agent's preferences between ``lotteries'' (which for our purposes are probability distributions in $\Delta(\Omega)$) can be represented as the comparison of the expected value under each lottery of a utility function $u$ unique up to affine transformation. That is, for lotteries $\prob{P}_\alpha$ and $\prob{P}_{\alpha'}$, there exists some $u:\Omega\to \mathbb{R}$ unique up to affine transformation such that $\mathbb{E}_{\prob{P}_\alpha}[u]> \mathbb{E}_{\prob{P}_{\alpha'}}[u]$ if and only if $\prob{P}_{\alpha} \succ \prob{P}_{\alpha'}$.

In the proposed setup, the set of lotteries forms a probability set -- what we call ``consequences''. The von Neumann-Morgenstern theorem establishes that preferences between consequences satisfy the vNM axioms, then these preferences can be represented as the expectation of some utility function. It does not provide reason to represent consequences with probability distributions in the first place, which is the main question we're interested in here.

\subsection{Savage's decision theory}

Unlike con Neumann-Morgenstern's theory, \citet{savage_foundations_1954} decision theory doesn't assume that probability models should be used to represent uncertainty over consequences. Instead, Savage decision theory posits as basic elements a measurable set of \emph{states} $(S,\sigalg{S})$, and sets of \emph{acts} $C$ (equivalent to choices in our language) and \emph{consequences} $F$. Furthermore, it is taken as a given that a known map $T:S\times C\to F$ is available. The preference relation $\succ$ is over acts $C$, and if the preference relation along with the map $T$ satisfies the \emph{savage axioms} (not given here, see \citet{savage_foundations_1954} or \citet{steele_decision_2020}), then there exists a unique probability function $\prob{P}\in \Delta(S)$ and a utility $u:F\to \mathbb{R}$ unique up to affine transformation such that
\begin{align}
    \alpha\succ \alpha' \iff \int_S u(T(s,\alpha))\prob{P}(\mathrm{d}s) > \int_S u(T(s,\alpha'))\prob{P}(\mathrm{d}s)
\end{align}

Note that if we equip acts and consequences with measures $(C,\sigalg{C})$ and $(F,\sigalg{F})$, $T$ and $\prob{P}$ together induce the probability function $\prob{Q}_\cdot:C\kto F$ given by
\begin{align}
    \prob{Q}_\alpha &:= \int_S T(s,\alpha)\prob{P}(\mathrm{d}s)\\
                    &=\prob{P}^{\RV{T}_\alpha}
\end{align}
where $\RV{T}_\alpha:S\to F$ is the function $s\mapsto T(s,\alpha)$. If so, we can then re-state Savage's criterion for preferences as
\begin{align}
    \alpha\succ \alpha' \iff \prob{Q}_\alpha u > \prob{Q}_{\alpha'} u
\end{align}
 If $\prob{Q}$ depends measurably on $S$, then it is a Markov kernel.

Thus Savage's theorem establishes that the relationship between acts and consequences can be represented by a probability function, and furthermore this probability function is unique. 

Savage's first axiom is:
\begin{itemize}
    \item P1: There is a complete preference relation $\succ,\prec,\sim$ on $C$
\end{itemize}

Our initial proposition was that the consequences $F$ are a set of things we know how to rank by preference, and acts $C$ are the things we want to rank. If we suppose that a complete preference relation on $F$, a complete preference relation on $C$ does not follow. P1 can be viewed as the requirement that the mathematical model offers a ``full solution'' to the decision problem at hand -- it provides us with a full ordering on the choices $C$, and (supposing a finite set) an unambiguous subset of choices that are all equally optimal. 

Without P1, we may propose models that yield a collection of incomparable choices and choices between them must be made according to some informal principle. On the other hand, with P1 we may end up having to add additional constraints to our model which have informal motivations themselves. That is, P1 makes model construction more demanding while making decisions easier.

Savage seems to take states $S$ and consequences $F$ as basic, while defining acts $C$ to be the set of all functions from states to consequences. However, it seems to be consistent with Savage's theory to consider acts $C$ and consequences $F$ as basic (as we do), and defining states to be the set of all functions from acts to consequences. In this view, the states can be identified with the extreme points of the set of all models we can consider.

A detailed consideration of Savage's axiomatisation is out of the scope of this work.

\subsection{Jeffrey's decision theory}

Jeffrey's decision theory is an alternative to Savage's that offers a slightly different axiomatisation. It is set out in \citet{jeffrey_logic_1990}, and the key representation theorem proved in \citet{bolker_functions_1966}. Two key differences between the theories are:
\begin{itemize}
    \item Jeffrey's theory considers preferences to be a function $\sigalg{F}\to \mathbb{R}$ defined over the $\sigma$-algebra on the set of consequences $(F,\sigalg{F})$, and holds that choices are a subset of $\sigalg{F}$
    \item The Bolker-Jeffrey representation theorem yields the existence of a generally nonunique probability $\prob{P}$ over $(F,\sigalg{F})$
\end{itemize}

We are not trying to derive models from preferences, so we don't see the second difference as particuarly important. Recall that our fundamental problem is relating a set $C$ of things we can choose to a set $F$ of things we can compare. Jeffrey's theory uses a different strategy to accomplish this than Savages'; where Savage employs the function $T:C\times S\to F$ and axioms that constrain the preference relation on $C$, Jeffrey embeds the choices in the algebra $\sigalg{F}$ and proposes axioms that constrain preferences on $(F,\sigalg{F})$. The ultimate result is, for our purposes, very similar.

The Bolker-Jeffrey representation theorem concludes: assuming a measurable set $(F,\sigalg{F})$ of consequences, if we have a complete preference relation on $\sigalg{F}$ that satisfies the \emph{Jeffrey axioms} then there exists a utility $u:\sigalg{F}\to\mathbb{R}$ and a probability distribution $\prob{P}\in \Delta(F)$ such that for $A,B\in \sigalg{F}$ and finite partition $D_1,...,D_n\in \sigalg{F}$ (the proof is given by \citet{bolker_functions_1966}):
\begin{align}
    (A \succ B) \iff \sum_{i}^n u(D_i) \prob{P}_A(D_i) > \sum_{i}^n u(D_i) \prob{P}_A(D_i) \label{eq:ev_dec_theory}
\end{align}
where $\prob{P}_A(D_i):=\frac{\prob{P}(A\cap D_i)}{\prob{P}(A)}$ for $\prob{P}(A)>0$, undefined otherwise (so if $A\succ B$, then $\prob{P}(A)>0$).

Now, Jeffrey says that \emph{choices} are special elements of $\sigalg{F}$ that we have the power to ``make true''. In this case, we have some $C\subset\sigalg{F}$ of choices that we want to compare, and the obvious probability function $\prob{P}_\cdot: C\to \Delta(\sigalg{F})$ given by $\alpha\mapsto \prob{P}_\alpha$. We are still in a slightly different position to Savage's original theory, as our ``utility function'' is a function defined on $\sigalg{F}$ rather than on $F$. However, we're interested in questions about the construction of the probability function $\prob{P}_\cdot$, and so this difference is not relevant to our work.