
%!TEX root = main.tex

\chapter{Models with choices and consequences}\label{ch:2p_statmodels}

Probability sets, introduced in Chapter \ref{ch:tech_prereq}, will be used to model \emph{decision problems}, which are problems that involve choices and consequences. In such problems, three things are given: a set of options, one of which must be chosen, a set of consequences and a means of judging which consequences are more desirable than others. Such a problem requires an understanding of how each choice corresponds to consequences, as far as this is able to be understood. The fundamental type of problem studied in this thesis is how to map choices to consequences. 

In practice causal inference is concerned with a wider variety of problems than this. A great deal of empirical causal analysis is concerned with problems a step removed from this: the purpose is to advise other decision makers on a course of action rather than to recommend an action directly. Nevertheless, many theorists of causal inference signal an awareness that much of their analysis is ultimately motivated by problems involving a choice among options, even if such problems are not always directly addressed. Section \ref{sec:whats_the_point} reviews briefly the role of decision problems according to theorists of causal inference and sets out the basic scheme by which probability sets are used to model problems of this type.

The reasons we provide for focusing on probability sets are not rigorous. The strongest motivation for this choice is \emph{convention}: this chapter shows how many varieties of decision theory induce probability set models, and Chapter \ref{ch:other_causal_frameworks} shows how many causal inference frameworks also induce probability set models. Some decision theories examined in this chapter seek to justify their modelling choices by suggesting axioms for rational theories of decision under uncertainty, and deriving a particular type of model from these axioms. However, despite the various attempts at axiomatisation, the nature of such a theory is contested -- there is no clear standard among the variety of theories surveyed here, nor among other proposed theories not considered in this work. This work is not trying to resolve this dispute. However, modelling choices must still be made and it is still reasonable to ask what is being assumed by making these choices, so Section \ref{sec:how_represent_conseqeunces} provides an overview of several major decision theories along with their axiomatisations.

Section \ref{sec:how_represent_conseqeunces} describes in particular detail the connections between \emph{statistical decision theory} \citep{wald_statistical_1950} and probability set models of decision problems. There are two reasons for this: firstly, we are able to demonstrate a close connection between probability set models of decision problems and the classical statistical notion of \emph{risk} of a decision rule, even though causal considerations are often not central to classical statistics. Secondly, the kind of probability set model -- which we call a \emph{see-do model} -- shows up again in Chapter \ref{ch:evaluating_decisions} where we consider the question of when a probability set model supports a certain notion of ``the causal effect of a variable'', and again in Chapter \ref{ch:other_causal_frameworks} where we consider the kinds of probability set models induced by other causal reasoning frameworks.

The formal definition of a variable in a probabilistic model is straightforward (Definition \ref{def:variable}). However, in practice the definitions of variables often includes informal content that enables the interpretation of a probabilistic model. In the field of causal models, one is likely to come across many different ``kinds'' of variables: for example, observed variables, unobserved variables, counterfactual variables and causal variables all play important roles in various causal inference frameworks. However, there is no formal distinction between these different kinds of variables -- Definition \ref{def:variable} applies to them all. Section \ref{sec:variable} is an attempt to clarify an understanding of the informal role of variables as ``pointing to the parts of the world that the model is about''. In comparison to the wide variety of variable types encountered in the causal literature, it offers a very limited theory of the informal semantics of variables. In short, observed variables correspond to a measurement procedure (in a sense that will be made precise), and unobserved variables do not.

\section{What is the point of causal inference?}\label{sec:whats_the_point}

\citet{pearl_book_2018} argues forcefully that causal reasoning frameworks should be understood by the questions that they answer. He also posits a ``ladder'' of types of causal question, where the $n$th level of question type also subsumes all lower levels. The question types are \citep{barenboim_foundations_2020}:
\begin{enumerate}
    \item \emph{Associational}: informally, ``questions about relationships and predictions''; formally defined as queries that can be answered by a single probability distribution
    \item \emph{Interventional}: informally, ``questions about the consequences of interventions''; formally defined as queries that can be answered by a causal Bayesian network (CBN)
    \item \emph{Counterfactual}: informally, ``questions concerning imagining alternate worlds''; formally defined as queries that can be answered by a structural causal model (SCM)
\end{enumerate}

Given that counterfactual questions are suggested to be the most general kind of causal question, one might ask why this work focuses on questions of an interventional nature. There are two reasons for this: Firstly, a class of informal questions is being used to motivate the theory of causal inference with probability sets. I have much stronger intuitions about informal decision problems than informal counterfactual queries. This does not seem to be a purely personal taste: questions about how decision problems should be represented have been studied much more than similar questions regarding counterfactual queries. Secondly, problems that involve comparing different choices on the basis of their consequences are an important class of problems on their own. Even within the causal inference literature, ``interventional'' questions and interpretations are much more prominent than counterfactual questions. For example, \citet{rubin_causal_2005} points out that causal inference often informs a decision maker by providing ``scientific knowledge'', but does not make recommendations by itself. \citep{imbens_causal_2015} introduces causal inference as the study of ``outcomes of manipulations'' and \citep{spirtes_causation_1993} highlights the universal relevance of understanding how to control certain outcomes, while further arguing that clarifying commonsense ideas of causation is also an important aim of causal inference. \citet{hernan_whatif_2020} present causal knowledge as critical for assessing the consequences of actions.

Speculatively, counterfactual queries may be able to be interpreted as decision problems with fanciful options. Consider an informal decision problem and a counterfactual query addressing similar material:
\begin{itemize}
    \item Decision problem: I want my headache to go away. If I take Aspirin, will it do so?
    \item Counterfactual query: I wish I didn't have headache. If I had taken the Aspirin, would I still have it?
\end{itemize}
If I haven't taken aspirin, then there's nothing I can actually choose to do to make it so that I had. However, if I imagine that I did have some option available that accomplished this, then the structure of the two questions seems rather similar. Both ask: if I take the option, what will the consequence be? Of course, it's hard to say what makes a correct answer to the second question, but this is a feature of counterfactual questions in general.

\subsection{Modelling decision problems}\label{sec:modelling_decision_problems}

People who need to make a decisions might (and often do) make them with no mathematical reasoning at all. However, this work is concerned with making decisions assisted by mathematical reasoning. In order to reason mathematically about a decision to be made, we assume that somehow, we have access to two sets:
\begin{enumerate}
    \item There is a set of choices $C$ that need to be compared
    \item There is a set of consequences $\Omega$ along with a utility function $u:\Omega\to \mathbb{R}$
\end{enumerate}

Given some means of relating between $C$ and $\Omega$, the order on $\Omega$ will induce some order on $C$. There are a great number of different ways that of relating elements of $C$ to $\Omega$. For example, a binary relation between the two sets will, given a total order on $\Omega$, induce a preorder on $C$. However, in this work the assumption is made that the relevant kinds of relations are either
\begin{itemize}
    \item A Markov kernel $C\kto \Omega$
    \item A Markov kernel $C\times H\kto \Omega$ for some set of hypotheses $H$
\end{itemize}
That is, for each choice $c\in C$ we have either a probability distribution in $\Delta(\Omega)$ or a set of probability distributions indexed by $h\in H$. Sections \ref{sec:cons_to_sdp} and \ref{sec:cc_theorem} discuss each choice in more detail. Where it is needed, we also assume that a utility function $\Omega\to \mathbb{R}$ is available and that choices are evaluated using the principle of expected utility.

Usually, someone confronted with a decision problem will not know for certain the consequences that arise from any given choice, and yet they may have some views about which consequences are more likely than others. Probability has a long and successful history of representing uncertain knowledge of this type. There are many works that aim to show that any method for representing uncertain knowledge that adheres to certain principles must be a probability distribution \citet{de_finetti_foresight_1992,horvitz_framework_1986}, along with criticism of these principles \citet{halpern_counter_1999}. A notable alternative to representing uncertainty with a single probability distribution represents uncertainty with a set of probability distributions, which is a type of \emph{vague probability} model \citep{walley_statistical_1991}. 

More relevant to the question of modelling decision problems are a number of works that establish conditions under which ``desirability'' or ``preference'' relations over sets of choices or propositions must be represented by a probability distribution along with a utility function. These works are surveyed in Section \ref{sec:how_represent_conseqeunces}. Ultimately, however, the question of whether probability is the right choice to represent uncertain knowledge in decision models is not a key focus of this work. It is a conventional choice, and one that is accepted here.

\subsection{Formal definitions}\label{sec:probability_sets}

We suppose that we are, at the outset, given a few basic ingredients: a set of choices $C$, a set of consequences $\Omega$ and a utility function $u:F\to \mathbb{R}$. We call these ingredients a ``decision problem''.

\begin{definition}[Decision problem]
A decision problem is a triple $(C,\Omega,u)$ consisting of a measurable set $(C,\sigalg{C})$ of choices, $(\Omega,\sigalg{F})$ consequences and a utility function $u:F\to \mathbb{R}$.
\end{definition}

Our task is to find a \emph{model} that relates $C$ to $\Omega$. We assume two forms of model -- a \emph{sharp model} associates each choice with a unique probability distribution, and a \emph{vague model} associates each choice with a set of probability distributions.

\begin{definition}[Sharp model]
Given a decision problem $(C,\Omega,u)$, a \emph{sharp model} is a function $C\kto \Omega$.
\end{definition}

\begin{definition}[Vague model]
Given a decision problem $(C,\Omega,u)$, a \emph{vague model} is a function $C\times H\kto \Omega$ for some hypothesis set $H$.
\end{definition}

Both sharp models and vague models have probability sets induced by the range of the model.

\begin{definition}[Induced probability set]\label{def:induced_pset_1}
Given a decision problem $(C,\Omega,u)$ and a model $\prob{P}_\cdot:C\times H\kto \Omega$, the induced probability set is $\prob{P}_{C\times H}:=\{\prob{P}_\alpha|\alpha\in C\times H\}$.
\end{definition}


\section{Representation theorems for decision problems }\label{sec:how_represent_conseqeunces}

We assume decision models are probabilistic functions $C\to \Delta(\Omega)$ for some sample space $(\Omega,\sigalg{F})$ of ``consequences''. Probability distributions, and the principle of expected utility in particular, are common choices for evaluation under uncertainty. Representation theorems offer a more formal justification for this choice; they propose a collection of axioms regulating the sets of evaluations we want some decision evaluation model to admit, and then show that this model can be represented with (for example) a probability distribution along with a utility function. The desirability of some of the axioms in these theorems is not obvious. 

Here we review the representation theorems of \citet{savage_foundations_1954} and \citet{jeffrey_logic_1990}. We establish that both imply that choices are compared using a probabilistic function $C\to \Delta(\Omega)$ for a suitable selection of $C$ and $(\Omega,\sigalg{F})$, along with a ``desirability'' function which differs in type between the two theorems.

Lewis' \emph{causal decision theory} is also briefly reviewed. While the particular considerations that motivated this theory are not examined, this theory introduces \emph{dependency hypotheses}, which play a key role in the rest of this work.

The following discussion will often make reference to \emph{complete preference relations}. A complete preference relation is a relation $\succ,\prec,\sim$ on a set $A$ such that for any $a,b,c$ in $A$ we have:
\begin{itemize}
    \item Exactly one of $a\succ b$, $a\prec b$, $a\sim b$ holds
    \item $(a\succ b)\iff(b\prec a)$
    \item $a\succ b$ and $b\succ c$ implies $a\succ c$
\end{itemize}
In short, it is a total order without antisymmetry ($a$ and $b$ can be equally preferred even if they are not in fact equal).

This definition is meant to correspond to the common sense idea of having preferences over some set of things, where $\succ$ can be read as ``strictly better than'', $\prec$ read as ``strictly worse than'' and $\sim$ read as ``as good as''. Given any two things from the set, I can say which one I prefer, or if I prefer neither (and all of these are mutually exclusive). If I prefer $a$ to $a'$ then I think $a'$ is worse than $a$. Furthermore, if I prefer $a$ to $a'$ and $a'$ to $a''$ then I prefer $a$ to $a''$.

Define $a\preceq b$ to mean $a\prec b$ or $a \sim b$.

\subsection{von Neumann-Morgenstern utility}

\citet{von_neumann_theory_1944} proved that when the \emph{vNM axioms} hold (not defined here; see the original reference or \citet{steele_decision_2020}), an agent's preferences between ``lotteries'' (probability distributions in $\Delta(\Omega)$ for some $(\Omega,\sigalg{F})$) can be represented as the comparison of the expected value under each lottery of a utility function $u$ unique up to affine transformation. That is, for lotteries $\prob{P}_\alpha$ and $\prob{P}_{\alpha'}$, there exists some $u:\Omega\to \mathbb{R}$ unique up to affine transformation such that $\mathbb{E}_{\prob{P}_\alpha}[u]> \mathbb{E}_{\prob{P}_{\alpha'}}[u]$ if and only if $\prob{P}_{\alpha} \succ \prob{P}_{\alpha'}$.

In vNM theory, the set of lotteries is is the set of all probability measures on $(\Omega,\sigalg{F})$. Thus von Neumann-Morgenstern theorem gives conditions under which preferences \emph{over distributions of consequences} can be represented using expected utility. It is silent on the question of whether each choice should be mapped to a unique probability distribution over consequences.

\subsection{Savage decision theory}

Savage's decision theory establishes conditions under which, given \emph{acts} $C$, \emph{consequences} $\Omega$ and \emph{states} $(S,\sigalg{S})$ (which are ``possible mappings from acts to consequences''), the preference relation over acts can be represented with a probability distribution over states and a utility function $\Omega\to \mathbb{R}$. This is much closer to the subject of this work than the theorem of von Neumann and Morgernstern.

\begin{definition}[Elements of a Savage decision problem]
A \emph{Savage decision problem} features a measurable set of states $(S,\sigalg{S})$, a set of consequences $\Omega$ and a set of acts $C$ such that $|C|=\Omega^S$ and an evaluation function $T:S\times C\to F$ such that for any $f:S\to \Omega$ there exists $c\in C$ such that $T(\cdot,c)=f$.
\end{definition}

\begin{theorem}
Given any Savage decision problem $(S,\Omega,C,T)$ with a preference relation $(\prec,\sim)$ on $C$ that satisfies the \emph{Savage axioms} \ref{sec:savage_axioms}, there exists a unique probability distribution $\mu\in\Delta(\sigalg{S})$ and a utility $u:\Omega\to \mathbb{R}$ unique up to affine transformation such that
\begin{align}
    \alpha\preceq \alpha' &\iff \int_S u(T(s,\alpha))\mu(\mathrm{d}s) \leq \int_S u(T(s,\alpha'))\mu(\mathrm{d}s)&\forall \alpha,\alpha'\in C
\end{align}
\end{theorem}

\begin{proof}
\citet{savage_foundations_1954}
\end{proof}

If we equip consequences with a measures $(\Omega,\sigalg{F})$, Savage's setup implies the existence of a unique probabilistic function $C\to \Delta(\Omega)$ representing the ``probabilistic consequences'' of each choice.

\begin{theorem}
Given any Savage decision problem $(S,\Omega,C,T)$ with a preference relation $(\prec,\sim)$ on $C$ that satisfies the \emph{Savage axioms}, and a $\sigma$-algebra $\sigalg{F}$ on $\Omega$ such that $T$ is measurable, there is a probabilistic function $\prob{P}_{\cdot}:C\to \Delta(\Omega)$ and a utility $u:\Omega\to \mathbb{R}$ unique up to affine transformation such that
\begin{align}
    \alpha\preceq \alpha' &\iff \int_\Omega u(f)\prob{P}_\alpha(\mathrm{d}f) \leq \int_\Omega u(f)\prob{P}_{\alpha'}(\mathrm{d}f)&\forall \alpha,\alpha'\in C
\end{align}
\end{theorem}

\begin{proof}
Define $\prob{P}_\cdot:C\to \Delta(\Omega)$ by
\begin{align}
    \prob{P}_\alpha(A) &:= \mu (T_\alpha^{-1}(A))&\forall A\in \sigalg{F}
\end{align}
where $\RV{T}_\alpha:S\to F$ is the function $s\mapsto T(s,\alpha)$. $\prob{P}_\alpha$ is the pushforward of $T_\alpha$ under $\mu$.

Then 
\begin{align}
    \int_\Omega u(f)\prob{P}_\alpha(\mathrm{d}f) &= \int_S u \circ T_\alpha (s)\mu(\mathrm{d}s)\\
    &= \int_S u(T(s,\alpha))\mu(\mathrm{d}s)
\end{align}
\end{proof}

\subsubsection{Savage axioms}\label{sec:savage_axioms}

Careful analysis of Savage's theorem is outside the scope of this work, but given the relevant of Savage's representation theorem we will reproduce the axioms from \citet{savage_foundations_1954} with a small amount of commentary. Keep in mind that Savage's theorem establishes that the following are sufficient for representation with a probability set, not necessary, and furthermore the probability set representation of preferences satisfying these axioms is unique.

Given acts $C$, states $(S,\sigalg{S})$ and consequences $F$ and a map $T:S\times C\to F$, let all greek letters $\alpha,\beta$ etc. be elements of $C$. Savage's axioms are:
\begin{enumerate}[P1:]
    \item There is a complete preference relation $\preceq$ on $C$
    \begin{enumerate}[D1:]
        \item $\alpha\preceq \beta$ given $B\in \sigalg{S}$ if and only if $\alpha'\preceq \beta'$ for every $\alpha'$ and $\beta'$ such that $T(\alpha,s)=T(\alpha',s)$ for $s\in B$ and $T(\alpha',r)=T(\beta',r)$ for $r\not\in B$, and $\beta'\preceq \alpha'$ either for every such pair or for none.
    \end{enumerate}
    \item For every $\alpha,\beta$ and $B\in \sigalg{S}$, $\alpha\preceq \beta$ given $B$ or $\beta\preceq \alpha$ given $B$
    \begin{enumerate}[D2:]
        \item for $q,q'\in F$, $q\preceq q'$ if and only if $\alpha\preceq \alpha'$ where $T(\alpha,s)=q$ and $T(\alpha',s)=q'$ for all $s\in S$
        \item $B\in \sigalg{S}$ is null if and only if $\alpha\preceq \beta$ given B for every $\alpha,\beta\in C$
    \end{enumerate}
    \item If $T(\alpha,s)=q$ and $T(\alpha',s)=q'$ for every $s\in B$, $B\in \sigalg{S}$ non-null, then $\alpha\preceq \alpha'$ given $B$ if and only if $q\preceq q'$
    \begin{enumerate}[D4:]
        \item For $A,B\in \sigalg{S}$, $A\leqslant B$ if and only if $\alpha_A\preceq \alpha_B$ or $q\preceq q'$ for all $\alpha_A,\alpha_B\in C$, $q,q'\in F$ such that $T(\alpha_A,s) = q$ for $s\in A$, $T(\alpha_A,s')=q'$ for $s'\not\in A$, $T(\alpha_B,s)=q$ for $s\in B$, $T(\alpha_B,s')=q'$ for $s'\not\in B$. Read $\leqslant$ as ``is less probable than''
    \end{enumerate}
    \item For every $A,B\in\sigalg{S}$, $A\leqslant B$ or $B\leqslant A$
    \item For some $\alpha,\beta$, $\alpha\prec \beta$
    \item Suppose $\alpha\not\preceq \beta$. Then for every $\gamma$ there is a finite partition of $S$ such that if $\alpha'$ agrees with $\alpha$ and $\beta'$ agrees with $\beta$ except on some element $B$ of the partition, $\alpha'$ and $\beta'$ being equal to $\gamma$ on $B$, then $\alpha\not\preceq \beta'$ and $\alpha'\not\preceq \beta$
    \begin{enumerate}[D5:]
        \item $\alpha\preceq q$ for $q\in F$ given $B$ if and only if $\alpha\preceq \beta$ given $B$ where $T(\beta,s)=q$ for all $s\in S$
    \end{enumerate}
    \item If $\alpha\preceq T(\beta,s)$ given $B$ for every $s\in B$, then $\alpha\preceq \beta$ given $B$
    \begin{enumerate}[P7':]
        \item The proposition given by inverting every expression in D5 and P7
    \end{enumerate}
\end{enumerate}

Our initial view of decision problems was that the consequences $\Omega$ are a set of things we know how to rank and choices $C$ are the things we want to rank. This is not exactly Savage's setup -- he assumes a preference relation ranking ``acts'' $C$ to begin with. Furthermore, Savage also introduces a set of states $S$ and assumes that the set of acts corresponds to the set of all function $S\to \Omega$. Many decision problems might be able to be extended with states and the set of acts enriched so as to satisfy these requirements, but it is not obvious that this is always possible.

D1 formalises the idea of one act $\alpha$ being not preferred to another $\beta$ given the knowledge that the true state lies in the set $B$ (in short: ``given $B$'' or ``conditional on $B$''). P2 is sometimes called the ``sure thing principle'', as it implies the following: for any $\alpha, \beta$ if $\alpha$ is better than $\beta$ on some states and no worse on any other, then $\alpha\succ \beta$. In Savage's model, the ``likelihood'' that of any state cannot depend on the act chosen.

D4 + P4 defines the ``probability preorder'' $\leqslant$ on $(S,\sigalg{S})$ and assumes it is complete.

P5 is the requirement that the preference relation is non-trivial; not everything is equally desirable. This doesn't seem like it should be a practical requirement to me; we might hope that a model can distinguish between some of our options, but that doesn't mean we should assume it can. Savage claims that this requirement is ``innocuous'' because any exception must be trivial, but I'm not sure I agree.

P6 is a requirement of continuity; for any $\alpha\preceq \beta$, we can divide $S$ finely enough to squeeze a ``small slice'' of any third outcome $\gamma$ into the gap between the two.

P7 in combination with the other axioms forces preferences to be bounded.

\subsection{Jeffrey's decision theory}

Jeffrey's decision theory is an alternative to Savage's that starts from a different set of assumptions. One of the key differences is in what is assumed at the outset: where Savage assumes a set of states $S$, acts $C$ and consequences $\Omega$, Jeffrey's theory only considers a single space $\underline{\sigalg{F}}$, which is a complete atomless boolean algebra. Elements of $\underline{\sigalg{F}}$ are said to be propositions, although the structure of $\underline{\sigalg{F}}$ means we can't understand it as, for example, a set of propositions regarding the result of a particular measurement procedure (Section \ref{sec:variable}). The theory is set out in \citet{jeffrey_logic_1990}, and the key representation theorem proved in \citet{bolker_functions_1966}.

Recall that our fundamental problem is relating a set $C$ of things we can choose to a set $F$ of things we can compare. Jeffrey's theory uses a different strategy to accomplish this than Savages'; where identifies a set of acts $C$ with all functions $S\to F$ and proposes axioms that constrain a preference relation on $C$, Jeffrey assumes that choices are elements of the algebra $\underline{\sigalg{F}}$, along with propositions that do not correspond to choices. Jeffrey's axioms pertain to a preference relation on $\underline{\sigalg{F}}$. The ultimate result is, for our purposes, very similar.

\begin{theorem}\label{th:bolker_jeffrey}
Suppose there is a complete atomless Boolean algebra $\underline{\sigalg{F}}$ with a preference relation $\preceq$. If $\preceq$ satisfies the \emph{Bolker axioms} (Section \ref{sec:bolker_axioms}) then there exists a desirability function $\text{des}:\underline{\sigalg{F}}\to\mathbb{R}$ and a probability distribution $\mu\in \Delta(\underline{\sigalg{F}})$ such that for $A,B\in \underline{\sigalg{F}}$ and finite partition $D_1,...,D_n\in \underline{\sigalg{F}}$:
\begin{align}
    (A \preceq B) \iff \sum_{i}^n \text{des}(D_i) \mu(D_i|A) \leq \sum_{i}^n \text{des}(D_i) \mu(D_i|B) \label{eq:ev_dec_theory}
\end{align}
where $\mu(D_i|A):=\frac{\mu(A\cap D_i)}{\mu(A)}$ for $\mu(A)>0$, undefined otherwise.
\end{theorem}

\begin{proof}
\citet{bolker_functions_1966})
\end{proof}

As mentioned, in Jeffrey's theory the \emph{choices} $C$ are a subset of $\underline{\sigalg{F}}$. Thus we can deduce from a Jeffrey model a function $C\to \Delta(\underline{\sigalg{F}})$ that ``represents the consequences of choices'' in the sense of Theorem \ref{th:jeffrey_with_choices}.

\begin{theorem}\label{th:jeffrey_with_choices}
Suppose there is a complete atomless Boolean algebra $\underline{\sigalg{F}}$ with a preference relation $\preceq$ that satisfies the Bolker axioms, and a set of choices $C$ over which a preference relation is sought with $\mu(\alpha)>0$ for all $\alpha\in C$. Then there is a function $\prob{P}_\cdot:C\to \Delta(\underline{\sigalg{F}})$ such that for any $\alpha,\alpha'\in C$ and finite partition $D_1,...,D_n\in \underline{\sigalg{F}}$:
\begin{align}
    \alpha \preceq \alpha'\iff \sum_{i}^n \text{des}(D_i) \prob{P}_\alpha(D_i) \leq \sum_{i}^n \text{des}(D_i) \prob{P}_{\alpha'}(D_i)\label{eq:ev_with_choices}
\end{align}
Where $\mu$ and $\mathrm{des}$ are as in Theorem \ref{th:bolker_jeffrey}
\end{theorem}

\begin{proof}
Define $\prob{P}_\cdot$ by $\alpha\mapsto \mu(\cdot|\alpha)$. Then Equation \ref{eq:ev_with_choices} follows from Equation \ref{eq:ev_dec_theory}.
\end{proof}

\subsubsection{Bolker axioms}\label{sec:bolker_axioms}

$\underline{\sigalg{F}}$ a complete, atomless Boolean algebra with the impossible proposition. An example of such a set is constructed from the set of Lebesgue measurable sets on $[0,1]$ identifying any two sets that differ by a set of measure zero identified \citet{bolker_simultaneous_1967}. This is not a $\sigma$-algebra.
 
\begin{enumerate}[A1:]
    \item $\preceq$ is a complete preference relation
    \item $\underline{\sigalg{F}}$ is a complete, atomless Boolean algebra with the impossible proposition removed
    \item For $A,B\in \underline{\sigalg{F}}$, if $A\cap B=\emptyset$, then
    \begin{enumerate}[a)]
        \item If $A\succ B$ then $A\succ A\cup B \succ B$
        \item If $A\sim B$ then $A\sim A\cup B \sim B$
    \end{enumerate}
    \item Given $A\cap B=\emptyset$ and $A\sim B$, if $A\cup G\sim B\cup G$ for some $G$ where $A\cap G=B\cap G=\emptyset$ and $G\not\sim A$, then $A\cup G\sim B\cup G$ for every such $G$
    \begin{enumerate}[D1:]
        \item The supremum (infimum) of a subset $W\subset \underline{\sigalg{F}}$ is a set $G$ ($D$) such that for all $A\in W$, $G\subset A$ ($A\subset D$), and for any $E$ that also has this property, $G\subset E$ ($E\subset D$)
    \end{enumerate}
    \item Given $W:= \{W_i\}_{i\in M\subset \mathbb{N}}$ with $i<j\implies W_j\subset W_i$ and $W\subset \underline{\sigalg{F}}$ with supremum $G$ (infimum $D$), whenever $A\prec G \prec B$ ($A\prec D\prec B$) then there exists some $k\in M$ such that $i\geq k$ ($i\leq k$) implies $A\prec W_i \prec B$.
\end{enumerate}

Like Savage's theory, A1 requires the preference relation to be complete.

A3 is the assumption that the desirability of disjunctions of events lies between the desirability of each event; it is sometimes called ``averaging''. It notably rules out the following: if $A\succ B$ we cannot have $A\cup B\sim A$. In the Jeffrey-Bolker theory, propositions all have positive probabilities.

A4 allows a probability order to be defined on $\underline{\sigalg{F}}$. The conditions $A\cap B=\emptyset$, $A\sim B$, $A\cup G\sim B\cup G$ for some $G$ where $A\cap G=B\cap G=\emptyset$ and $G\not\sim A$ can be seen as a test for $A$ and $B$ being ``equally probable''. A4 requires that if $A$ and $B$ are rated as equally probable by one such test, then they are rated as equally probable by all such tests.

A5 is an axiom of continuity.

\subsection{Causal decision theory}

Causal decision theory was developed after both Jeffrey's and Savage's theory. A number of authors \citet{lewis_causal_1981,skyrms_causal_1982} felt that Jeffrey's theory erred by treating the consequences of a choice as an ``ordinary conditional probability''. \citet{lewis_causal_1981} suggested that causal decision theory can be used to evaluate choices when we are given a set $\Omega$ of consequences over which preferences are known, a set $C$ of choices and a set $H$ of dependency hypotheses (the letters have been changed to match usage in this work; in the original the consequences were called $S$, the choices $A$ and the dependency hypotheses $H$). Choices are then evaluated according to the causal decision rule. We have taken the liberty to state Lewis' rule in the language of the present work.

\begin{definition}[Causal decision rule]
Given a set $C$ of choices, sample space $(\Omega,\sigalg{F})$, variables $\RV{H}:\Omega\to H$ (the \emph{dependency hypothesis}) and $\RV{S}:\Omega\to S$ (the \emph{consequence}) and a utility $u:\Omega\to \mathbb{R}$, the \emph{causal utility} of a choice $\alpha\in C$ is given by
\begin{align}
    U(\alpha) := \int_S \int_H u(s) \prob{P}_\alpha^{\RV{S}|\RV{H}}(\mathrm{d}s|h) \prob{P}_C^{\RV{H}}(\mathrm{d}h)\label{eq:lewis_cdt}
\end{align}
For some probabilistic function $\prob{P}_\cdot:C\to \Delta(\Omega)$.
\end{definition}

The reasons why Lewis wanted to introduce dependency hypothesis and modify Jeffrey's rule to Equation \ref{eq:lewis_cdt} are controversial and do not come up in this work. However, causal decision theory is still relevant to this work in two ways: firstly, once again is a probabilistic function $\prob{P}_\cdot:C\to \Delta(\Omega)$. Secondly, causal decision theory introduces the notion of the dependency hypothesis $\RV{H}$. The dependency hypothesis is similar to the state in Savage's theory, however Lewis does not require a deterministic map from dependency hypotheses to consequences, nor does he require a choice to correspond to every possible function from dependency hypotheses to states.

Dependency hypotheses are quite an important idea in causal reasoning. Together Lewis' decision rule connect the theory of probability sets with \emph{statistical decision theory}, as Section \ref{sec:sdt} will show. Chapter \ref{ch:evaluating_decisions} goes into considerable detail concerning the question of when probability sets support certain types of dependency hypothesis. While they are typically not explicitly represented in common frameworks for causal inference, Chapter \ref{ch:other_causal_frameworks} discusses how dependency hypotheses are often implicit in these approaches, and shows how they can be made explicit.

\subsection{Statistical decision theory}\label{sec:sdt}

Statistical decision theory (SDT), created by \citet{wald_statistical_1950}, predates all of the decision theories discussed above. Savage's theory appears to have developed in part to explain some features of SDT \citet{savage_theory_1951}, and Jeffrey's theory and subsequent causal decision theories were in turn influenced by Savage's decision theory. While the later decision theories were concerned with articulating why their theory fit the role of a theory for rational decision under uncertainty, Wald focused much more on the mathematical formalism and solutions to statistical problems. Statistical decision theory introduced many fundamental ideas that have since entered the ``water supply'' of machine learning theory, such as \emph{decision rules} and \emph{risk} as a measure of the quality of a decision rule.

In contrast to the later decision theories, SDT has no explicit representation of the ``consequences'' of a decision. Rather, it is assumed that a loss function is given that maps decisions and hypotheses directly to a loss, which is a kind of desirability score similar to a utility (although it is minimised rather than maximised).

\begin{definition}[Statistical decision problem]
A statistical decision problem (SDP) is a tuple $(X, H, D, l, \prob{P}_\cdot)$ where $(X,\sigalg{X})$ is a set of outcomes, $(H,\sigalg{H})$ is a set of hypotheses, $(D,\sigalg{D})$ is a set of decisions, $l:D\times H\to \mathbb{R}$ is a loss function and $\prob{P}_\cdot:H\kto X$ is a Markov kernel from hypotheses to to outcomes.
\end{definition}

Statistical decision theory is concerned with the selection of \emph{decision rules}, rather than the selection of decisions directly. A decision rule maps observations to decisions, and may be deterministic or stochastic.

\begin{definition}[Decision rule]
Given a statistical decision problem $(X, H, D, l, \prob{P}_\cdot)$, a decision rule is a Markov kernel $\kernel{D}_\alpha:\Omega\kto D$.
\end{definition}

Because decision rules in SDT play the role of what we call \emph{choices}, we denote the set of all available decision rules by $C$. A further feature of SDT that is unlike the later decision theories is that SDT does not offer a single rule for assessing the desirability of any choice in $C$. Instead, it offers a definition of the risk, which assesses the desirability of a choice \emph{relative to a particular hypothesis}. The risk function completely characterises the problem of choosing a decision function. Two different rules are for turning this ``intermediate assessment'' into a final assessment of the available choices - Bayes optimality and minimax optimality. Bayes optimality reuquires a prior over hypotheses, while minimax optimality does not.

\begin{definition}[SDP Risk]\label{def:risk}
Given a statistical decision problem $(X, H, D, l, \prob{P}_\cdot)$ and decision functions $C$, the \emph{risk} functional $R:C\times H\to \mathbb{R}$ is defined by
\begin{align}
    R(\kernel{D}_\alpha,h):= \int_X \int_D l(d,h) \kernel{D}_\alpha(\mathrm{d}d|f)\kernel{P}_h(\mathrm{d}f)
\end{align}
\end{definition}

It is possible to find risk functions in problems that aren't SDPs. The definitions of Bayes and Minimax optimality still apply to risk functions obtained on other manners. Thus Bayes optimality and minimax optimality are defined in terms of risk functions in general, not SDP risk functions.

\begin{definition}[Bayes risk]
Given decision functions $C$, hypotheses $(H,\sigalg{H})$, risk $R:C\times H\to \mathbb{R}$ and prior $\mu\in \Delta(H)$, the $\mu$-\emph{Bayes risk} is
\begin{align}
    R_\mu(\kernel{D}_\alpha) &:= \int_{H} R(\kernel{D}_\alpha,h)\mu(\mathrm{d}h)
\end{align}
\end{definition}

\begin{definition}[Bayes optimal]
Given decision functions $C$, hypotheses $(H,\sigalg{H})$, risk $R:C\times H\to \mathbb{R}$ and prior $\mu\in \Delta(H)$, $\alpha\in C$ is $\mu$-Bayes optimal if
\begin{align}
    R_\mu(\kernel{D}_\alpha) &= \inf_{\alpha'\in C} R_{\mu}(\kernel{D}_{\alpha'})
\end{align}
\end{definition}

\begin{definition}[Minimax optimal]
Given decision functions $C$, hypotheses $(H,\sigalg{H})$, risk $R:C\times H\to \mathbb{R}$, a \emph{minimax decision function} is any decision function $\kernel{D}_\alpha$ satisfying
\begin{align}
    \sup_{h\in H}  R(\kernel{D}_\alpha,h) &= \inf_{\alpha' in C} \sup_{h\in H} R(\kernel{D}_{\alpha'},h)
\end{align}
\end{definition}

\subsubsection{From consequences to statistical decision problems}\label{sec:cons_to_sdp}

Statistical decision theory ignores the notion of consequences beyond the loss received for a particular decision given a particular hypothesis. The kinds of probability set models studied here probabilistically map decisions to consequences, and the set of consequences is understood to have a utility function to allow for assessment of the desirability of different choices via the principle of expected utility. Not every probability set model induces a statistical decision problem in this manner. A family of models that does are what we call \emph{conditionally independent see-do models}. These models feature observations (the ``see'' part) along with decisions and consequences (the ``do'' part), and the observations come ``before'' the decisions (hence see-do). Examples of this type of model will be encountered again in Chapters \ref{ch:evaluating_decisions} and \ref{ch:other_causal_frameworks}. Furthermore, there is a hypothesis such that consequences are assumed to be independent of observations conditional on the decision and the hypothesis. This is why they are qualified as ``conditionally independent'' see-do models.
\begin{definition}[See-do model]
A probability set model of a statistical decision problem, or a \emph{see-do model} for short, is a tuple $(\prob{P}_{C\times H}, \RV{X},\RV{Y},\RV{D})$ where $\prob{P}_{C\times H}$ is a probability set indexed by elements of $C\times H$ on $(\Omega,\sigalg{F})$, $\RV{X}:\Omega\to X$ are the observations, $\RV{Y}:\Omega\to Y$ are the consequences and $\RV{D}:\Omega\to D$ are the decisions. $\prob{P}_{C\times H}$ must observe the following conditional independences:
\begin{align}
    \RV{X}&\CI^e_{\prob{P}_{C\times H}} \RV{C}|\RV{H}\\
    \RV{D}&\CI^e_{\prob{P}_{C\times H}} \RV{H}|\RV{C}
\end{align}
where $\RV{C}:C\times H\to C$ and $\RV{H}:C\times H\to H$ are the respective projections (see Definition \ref{def:eci_orig} for the definition of extended conditional independence).
\end{definition}
\begin{definition}[Conditionally independent see-do model]
A conditionally independent see-do model is a see do model $(\prob{P}_{C\times H}, \RV{X},\RV{Y},\RV{D})$ where the following additional conditional independence holds:
\begin{align}
    \RV{Y}&\CI^e_{\prob{P}_{C\times H}} (\RV{X,C})|(\RV{D},\RV{H})
\end{align}
\end{definition}
We assume that a utility function is available depending on the consequence $\RV{Y}$ only, and identify the loss with the negative expected utility, conditional on a particular decision and hypothesis.
\begin{definition}[Induced loss]
Given a see-do model $(\prob{P}_{C\times H}, \RV{X},\RV{Y},\RV{D})$ and a utility $u:Y\to \mathbb{R}$, the induced loss $l:D\times H\to \mathbb{R}$ is defined as
\begin{align}
    l(d,h)&:= -\int_Y u(y) \prob{P}_{C\times\{h\}}^{\RV{Y}|\RV{D}}(\mathrm{d}y|d)
\end{align}
\end{definition}
where the uniform conditional $\prob{P}_{C\times\{h\}}^{\RV{Y}|\RV{D}}$'s existence is guaranteed by $\RV{Y}\CI^e_{\prob{P}_{C\times H}} (\RV{X,C})|(\RV{D},\RV{H})$.

A see-do model induces a set of decision functions: for each $\alpha\in C$, there is an associated probability distribution $\prob{P}_\alpha^{\RV{D}|\RV{X}}$. Using the above definition of loss, the expected loss of a decision function in a conditionally independent see-do model induces a risk function identical to the SDP risk.
\begin{theorem}[Induced SDP risk]\label{th:ind_risk}
Given a conditionally independent see-do model $(\prob{P}_{C\times H}, \RV{X},\RV{Y},\RV{D})$ along with a utility $u:Y\to \mathbb{R}$, the expected utility for each choice $\alpha\in C$ and hypothesis $h\in H$ is equal to the negative SDP risk of the associated decision rule $\prob{P}_\alpha^{\RV{D}|\RV{X}}$ and hypothesis $h$.
\begin{align}
    \prob{P}_{\alpha,h}^{\RV{Y}}u &= -R(\prob{P}_{\{\alpha\}\times H}^{\RV{D}|\RV{X}},h)
\end{align}
\end{theorem}

\begin{proof}
The expected utility given $\alpha$ and $h$ is
\begin{align}
    \int_Y u(y)\prob{P}_{\alpha,h}^{\RV{Y}}(\mathrm{d}y) &= \int_Y  \int_D \int_{X} u(y)\prob{P}_{\alpha,h}^{\RV{Y}|\RV{DX}}(\mathrm{d}y|d,x)\prob{P}_{\alpha,h}^{\RV{D}|\RV{X}}(\mathrm{d}d|x)\prob{P}_{\alpha,h}^{\RV{X}}(\mathrm{d}x) \\
    &= \int_X  \int_D \int_{Y} u(y) \prob{P}_{\alpha,h}^{\RV{Y}|\RV{D}}(\mathrm{d}y|d)\prob{P}_{\alpha,h}^{\RV{D}|\RV{X}}(\mathrm{d}d|x)\prob{P}_{\alpha,h}^{\RV{X}}(\mathrm{d}x)\label{eq:because_of_ci}\\
    &= \int_{X} \int_D \int_Y u(y) \prob{P}_{C\times\{h\}}^{\RV{Y}|\RV{D}}(\mathrm{d}y|d)\prob{P}_{\{\alpha\}\times H}^{\RV{D}|\RV{X}}(\mathrm{d}d|x)\prob{P}_{C\times\{h\}}^{\RV{X}}(\mathrm{d}x)\\
     &= -\int_D\int_X l(d,h)\prob{P}_{\{\alpha\}\times H}^{\RV{D}|\RV{X}}(\mathrm{d}d|x)\prob{P}_{C\times\{h\}}^{\RV{X}}(\mathrm{d}x)\\
    &= -R(\prob{P}_{\{\alpha\}\times H}^{\RV{D}|\RV{X}},h)
\end{align}
where Equation \ref{eq:because_of_ci} follows from $\RV{Y}\CI^e_{\prob{P}_{C\times H}} (\RV{X,C})|(\RV{D},\RV{H})$, the uniform conditional $\prob{P}_{\{\alpha\}\times H}^{\RV{D}|\RV{X}}$ exists due to $\RV{D}\CI^e_{\prob{P}_{C\times H}} \RV{H}|\RV{C}$ and the uniform conditional $\prob{P}_{C\times\{h\}}^{\RV{X}}$ exists due to $\RV{X}\CI^e_{\prob{P}_{C\times H}} \RV{C}|\RV{H}$.
\end{proof}

Theorem \ref{th:ind_risk} does \emph{not} hold for general see-do models. General see-do models allow for the utility to depend on $\RV{X}$ even after conditioning on $\RV{D}$ and $\RV{H}$, while the form of the loss function in SDT forces no direct dependence on observations. The generic ``see-do risk'' (Definition \ref{def:see_do_risk}) provides a notion of risk for the more general case, while Theorem \ref{th:ind_risk} shows it reduces to SDP risk in the case of conditionally independent see-do models with a utility that depends only on the consequences $\RV{Y}$.
\begin{definition}[See-do risk]\label{def:see_do_risk}
Given a see-do model $(\prob{P}_{C\times H}, \RV{X},\RV{Y},\RV{D})$ along with a utility $u:X\times Y\to \mathbb{R}$, the \emph{see-do risk} $R:C\times H\to \mathbb{R}$ is given by
\begin{align}
    R(\alpha,h) &:= -\prob{P}_{\alpha,h}^{\RV{XY}}u&\forall \alpha\in C,h\in H
\end{align}
\end{definition}

Section \ref{sec:modelling_decision_problems} noted that two types of probability set model are considered: probability sets $\prob{P}_C$ indexed by choices alone, and probability sets $\prob{P}_{C\times H}$ jointly indexed by choices and hypotheses. See-do models are an instance of the second kind, jointly indexed by choices and hypotheses. Bayesian see-do models are of the former type, indexed by choices alone. A see-do model $(\prob{P}_{C\times H}, \RV{X},\RV{Y},\RV{D})$ and a prior over hypotheses $\mu\in \Delta(H)$ can by combined to form a Bayesian see-do model, and under the right conditions the risk of the Bayesian model reduces to the Bayes risk of the original see-do model.

\begin{definition}[Bayesian see-do model]
A Bayesian see-do model is a tuple $(\prob{P}_{C}, \RV{X},\RV{Y},\RV{D},\RV{H})$ where $\prob{P}_C$ is a probability set on $(\Omega,\sigalg{F})$, $\RV{X}:\Omega\to X$ are the observations, $\RV{Y}:\Omega\to Y$ are the consequences, $\RV{D}:\Omega\to D$ are the decisions and $\RV{H}:\Omega\to H$ is the hypothesis. $\prob{P}_C$ must observe the following conditional independences:
\begin{align}
    \RV{X}&\CI^e_{\prob{P}_{C}} \RV{C}|\RV{H}\\
    \RV{D}&\CI^e_{\prob{P}_{C}} \RV{H}|\RV{C}\\
    \RV{H}&\CI^e_{\prob{P}_C} \RV{C}
\end{align}
\end{definition}

\begin{definition}[Induced Bayesian see-do model]
Given a see-do model $(\prob{P}_{C\times H}, \RV{X},\RV{Y},\RV{D},\RV{H})$ on $(\Omega,\sigalg{F})$ and a prior $\mu\in \Delta(H)$, the induced Bayesian see-do model $\prob{P}_C$ on $(\Omega\times H,\sigalg{F}\otimes \sigalg{H})$ is
\begin{align}
    \prob{P}_C(A) &= \int_{\RV{H}^{-1}(A)} \prob{P}_{C\times \{h\}}(\Pi_{\Omega}^{-1}(A))\mu(\mathrm{d}h)&\forall A\in \sigalg{F}\otimes\sigalg{H}
\end{align}
Where $\Pi_\Omega:\Omega\times H\to \Omega$ is the projection onto $\Omega$.
\end{definition}

\begin{theorem}[Induced SDP Bayes risk]
Given a conditionally independent see-do model $(\prob{P}_{C}, \RV{X},\RV{Y},\RV{D},\RV{H})$ along with a utility $u:Y\to \mathbb{R}$ and a prior $\mu\in \Delta(H)$, the expected utility for each choice $\alpha\in C$ under the induced Bayesian see-do model is equal to the negative $\mu$-Bayes risk of that decision rule.
\end{theorem}

\begin{proof}
First, note that $h\mapsto \prob{P}_{C\times \{h\}}^{\RV{Y}|\RV{XD}}$ is a version of $\prob{P}_C^{\RV{Y}|\RV{XD}}$ and hence $\RV{Y}\CI^e_{\prob{P}_C} (\RV{X},\RV{C})|(\RV{H},\RV{D})$, a property it inherits from the underlying see-do model.

Also, note that $\prob{P}_C^{\RV{H}}=\mu$, by construction.

The expected utility of $\alpha\in C$ is 
\begin{align}
    \prob{P}_\alpha^{\RV{Y}} u &= \int_Y u(y) \prob{P}_{\alpha}^{\RV{Y}}(\mathrm{d}y) \\
    &= \int_Y  \int_D \int_{X} \int_H u(y)\prob{P}_{\alpha}^{\RV{Y}|\RV{DXH}}(\mathrm{d}y|d,x,h)\prob{P}_{\alpha}^{\RV{D}|\RV{XH}}(\mathrm{d}d|x,h)\prob{P}_{\alpha}^{\RV{X}|\RV{H}}(\mathrm{d}x|h)\prob{P}_\alpha^{\RV{H}}(\mathrm{d}h) \\
    &= \int_X  \int_D \int_{Y} \int_H u(y) \prob{P}_{\alpha}^{\RV{Y}|\RV{DH}}(\mathrm{d}y|d,h)\prob{P}_{\alpha}^{\RV{D}|\RV{X}}(\mathrm{d}d|x)\prob{P}_{\alpha}^{\RV{X}|\RV{H}}(\mathrm{d}x|h)\prob{P}_\alpha^{\RV{H}}(\mathrm{d}h)\label{eq:because_of_ci}\\
    &=  \int_X  \int_D \int_{Y} \int_H u(y) \prob{P}_{C}^{\RV{Y}|\RV{DH}}(\mathrm{d}y|d,h)\prob{P}_{\alpha}^{\RV{D}|\RV{X}}(\mathrm{d}d|x)\prob{P}_{C}^{\RV{X}|\RV{H}}(\mathrm{d}x|h)\mu(\mathrm{d}h)\\
     &= -\int_D\int_X\int_H l(d,h)\prob{P}_{\alpha}^{\RV{D}|\RV{X}}(\mathrm{d}d|x)\prob{P}_{C}^{\RV{X}|\RV{H}}(\mathrm{d}x|h)\mu(\mathrm{d}h)\\
    &= -\int_H R(\prob{P}_{\alpha}^{\RV{D}|\RV{X}},h)\mu(\mathrm{d}h)\\
    &= -R_{\mu}(\prob{P}_{\alpha}^{\RV{D}|\RV{X}})
\end{align}
\end{proof}

\subsubsection{Complete class theorem}\label{sec:cc_theorem}

The \emph{complete class theorem} establishes that, under certain conditions, any \emph{admissible} decision rule (Definition \ref{def:admissible_decision}) for a see-do model $\prob{P}_{C\times H}$ with a utility $u$ must minimise the Bayes risk for a Bayesian model constructed from $\prob{P}_{C\times H}$ and a prior over hypotheses $\mu\in \Delta(H)$. This can be interpreted in a similar way to the decision theoretic representation discussed above: if you accept that the relevant assumptions apply to the decision problem at hand, than there is a Bayesian see-do model along with $u$ that captures the important features of this problem. The assumptions are that a see-do model $\prob{P}_{C\times H}$ with a utility $u$ that satisfies the relevant conditions is available, and that the principle used to evaluate decision rules should yield an admissible decision rule (though it may also be desired to satisfy other properties as well).

If there are auxiliary requirements for choosing the decision rule, the complete class theorem does not prove that it is easy to find a Bayesian model that will yield rules satisfying these requirements.

See-do models (and statistical decision problems) have a lot of structure -- the loss, the assumption that consequences are conditionally independent of observations -- that is not actually critical to the complete class theorem. The complete class theorem is a theorem about risk function $R:C\times H\to \mathbb{R}$ that have certain properties. Theorem \ref{th:ind_risk} shows one way that a risk function can be derived from a see-do model along with a utility. However, it is also possible to derive risk functions from other classes of probability set models with utilities, and if the resulting risk function satisfies the appropriate conditions then the complete class theorem also applies to that class of model. For example, the complete class theorem also applies to see-do models without the assumption that consequences are conditionally independent of observations given the hypothesis and the decision, even though in this case the risk calculation is not the standard calculation for a statistical decision problem.

\begin{definition}[Risk function]
Given a set of choice $C$ and a set of hypotheses $H$, a risk function is a map $R:H\times C\to \mathbb{R}$.
\end{definition}

If the second set $H$ were, instead of hypotheses about nature, a set of options available to a second player playing a game, then a ``risk function'' defines a two-player zero-sum game \citet{toutenburg_ferguson_1967}.

\begin{definition}[Admissible choice]\label{def:admissible_decision}
Given a risk function $R:C\times H\to \mathbb{R}$, a choice r $\alpha\in C$ dominates a choice $\alpha'\in C$ if for all $h\in H$, $R(\alpha,h)\leq R(\alpha',h)$ and for at least on $h^*$, $R(\alpha,h)<R(\alpha,h^*)$. An \emph{admissible choice} is a choice $\alpha\in C$ such that there is no $\alpha'\in C$ dominating $\alpha$.
\end{definition}

\begin{definition}[Complete class]
A \emph{complete class} is any $B\subset C$ such that, for any $\alpha'\not in B$ there is some $\alpha\in B$ that dominates $\alpha'$. A \emph{minimal complete class} is a complete class $B$ such that no proper subset of $B$ is complete
\end{definition}

\begin{theorem}
If a minimal complete class $B\subset C$ exists then $B$ is the set consisting of all the admissible decision rules.
\end{theorem}

\begin{proof}
See \citet[Theorem 2.1]{toutenburg_ferguson_1967}
\end{proof}

\begin{definition}[Risk set]
Given a finite set of hypotheses $H$, a set of choices $C$ and a risk function $R:C\times H\to \mathbb{R}$, the risk set is the subset of $\mathbb{R}^{|H|}$ given by
\begin{align}
    S := \{(R(\alpha,h))_{h\in H}|\alpha\in C\}
\end{align}
\end{definition}

\begin{theorem}[Complete class theorem]
Given a risk function $R:C\times H\to \mathbb{R}$, if the risk set S is convex, bounded from below and closed downwards, and H is finite, then the set of Bayes optimal choices is a minimal complete class.
\end{theorem}

\begin{proof}
See \citet[~Theorem 2.10.2]{toutenburg_ferguson_1967}
\end{proof}

Two examples of the application of the complete class theorem will be presented (Examples \ref{ex:cc_sdt} and \ref{ex:cc_nonsdt}). In order to explain them, we need a few lemmas.

\begin{lemma}\label{lem:convex_closed}
Given $H$ and $C$ both finite and a risk function $R:C\times H\to \mathbb{R}$ and an associated probability set $\prob{P}_C$ on $(\Omega,\sigalg{F})$, $\Omega$ finite, if the function
\begin{align}
    \prob{P}_{\alpha,h}^{\RV{D}|\RV{X}}\mapsto R(\alpha,h)
\end{align}
is linear and 
\begin{align}
    Q:= ((\prob{P}_{\alpha,h}^{\RV{D}|\RV{X}})_{h\in H})_{\alpha\in C}
\end{align}
is convex closed, then the risk set $S$ is convex closed.
\end{lemma}

\begin{proof}
By linearity of 
\begin{align}
    \prob{P}_{\alpha,h}^{\RV{D}|\RV{X}}\mapsto R(\alpha,h)
\end{align}
we also have linearity of 
\begin{align}
    (\prob{P}_{\alpha,h}^{\RV{D}|\RV{X}})_{h\in H}\mapsto (R(\alpha,h))_{h\in H}
\end{align}
Furthermore, $Q$ is bounded when viewed as an element of $\mathbb{R}^{\Omega\times H\times C}$, and so $S$ is the linear image of a compact convex set, and is therefore also compact convex.
\end{proof}

\begin{lemma}\label{lem:linear}
For a see-do model $(\prob{P}_{C\times H}, \RV{X},\RV{Y},\RV{D},\RV{H})$ with utility $u:X\times Y\to \mathbb{R}$, the map
\begin{align}
    \prob{P}_{\alpha,h}^{\RV{D}|\RV{X}}\mapsto R(\alpha,h)
\end{align}
is linear.
\end{lemma}

\begin{proof}
By definition,
\begin{align}
    R(\alpha,h) &= - \prob{P}_{\alpha,h}^{\RV{XY}} u\\
    &= -\prob{P}_{C\times\{h\}}^{\RV{X}} \odot \prob{P}_{\alpha\times h}^{\RV{D}|\RV{X}} \odot \prob{P}_{C\times\{h\}}^{\RV{Y}|\RV{DX}} u
\end{align}
Which is a composition of kernel products involving $\prob{P}_{\alpha\times H}^{\RV{D}|\RV{X}}$, and kernel products are linear, hence this function is linear.
\end{proof}

The preceding theorem does \emph{not} hold for a utility defined on $\Omega$ rather than on $X\times Y$. In this case we have instead
\begin{align}
    -\prob{P}_{C\times\{h\}}^{\RV{X}} \odot \prob{P}_{\alpha\times h}^{\RV{D}|\RV{X}} \odot \prob{P}_{\alpha,h}^{\Omega|\RV{DX}} u
\end{align}
where $\alpha$ appears twice on the right hand side, rendering the map nonlinear.

\begin{lemma}\label{lem:all_kernels_is_convex_hull}
For finite $X$ and $D$, the set of all Markov kernels $X\kto D$ is convex closed.
\end{lemma}

\begin{proof}
From \citet{blackwell_theory_1979}, the set of all Markov kernels $X\kto D$ is the convex hull of the set of all deterministic Markov kernels $X\kto D$. There are a finite number of deterministic Markov kernels, and so the convex hull of this set is closed.
\end{proof}

\begin{example}\label{ex:cc_sdt}
Suppose we have a conditionally independent see-do model $(\prob{P}_{C}, \RV{X},\RV{Y},\RV{D},\RV{H})$ along with a bounded utility $u:Y\to \mathbb{R}$ where $H,D,X$ and $Y$ are all finite, and $\{\prob{P}_\alpha^{\RV{D}|\RV{X}}|\alpha\in C\}$ is the set of all Markov kernels $X\kto D$. Then the risk set is convex and closed downwards, and so the set of Bayes optimal choices is exactly the set of admissible choices.

The boundedness of the risk set $S$ follows from the boundedness of the utility $u$; if $u$ is bounded above by $k$, then $S$ is bounded below in every dimension by $-k$.

The fact that $S$ is convex and closed follows from Lemmas \ref{lem:convex_closed}, \ref{lem:linear} and \ref{lem:all_kernels_is_convex_hull}.
\end{example}

\begin{example}\label{ex:cc_nonsdt}
As before, but suppose we have the see-do model is not conditionally independent. Because none of the lemmas \ref{lem:convex_closed}, \ref{lem:linear} and \ref{lem:all_kernels_is_convex_hull} made use of the conditional independence assumption, the risk set is still convex and closed downwards and so the set of Bayes optimal choices is also exactly the set of admissible choices.
\end{example}

\section{Variables}\label{sec:variable}

In probability theory, it is standard to assume the existence of a probability space $(\mu,\Omega,\sigalg{F})$ and to define \emph{random variables} as measurable functions from $(\Omega,\sigalg{F})$ to $(\mathbb{R},\mathcal{B}(\mathbb{R}))$. However, variables aren't \emph{just} functions -- they're also typically understood to correspond to some measured aspect of the real world. For example, \citet{pearl_causality:_2009} offers the following two, purportedly equivalent, definitions of variables:
\begin{quote}
By a \emph{variable} we will mean an attribute, measurement or inquiry that may take on one of several possible outcomes, or values, from a specified domain. If we have beliefs (i.e., probabilities) attached to the possible values that a variable may attain, we will call that variable a random variable.
\end{quote}

\begin{quote}
This is a minor generalization of the textbook definition, according to which a random variable is a mapping from the sample space (e.g., the set of elementary events) to the real line. In our definition, the mapping is from the sample space to any set of objects called ``values,'' which may or may not be ordered.
\end{quote}

However, these are actually two different things. The first is a \emph{measurement}, which is something we can do in the real world that produces as a result an element of a mathematical set. The second is a \emph{function}, a purely mathematical object with a domain and a codomain and a mapping from the former into the latter. Measurement procedures play the extremely important role of ``pointing to the parts of the world'' that the model addresses.

The general scheme considered in this work is to assume that there is a collection of  ``complete measurement procedure'' $\proc{S}_\alpha$, one for each choice $\alpha\in C$. $\proc{S}_\alpha$ is considered to be the procedure that measures all quantities of interest, and any subprocedure corresponding to a particular quantity of interest reconstructed from the result of $\proc{S}$ by applying a function to its result. The function $\RV{X}$ that, when applied to the result of $\proc{S}$, yields the result of a measurement subprocedure $\proc{X}$ is the \emph{variable} associated with the measurement procedure $\proc{X}$. In this way, a variable $\RV{X}$ -- which is by itself just a mathematical function -- is associated with a measurement procedure in the real world.

\subsection{Variables and measurement procedures}

Consider Newton's second law in the form $\RV{F}=\RV{MA}$. This model relates ``variables'' $\RV{F}$, $\RV{M}$ and $\RV{A}$. As \citet{feynman_feynman_1979} noted, in order to understand this law, some pre-existing understanding of force, mass and acceleration is required. In order to offer a numerical value for the net force on a given object is, even the most knowledgeable physicist will have to go and do a measurement, which involves interacting with the object in some manner that cannot be completely mathematically specified, and which will return a numerical value that will be taken to be the net force.

In order to make sense of the equation $\RV{F}=\RV{MA}$, it must be understood relative to some measurement procedure $\proc{S}$ that simultaneously measures the force on an object, its mass and its acceleration, which can be recovered by the functions $\RV{F}$, $\RV{M}$ and $\RV{A}$ respectively. The equation then says that, whatever result $s$ this procedure yields, $\RV{F}(s)=\RV{M}(s)\RV{A}(s)$ will hold.

A measurement procedure $\proc{S}$ is akin to \citet{menger_random_2003}'s notion of variables as ``consistent classes of quantities'' that consist of pairing between real-world objects and quantities of some type. $\proc{S}$ itself is not a well-defined mathematical thing. At the same time, the set of values it may yield \emph{is} a well-defined mathematical set. No actual procedure can be guaranteed to return elements of a mathematical set known in advance -- anything can fail -- but we assume that we can study procedures reliable enough that we don't lose much by ignoring this possibility.

Note that, because $\proc{S}$ is not a purely mathematical thing, we cannot perform mathematical reasoning with $\proc{S}$ directly. It is much more practical to relegate $\proc{S}$ to the background, and reason in terms of the functions $\RV{F}$, $\RV{M}$ and $\RV{A}$. However, even if we don't talk about it much, $\proc{S}$ remains an important element of the law.

\subsection{Measurement procedures}\label{sec:mprocs}

\begin{definition}[Measurement procedure]
A \emph{measurement procedure} $\proc{B}$ is a procedure that involves interacting with the real world somehow and delivering an element of a mathematical set $X$ as a result. A procedure $\proc{B}$ is said to takes values in a set $B$.
\end{definition}

We adopt the convention that the procedure name $\proc{B}$ and the set of values $B$ share the same letter.

\begin{definition}[Values yielded by procedures]
$\proc{B}\yields x$ is the proposition that the the procedure $\proc{B}$ will yield the value $x\in X$. $\proc{B}\yields A$ for $A\subset X$ is the proposition $\lor_{x\in A} \proc{B}\yields x$.
\end{definition}

\begin{definition}[Equivalence of procedures]\label{def:equality}
Two procedures $\proc{B}$ and $\proc{C}$ are equal if they both take values in $X$ and $\proc{B}\yields x\iff \proc{C}\yields x$ for all $x\in X$.
\end{definition}

If two involve different measurement actions in the real world but necessarily yield the same result, we say they are equivalent.

It is worth noting that this notion of equivalence identifies procedures with different real-world actions. For example, ``measure the force'' and ``measure everything, then discard everything but the force'' are often different -- in particular, it might be possible to measure the force only before one has measured everything else. Thus the result yielded by the first procedure could be available before the result of the second. However, if the first is carried out in the course of carrying out the second, they both yield the same result in the end and so we treat them as equivalent. 

Measurement procedures are like functions without well-defined domains. Just like we can compose functions with other functions to create new functions, we can compose measurement procedures with functions to produce new measurement procedures.

\begin{definition}[Composition of functions with procedures]
Given a procedure $\proc{B}$ that takes values in some set $B$, and a function $f:B\to C$, define the ``composition'' $f\circ \proc{B}$ to be any procedure $\proc{C}$ that yields $f(x)$ whenever $\proc{B}$ yields $x$. We can construct such a procedure by describing the steps: first, do $\proc{B}$ and secondly, apply $f$ to the value yielded by $\proc{B}$.
\end{definition}

For example, $\proc{MA}$ is the composition of $h:(x,y)\mapsto xy$ with the procedure $(\proc{M},\proc{A})$ that yields the mass and acceleration of the same object. Measurement procedure composition is associative:

\begin{align}
    (g\circ f)\circ\proc{B}\text{ yields } x &\iff B\text{ yields } (g\circ f)^{-1}(x) \\
    &\iff B\text{ yields } f^{-1}(g^{-1}(x))\\
    &\iff f\circ B \text{ yields } g^{-1}(x)\\
    &\iff g\circ(f\circ B)\text{ yields } x
\end{align}


One might wonder whether there is also some kind of ``tensor product'' operation that takes a standalone $\proc{M}$ and a standalone $\proc{A}$ and returns a procedure $(\proc{M},\proc{A})$. Unlike function composition, this would be an operation that acts on two procedures rather than a procedure and a function. Thus this ``append'' combines real-world operations somehow, which might introduce additional requirements (we can't just measure mass and acceleration; we need to measure the mass and acceleration of the same object at the same time), and may be under-specified. For example, measuring a subatomic particle's position and momentum can be done separately, but if we wish to combine the two procedures then we can get different results depending on the order in which we combine them.

Our approach here is to suppose that there is some complete measurement procedure $\proc{S}$ to be modeled, which takes values in the observable sample space $(\Psi,\sigalg{E})$ and for all measurement procedures of interest there is some $f$ such that the procedure is equivalent to $f\circ \proc{S}$ for some $f$. In this manner, we assume that any problems that arise from a need to combine real world actions have already been solved in the course of defining $\proc{S}$.

Given that measurement processes are in practice finite precision and with finite range, $\Psi$ will generally be a finite set. We can therefore equip $\Psi$ with the collection of measurable sets given by the power set $\sigalg{E}:=\mathscr{P}(\Psi)$, and $(\Psi,\sigalg{E})$ is a standard measurable space. $\sigalg{E}$ stands for a complete collection of logical propositions we can generate that depend on the results yielded by the measurement procedure $\proc{S}$.

One could also consider measurement procedures to produce results in $(\mathbb{R},\mathcal{B}(\mathbb{R}))$ (i.e. the reals with the Borel sigma-algebra) or a set isomorphic to it. This choice is often made in practice, and following standard practice we also often consider variables to take values in sets isomorphic to $(\mathbb{R},\mathcal{B}(\mathbb{R}))$. However, for measurement in particular this seems to be a choice of convenience rather than necessity -- for any measurement with finite precision and range, it is possible to specify a finite set of possible results.

\subsection{Observable variables}

Our \emph{complete} procedure $\proc{S}$ represents a large collection of subprocedures of interest, each of which can be obtained by composition of some function with $\proc{S}$. We call the pair consisting of a subprocedure of interest $\proc{X}$ along with the variable $\RV{X}$ used to obtain it from $\proc{S}$ an \emph{observable variable}.

\begin{definition}[Observable variable]
Given a measurement procedure $\proc{S}$ taking values in $(\Psi,\sigalg{E})$, an observable variable is a pair $(\RV{X}\circ \proc{S},\RV{X})$ where $\RV{X}:(\Psi,\sigalg{E})\to (X,\sigalg{X})$ is a measurable function and $\proc{X}:=\RV{X}\circ \proc{S}$ is the measurement procedure induced by $\RV{X}$ and $\proc{S}$.
\end{definition}

For the model $\RV{F}=\RV{MA}$, for example, suppose we have a complete measurement procedure $\proc{S}$ that yields a triple (force, mass, acceleration) taking values in the sets $X$, $Y$, $Z$ respectively. Then we can define the ``force'' variable $(\proc{F},\RV{F})$ where $\proc{F}:=\RV{F}\circ \proc{S}$ and $\RV{F}:X\times Y\times Z\to X$ is the projection function onto $X$.

A measurement procedure yields a particular value when it is completed. We will call a proposition of the form ``$\proc{X}$ yields $x$'' an \emph{observation}. Note that $\proc{X}$ need not be a complete procedure here. Given the complete procedure $\proc{S}$, a variable $\RV{X}:\Psi\to X$ and the corresponding procedure $\proc{X}=\RV{X}\circ\proc{S}$, the proposition ``$\proc{X}$ yields $x$'' is equivalent to the proposition ``$\proc{S}$ yields a value in $\RV{X}^{-1}(x)$''. Because of this, we define the \emph{event} $\RV{X}\yields x$ to be the set $\RV{X}^{-1}(x)$.

\begin{definition}[Event]
Given the complete procedure $\proc{S}$ taking values in $\Psi$ and an observable variable $(\RV{X}\circ \proc{S},\RV{X})$ for $\RV{X}:\Psi\to X$, the \emph{event} $\RV{X}\yields x$ is the set $\RV{X}^{-1}(x)$ for any $x\in X$.
\end{definition}

If we are given an observation ``$\proc{X}$ yields $x$'', then the corresponding event $\RV{X}\yields x$ is \emph{compatible with this observation}.

It is common to use the symbol $=$ instead of $\bowtie$ to stand for ``yields'', but we want to avoid this because $\RV{Y}=y$ already has a meaning, namely that $\RV{Y}$ is a constant function everywhere equal to $y$.

An \emph{impossible event} is the empty set. If $\RV{X}\yields x=\emptyset$ this means that we have identified no possible outcomes of the measurement process $\proc{S}$ compatible with the observation ``$\proc{X}$ yields $x$''. 

\subsection{Model variables}

Observable variables are special in the sense that they are tied to a particular measurement procedure $\proc{S}$. However, the measurement procedure $\proc{S}$ does not enter into our mathematical reasoning; it guides our construction of a mathematical model, but once this is done mathematical reasoning proceeds entirely with mathematical objects like sets and functions, with no further reference to the measurement procedure.

A \emph{model variable} is simply a measurable function with domain $(\Psi,\sigalg{E})$.

Model variables do not have to be derived from observable variables. We may instead choose a sample space for our model $(\Omega,\sigalg{F})$ that does not correspond to the possible values that $\proc{S}$ might yield. In that case, we require a surjective model variable $\RV{S}:\Omega\to \Psi$ called the complete observable variable, and every observable variable $(\RV{X}'\circ \proc{S},\RV{X}')$ is associated with the model variable $\RV{X}:=\RV{X}'\circ \RV{S}$.

An \emph{unobserved variable} is a variable whose set of possible values is not constrained by the results of the measurement procedure.

\begin{definition}[Unobserved variable]\label{def:unobserved_variable}
Given a sample space $(\Omega,\sigalg{F})$ and a complete observable variable $\RV{S}:\Omega\to\Psi$, a model variable $\RV{Y}:\Omega\to Y$ is \emph{unobserved} if $\RV{Y}(\RV{S}\yields s)=Y$ for all $s\in \Psi$.
\end{definition}

\subsection{Variable sequences and partial order}

Given $\RV{Y}:\Omega\to X$, we can define a sequence of variables: $(\RV{X},\RV{Y}):=\omega\mapsto (\RV{X}(\omega),\RV{Y}(\omega))$. $(\RV{X},\RV{Y})$ has the property that $(\RV{X},\RV{Y})\yields (x,y)= \RV{X}\yields x\cap \RV{Y}\yields y$, which supports the interpretation of $(\RV{X},\RV{Y})$ as the values yielded by $\RV{X}$ and $\RV{Y}$ together.

Define the partial order on variables $\varlessthan$ where $\RV{X}\varlessthan \RV{Y}$ can be read ``$\RV{X}$ is completely determined by $\RV{Y}$''.

\begin{definition}[Variables determined by another variable]\label{def:variable_po}
Given a sample space $(\Omega,\sigalg{F})$ and variables $\RV{X}:\Omega\to X$, $\RV{Y}:\Omega\to Y$, $\RV{X}\varlessthan \RV{Y}$ if there is some $f:Y\to X$ such that $\RV{X}=f\circ \RV{Y}$.
\end{definition}

Clearly, $\RV{X}\varlessthan(\RV{X},\RV{Y})$ for any $\RV{X}$ and $\RV{Y}$.

\subsection{Decision procedures}\label{sec:actions}

The kind of problem we want to solve requires us to compare the consequences of different choices from a set of possibilities $C$. We take the \emph{consequences of} $\alpha\in C$ to refer to the values obtained by some measurement procedure $\proc{S}_\alpha$ associated with the choice $\alpha$.

As we have said, what exactly a ``measurement procedure'' is is a bit vague -- it's ``what we actually do to get the numbers we associate with variables''. It seems we could describe the above in terms of a single measurement procedure $\proc{S}$, which involves:

\begin{enumerate}
    \item Choose $\alpha$
    \item Proceed according to $\proc{S}_\alpha$
\end{enumerate}

However, $\proc{S}$ is problematic to model. The model is often part of the process of choosing $\alpha$, and so a model of $\proc{S}$ that involves the step ``choose $\alpha$'' will be self-referential. Because of this, we don't try to model $\proc{S}$, and whether this changes anything is an open question.

\begin{definition}[Decision procedure]
A decision procedure is a collection $\{\proc{S}_\alpha\}_{\alpha\in C}$ of measurement procedures.
\end{definition}

Like measurement procedures, a decision procedure $\{\proc{S}_\alpha\}_{\alpha\in A}$ isn't a well-defined mathematical object; it's not really a ``set'', because the contents are real-world actions.