
%!TEX root = main.tex

\chapter{Chapter 2: See-do models}

A \emph{statistical model} is an indexed set of probability distributions $\{\prob{P}_\theta|\theta\in \Theta\}$ where each distribution shares a sample space $\prob{P}_\theta\in \Delta(\sigalg{E})$, $\All \theta$. Equivalently, they are maps $\kernel{P}:\Theta\to \Delta(\sigalg{E})$, as any indexed set is essentially a function. Statistical models are ubiquitous -- they are found in statistical decision theory where the elements of $\Theta$ are typically called ``states''\citep{wald_statistical_1950}, in Bayesian inference where the elements of $\Theta$ may be called ``parameters'' \citep{freedman_asymptotic_1963} and in frequentist inference they may be called ``hypotheses'' \citep{fisher_statistical_1992}. 

Though the precise terminology might differ, in all cases described a statistical model can be interpreted as telling us that if the true state/hypothesis/parameter is given by $\theta$, then the observations we receive are distributed according to $\prob{P}_\theta$. Furthermore, no matter how much you or me might want a particular $\theta^*$ to be the true state, neither of us can have any influence over whether or not $\theta^*$ actually \emph{is} the true state. This choice is Mother Nature's alone.

Here we are interested in a class of model where ``we'' (specifically, the model's user) \emph{can} make a choice that influences whatever happens in the end. Not all outcomes can be influenced -- some may be \emph{observations} collected prior to making a choice, for example. In these models, \emph{hypotheses} serve two roles: firstly, given a hypothesis $\theta$ we know that \emph{observations} are distributed according to $\kernel{P}_\theta$ for some $\kernel{P}:\Theta\to \Delta(\sigalg{E})$, just as with statistical models. Secondly, a hypothesis $\theta$ \emph{also} tells us that the consequences of each choice $d\in D$, given by $\kernel{C}_{\theta,d}\in \Delta(\sigalg{F})$ for some $\kernel{C}:\Theta\to \Delta(\sigalg{F})$. Thus the model tells us, for each hypothesis $\theta$, what we expect to \emph{see} and what we expect to happen as a result of \emph{doing} something, hence the name ``see-do model''.

See-do models are a straightforward extension of statistical models, adding a ``do model'' to the ``see model'' that statistical models already provide. We shall also show that they are closely related to \emph{statistical decision problems}, which also consider the possibility of making a choice after observing a given dataset, but unlike see-do models statistical decision problems do not allow for general consequences of decisions.

See-do models can formalise causal questions, whether of the ``associational'', ``interventional'' or ``counterfactual'' types, in the terminology of \citet{pearl_book_2018}. In \ref{ch:4}, I show that see-do models overlap significantly with existing approaches to formalising questions of these types and argue that where see-do models differ, they are favoured by the difference.

\subsection{See-Do Models with diagrams}



A causal statistical decision problem adds data to this basic mix. If this data has any influence on the option that I ultimately select then -- supposing that my preferences do not change -- I must be \emph{uncertain} about how options lead to consequences, and the data may affect my view on which consequences are likely. In order to formally specify everything needed to answer a causal statistical decision problem, I need to formalise the following notions:

\begin{itemize}
    \item \emph{Option sets}; the set of options I have available
    \item \emph{Observations}; the observations I might be given
    \item \emph{Consequences}; what might arise as a result of my having chosen a particular option
    \item \emph{Preferences}; which consequences I want, and which I do not want
    \item \emph{Uncertainty}; I am uncertain about which consequences are brought about by my available options, and I may be able to become less uncertain after considering the data I have been given
\end{itemize}

\emph{See-Do Models} formalise the above notions. See-Do Models use expected utility theory to formalise preferences and probability theory to model noisy observations and uncertainty over consequences. We make these choices because these are well understood and widely accepted tools for modelling preferences and uncertainty respectively. For a given CSDP, See-Do Models regard the \emph{option set}, the \emph{observation space} and the \emph{consequence space} to be fixed.

\begin{definition}[Option set]
An \emph{option set} $D$ is a finite or countable set of options. A decision maker may select any option from $D$ and in addition may select any mixture of options from $\Delta(\sigalg{D})$.
\end{definition}

\begin{definition}[Observation space]
The decision maker receives an \emph{observation}, which is an element of a standard measurable set $(X,\sigalg{X})$.
\end{definition}

\begin{definition}[Consequence space]
The decision maker's choice of option results in a \emph{consequence}, which is an element of a standard measurable set $(Y,\sigalg{Y})$.
\end{definition}

We allow for two types of uncertainty over which consequences will actually take place given the selection of a particular option. Consequences may be stochastic functions of the option selected and of the given data. Secondly, a decision maker may entertain a number of different \emph{hypotheses} about the relationship between data, decision and consequences. A decision maker may select a particular distribution of hypotheses called a \emph{prior} so that the only remaining uncertainty is stochastic, but we avoid assuming that a canonical prior is available at the outset.

\begin{definition}[See-Do Model]
A See-Do Model takes a hypothesis $\theta\in \Theta$ and an option $d\in D$ and returns a joint distribution over observations and consequences with the restriction that, given a particular hypothesis, observations are independent of options. Formally, a See-Do Model is a Markov kernel $\kernel{T}:\Theta\times D\to \Delta(\sigalg{X}\otimes\sigalg{Y})$ where, given the obvious definitions of observations $\RV{X}$, consequences $\RV{Y}$, options $\RV{D}$ and hypothesis $\Theta$, $\RV{X}\CI_{\kernel{T}}\RV{D}|\Theta$. 

Letting $\kernel{O}$ be a version of $\kernel{T}^{\RV{X}|\Theta}$ and letting $\kernel{S}$ be a version of $\kernel{T}^{\RV{Y}|\RV{D}\RV{X}\Theta}$ we can write

\begin{align}
    \kernel{T} = 
    \begin{tikzpicture} \path (0,0) node (T) {$\Theta$}
        + (0,-1) node (D) {$\RV{D}$}
        ++ (0.5,0) coordinate (copy0)
        ++ (0.5,0) node[kernel] (O) {$\kernel{O}$}
        ++ (0.7,0) coordinate (copy1)
        +  (0.4,-1) node[kernel] (C) {$\kernel{S}$}
        ++ (1.1,0) node (X) {$\RV{X}$}
        +  (0,-1) node (Y) {$\RV{Y}$};
        \draw (T) -- (O) -- (X);
        \draw (copy0) to [out=-90,in=180] ($(C.west) + (0,0)$);
        \draw (D) to [out=0,in=180] ($(C.west) + (0,-0.15)$);
        \draw (copy1) to [out=-60,in=180] ($(C.west)+ (0,0.15)$);
        \draw (C) -- (Y);
    \end{tikzpicture}
\end{align}
\end{definition}

\begin{definition}[Hypothesis sufficiency]
According to the general definition, observations provide evidence about which hypothesis $\theta$ is correct \emph{and also} may directly affect the consequences. The hypotheses are \emph{sufficient} for a See-Do Model $\kernel{T}:\Theta\times D\to \Delta(\sigalg{X}\otimes\sigalg{Y})$ if $\RV{Y}\CI_{\kernel{T}} \RV{X}|\Theta\RV{D}$. In this case, letting $\kernel{O}$ be a version of $\kernel{T}^{\RV{X}|\Theta}$ and $\kernel{C}$ be a version of $\kernel{T}^{\RV{Y}|\RV{D}\Theta}$ we can write

\begin{align}
    \kernel{T} = 
    \begin{tikzpicture} \path (0,0) node (T) {$\Theta$}
        + (0,-1) node (D) {$\RV{D}$}
        ++ (0.5,0) coordinate (copy0)
        ++ (0.5,0) node[kernel] (O) {$\kernel{O}$}
        +  (0.,-1) node[kernel] (C) {$\kernel{C}$}
        ++ (1,0) node (X) {$\RV{X}$}
        +  (0,-1) node (Y) {$\RV{Y}$};
        \draw (T) -- (O) -- (X);
        \draw (copy0) to [out=-90,in=180] ($(C.west) + (0,0.1)$);
        \draw (D) to [out=0,in=180] ($(C.west) + (0,-0.1)$);
        \draw (C) -- (Y);    
    \end{tikzpicture}
\end{align}

To specify a see-do model with sufficient hypotheses we require an observation model $\kernel{O}:\Theta\to \Delta(\sigalg{X})$ and a consequence map $\kernel{C}:\Theta\times D\to \Delta(\sigalg{Y})$. To specify an insufficient model, we require an observation model (as before) and a \emph{state-dependent} consequence map $\kernel{S}:\Theta\times X\times D\to \Delta(\sigalg{Y})$.
\end{definition}

\begin{definition}[Bayesian See-Do Model]
A Bayesian See-Do Model $\kernel{T}$ is a Markov kernel $D\to \Delta(\sigalg{X}\otimes\sigalg{Y})$ with the property $\RV{X}\CI_{\kernel{T}}\RV{D}$. Letting $\kernel{V}\in\kernel{T}^{\RV{Y}|\RV{X}\RV{D}}$, it can be written in the form

\begin{align}
    \kernel{T} = 
    \begin{tikzpicture} \path (0,0) node[dist] (T) {$\gamma$}
        + (0,-1) node (D) {$\RV{D}$}
        ++ (0.5,0) coordinate (copy0)
        ++ (0.5,0) coordinate (O)
        +  (0.3,-1) node[kernel] (C) {$\kernel{V}$}
        ++ (1,0) node (X) {$\RV{X}$}
        +  (0,-1) node (Y) {$\RV{Y}$};
        \draw (T) -- (O) -- (X);
        \draw (copy0) to [out=-90,in=180] ($(C.west) + (0,0)$);
        \draw (D) to [out=0,in=180] ($(C.west) + (0,-0.15)$);
        \draw (C) -- (Y);
    \end{tikzpicture}
\end{align}

A Bayesian See-Do Model can be constructed from a See-Do Model $\kernel{T}'$ by choosing an arbitrary prior $\gamma\in \Delta(\Theta)$ and taking the product:

\begin{align}
    \kernel{T} &= (\gamma\otimes\mathrm{Id}^D)\kernel{T}'\\ 
               &= \begin{tikzpicture} \path (0,0) node[dist] (T) {$\gamma$}
        + (0,-1) node (D) {$\RV{D}$}
        ++ (0.5,0) coordinate (copy0)
        ++ (0.5,0) node[kernel] (O) {$\kernel{O}$}
        ++ (0.7,0) coordinate (copy1)
        +  (0.4,-1) node[kernel] (C) {$\kernel{S}$}
        ++ (1.1,0) node (X) {$\RV{X}$}
        +  (0,-1) node (Y) {$\RV{Y}$};
        \draw (T) -- (O) -- (X);
        \draw (copy0) to [out=-90,in=180] ($(C.west) + (0,0)$);
        \draw (D) to [out=0,in=180] ($(C.west) + (0,-0.15)$);
        \draw (copy1) to [out=-60,in=180] ($(C.west)+ (0,0.15)$);
        \draw (C) -- (Y);
    \end{tikzpicture}\label{eq:t_with_prior}\\
               &= \begin{tikzpicture} \path (0,0) node[dist] (T) {$\gamma \kernel{O}$}
        + (0,-1.3) node (D) {$\RV{D}$}
        ++ (0.8,0) coordinate (copy0)
        + (-0.2,-0.2) coordinate (r1)
        + (0,-0.3) coordinate (copy1)
        +  (0.9,-0.3) coordinate (c2)
        + (0.8,-1) node[kernel] (Tg) {$\kernel{T}^{\Theta|\RV{X}}$}
        +  (1.8,-1.3) node[kernel] (C) {$\kernel{S}$}
        ++ (2.4,0) node (X) {$\RV{X}$}
        +  (0,-1.3) node (Y) {$\RV{Y}$}
        + (0.3,-1.6) coordinate (r2);
        \draw (T) -- (X);
        \draw (copy0) to [out=-85,in=180] (Tg) (Tg) to [out=0,in=180] ($(C.west) + (0,0)$);
        \draw (D) to [out=0,in=180] ($(C.west) + (0,-0.15)$);
        \draw (copy1) to (c2) to [out=-0,in=180] ($(C.west)+ (0,0.15)$);
        \draw[red] (r1) rectangle (r2);
        \draw (C) -- (Y);
    \end{tikzpicture}
\end{align}

The existence of $\kernel{T}^{\Theta|\RV{X}}$ follows from the fact that $\Theta$ is independent of $\RV{D}$ in \ref{eq:t_with_prior}. Define $\kernel{V}$ as the kernel in the red box. Then

\begin{align}
    \kernel{T} &= \begin{tikzpicture} \path (0,0) node[dist] (T) {$\gamma\kernel{O}$}
        + (0,-1) node (D) {$\RV{D}$}
        ++ (0.5,0) coordinate (copy0)
        ++ (0.5,0) coordinate (O)
        +  (0.3,-1) node[kernel] (C) {$\kernel{V}$}
        ++ (1,0) node (X) {$\RV{X}$}
        +  (0,-1) node (Y) {$\RV{Y}$};
        \draw (T) -- (O) -- (X);
        \draw (copy0) to [out=-90,in=180] ($(C.west) + (0,0)$);
        \draw (D) to [out=0,in=180] ($(C.west) + (0,-0.15)$);
        \draw (C) -- (Y);
    \end{tikzpicture}
\end{align}

\end{definition}

\subsubsection{Examples of see-do models}

Suppose we are betting on the outcome of the flip of a possibly biased coin with payout 1 for a correct guess and 0 for an incorrect guess, and we are given $N$ previous flips of the coin to inspect. This situation can be modeled by a hypothesis sufficient see-do model. Define $\kernel{B}:(0,1)\to \Delta(\{0,1\})$ by $\kernel{B}:\theta\mapsto \mathrm{Bernoulli}(\theta)$. Then define $\kernel{T}^1$ by:

\begin{itemize}
    \item $D=\{0,1\}$
    \item $X=\{0,1\}^N$
    \item $Y=\{0,1\}$
    \item $\Theta=(0,1)$
    \item $\kernel{O}:\splitter{0.1}^N\kernel{B}$
    \item $\kernel{C}:(\theta,d)\mapsto \mathrm{Bernoulli}(1-|d-\theta|)$
\end{itemize}

Where $\otimes^N$ indicates the tensor product copied $N$ times. The chance $\theta$ of the coin landing on heads is as much as we can hope to know about how our bet will work out.

Suppose instead that in addition to the $N$ prior flips, we manage to sneak a look at the outcome of the flip on which we will bet. In this case, the situation can be modeled by the following hypothesis insufficient see-do model $\kernel{T}^2$:

\begin{itemize}
    \item $D=\{0,1\}$
    \item $X=\{0,1\}^{N+1}$
    \item $Y=\{0,1\}$
    \item $\Theta=(0,1)$
    \item $\kernel{O}:\splitter{0.1}^{N+1}\kernel{B}$
    \item $\kernel{S}:(\theta,\mathbf{x},d)\mapsto \delta_{1-|d-x_{N+1}|}$
\end{itemize}

It is also possible to model the second situation with a hypothesis sufficient model if we include the result of the $N+1$th flip in the hypothesis. Define $\kernel{T}^3$ by the elements:

\begin{itemize}
    \item $D=\{0,1\}$
    \item $X=\{0,1\}^{N+1}$
    \item $Y=\{0,1\}$
    \item $\Theta=(0,1)\times\{0,1\}$
    \item $\kernel{O}:(\splitter{0.1}^N\kernel{B}\otimes \delta_{x_{N+1}}$
    \item $\kernel{S}:(\theta,x_{N+1},d)\mapsto \delta_{1-|d-x_{N+1}|}$
\end{itemize}

However, $\RV{X}_{N+1}$ is related to the prior flips $\vecRV{X}_{<N}$. In particular, given $\theta\in \Theta$, $\RV{X}_{N+1}$ should be distributed according to Bernoulli($\theta$). If we require that any prior $\gamma$ over $\Theta\times \{0,1\}$ have this property, then definining $\kernel{B}^N:=\splitter{0.1}^N\kernel{B}$, the model will factorise as

\begin{align}
    (\gamma\otimes\mathrm{Id}^D)\kernel{T}^3 &= 
    \begin{tikzpicture} \path (0,0) node[dist,inner sep=-1pt] (T) {$\gamma^\Theta$}
        + (0,-1) node (D) {$\RV{D}$}
        ++ (0.5,0) coordinate (copy0)
        ++ (0.7,0) node[kernel] (O) {$\kernel{B}$}
        + (0,0.5) node[kernel] (B) {$\kernel{B}^N$}
        ++ (0.7,0) coordinate (copy1)
        +  (0.4,-1) node[kernel] (C) {$\kernel{S}$}
        ++ (1.1,0) node (X) {$\RV{X}_{N+1}$}
        +  (0,-1) node (Y) {$\RV{Y}$}
        + (0,0.5) node (Xn) {$\vecRV{X}_{\leq N}$};
        \draw (T) -- (O) -- (X);
        \draw (copy0) to [out=0,in=180] (B) -- (Xn);
        \draw (copy0) to [out=-90,in=180] ($(C.west) + (0,0)$);
        \draw (D) to [out=0,in=180] ($(C.west) + (0,-0.15)$);
        \draw (copy1) to [out=-60,in=180] ($(C.west)+ (0,0.15)$);
        \draw (C) -- (Y);
    \end{tikzpicture}\\
    &= (\gamma^\Theta\otimes\mathrm{Id}^D)\kernel{T}^2
\end{align}

The only real choice that can be made about the prior is $\gamma^\Theta$, and adding the requirement that $\RV{X}_{N+1}$ is distributed as Bernoulli($\theta$) to $\kernel{T}^3$ yields $\kernel{T}^2$.

\todo[inline]{I don't have a theory of stochastic vs non-stochastic uncertainty, but it is the case in general that a hypothesis sufficient model with additional restrictions on the prior can be replaced by a hypothesis insufficient model with no restrictions on the prior. This is mainly relevant with regard to counterfactuals}

\section{Causal Questions}

\citet{pearl_book_2018} has proposed three types of causal question:
\begin{enumerate}
    \item Association: How are $\RV{X}$ and $\RV{Y}$ related? How would observing $\RV{X}$ change my beliefs about $\RV{Y}$?
    \item Intervention: What would happen if I do ... ? How can I make $E$ happen?
    \item Counterfactual: What if I had done ... instead of what I actually did?
\end{enumerate}

I will initially focus on the second question type: ``How can I make $E$ happen?'', and later show how the approach for this type of question can be generalised to handle questions of the third type. Call this kind of question a \emph{causal decision problem}:

\begin{quote}\label{def:causal_decision_problem}
    Given available options, which ones are most likely to lead to a desirable result?
\end{quote}

Causal \emph{statistical} causal decision problems extend causal decision problems by introducing data:

\begin{quote}\label{def:causal_statistical_decision_problem}
    Given my available options and data, which options are likely to lead to a desirable result?
\end{quote}


\subsubsection{Decision rules}

See-do models encode the relationship between observed data and consequences of decisions. In order to actually make decisions, we also require preferences over consequences. We suppose that a \emph{utility function} is given, and evaluate the desirability of consequences using \emph{expected utility}. A see-do model along with a utility allows us to evaluate the desirability of \emph{decisions rules} according to each hypothesis.

\begin{definition}[Utility function]
Given a See-Do Model $\kernel{T}:\Theta\times D\to \Delta(\sigalg{X}\otimes\sigalg{Y})$, a \emph{utility function} $u$ is a measurable function $Y\to \mathbb{R}$. 
\end{definition}

\begin{definition}[Expected utility]
Given a utility function $u:Y\to \mathbb{R}$ and probability measures $\mu,\nu\in \Delta(\sigalg{Y})$, the \emph{expected utility} of $\mu$ is $\mathbb{E}_{\mu}[u]$.

$\mu$ is \emph{preferred} to $\nu$ if $\mathbb{E}_{\mu}[u]\geq \mathbb{E}_{\nu}[u]$, and \emph{strictly preferred} if $\mathbb{E}_{\mu}[u]>\mathbb{E}_{\nu}[u]$.
\end{definition}

\begin{definition}[Decision rule]
Given a see-to map $\kernel{T}:\Theta\times D\to \Delta(\sigalg{X}\otimes\sigalg{Y})$, a \emph{decision rule} is a Markov kernel $X\to \Delta(\sigalg{D})$. A \emph{deterministic decision rule} is a decision rule that is deterministic.

\todo[inline]{Define deterministic Markov kernels}
\end{definition}

Expected utility together with a decision rule gives rise to the definition of \emph{risk}, which connects CSDT to classical statistical decision theory (SDT). For historical reasons, risks are minimised while utilities are maximised.

\begin{definition}[Risk]
Given a see-to map $\kernel{T}:\Theta\times D\to \Delta(\sigalg{X}\otimes\sigalg{Y})$, a utility $u:Y\to \mathbb{R}$ and the set of decision rules $\mathscr{U}$, the \emph{risk} is a function $l:\Theta\times \mathscr{U}\to \mathbb{R}$ given by

\begin{align}
    R(\theta,\kernel{U}) := - \int_X  \kernel{U}_x \kernel{T}^{\RV{Y}|\RV{D}\RV{X}\Theta}_{\cdot,x,\theta} u d\kernel{T}^{\RV{X}|\Theta}_\theta(x)
\end{align}

for $\theta\in \Theta$, $\kernel{U}\in \mathscr{U}$. Here $\kernel{U}_x \kernel{T}^{\RV{Y}|\RV{D}\RV{X}\Theta}_{\cdot,x,\theta} u$ is the product of the measure $\kernel{U}_x$, the kernel $\kernel{T}^{\RV{Y}|\RV{D}\RV{X}\Theta}_{\cdot,x,\theta}:D\to \Delta(\sigalg{Y})$ and the function $u$.
\end{definition}

The loss induces a partial order on decision rules. If for all $\theta$, $l(\theta,\kernel{U})\leq l(\theta,\kernel{U}')$ then $\kernel{U}$ is at least as good as $\kernel{U}'$. If, furthermore, there is some $\theta_0$ such that $l(\theta_0,\kernel{U})<l(\theta_0,\kernel{U}')$ then $\kernel{U}$ is preferred to $\kernel{U}'$.

\begin{definition}[Induced statistical decision problem]
A see-do model $\kernel{T}:\Theta\times D\to \Delta(\sigalg{X}\otimes\sigalg{Y})$ along with a utility $u$ induces the \emph{statistical decision problem} $(\Theta,\mathscr{U},R)$ with states $\Theta$, decisions $\mathscr{U}$ and risks $R$.

\todo[inline]{Statistical decision problems usually define the risk via the loss, but it is only possible to define a loss with a hypothesis sufficient model. We don't actually need a loss, though: the complete class theorem still holds via the induced risk and Bayes risk}

\end{definition}


\todo[inline]{An alternative method of converting hypothesis insufficient to hypothesis sufficient models involves expanding the decision set; this is not appliccable to counterfactual models.}


A key difference between CSDT and other approaches to causal inference is that diagrams in CSDT feature two coupled maps $\kernel{O}$ and $\kernel{C}$, while most other approaches to causal inference represent both $\kernel{O}$ and $\kernel{C}$ in one diagram. \citet{lattimore_replacing_2019} is the only other example I am aware of that represents both $\kernel{O}$ and $\kernel{C}$. Nevertheless, ``one-picture'' causal models such as Causal Bayesian Networks, Single World Intervention Graphs \emph{do} represent observational distributions and interventional maps, and the two differ (see Section \ref{sec:single_double_representation})

A causal hypothesis class $\Theta$ induces a binary relation between observed probability distributions $\prob{O}_\theta$ and consequence maps $\prob{C}_\theta$. This approach is very agnostic about the actual relation induced -- we do not even insist that the range of the observed data $X$ is the same as the range of possible consequences $Y$ (though we will generally limit our attention to cases where the two coincide). 

In common with \citet{heckerman_decision-theoretic_1995}, decisions (or ``acts'') are primitive elements of See-Do Models. In contrast to our work, \citet{heckerman_decision-theoretic_1995} only discuss deterministic \emph{consequence maps}, while See-Do Models represent relations between consequence maps and observed probability.

Decisions are similar to the ``regime indicators'' found in \citet{dawid_decision-theoretic_2020}. They coincide precisely if we suppose that the observation and consequence spaces coincide ($X=Y$) and there exists an ``idle'' decision $d^*\in D$ such that $\kernel{C}_{(\cdot,d^*)} = \kernel{O}_{\cdot}$. However, in general we don't require that $\kernel{O}$ and $\kernel{C}$ are related in this manner. This assumption will be revisited in \todo[inline]{A section I haven't written yet}.

\subsection{D-causation}

While we take $D$ to be a primitive element of causal decision problems, and therefore a primitive of See-Do Models. Causes are not primitive, but we can offer a secondary notion of causation. We call this $D$-causation to stress the fact that it arises in a theory of causal inference in which the set $D$ of available decisions is primitive. A similar idea is discussed extensively in \citet{heckerman_decision-theoretic_1995}. The main differences are that what we call ``consequence maps'' map decisions to probability distributions over possible consequences while Heckerman and Shachter work with ``states'' that map decisions deterministically to consequences. In addition, while we define $D$-causation relative to a particular consequence map $\kernel{C}_\theta$, Heckerman and Shachter define it with respect to a \emph{set} of states.

Section \ref{sec:cbns_without_d} explores the difficulty of defining ``objective causation'' without reference to a set of basic decisions, acts or operations. $D$ need not be interpreted as the set of decisions an agent may make, but whatever interpretation it is assigned, all existing examples of causal models seem to require a ``domain set''.

See Section \ref{ssec:random_variables} for the definition of random variables.

\todo[inline]{Add definition of conditional independence, revise wire label definitions}

One way to motivate the notion of $D$-causation is to observe that for many decision problems, the full set $D$ may be extremely large. Suppose I aim to have my light switched on, and there is a switch that controls the light. Often, the relevant choice of acts for such a problem would appear to be $D_0=\{\text{flip the switch},\text{don't flip the switch}\}$. However, in principle I have a much larger range of options to choose from. For simplicity's sake, suppose I have instead the following set of options:

\begin{align*}
D_1:=&\{``\text{walk to the switch and press it with my thumb}'', \\
    &``\text{trip over the lego on the floor, hop to the light switch and stab my finger at it}'',\\
    &``\text{stay in bed}''\}
\end{align*}

If having the light turned on is all that matters, I could consider any acts in $D_1$ to be equivalent if they have the same ultimate impact on the position of the light switch. $D_0$ is a quotient over $D_1$ under this equivalence relation. 

If I hypothesize that, relative to $D_1$, the ultimate state of the light switch is all that matters to determine the ultimate state of the light, I can say that the light switch $D_1$-causes the state of the light. Given this $D_1$-causation, the $D_1$ decision problem can (subject to my hypothesis) be reduced to a $D_0$ decision between states of the light switch.

If I consider an even larger set of possible acts $D_2$, I might not accept the hypothesis of $D_2$-causation. Let $D_2$ be the following acts:

\begin{align*}
D_2:=&\{``\text{walk to the switch and press it with my thumb}'', \\
    &``\text{trip over the lego on the floor, hop to the light switch and stab my finger at it}'',\\
    &``\text{stay in bed}'',
    &``\text{toggle the mains power, then flip the light switch}''\}
\end{align*}

In this case, it would be unreasonable to hypothesize that all acts that left the light switch in the ``on'' position would also result in the light being ``on''. Thus the switch does not $D_2$-cause the light to be on.

Formally, $D$-causation is defined in terms of conditional independence:

\begin{definition}[$D$-causation]\label{def:d_cause}
Given a consequence map $\kernel{C}_\theta:D\to \Delta(\mathcal{Y})$, random variables $\RV{Y}_1:Y\times D\to Y_1$, $\RV{Y}_2:Y\times D\to Y_2$ and domain variable $\RV{D}:Y\times D\to D$ (Definition \ref{def:domain_variable}), $\RV{Y}_1$ $D$-causes $\RV{Y}_2$ iff $\RV{Y}_2\CI_{\kernel{C}_\theta} \RV{D}|\RV{Y}_1$.
\end{definition}

\subsection{D-causation vs Heckerman and Shachter}

Heckerman and Shachter study deterministic ``consequence maps''. Furthermore, what we call hypotheses $\theta\in\Theta$, Heckerman and Schachter call states $s\in S$. One could consider a state to be a hypothesis that is specific enough to yield a deterministic map from decisions to outcomes. Heckerman and Shachter's notion of causation is defined by \emph{limited unresponsiveness} rather than \emph{conditional independence}, which depends on a partition of states rather than a particular hypothesis.

\begin{definition}[Limited unresponsiveness]
    Given states $S$, deterministic consequence maps $\kernel{C}_s:D\to \Delta(F)$ for each $s\in A$ and a random variables $\RV{X}:F\to X$, $\RV{Y}:F\to Y$, $\RV{Y}$ is unresponsive to $\RV{D}$ in states limited by $\RV{X}$ if $\kernel{C}_{(s,d)}^{\RV{X}|\RV{D}}=\kernel{C}_{(s,d')}^{\RV{X}|\RV{D}\RV{S}}\implies \kernel{C}_{(s,d)}^{\RV{Y}|\RV{D}\RV{S}}=\kernel{C}_{(s,d')}^{\RV{Y}|\RV{D}\RV{S}}$ for all $d,d'\in D$, $s\in S$. Write $\RV{Y}\not\hookleftarrow_{\RV{X}} \RV{D}$
\end{definition}

\begin{lemma}[Limited unresponsiveness implies $D$-causation]
For deterministic consequence maps, $\RV{Y}\not\hookleftarrow_{\RV{X}} \RV{D} $ implies $\RV{X}$ $D$-causes $\RV{Y}$ in every state $s\in S$.
\end{lemma}

\begin{proof}
By the assumption of determinism, for each $s\in S$ and $d\in D$ there exists $x(s,d)$ and $y(s,d)$ such that $\kernel{C}^{\RV{X}\RV{Y}|\RV{D}\RV{S}}_{d,s} = \delta_{x(s,d)}\otimes\delta_{y(s,d)}$.

By the assumption of limited unresponsiveness, for all $d,d'$ such that $x(s,d)=x(s,d')$, $y(s,d)=y(s,d')$ also. Define $f:X\times S\to Y$ by $(s,x)\mapsto y(s,[x(s,\cdot)]^{-1}(x(s,d)))$ where $[x(s,\cdot)]^{-1}(a)$ is an arbitrary element of $\{d|x(s,d)=a\}$. For all $s,d$, $f(x(s,d),s)=y(s,d)$. Define $\kernel{M}:X\times D\times S\to \Delta(\mathcal{Y})$ by $(x,d,s)\mapsto \delta_{f(x,s)}$. $\kernel{M}$ is a version of $\kernel{C}^{\RV{Y}|\RV{X},\RV{D},\RV{S}}$ because, for all $A\in \mathcal{X}$, $B\in \mathcal{Y}$, $s\in S$, $d\in D$:

\begin{align}
    \kernel{C}^{\RV{X}|\RV{D}\RV{S}}_{(d,s)}\splitter{0.1}(\kernel{M}\otimes\mathrm{Id}) &= \int_A \kernel{M}(x',d,s;B) d\delta_{x(s,d)}(x') \\
                                                                                        &= \int_A \delta_{f(x',s)}(B) d\delta_{x(s,d)}(x') \\
                                                                                        &= \delta_{f(x(s,d),s)}(B)\delta_{x(s,d)}(A) \\
                                                                                        &= \delta_{y(s,d)}(B)\delta_{x(s,d)}(A)\\
                                                                                        &= \delta_{x(s,d)}\otimes\delta_{y(s,d)}(A\times B)
\end{align}

$\kernel{M}$ is also independent of $\RV{D}$, given the obvious labeling of inputs. Therefore $\RV{Y}\CI_{\kernel{C}_s}\RV{D}|\RV{X}$.
\end{proof}

However, despite limited unresponsiveness implying $D$-causation within every state, it does not imply $D$-causation in mixtures of states. Suppose $D=\{0,1\}$ where $1$ stands for ``toggle light switch'' and $0$ stands for ``do nothing''. Suppose $S=\{[0,0],[0,1],[1,0],[1,1]\}$ where $[0,0]$ represents ``switch initially off, mains off'' the other states generalise this in the obvious way. Finally, $\RV{F}\in\{0,1\}$ is the final position of the switch and $\RV{L}\in\{0,1\}$ is the final state of the light. We have

\begin{align}
    \kernel{C}^{\RV{L}\RV{F}|\RV{D}\RV{S}}_{d,[i,m]} = \delta_{(d\text{ XOR }i)\text{ AND }m}\otimes \delta_{(d\text{ XOR }i)\text{ AND }m}
\end{align}

Within states $[0,0]$ and $[1,0]$, the light is always off, so $\RV{F}=a\implies \RV{L}=0$ for any $a$. In states $[0,1]$ and $[1,1]$, $\RV{F}=1\implies \RV{L}=1$ and $\RV{F}=0\implies \RV{L}=0$. Thus $\RV{L}\not\hookleftarrow_{\RV{F}} \RV{D}$. However, suppose we take a mixture of consequence maps:
\begin{align}
    \kernel{C}_\gamma &= \frac{1}{4}\kernel{C}_{\cdot,[0,0]} + \frac{1}{4}\kernel{C}_{\cdot,[0,1]} + \frac{1}{2}\kernel{C}_{\cdot,[1,1]}\\
    \kernel{C}^{\RV{F}\RV{L}|\RV{D}}_\gamma &= \frac{1}{4} \left[\begin{matrix}
                        1 & 0\\ 0 & 1
                      \end{matrix}\right]\otimes \left[\begin{matrix}
                        1 & 0\\ 1 & 0
                      \end{matrix}\right] + \frac{1}{4} \left[\begin{matrix}
                        1 & 0\\ 0 & 1
                      \end{matrix}\right]\otimes \left[\begin{matrix}
                        1 & 0\\ 0 & 1
                      \end{matrix}\right] + \frac{1}{2}\left[\begin{matrix}
                        0 & 1\\ 1 & 0
                      \end{matrix}\right]\otimes \left[\begin{matrix}
                        0 & 1\\ 1 & 0
                      \end{matrix}\right]
\end{align}

Then

\begin{align}
    [1,0]\kernel{C}^{\RV{F}\RV{L}|\RV{D}}_{\gamma} &= \frac{1}{4}[0,1]\otimes[1,0]+\frac{1}{4}[0,1]\otimes[0,1]+\frac{1}{2}[1,0]\otimes[1,0]\\
    [1,0]\splitter{0.1}(\kernel{C}^{\RV{F}|\RV{D}}_\gamma\otimes \kernel{C}^{\RV{L}|\RV{D}}_\gamma) &= (\frac{1}{2}[0,1]+\frac{1}{2}[1,0])\otimes(\frac{1}{4}[0,1]+\frac{3}{4}[1,0])\\
    \implies [1,0]\kernel{C}^{\RV{F}\RV{L}|\RV{D}}_{\gamma} &\neq [1,0] \splitter{0.1} (\kernel{C}^{\RV{F}|\RV{D}}_\gamma\otimes \kernel{C}^{\RV{L}|\RV{D}}_\gamma)
\end{align}

Thus under hypothesis mixture $\gamma$, $\RV{F}$ does not $D$-cause $\RV{L}$ even though $\RV{F}$ $D$-causes $\RV{L}$ in all states $S$. The definition of $D$-causation was motivated by the idea that we could reduce a difficult decision problem with a large set $D$ to a simpler problem with a smaller ``effective'' set of decisions by exploiting conditional independence. Even if $\RV{X}$ $D$-causes $\RV{Y}$ in every $\theta\in S$, $\RV{X}$ does not necessarily $D$-cause $\RV{Y}$ in mixtures of states in $S$. For this reason, we do not say that $\RV{X}$ $D$-causes $\RV{Y}$ in $S$ if $\RV{X}$ $D$-causes $\RV{Y}$ in every $\theta\in S$, and in this way we differ substantially from \citet{heckerman_decision-theoretic_1995}.

Instead, we simply extend the definition of $D$-causation to mixtures of hypotheses: if $\gamma\in \Delta(\Theta)$ is a mixture of hypotheses, define $\kernel{C}_\gamma:= (\gamma\otimes\textbf{Id})\kernel{C}$. Then $\RV{X}$ $D$-causes $\RV{Y}$ relative to $\gamma$ iff $\RV{Y}\CI_{\kernel{C}_\gamma} \RV{D}|\RV{X}$.

Theorem \ref{th:univ_d_causation} shows that under some conditions, $D$-causation can hold for arbitrary mixtures over subsets of the hypothesis class $\Theta$.

\begin{theorem}[Universal $D$-causation]\label{th:univ_d_causation}
If $\kernel{C}^{\RV{X}|\RV{D}}_{\theta} = \kernel{C}^{\RV{X}|\RV{D}}_{\theta'}$ for all $\theta,\theta'\in S\subset \Theta$ and $\RV{X}$ $D$-causes $\RV{Y}$ in all $\theta\in S$, then $\RV{X}$ $D$-causes $\RV{Y}$ with respect to all mixed consequence maps $\kernel{C}_\gamma$ for all $\gamma\in \Delta(\Theta)$ with $\gamma(S)=1$.
\end{theorem}

\begin{proof}

For $\gamma\in \Delta(\Theta)$, define the mixture

\begin{align}
\kernel{C}_\gamma := \begin{tikzpicture}
    \path (0,0) node[dist] (g) {$\gamma$}
    + (0,-0.45) node (D) {$\RV{D}$}
    ++ (1,-0.3) node[kernel] (C) {$\kernel{C}$}
    ++ (1,0) node (F) {$\RV{F}$};
    \draw (g) to [out=0,in=180] ($(C.west) + (0,0.15)$) (D) -- ($(C.west) + (0,-0.15)$) (C) -- (F);
\end{tikzpicture}
\end{align}

Because $\kernel{C}_\theta^{\RV{X}|\RV{D}} = \kernel{C}_{\theta'}^{\RV{X}|\RV{D}}$ for all $\theta,\theta'\in \Theta$, we have

\begin{align}
\begin{tikzpicture}
    \path (0,0) node[dist] (g) {$\gamma$}
    + (0.7,-0.15) coordinate (copy0)
    + (0,-0.45) node (D) {$\RV{D}$}
    ++ (1.5,-0.3) node[kernel] (C) {$\kernel{C}^{\RV{X}|\RV{D}\Theta}$}
    ++ (1,0) node (X) {$\RV{X}$}
    + (0,0.5) node (T) {$\Theta$};
    \draw (g) to [out=0,in=180] (copy0) -- ($(C.west) + (0,0.15)$) (D) -- ($(C.west) + (0,-0.15)$);
    \draw (C) -- (X);
    \draw (copy0) to [out=90,in=180] (T);
\end{tikzpicture} &= \begin{tikzpicture}
    \path (0,0) node[dist] (g) {$\gamma$}
    + (0,0.5) node[dist] (g2) {$\gamma$}
    + (0.7,-0.15) coordinate (copy0)
    + (0,-0.45) node (D) {$\RV{D}$}
    ++ (1.5,-0.3) node[kernel] (C) {$\kernel{C}^{\RV{X}|\RV{D}\Theta}$}
    ++ (1,0) node (X) {$\RV{X}$}
    + (0,0.3) node (T) {$\Theta$};
    \draw (g) to [out=0,in=180] (copy0) -- ($(C.west) + (0,0.15)$) (D) -- ($(C.west) + (0,-0.15)$);
    \draw (C) -- (X);
    \draw (g2) to [out=0,in=180] (T);
\end{tikzpicture} \label{eq:decompose_condi_x}
\end{align}

Also

\begin{align}
    \kernel{C}_\gamma^{\RV{XY}|\RV{D}} &= \begin{tikzpicture}
    \path (0,0) node[dist] (g) {$\gamma$}
    + (0,-0.45) node (D) {$\RV{D}$}
    ++ (1,-0.3) node[kernel] (C) {$\kernel{C}$}
    ++ (1,0) node[kernel] (F) {$\kernel{F}^{\RV{X}\utimes\RV{Y}}$}
    ++ (1,0.15) node (X) {$\RV{X}$}
    + (0,-0.3) node (Y) {$\RV{Y}$};
    \draw (g) to [out=0,in=180] ($(C.west) + (0,0.15)$) (D) -- ($(C.west) + (0,-0.15)$) (C) -- (F);
    \draw ($(F.east) + (0,0.15)$) -- (X) ($(F.east) + (0,-0.15)$) -- (Y);
\end{tikzpicture}\\
    &= \begin{tikzpicture}
    \path (0,0) node[dist] (g) {$\gamma$}
    + (0,-0.45) node (D) {$\RV{D}$}
    ++ (1,-0.3) node[kernel] (C) {$\kernel{C}^{\RV{XY}|\RV{D}\Theta}$}
    ++ (1,0.15) node (X) {$\RV{X}$}
    + (0,-0.3) node (Y) {$\RV{Y}$};
    \draw (g) to [out=0,in=180] ($(C.west) + (0,0.15)$) (D) -- ($(C.west) + (0,-0.15)$);
    \draw ($(C.east) + (0,0.15)$) -- (X) ($(C.east) + (0,-0.15)$) -- (Y);
\end{tikzpicture}\\
 &= \begin{tikzpicture}
    \path (0,0) node[dist] (g) {$\gamma$}
    + (0,-0.45) node (D) {$\RV{D}$}
    + (0.7,-0.45) coordinate (copy0)
    + (0.7,-0.15) coordinate (copy1)
    ++ (1.4,-0.3) node[kernel] (C) {$\kernel{C}^{\RV{X}|\RV{D}\Theta}$}
    + (0,0.6) coordinate (via0)
    + (0,-0.6) coordinate (via1)
    ++ (0.9,0) coordinate (copy2)
    ++ (0.7,0) node[kernel] (Yx) {$\kernel{C}^{\RV{Y}|\RV{X}\RV{D}\Theta}$}
    ++ (1.2,0.15) node (X) {$\RV{Y}$}
    + (0,-0.5) node (Y) {$\RV{X}$};
    \draw (g) to [out=0,in=180] (copy1) -- ($(C.west) + (0,0.15)$) (D) -- ($(C.west) + (0,-0.15)$) (C)--(Yx);
    \draw (copy0) to [out=-90,in=180] (via1) to [out=0,in=180] ($(Yx.west) + (0,-0.15)$) (copy1) to [out=90,in=180] (via0) to [out=0,in=180] ($(Yx.west) + (0,0.15)$);
    \draw ($(Yx.east) + (0,0.15)$) -- (X) (copy2) to [out=-90,in=180] (Y);
 \end{tikzpicture}\\
 &\overset{\RV{Y}\CI \RV{D}|\RV{X}\Theta}{=} \begin{tikzpicture}
    \path (0,0) node[dist] (g) {$\gamma$}
    + (0,-0.45) node (D) {$\RV{D}$}
    + (0.7,-0.15) coordinate (copy1)
    ++ (1.4,-0.3) node[kernel] (C) {$\kernel{C}^{\RV{X}|\RV{D}\Theta}$}
    ++ (0.9,0.1) coordinate (copy2)
    ++ (0.7,0.3) node[kernel] (Yx) {$\kernel{C}^{\RV{Y}|\RV{X}\Theta}$}
    ++ (1.2,0.15) node (X) {$\RV{Y}$}
    + (0,-0.5) node (Y) {$\RV{X}$};
    \draw (g) to [out=0,in=180] (copy1) -- ($(C.west) + (0,0.15)$) (D) -- ($(C.west) + (0,-0.15)$) (C) to [out=0,in=180] (copy2) to [out=0,in=180] (Yx);
    \draw (copy1) to [out=90,in=180] ($(Yx.west) + (0,0.15)$);
    \draw ($(Yx.east) + (0,0.15)$) -- (X) (copy2) to [out=-90,in=180] (Y);
 \end{tikzpicture} \\
 &\overset{\ref{eq:decompose_condi_x}}{=} \begin{tikzpicture}
    \path (0,0) node[dist] (g) {$\gamma$}
    + (0,-0.45) node (D) {$\RV{D}$}
    + (0.7,-0.15) coordinate (copy1)
    ++ (1.4,-0.3) node[kernel] (C) {$\kernel{C}^{\RV{X}|\RV{D}\Theta}$}
    + (1,0.6) node[dist] (g2) {$\gamma$}
    ++ (0.9,0.1) coordinate (copy2)
    ++ (1,0.3) node[kernel] (Yx) {$\kernel{C}^{\RV{Y}|\RV{X}\Theta}$}
    ++ (1.2,0.15) node (X) {$\RV{Y}$}
    + (0,-0.5) node (Y) {$\RV{X}$};
    \draw (g) to [out=0,in=180] (copy1) -- ($(C.west) + (0,0.15)$) (D) -- ($(C.west) + (0,-0.15)$) (C) to [out=0,in=180] (copy2) to [out=0,in=180] (Yx);
    \draw (g2) to [out=0,in=180] ($(Yx.west) + (0,0.15)$);
    \draw ($(Yx.east) + (0,0.15)$) -- (X) (copy2) to [out=-90,in=180] (Y);
 \end{tikzpicture}\\
 &= \overset{\ref{eq:decompose_condi_x}}{=} \begin{tikzpicture}
    \path (0,0) node (g) {}
    + (0,-0.45) node (D) {$\RV{D}$}
    + (0.7,-0.45) coordinate (copy1)
    ++ (1.4,-0.3) node[kernel] (C) {$\kernel{C}_\gamma^{\RV{X}|\RV{D}\Theta}$}
    + (1,0.6) node[dist] (g2) {$\gamma$}
    ++ (0.9,0.1) coordinate (copy2)
    ++ (1,0.3) node[kernel] (Yx) {$\kernel{C}^{\RV{Y}|\RV{X}\Theta}$}
    + (-0.5,0.6) coordinate (stop0)
    ++ (1.2,0.15) node (X) {$\RV{Y}$}
    + (0,-0.5) node (Y) {$\RV{X}$};
    \draw (D) -- ($(C.west) + (0,-0.15)$) (C) to [out=0,in=180] (copy2) to [out=0,in=180] (Yx);
    \draw (g2) to [out=0,in=180] ($(Yx.west) + (0,0.15)$);
    \draw ($(Yx.east) + (0,0.15)$) -- (X) (copy2) to [out=-90,in=180] (Y);
    \draw[-{Rays[n=8]}] (copy1) to [out=90,in=180] (stop0);
 \end{tikzpicture}\label{eq:is_conditional}
\end{align}
Equation \ref{eq:is_conditional} establishes that $(\gamma\otimes\textbf{Id}_X\otimes\stopper{0.3}_D)\kernel{C}^{\RV{Y}|\RV{X}\Theta}$ is a version of $\kernel{C}_\gamma^{\RV{Y}|\RV{X}\RV{D}}$, and thus $\RV{Y}\CI_{\kernel{C}_\gamma} \RV{D}|\RV{X}$.

This can also be derived from the semi-graphoid rules:

\begin{align}
    \Theta\CI \RV{D} \land \Theta\CI \RV{X} | \RV{D} &\implies \Theta\CI \RV{XD}\\
    &\implies \Theta\CI \RV{D}|\RV{X}\\
    \RV{D} \CI \Theta|\RV{X} \land \RV{D}\CI \RV{Y}|\RV{X}\Theta &\implies \RV{D}\CI \RV{Y}|\RV{X}\\
    &\implies \RV{Y}\CI\RV{D}|\RV{X}
\end{align}
\end{proof}

\subsection{Properties of D-causation}

If $\RV{X}$ D-causes $\RV{Y}$ relative to $\kernel{C}_\theta$, then the following holds:

\begin{align}
    \kernel{C}_{\theta}^{\RV{X}|\RV{D}} &= \begin{tikzpicture}
    \path (0,0) node (D) {$\RV{D}$}
    ++ (0.9,0) node[kernel] (Xd) {$\kernel{C}^{\RV{X}|\RV{D}}$}
    ++ (1.3,0) node[kernel] (Yd) {$\kernel{C}^{\RV{Y}|\RV{X}}$}
    ++ (0.9,0) node (Y) {$\RV{Y}$};
    \draw (D) -- (Xd) -- (Yd) -- (Y); 
    \end{tikzpicture}
\end{align}

This follows from version (2) of Definition \ref{def:conditional_independence}:

\begin{align}
    \kernel{C}_\theta^{\RV{X}|\RV{D}} &= \begin{tikzpicture}
    \path (0,0) node (D) {$\RV{D}$}
    ++ (0.7,0) coordinate (copy0)
    ++ (0.7,0) node[kernel] (Xd) {$\kernel{C}^{\RV{X}|\RV{D}}$}
    + (0,0.5) coordinate (via1)
    ++ (1.3,0) node[kernel] (Yd) {$\kernel{C}^{\RV{Y}|\RV{X}\RV{D}}$}
    ++ (0.9,0) node (Y) {$\RV{Y}$};
    \draw (D) -- (Xd) -- (Yd) -- (Y);
    \draw (copy0) to [out=90,in=180] (via1) to [out=0,in=180] ($(Yd.west)+(0,0.15)$); 
    \end{tikzpicture}\\
     &= \begin{tikzpicture}
    \path (0,0) node (D) {$\RV{D}$}
    ++ (0.7,0) coordinate (copy0)
    ++ (0.7,0) node[kernel] (Xd) {$\kernel{C}^{\RV{X}|\RV{D}}$}
    + (1.3,0.5) coordinate (via1)
    ++ (1.3,0) node[kernel] (Yd) {$\kernel{C}^{\RV{Y}|\RV{X}}$}
    ++ (0.9,0) node (Y) {$\RV{Y}$};
    \draw (D) -- (Xd) -- (Yd) -- (Y);
    \draw[-{Rays[n=8]}] (copy0) to [out=90,in=180] (via1); 
    \end{tikzpicture}\\
    &= \begin{tikzpicture}
    \path (0,0) node (D) {$\RV{D}$}
    ++ (0.9,0) node[kernel] (Xd) {$\kernel{C}^{\RV{X}|\RV{D}}$}
    ++ (1.3,0) node[kernel] (Yd) {$\kernel{C}^{\RV{Y}|\RV{X}}$}
    ++ (0.9,0) node (Y) {$\RV{Y}$};
    \draw (D) -- (Xd) -- (Yd) -- (Y); 
    \end{tikzpicture}
\end{align}

D-causation is not transitive: if $\RV{X}$ D-causes $\RV{Y}$ and $\RV{Y}$ D-causes $\RV{Z}$ then $\RV{X}$ doesn't necessarily D-cause $\RV{Z}$.