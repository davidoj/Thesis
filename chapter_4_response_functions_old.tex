%!TEX root = main.tex

\chapter[Repeatable decision problems]{Models of repeatable decision problems}\label{ch:evaluating_decisions}

Chapter \ref{ch:tech_prereq} introduced probability sets as generic tools for causal modelling, while Chapter \ref{ch:2p_statmodels} examined how probability set models can be used in decision problems and Section \ref{sec:cons_to_sdp} in particular introduced \emph{see-do} models, which featured variables representing observations and consequences, choices and hypotheses. So far, no attention has been paid to how exactly anybody might use a see-do model to help them ``learn from observations''. Observations and consequences in see-do models could be just about anything -- they need not take values in the same set, nor be related to one another in any particular way. While there might be interesting problems involving models where observations and consequences are different kinds of things, causal inference in practice is usually concerned with problems where a sequence of observations is available and the consequences of interest are of the ``same type'' as observations. Such problems involve \emph{repeatable} measurement procedures - the procedure can be broken down into a sequence sub-procedures that all have something in common with one another. This chapter aims to better understand the ``something in common'' that these sub-procedures share.

Repeatable procedures in classical statistics are associated with the assumption of \emph{exchangeability of observations}. Exchangeable models express the assumption that the results of sub-procedures can be swapped without changing the problem in any important way -- this is the ``something in common'' that exchangeable sub-procedures have (note that in a non-Bayesian setting, this assumption is called permutability and is weaker than exchangeability, see \citet[pg. 463]{walley_statistical_1991}). Decision problem models usually aren't compatible with this assumption; often, the decision maker's choices will affect some sub-procedures in different ways to others. For example, the decision maker might choose to record $10$ values without interaction, then tweak the system of interest before recording their $11$th value -- in this case, swapping the results of the $11$th sub-procedure with the $1$st \emph{does} change the problem in an important way, because it changes the rank order of the ``tweaked'' sub-procedure. The need to account for different possible ``inputs'' to each sub-procedure makes the question of commonality between these sub-procedures substantially more complicated than the assumption of exchangeability.

This chapter introduces \emph{conditionally independent and identical response functions} as the ``something in common'' that a sequence of subprocedures might have. Section \ref{sec:response_functions} defines conditionally independent and identical response functions and explores different examples of models that feature them. The assumption of conditionally independent and identical response functions can be combined with additional assumptions, and these yield different classes of models.

Section \ref{sec:ccontracibility} shows that a Markov kernel can be represented as a mixture of conditionally independent and identical response functions if and only if it is \emph{causally contractible} (Theorem \ref{th:ciid_rep_kernel}). Additionally, Theorem \ref{th:ciid_rep_kernel_nolocal} shows that a Markov kernel that satisfies the weaker condition under an additional condition can be represented as a mixture of conditionally independent and identical response functions where the mixture depends on a symmetric function of the entire sequence of inputs.

Probability set models with conditionally independent and identical response functions all feature causally contractible Markov kernels, but depending on the class of model under consideration, these Markov kernels can play different roles. The simplest case is where each input is assumed to be independent of all data that has come previously and the hypothesis (Definition \ref{def:cii_rf}) is assumed independent of the inputs. A model satisfying these assumptions features a causally contractible uniform conditional probability. Such models are explored in Section \ref{sec:data_independent_actions}. Relaxing the assumption of independence between the hypothesis and the inputs yields a causally contractible \emph{uniform 2-comb} (Definition \ref{def:uniform_comb}). This is explored in Section \ref{sec:nonu_input}. Finally, we explore models where inputs can depend on previous data in Section \ref{sec:data_dependent}, in which case the model features a causally contractible \emph{uniform $n$-comb}.

\section{Previous work}\label{sec:prev_work}

This chapter draws on three different lines of work. The first is the study of representations of symmetric of probability models. The equivalence between infinite exchangeable probability models and mixtures of independent and identically distributed models was shown by \cite{de_finetti_foresight_1992}. This result has been extended in many ways, including to finite sequences \citet{kerns_definettis_2006,diaconis_finite_1980} and for partially exchangeable arrays \citet{aldous_representations_1981}. A comprehensive overview of results is presented in \citet{kallenberg_probabilistic_2005}. Particularly similar to our result is the notion of ``partial exchangeability'' from \citet{diaconis_recent_1988}.

The second line of work is the study of exchangeability-like assumptions in causal models. \citet{lindley_role_1981} discuss models consisting of a sequence of exchangeable observations along with ``one more observation'', a structure that is similar to the models with observations and consequences discussed in section \ref{sec:weaker_assumptions}. Lindley discusses the application of this model to questions of causation, but does not explore this deeply due to the perceived difficulty of finding a satisfactory definition of causation. \citet{rubin_causal_2005}'s overview of causal inference with potential outcomes along with the text \citet{imbens_causal_2015} make use of the assumption of exchangeable potential outcomes to prove several identification results. \citet{saarela_role_2020}, using structural causal models, proposes \emph{conditional exchangeability}, which refers to the invariance of a joint distribution over outcomes under ``surgical switches'' of the values of causal variables of interest. This definition depends on having a structural model, a property not shared by the current work.

Exchangeability in the setting of causal models is often discussed in terms of the exchangeability of \emph{people} (or more generically, \emph{experimental units}). \citet{hernan_beyond_2012,greenland_identifiability_1986,banerjee_chapter_2017,dawid_decision-theoretic_2020} all discuss assumptions along these lines.

A stronger assumption than commutativity of exchange is \emph{causal contractibility} (Definition \ref{def:caus_cont}), which adds the assumption of \emph{locality}. This additional assumption appears to have similarities to the stable unit treatment distribution assumption (SUTDA) in \citet{dawid_decision-theoretic_2020}, and the stable unit treatment value assumption (SUTVA) in \citep{rubin_causal_2005}:
\begin{blockquote}
(SUTVA) comprises two sub-assumptions. First, it assumes that \emph{there is no interference between units (Cox 1958)}; that is, neither $Y_i(1)$ nor $Y_i(0)$ is affected by what action any other unit received. Second, it assumes that \emph{there are no hidden versions of treatments}; no matter how unit $i$ received treatment $1$, the outcome that would be observed would be $Y_i(1)$ and similarly for treatment $0$.
\end{blockquote}

Finally, the idea of \emph{combs} in probabilistic models was first proposed by \citet{chiribella_quantum_2008} and an application to causal models was developed by \citet{jacobs_causal_2019}.

\section[Response functions]{Conditionally independent and identical response functions}\label{sec:response_functions}

Suppose a decision maker is implementing a decision procedure where they'll make a choice and subsequently receive a sequence of paired values $(\proc{D}_i,\proc{Y}_i)$, with their objective depending on the output values yielded by $\proc{Y}_i$s only. Usually the $\proc{D}_i$s, which we call ``inputs'', are under the decision maker's control to some extent, but this might not always be the case. For example, perhaps the first $m$ pairs come from data collected by someone else, where the decision maker has no control over inputs, and the next $n$ depend on their own actions where they have complete control over the inputs.

Suppose the decision maker uses a probability set $\prob{P}_C$ to model such a procedure, and variables $(\RV{D}_i,\RV{Y}_i)$ are associated with the inputs and outputs. There are two different relationships between $\RV{D}_i$ and $\RV{Y}_i$ that might be of interest to the decision maker:
\begin{itemize}
    \item For some choice $\alpha$, $j>m$ and some fixed value of $\RV{D}_j$, what are the \emph{likely consequences} with regard to $\RV{Y}_j$?
    \item For some choice $\alpha$, all $i\leq m$ with some fixed value of $\RV{D}_i$, what is the \emph{relative frequency} of different values of $\RV{Y}_i$?
\end{itemize}
The first is what the decision maker wants to know in order to make a good decision, and the second is something they can learn from the data before taking any actions. In particular, if the decision maker has a good reason to thing that the two relationships should be (approximately) the same, then they may choose to set (or influence) $\RV{D}_j$ for $j>m$ to whatever value appears the most favourable according to the preceding data.

More precisely, we are interested in models $\prob{P}_C$ where the probabilistic relationship between each $\RV{D}_i$ and the corresponding $\RV{Y}_i$ is unknown but identical for all indices $i$. To model this, we introduce a hypothesis $\RV{H}$ that represents this unknown relationship, and assert that the distribution of $\RV{Y}_i$ given $(\RV{D}_i,\RV{H})$ is identical for all $i$. Refer to Section \ref{pgph:plates} for the definition of the plate notation.

\begin{definition}[Conditionally independent and identical response functions]\label{def:cii_rf}
A probability set $\prob{P}_C$ on $(\Omega,\sigalg{F})$ with variables $\RV{Y}:=(\RV{Y}_i)_{i\in \mathbb{N}}$ and $\RV{D}:=(\RV{D}_i)_{i\in\mathbb{N}}$ has \emph{conditionally independent and identical response functions} if there is some hypothesis $\RV{H}:\Omega\to H$ and $\kernel{L}:H\times D\kto Y$ such that
\begin{align}
    \prob{P}_C^{\RV{Y}|\RV{DH}} &\overset{\prob{P}_C}{\cong} \tikzfig{do_model_representation_conditional}\label{eq:do_model_rep}\\
    &\iff\\
    \prob{P}_C^{\RV{Y}|\RV{DH}}(\bigtimes_{i\in\mathbb{N}} A|d,h) &= \prod_{i=1}^{\infty} \kernel{L}(A_i{}|d_i,h)&\forall A_i\in\sigalg{Y},h\in H,d\in D^{\mathbb{N}}
\end{align}
\end{definition}

Definition \ref{def:cii_rf} implies $\RV{Y}_i\CI_{\prob{P}_C}^e (\RV{D}_{\neq i},\RV{Y}_{\neq i},\RV{C})|(\RV{D}_i,\RV{H})$. We can consider a number of different kinds of models where this holds. A simple family of models can be obtained by ``attaching'' a prior $\mu$ over $\RV{H}$ to Equation \ref{eq:do_model_rep}. This yields the Markov kernel $\kernel{K}:D^{\mathbb{N}}\kto Y^{\mathbb{N}}$
\begin{align}
    \kernel{K} := \tikzfig{do_model_representation_mu}\label{eq:data_independent_kernel}
\end{align}
$\kernel{K}=\prob{P}_C^{\RV{Y}|\RV{D}}$ if $\RV{H}\CI^e_{\prob{P}_C} (\RV{D},\RV{C})$ and $\RV{D}_i \CI^e_{\prob{P}_C} \RV{Y}_{[i-1]}|(\RV{D}_{[i-1]},\RV{C})$. We say that such models have an \emph{input-independent hypothesis} and \emph{data-independent inputs}, and they are considered in more detail in Section \ref{sec:data_independent_actions}. This is a limited family of models, but it can be a helpful starting point to understand more general ones. Theorem \ref{th:data_ind_CC} establishes that given a data-independent model $\prob{P}_C$, $\prob{P}_C^{\RV{Y}|\RV{D}}$ is \emph{causally contractible} if and only if it can be represented in the form given by Equation \ref{eq:data_independent_kernel}. 

Dropping the assumption $\RV{H}\CI^e_{\prob{P}_C} (\RV{D},\RV{C})$ yields models that simply have data-independent inputs. In this case, we can write for any $\alpha\in C$
\begin{align}
    \prob{P}_\alpha^{\RV{YD}} &= \tikzfig{do_model_representation_mu_nonuniform}
\end{align}
Of particular interest is the case where $\prob{P}_\alpha^{\RV{D}_i|\RV{H}}\neq \prob{P}_\alpha^{\RV{D}_j|\RV{H}}$ for some $i\neq j$. In this case, we have a model with \emph{varying inputs}, and this class of models is explored in Section \ref{sec:nonu_input}. For models with varying inputs, the 2-comb $\prob{P}_C^{\RV{Y}|\RV{D}\combgap \RV{H}}$ is causally contractible (see Section \ref{sec:def_combs} for an explanation of the notation), and this suggests a symmetry with respect to ``input switching'' (which we explain in the section). Models with varying inputs are particularly closely related to many standard causal inference problems, as we will see in Chapter \ref{ch:other_causal_frameworks}.

Dropping the assumption $\RV{D}_i \CI^e_{\prob{P}_C} \RV{Y}_{\neq i}|(\RV{C},\RV{H})$ yields models with \emph{data-dependent inputs}. An example of a model in this class is the multi-armed bandit, a standard toy problem in the field of reinforcement learning \citet{barto_reinforcement_1998}. Models with data-dependent inputs have causally contractible $\mathbb{N}$-combs $\prob{P}_C^{\RV{Y}\combbreak \RV{D}}$ (see Section \ref{sec:def_combs} again for an explanation of the notation).

\section[Causally contractible Markov kernels]{Causally contractible Markov kernels - definitions and explanation}\label{sec:ccontracibility}

In this section we prove representation theorems for Markov kernels that satisfy the assumptions of causal contractibility and exchange commutativity. These theorems will be applied to various families of probability set models in the following sections (specifically, Section \ref{sec:data_independent_actions}, Section \ref{sec:nonu_input} and Section \ref{sec:data_dependent}).

The assumptions of exchange commutativity and \emph{locality} together make causal contractibility. Exchange commutativity expresses a sense in which a Markov kernel $\kernel{K}:X^{\mathbb{N}}\kto Y^{\mathbb{N}}$ can be symmetric: if it yields the same result from shuffling inputs as it does from shuffling outputs. Locality is the assumption that subsets of outputs are probabilistically independent of the non-corresponding inputs, conditioned on the corresponding inputs.

Graphical notation can offer an intuitive picture of these two assumptions. In the simplified case of a sequence of length 2 (that is, $\kernel{K}:X^2\kto Y^2$), exchange commutativity for two inputs and outputs is given by the following equality:
\begin{align}
    \tikzfig{commutativity_of_exchange}
\end{align}
swapping the inputs is equivalent to applying the same swap to the outputs. Locality is given by the following pair of equalities:
\begin{align}
    \tikzfig{cons_locality_1}\\
    \tikzfig{cons_locality_2}
\end{align}
and expresses the idea that the outputs are independent of the non-corresponding input, conditional on the corresponding input.

The general definitions follow.

\begin{definition}[Locality]\label{def:caus_cont}
A Markov kernel $\kernel{K}:X^{\mathbb{N}}\kto Y^{\mathbb{N}}$ is \emph{local} if for all $n\in \mathbb{N}$, $A_i\in \sigalg{Y}$, $(x_{[n]},x_{[n]^C})\in\mathbb{N}$ there exists $\kernel{L}:X^n\kto Y^n$ such that
\begin{align}
    \tikzfig{local_lhs} &= \tikzfig{local_rhs}\\
    &\iff\\
    \kernel{K}(\bigtimes_{i\in [n]} A_i\times Y^{\mathbb{N}}|x_{[n]},x_{[n]^C}) &= \kernel{L}(\bigtimes_{i\in [n]} A_i|x_{[n]})
\end{align}
\end{definition}

\begin{definition}[Exchange commutativity]\label{def:caus_exch}
A Markov kernel $\kernel{K}:X^{\mathbb{N}}\kto Y^{\mathbb{N}}$ \emph{commutes with exchange} if for all finite permutations $\rho:\mathbb{N}\to\mathbb{N}$, $A_i\in \sigalg{Y}$, $(x_{[n]},x_{[n]^C})\in\mathbb{N}$
\begin{align}
    \kernel{K}\mathrm{swap}_{\rho,Y} &=  \mathrm{swap}_{\rho,X} \kernel{K}\\
    &\iff\\
    \kernel{K}(\bigtimes_{i\in\mathbb{N}} A_{\rho(i)}|(x_i)_{i\in {\mathbb{N}}}) &= \kernel{K}(\bigtimes_{i\in\mathbb{N}} A_{i}|(x_{\rho(i)})_{i\in {\mathbb{N}}})
\end{align}
\end{definition}

Causal contractibility is the conjunction of both assumptions.
\begin{definition}[Causal contractibility]
A Markov kernel $\kernel{K}:X^{\mathbb{N}}\kto Y^{\mathbb{N}}$ is \emph{causally contractible} if it is local and commutes with exchange.
\end{definition}

\subsection{Properties of causally contractible Markov kernels}

A causally contractible Markov kernel $\kernel{K}:X^{\mathbb{N}}\kto Y^{\mathbb{N}}$ can be mapped to ``contracted'' versions of the kernel using the $\text{del}$ map. Theorem \ref{th:equal_of_condits} establishes that any two contraction of $\kernel{K}$ with equivalent codomains yields the same result (see also Theorem \ref{th:equal_of_reduced_condits}). This feature is the motivation for the name \emph{causal contractibility}. 

Theorem \ref{th:no_implication} shows that exchange commutativity and locality are independent assumptions.

Before these theorems are proved, the following definition and Lemma will prove helpful.

All swaps can be written as a product of transpositions, so proving that a property holds for all finite transpositions is enough to show it holds for all finite swaps. It's useful to define a notation for transpositions.
\begin{definition}[Finite transposition]
Given two equally sized sequences $A=(a_i)_{i\in [n]}$, $B=(b_i)_{i\in [n]}$, ${A\leftrightarrow B}:\mathbb{N}\to \mathbb{N}$ is the permutation that sends the $i$th element of $A$ to the $i$th element of $B$ and vise versa. Note that $A\leftrightarrow B$ is its own inverse.
\end{definition}

Lemma \ref{lem:infinitely_extended_kernels} is used to extend finite sequences to infinite ones, and is used in a number of upcoming theorems.

\begin{lemma}[Infinitely extended kernels]\label{lem:infinitely_extended_kernels}
Given a collection of Markov kernels $\kernel{K}_i:X^i\kto Y^i$ for all $i\in \mathbb{N}$, if we have for every $j>i$
\begin{align}
    \kernel{K}_j(\text{id}_{X_i}\otimes \text{del}_{X_{j-i}}) &= \kernel{K}_i\otimes \text{del}_{X_{j-i}}\label{eq:marginalise_comb}
\end{align} 
then there is a unique Markov kernel $\kernel{K}:X^{\mathbb{N}}\kto Y^{\mathbb{N}}$ such that for all $i,j\in \mathbb{N}$,$j>i$
\begin{align}
    \kernel{K}(\text{id}_{X_i}\otimes \text{del}_{X_{j-i}})&= \kernel{K}_i\otimes \text{del}_{X_{j-i}}
\end{align}
\end{lemma}

\begin{proof}
Take any $x\in X^{\mathbb{N}}$ and let $x_{|m}\in X^n$ be the first $n$ elements of $x$. By Equation \ref{eq:marginalise_comb}, for any $A_i\in \sigalg{Y}$, $i\in [m]$
\begin{align}
    \kernel{K}_n(\bigtimes_{i\in [m]}A_i\times Y^{n-m}|x_{|n}) &= \kernel{K}_m(\bigtimes_{i\in [m]}A_i|x_{|m})
\end{align}

Furthermore, by the definition of the $\mathrm{swap}$ map for any permutation $\rho:[n]\to[n]$
\begin{align}
    \kernel{K}_n\mathrm{swap}_{\rho}(\bigtimes_{i\in [m]}A_{\rho(i)}\times Y^{n-m}|x_{|n}) &= \kernel{K}_n(\bigtimes_{i\in [m]}A_{i}\times Y^{n-m}|x_{|n})
\end{align}
thus by the Kolmogorov Extension Theorem \citep{cinlar_probability_2011}, for each $x\in X^{\mathbb{N}}$ there is a unique probability measure $\prob{Q}_x\in \Delta(Y^{\mathbb{N}}$ satisfying
\begin{align}
    \prob{Q}_d(\bigtimes_{i\in [n]}A_i\times Y^{\mathbb{N}}) &= \kernel{K}_n(\bigtimes_{i\in [n]}A_{\rho(i)}|d_{|n})\label{eq:q_is_Markov}
\end{align}

Furthermore, for each $\{A_i\in\sigalg{Y}|i\in \mathbb{N}\}$, $n\in \mathbb{N}$ note that for $p>n$
\begin{align}
\prob{Q}_d(\bigtimes_{i\in[n]} A_i \times Y^{\mathbb{N}})&\geq \prob{Q}_d(\bigtimes_{i\in [p]} A_i\times Y^{\mathbb{N}})\\
&\geq \prob{Q}_d(\bigtimes_{i\in \mathbb{N}} A_i)
\end{align}
so by the Monotone convergence theorem, the sequence $\prob{Q}_d(\bigtimes_{i\in[n]} A_i)$ converges as $n\to \infty$ to $\prob{Q}_d(\bigtimes_{i\in\mathbb{N}} A_i)$. $d\mapsto \prob{Q}_d^{\RV{Z}_n}(\bigtimes_{i\in[n]} A_i)$ is measurable for all $n$, $\{A_i\in\sigalg{Y}|i\in \mathbb{N}\}$ by Equation \ref{eq:q_is_Markov}, and so $d\mapsto Q_d$ is also measurable.

Thus $d\mapsto Q_d$ is the desired $\prob{P}_C^{\RV{Y}_{\mathbb{N}}\combbreak \RV{D}_{\mathbb{N}}}:D^\mathbb{N}\kto Y^\mathbb{N}$.
\end{proof}

Theorem \ref{th:equal_of_condits} shows that, given a causally contractible kernel, the following operations yield equivalent results:
\begin{itemize}
    \item Marginalising all but the first $n$ outputs
    \item Marginalising all outputs except for the positions $A\subset\mathbb{N}$ where $|A|=n$, and swapping the first $n$ inputs with the elements of $A$
\end{itemize}

\begin{definition}[Marginalising kernel]
Given $(X,\sigalg{X})$ and $A\subset\mathbb{N}$, $\mathrm{marg}_A:X^\mathbb{N}\kto X^A$ is the Markov kernel given by
\begin{align}
    \bigotimes_{i\in \mathbb{N}} \text{switch}_{A,i}
\end{align}
where
\begin{align}
    \text{switch}_A &= \begin{cases}
                        \text{id}_X&i\in A\\
                        \text{del}_X&i\not\in A
                        \end{cases}
\end{align} 
\end{definition}

\begin{theorem}[Equality of equally sized contractions]\label{th:equal_of_condits}
A Markov kernel $\kernel{K}:X^{\mathbb{N}}\kto Y^{\mathbb{N}}$ is \emph{causally contractible} if and only if for every $n\in \mathbb{N}$ and every $A\subset\mathbb{N}$ there exists some $\kernel{L}:X^n\kto Y^n$ such that
\begin{align}
    \kernel{K} \text{marg}_A &= \text{swap}_{[n]\leftrightarrow A} \kernel{L}\otimes \text{del}_{X^{\mathbb{N}}}
\end{align}
\end{theorem}

\begin{proof}
Only if:
By exchange commutativity
\begin{align}
    \text{swap}_{[n]\leftrightarrow A} \kernel{K} &= \kernel{K} \text{swap}_{[n]\leftrightarrow A}
\end{align}
multiply both sides by $\text{swap}_{[n]\leftrightarrow A}$ on the right and, because $\text{swap}_{[n]\leftrightarrow A}$ is its own inverse,
\begin{align}
        \text{swap}_{[n]\leftrightarrow A} \kernel{K}\text{swap}_{[n]\leftrightarrow A} &= \kernel{K}
\end{align}
so
\begin{align}
    \kernel{K}\text{marg}_A &= \text{swap}_{[n]\leftrightarrow A} \kernel{K}\text{swap}_{[n]\leftrightarrow A}\text{marg}_A\\
    &= \text{swap}_{[n]\leftrightarrow A} \kernel{K}\text{marg}_{[n]}
\end{align}
By locality, there exists some $\kernel{L}:X^n\kto Y^n$ such that
\begin{align}
    \kernel{K} \text{marg}_{[n]} &= \kernel{K}(\text{id}_{[n]}\otimes \text{del}_{X^{\mathbb{N}}})\\
     &= \kernel{L}\otimes \mathrm{del}_{X^{\mathbb{N}}}
\end{align}
If:
Taking $A=[n]$ for all $n$ establishes locality.

For exchange commutativity, note that for all $x\in X^{\mathbb{N}}$, $n\in\mathbb{N}$, we have
\begin{align}
    \text{swap}_{A\leftrightarrow [n]} \kernel{K} \text{marg}_A &= \text{swap}_{A\leftrightarrow [n]} \kernel{K} \text{swap}_{A\leftrightarrow [n]} (\text{id}_{[n]}\otimes \text{del}_{X^{\mathbb{N}}})\\
     &= \kernel{K} \text{marg}_{[n]}\\
     &= \kernel{K}(\text{id}_{[n]}\otimes \text{del}_{X^{\mathbb{N}}})
\end{align}
Then by Lemma \ref{lem:infinitely_extended_kernels}
\begin{align}
    \text{swap}_{A\leftrightarrow [n]} \kernel{K} \text{swap}_{A\leftrightarrow [n]} &= \kernel{K}
\end{align}
Consider an arbitrary finite permutation $\rho:\mathbb{N}\to \mathbb{N}$. $\rho$ can be decomposed into a finite set of cyclic permutations on disjoint orbits. Each cyclic permutation is simply the composition of some set of transpositions, and so $\rho$ itself can be written as a composition of a sequence of transpositions. Thus for any finite $\rho:\mathbb{N}\to\mathbb{N}$
\begin{align}
    \text{swap}_{\rho} \kernel{K} \text{swap}_{\rho} &= \kernel{K}
\end{align}
\end{proof}

Theorem \ref{th:no_implication} shows that neither locality nor exchange commutativity is implied by the other.

\begin{theorem}\label{th:no_implication}
Exchange commutativity does not imply locality or vise versa.
\end{theorem}

\begin{proof}
First, a Markov kernel that exhibits exchange commutativity but not locality. Suppose $D=Y=\{0,1\}$ and $\kernel{K}:D^2\kto Y^2$ is given by
\begin{align}
    \kernel{K}(y_1,y_2|d_1,d_2) &= \llbracket (y_1,y_2)= (d_1+d_2,d_1+d_2) \rrbracket
\end{align}
then 
\begin{align}
    \kernel{K}(y_1,Y|d_1,d_2) &= \llbracket y_1 = d_1+d_2 \rrbracket
\end{align}
and there is no function depending on $y_1$ and $d_1$ only that is equal to this. Thus $\kernel{K}$ does not satisfy locality. 

However, taking $\rho$ to be the unique nontrivial swap $\{0,1\}\to \{0,1\}$
\begin{align}
    \text{swap}_{\rho,D}\kernel{K}(y_1,y_2|d_1,d_2) &= \kernel{K}(y_1,y_2|d_2,d_1)\\
    &= \llbracket (y_1,y_2)= (d_2+d_1,d_2+d_1) \rrbracket\\
    &= \llbracket (y_1,y_2)= (d_1+d_2,d_1+d_2) \rrbracket\\
    &= \llbracket (y_2,y_1)= (d_1+d_2,d_1+d_2) \rrbracket\\
    &= \kernel{K}\text{swap}_{\rho,Y}(y_1,y_2|d_1,d_2)
\end{align}
so $\kernel{K}$ commutes with exchange.

Next, a Markov kernel that satisfies locality but does not commute with exchange. Suppose again $D=Y=\{0,1\}$ and $\kernel{K}:D^2\kto Y^2$ is given by
\begin{align}
    \kernel{K}(y_1,y_2|d_1,d_2) &= \llbracket (y_1,y_2)= (0,1) \rrbracket
\end{align}

Then:
\begin{align}
    \kernel{K}(y_1|d_1,d_2) &= \llbracket y_1= 0 \rrbracket\\
    &= \kernel{K}(y_1|d_1)\\
    \kernel{K}(y_2|d_1,d_2)&= \llbracket y_2= 1 \rrbracket\\
    &= \kernel{K}(y_2|d_2)
\end{align}
so $\kernel{K}$ satisfies locality.

However, $\kernel{K}$ does not commute with exchange.
\begin{align}
    \text{swap}_{\rho(\RV{D})} \kernel{K}(y_1,y_2|d_1,d_2) &= \kernel{K}(y_1,y_2|d_2,d_1)\\
    &=\llbracket (y_1,y_2)= (0,1) \rrbracket\\
    &\neq \llbracket (y_2,y_1)= (0,1) \rrbracket\\
    &= \kernel{K}\text{swap}_{\rho(\RV{D})}(y_1,y_2|d_1,d_2)
\end{align}
\end{proof}

Theorem \ref{th:no_implication} presents abstract counterexamples to show that the assumptions of exchange commutativity and locality are independent. For some more practical examples, a model of the treatment of several patients who are known to have different illnesses might satisfy consequence locality but not exchange commutativity. Patient B's treatment can be assumed not to affect patient A, but the same results would not be expected from giving patient A's treatment to patient B as from giving patient A's treatment to patient A.

A model of strategic behaviour might satisfy exchange commutativity but not locality. Suppose a decision maker is observing people playing a game where they press a red or green button, and (for reasons mysterious to the decision maker), receive a payout randomly of 0 or \$100. The decision maker might reason that the results should be the same no matter who presses a button, but also that people will be more likely to press the red button if the red button tends to give a higher payout. In this case, the decision maker's prediction for the payout of the $i$th attempt given the red button has been pressed will be higher if the proportion of red button presses in the entire dataset is higher. There are other reasons why exchange commutativity might hold but not locality -- \citet{dawid_causal_2000} offers the alternative example of herd immunity in vaccination campaigns. In this case, the overall proportion of the population vaccinated will affect the disease prevalence over and above an individual's vaccination status.

An aside: although locality could be described as an assumption that there is no interference between inputs and outputs of different indices, it actually allows for some models with certain kinds of interference between actions and outcomes of different indices. For example: consider an experiment where I first flip a coin and record the results of this flip as the outcome of the ``step 1''. Subsequently, I can choose either to copy the outcome from step 1 to be the input for ``step 2'' (this is the choice $\RV{D}_1=0$), or flip a second coin use this as the input for step 2 (this is the choice $\RV{D}_1=1$). At the second step, I may further choose to copy the provisional results ($\RV{D}_2=0$) or invert them ($\RV{D}_2=1$). Then
\begin{align}
    \prob{P}_S^{\RV{Y}_1|\RV{D}}(y_1|d_1,d_2) &= 0.5\\
    \prob{P}_S^{\RV{Y}_2|\RV{D}}(y_2|d_1,d_2) &= 0.5
\end{align}
\begin{itemize}
    \item The marginal distribution of both experiments in isolation is $\text{Bernoulli}(0.5)$ no matter what choices I make, so a model of this experiment would satisfy Definition \ref{def:caus_cont}
    \item Nevertheless, the choice at step 1 affects the result of step 2
\end{itemize}

\subsection[Representation theorems]{Representation theorems for causally contractible Markov kernels}\label{sec:rep_theorem}

Theorem \ref{th:table_rep_kernel} shows that a causally contractible Markov kernel can be represented as the product of a column exchangeable probability distribution and a ``lookup function''. This representation is identical to the representation of potential outcomes models (see, for example, \citet{rubin_causal_2005}), but Theorem \ref{th:table_rep_kernel} applies to arbitrary kernels and the resulting representation will usually not be interpretable as a potential outcomes models. This theorem allows De Finetti's theorem to be applied to the column exchangeable probability distribution, which is a key step in proving the main result (Theorem \ref{th:ciid_rep_kernel}).

Theorem \ref{th:ciid_rep_kernel_nolocal} then extends Theorem \ref{th:ciid_rep_kernel} to the case of a Markov kernel with commutativity of exchange only. In this case the latent conditioning variable may depend on a symmetric function of the inputs, assuming the distribution of inputs is dominated by some exchangeable distribution.

\begin{theorem}\label{th:table_rep_kernel}
A Markov kernel $\kernel{K}:X^{\mathbb{N}}\kto Y^{\mathbb{N}}$ is causally contractible if and only if there exists a column exchangeable probability distribution $\mu \Delta(Y^{|X|\times \mathbb{N}})$ such that
\begin{align}
    \kernel{K} &= \tikzfig{lookup_representation_kernel}\label{eq:lup_rep_kernel}\\
    &\iff\\
    \kernel{K}(A|(x_i)_{i\in \mathbb{N}}) &= \mu \Pi_{(x_i i)_{i\in\mathbb{N}}}(A)\forall A\in \sigalg{Y}^{\mathbb{N}}
\end{align}
Where $\Pi_{(d_i i)_{i\in\mathbb{N}}}:Y^{|X|\times \mathbb{N}}\to Y^{\mathbb{N}}$ is the function 
\begin{align}
    (y_{j i})_{j,i \in X\times  \mathbb{N}}\mapsto (y_{d_i i})_{i\in \mathbb{N}}
\end{align}
that projects the $(x_i,i)$ indices of $y$ for all $i\in \mathbb{N}$, and $\prob{F}_{\text{ev}}$ is the Markov kernel associated with the evaluation map
\begin{align}
    \text{ev}:X^\mathbb{N}\times Y^{X\times \mathbb{N}}&\to Y\\
    ((x_i)_\mathbb{N},(y_{ji})_{j,i\in X\times \mathbb{N}})&\mapsto (y_{x_i i})_{i\in \mathbb{N}}
\end{align}
\end{theorem}

\begin{proof}
Only if:
Choose $e:=(e_i)_{i\in\mathbb{N}}$ such that $e_{i+|X|j}$ is the $i$th element of $X$ for all $i,j\in \mathbb{N}$.

Define
\begin{align}
    \mu(\bigtimes_{(i,j)\in X\times \mathbb{N}} A_{ij}):=\kernel{K}(\bigtimes_{(i,j)\in X\times \mathbb{N}} A_{ij}|e)& \forall A_{ij}\in \sigalg{Y}
\end{align}

Now consider any $x:=(x_i)_{i\in \mathbb{N}}\in X^{\mathbb{N}}$. By definition of $e$, $e_{x_i i}=x_i$ for any $i,j\in \mathbb{N}$.

Define
\begin{align}
    \prob{Q}:X^{\mathbb{N}}\kto Y^{\mathbb{N}}\\
    \prob{Q}:= \tikzfig{lookup_representation_kernel}
\end{align}
and consider some $A\subset \mathbb{N}$, $|A|=n$ and $B:= (x_i,i))_{i\in A}$. Note that the subsequence of $e$ indexed by $B$, $e_B:=(e_{x_i i})_{i\in A}=x_A$. Thus given the swap map $\mathrm{swap}_{A\leftrightarrow B}:\mathbb{N}\to\mathbb{N}$ that sends the first element of $A$ to the first element of $B$ and so forth, $\mathrm{swap}_{A\leftrightarrow B} (e_B) = x_A$. For arbitrary $\{C_i\in \sigalg{Y}|i\in A\}$, define $C_A:=\mathrm{swap}_{[n]\leftrightarrow A} (\times_{i\in [n]} C_i\times Y^{\mathbb{N}})$. Then, for arbitrary $x\in X^{\mathbb{N}}$
\begin{align}
    \prob{Q}(C_A|x) &= \mu (\mathrm{ev}_x^{-1}(C_A))\label{eq:q_mu_rel}
\end{align}

The argument of $\mu$ is
\begin{align}
    \mathrm{ev}_x^{-1}(C_A)&=\{(y_{ji})_{j,i\in X\times\mathbb{N}}|(y_{x_i i})_{i\in\mathbb{N}}\in C_A\}\\
    &= \bigtimes_{i\in \mathbb{N}} \bigtimes_{j\in X} D_{ji}
\end{align}
where
\begin{align}
    D_{ji} = \begin{cases}
        C_i & (j,i)\in B\\
        Y & \text{otherwise}
    \end{cases}
\end{align}
and so
\begin{align}
    \text{swap}_{A\leftrightarrow B} (\mathrm{ev}_x^{-1}(C_A)) &= C_A\label{eq:swap_select_relation}
\end{align}

Substituting Equation \ref{eq:swap_select_relation} into \ref{eq:q_mu_rel}
\begin{align}
    \prob{Q}(C_A|x) &= \mu \text{swap}_{A\leftrightarrow B} (C_A)\\
    &= \kernel{K} \text{swap}_{A\leftrightarrow B} (C_A|e)\\
    &= \kernel{K}\text{swap}_{A\leftrightarrow B} (C_A|e_B,\text{swap}_{B\leftrightarrow A}(x)_B^C)&\text{by locality}\\
    &= \kernel{K}\text{swap}_{A\leftrightarrow B} (C_A|\text{swap}_{B\leftrightarrow A}(x))\\
    &= \text{swap}_{B\leftrightarrow A} \kernel{K}\text{swap}_{A\leftrightarrow B} (C_A|x)\\
    &= \kernel{K}(C_A|x)&\text{by commutativity of exchange}
\end{align}

Because this holds for all $x$, $A\subset\mathbb{N}$, by Lemma \ref{lem:infinitely_extended_kernels}

\begin{align}
    \prob{Q} &= \kernel{K}
\end{align}

Next we will show $\mu$ is column exchangeable. Consider any column swap $\text{swap}_{c}:X\times \mathbb{N}\to X\times \mathbb{N}$ that acts as the identity on the $X$ component and a finite permutation on the $\mathbb{N}$ component. From the definition of $e$, $\text{swap}_c(e)=e$. Thus by commutativity of exchange, for any $A\in \sigalg{Y}^{\mathbb{N}}$
\begin{align}
 \kernel{K}(A|e) &= \text{swap}_c\kernel{K}\text{swap}_c(A|e)\\
 &= \kernel{K}\text{swap}_c(A|\text{swap}_c(e))\\
 &= \kernel{K}\text{swap}_c(A|e)
\end{align}


If:
Suppose 
\begin{align}
    \kernel{K} &= \tikzfig{lookup_representation_kernel}
\end{align}
where $\mu$ is column exchangeable, and consider any two $x,x'\in X^{\mathbb{N}}$ such that some subsequences are equal $x_S=x'_T$ with $S,T\subset \mathbb{N}$ and $|S|=|T|=[n]$.

For any $\{A_i\in\sigalg{Y}|i\in S\}$, let $A_S = \text{swap}_{[n]\leftrightarrow S} \times_{i\in [n]} A_i\times Y^{\mathbb{N}}$, $A_T = \text{swap}_{S\leftrightarrow T} (A_S)$, $B=(x_i i)_{i\in S}$ and $C=(x_i i)_{i\in T}=(x_{\text{swap}_{S\leftrightarrow T}}(i) i)_{i\in S}$. By Equations \ref{eq:q_mu_rel} and \ref{eq:swap_select_relation}
\begin{align}
    \kernel{K}(A_S|x) &= \mu \text{swap}_{S\leftrightarrow B} (A_S)\\
    &= \mu \text{swap}_{T\leftrightarrow C} (A_T)&\text{ by column exchangeability of }\mu\\
    &= \kernel{K}(A_T|\text{swap}_{S\leftrightarrow T}(x))\\
    &=  \text{swap}_{S\leftrightarrow T}\kernel{K}(A_T| x)\\
    &= \text{swap}_{S\leftrightarrow T} \kernel{K} \text{swap}_{S\leftrightarrow T} (A_S| x)
\end{align}
so $\kernel{K}$ is causally contractible by Theorem \ref{th:equal_of_condits}.
\end{proof}

Theorem \ref{th:ciid_rep_kernel} is the main result of this section. It shows that a causally contractible Markov kernel $X^{\mathbb{N}}\kto Y^{\mathbb{N}}$ is representable as a ``prior'' $\mu\in \Delta(H)$ and a ``parallel product'' of Markov kernels $H\times X\kto Y$. These will be the response conditionals when Theorem \ref{th:ciid_rep_kernel} is applied to probability set models.

\begin{definition}[Measurable set of probability distributions]
Given a measurable set $(\Omega,\sigalg{F})$, the measurable set of distributions on $\Omega$, $\mathcal{M}_1(\Omega)$, is the set of all probability distributions on $\Omega$ equipped with the coarsest $\sigma$-algebra such that the evaluation maps $\eta_B:\nu\mapsto \nu(B)$ are measurable for all $B\in \sigalg{F}$.
\end{definition}

\begin{lemma}[Exchangeable table to response functions]\label{lem:extabl_to_respf}
Given $\kernel{K}:X^{\mathbb{N}}\kto Y^{\mathbb{N}}$, $X$ and $Y$ standard measurable, if
\begin{align}
    \kernel{K} &= \tikzfig{lookup_representation_kernel}
\end{align}
for $\mu\in \Delta(Y^{X\times\mathbb{N}})$ column exchangeable, then defining $(H,\sigalg{H}):=\mathcal{M}_1(Y^{X\times\mathbb{N}})$ there is some $\RV{H}:Y^{X\times\mathbb{N}}\to H$ and $\kernel{L}:H\times X\kto Y$ such that
\begin{align}
    \kernel{K} &= \tikzfig{do_model_representation_kernel}
\end{align}
\end{lemma}

\begin{proof}
As a preliminary, we will show
\begin{align}
    \kernel{F}_{\mathrm{ev}} &= \tikzfig{lookup_rep_intermediate_kernel}\label{eq:ev_alternate_rep}
\end{align}
where  $\mathrm{evs}_{Y^X\times X}:Y^X\times X\to Y$ is the single-shot evaluation function
\begin{align}
    (x,(y_i)_{i\in X})\mapsto y_x
\end{align}

Recall that $\mathrm{ev}$ is the function
\begin{align}
    ((x_i)_\mathbb{N},(y_{ji})_{j,i\in X\times \mathbb{N}})&\mapsto (y_{x_i i})_{i\in \mathbb{N}}
\end{align}
By definition, for any $\{A_i\in\sigalg{Y}|i\in \mathbb{N}\}$
\begin{align}
    \kernel{F}_{\mathrm{ev}}(\bigtimes_{i\in \mathbb{N}}A_i|(x_i)_\mathbb{N},(y_{ji})_{i\in X\times \mathbb{N}}) &= \delta_{(y_{x_i i})_{i\in \mathbb{N}}}(\bigtimes_{i\in \mathbb{N}}A_i)\\
        &= \prod_{i\in \mathbb{N}} \delta_{y_{x_i i}} (A_i)\\
        &= \prod_{i\in \mathbb{N}} \kernel{F}_{\text{evs}} (A_i|x_i,(y_{ji})_{j\in X})\\
        &= \left(\bigotimes_{i\in\mathbb{N}} \kernel{F}_{\mathrm{evs}} \right)(\bigtimes_{i\in \mathbb{N}}A_i|(x_i)_\mathbb{N},(y_{ji})_{j\in X\times \mathbb{N}})
\end{align}
which is what we wanted to show.

Define $\kernel{M}:H\kto Y^X$ by $\kernel{M}(A|h)=h(A)$ for all $A\in\sigalg{Y}^X$, $h\in H$. By the column exchangeability of $\mu$, from \citet[Prop. 1.4]{kallenberg_basic_2005} there is a directing random measure $\RV{H}:Y^{X\times\mathbb{N}}\to H$ such that
\begin{align}
    \mu(\kernel{F}_{\RV{H}}\otimes \mathrm{id}_{Y^{X\times\mathbb{N}}}) &= \tikzfig{de_finetti_representation_kernel}\label{eq:df_rep_mu}\\
    &\iff\\
    \mu(\bigtimes_{i\in \mathbb{N}} A_i\times B) &= \int_B \prod_{i\in \mathbb{N}} \kernel{M}(A_i|h) \mu\kernel{F}_{\RV{H}}(\mathrm{d}h)&\forall A_i\in\sigalg{Y}^X
\end{align}

By Equations \ref{eq:lookup_representation} and \ref{eq:ev_alternate_rep}
\begin{align}
    \kernel{K} &= \tikzfig{do_model_representation_kernel_pre}\\
    &:= \tikzfig{do_model_representation_kernel}\label{eq:lup_rep_combined}
\end{align}
Where we can connect the copied outputs of $\mu\kernel{F}_{\RV{H}}$ to the inputs of each $\kernel{M}$ ``inside the plate'' as the plates in Equations \ref{eq:ev_alternate_rep} and \ref{eq:df_rep_mu} are equal in number and each connected wire represents a single copy of $Y^D$.
\end{proof}

\begin{theorem}\label{th:ciid_rep_kernel}
Given a kernel $\kernel{K}:X^{\mathbb{N}}\kto Y^{\mathbb{N}}$, let $(H,\sigalg{H}):=\mathcal{M}_1(Y^X)$ be the set of probability distributions on $(Y^X,\sigalg{Y}^X)$. $\kernel{K}$ is causally contractible if and only if there is some $\mu\in \Delta(H)$ and $\kernel{L}:H\times X\kto Y$ such that
\begin{align}
    \kernel{K} &= \tikzfig{do_model_representation_kernel}\\
    &\iff\\
    \kernel{K}(\bigtimes_{i\in\mathbb{N}}A_i|(x_i)_{i\in\mathbb{N}}) &= \int_H \prod_{i\in\mathbb{N}} \kernel{L}(A_i|h,x_i)\mu(\mathrm{d}h)
\end{align}
\end{theorem}

\begin{proof}
Only if:
By Theorem \ref{th:table_rep_kernel}, we can represent the conditional probability $\kernel{K}$ as
\begin{align}
        \kernel{K} &= \tikzfig{lookup_representation_kernel}\label{eq:lookup_representation}
\end{align}
where $\nu\in \Delta(Y^{X\times\mathbb{N}})$ is column exchangeable.

Applying Lemma \ref{lem:extabl_to_respf} yields the desired result.

If:
By assumption, for any $\{A_i\in \sigalg{Y}|i\in\mathbb{N}\}$, $x:=(x_i)_{i\in\mathbb{N}}\in X^{\mathbb{N}}$
\begin{align}
    \kernel{K}(\bigtimes_{i\in \mathbb{N}} A_i|x) &= \int_H \prod_{i\in \mathbb{N}}\kernel{L}(A_i|h,x_i)\mu(\mathrm{d}h)
\end{align}

Consider any $S,T\subset\mathbb{N}$ with $|S|=|T|$, and define $A_S:=\times_{i\in\mathbb{N}} B_i$ where $B_i=Y$ if $i\not\in S$, otherwise $A_i$ is an arbitrary element of $\sigalg{Y}$. Define $A_T:=\times_{i\in\mathbb{N}} B_{\mathrm{swap}_{S\leftrightarrow T}(i)}$.

\begin{align}
    \kernel{K}(A_S|x) &= \int_H \prod_{i\in S}\kernel{L}(A_i|h,x_i)\mu(\mathrm{d}h)\\
                      &= \int_H\prod_{i\in T}\kernel{L}(A_i|h,x_{\mathrm{swap}_{S\leftrightarrow T}(i)})\mu(\mathrm{d}h)\\
                      &= \mathrm{swap}_{S\leftrightarrow T}\kernel{K}(A_T|x)\\
                      &= \mathrm{swap}_{S\leftrightarrow T}\kernel{K}\mathrm{swap}_{S\leftrightarrow T}(A_S|x)
\end{align}
So by Theorem \ref{th:equal_of_condits}, $\kernel{K}$ is causally contractible.
\end{proof}

\begin{lemma}\label{lem:exch_prod_ciid}
A kernel $\kernel{K}:X^{\mathbb{N}}\kto Y^{\mathbb{N}}$ with $X$, $Y$ standard measurable commutes with exchange if and only if there is some $\kernel{L}:X^{\mathbb{N}}\kto Z$ symmetric in its inputs and $\kernel{M}:Z\times X\kto Y$ such that $\kernel{M}(\cdot|z,\cdot)$ is causally contractible for every $z\in Z$ and
\begin{align}
    \kernel{K} \overset{\mu}{\cong} \tikzfig{commutative_exchange}
\end{align}
for any exchangeable $\mu$.
\end{lemma}

\begin{proof}
If:
Swap maps are deterministic, so by Theorem \ref{th:fong_det_kerns}
\begin{align}
    \tikzfig{commutative_exchange_with_swap} &= \tikzfig{commutative_exchange_with_swap_past_copy}\\
    &= \tikzfig{commutative_exchange_with_swap_symmed}\label{eq:sym}\\
    &= \tikzfig{commutative_exchange_with_swap_cconted}\label{eq:ccont}
\end{align}
where Equation \ref{eq:sym} follows from symmetry of $\kernel{L}$ and \ref{eq:ccont} follows from causal contractibility of $\kernel{M}(\cdot|z,\cdot)$

Only if:
Construct the probability space $(\prob{Q},X^{\mathbb{N}}\times Y^{\mathbb{N}},\sigalg{X}^{\mathbb{N}}\otimes\sigalg{Y}^{\mathbb{N}})$ with $\prob{P}=\mu\odot\kernel{K}$.

Take all sets in $\sigalg{X}^{\mathbb{N}}$ invariant under any finite permutation, and call this the \emph{exchangeable $\sigma$-algebra} $\sigalg{E}$ \citep[pg. 29]{kallenberg_basic_2005}. Let $\sigalg{E}_{\RV{X}}:=\RV{X}^{-1}(\sigalg{E})$.

Consider $\prob{Q}^{\RV{Y}_{[n]}|\RV{X}}$ for some $n\in \mathbb{N}$. This is equivalent to $\kernel{K}(\text{id}_{Y^n}\otimes \text{del}_{Y^{\mathbb{N}}})$. By assumption, $\kernel{K}(\text{id}_{Y^n}\otimes \text{del}_{Y^{\mathbb{N}}})$ is invariant to any finite permutation of $\RV{X}$ that only affects indices after the $n$th. That is, $\prob{Q}^{\RV{Y}_{[n]}|\RV{X}}$ is $\sigalg{E}_{\RV{X}_[n+1,\infty)}\vee\sigma(\RV{X}_{[n]}$-measurable.

Let $\sigalg{T}_{\RV{X}}$ be the tail $\sigma$-algebra on $X^{\mathbb{N}}$. $\sigalg{T}_{\RV{X}}$ is defined as the intersection $\cap_{i=1}^{\infty} \sigma(\RV{X}_{[i,\infty)})$. Note that $\sigalg{T}_{\RV{X}_{[n+1,\infty)}}=\sigalg{T}_{\RV{X}}$. By \citet[Corollary 1.6]{kallenberg_basic_2005}, $\sigalg{E}_{\RV{X}_[n+1,\infty)}=\sigalg{T}_{\RV{X}_{[n+1,\infty)}}$ almost surely, and so $\sigalg{E}_{\RV{X}_[n,\infty)}=\sigalg{T}_{\RV{X}}$ almost surely and by \citet[Corollary 1.6]{kallenberg_basic_2005} again $\sigalg{E}_{\RV{X}_[n,\infty)}=\sigalg{E}_{\RV{X}}$ $\mu$-almost surely.

Thus $\prob{Q}^{\RV{Y}_{[n]}|\RV{X}}$ is $\sigalg{E}_{\RV{X}}\vee\sigma(\RV{X}_{[n]})$-measurable. By \citet[Corollary 1.6]{kallenberg_basic_2005} again, there is a random $\RV{J}$ taking values in the set of distributions on $X$ such that $\sigma(\RV{J})=\sigalg{E}_{\RV{X}}$ $\mu$-almost surely. Thus
\begin{align}
    \prob{Q}^{\RV{Y}_{[n]}|\RV{X}\RV{J}} &= \prob{Q}^{\RV{Y}_{[n]}|\RV{X}_{[n]}\RV{J}}\otimes \text{erase}_{X^{\mathbb{N}}}
\end{align}
That is, $\RV{Y}_{[n]}\CI_{\prob{Q}} \RV{X}_{[n+1,\infty)}|(\RV{X}_{[n]},\RV{J})$. In particular, $\prob{Q}^{\RV{Y}|\RV{J}\RV{X}}(\cdot|z,\cdot)$ is local for all $z\in \Delta(X)$. By disintegration
\begin{align}
    \kernel{K} \overset{\mu}{\cong} \tikzfig{commutative_exchange_Q}
\end{align}
which completes the proof.
\end{proof}

\begin{theorem}\label{th:ciid_rep_kernel_nolocal}
Given a kernel $\kernel{K}:X^{\mathbb{N}}\kto Y^{\mathbb{N}}$, let $(H,\sigalg{H}):=\mathcal{M}_1(Y^X)$ be the measurable set of probability distributions on $(Y^X,\sigalg{Y}^X)$. $\kernel{K}$ is exchange commutative if and only if there is some $\RV{H}:Y^{X\times\mathbb{N}}\to H$, some $\kernel{M}:X^{\mathbb{N}}\kto H$ symmetric in its inputs and some $\kernel{L}:H\times X\kto Y$ such that
\begin{align}
    \kernel{K} &\overset{\nu}{\cong} \tikzfig{do_model_representation_kernel_nolocal}\\
    &\iff\\
    \kernel{K}(\bigtimes_{i\in\mathbb{N}}A_i|x) &\overset{\nu}{\cong} \int_H \prod_{i\in\mathbb{N}} \kernel{M}(A_i|h,x_i)\kernel{L}(\mathrm{d}h|x)
\end{align}
for arbitrary exchangeable $\nu$, where $\Pi_i:X^{\mathbb{N}}\kto X$ is the Markov kernel associated with the $i$-th projection map.
\end{theorem}

\begin{proof}
Only if:
By Lemma \ref{lem:exch_prod_ciid}, $\kernel{K}=(\mathrm{id}_{X^{\mathbb{N}}}\odot \kernel{L})\kernel{M}$ where $\kernel{M}:J\times X^{\mathbb{N}}\kto Y$ is such that $\kernel{M}(\cdot|z,\cdot)$ is causally contractible for each $z\in J$. Thus by Theorem \ref{th:ciid_rep_kernel}, for each $z$ we have
\begin{align}
    \kernel{M}_z:&=\kernel{M}(\cdot|z,\cdot)\\
      &= \tikzfig{do_model_representation_kernel_z}
\end{align}
Where, in particular, $\kernel{L}$ is the same for all $z$. Thus
\begin{align}
    \kernel{M} = \tikzfig{do_model_representation_kernel_zcut}
\end{align}
Hence
\begin{align}
    \kernel{K} &\overset{\nu}{\cong} \tikzfig{do_model_representation_kernel_nolocal}
\end{align}

If:
Apply Lemma \ref{lem:exch_prod_ciid} identifying $\kernel{M}$ with $\kernel{L}$. By Theorem \ref{th:ciid_rep_kernel},
\begin{align}
    \tikzfig{do_model_representation_kernel_truncated}
\end{align}
is causally contractible for every $h\in H$.
\end{proof}

\subsection[Data-independent inputs]{Conditionally independent and identical response functions with data-independent inputs}\label{sec:data_independent_actions}

In this section, we apply the theorems from the previous part to models $\prob{P}_C$ with ``input'' variables $\RV{D}:=(\RV{D}_i)_{i\in \mathbb{N}}$ and corresponding ``output'' variables $\RV{Y}:=(\RV{Y}_i)_{i\in \mathbb{N}}$ where the inputs are independent of non-corresponding output variables. Data-independent models are interesting because they are precisely the class of models with a uniform conditional distribution $\prob{P}_C^{\RV{Y}|\RV{D}}$ that is causally contractible. This makes them simple to work with in comparison with more general models. The assumption of data independence is a very limiting assumption -- see-do models, for example (Definition \ref{def:see_do_model}) do not satisfy it. More general cases are considered in Sections \ref{sec:nonu_input} and \ref{sec:data_dependent}.

Call a model $\prob{P}_C$ with sequential outputs $\RV{Y}$ and a corresponding sequence of inputs $\RV{D}$ a ``sequential input-output model''.

\begin{definition}[Sequential input-output model]
A \emph{sequential input-output model} is a triple $(\prob{P}_C,\RV{D},\RV{Y})$ where $\prob{P}_C$ is a probability set on $(\Omega,\sigalg{F})$, $\RV{D}$ is a sequence of ``inputs'' $\RV{D}:=(\RV{D}_i)_{i\in\mathbb{N}}$ and $\RV{Y}$ is a corresponding sequence of ``outputs'' $\RV{Y}=(\RV{Y}_i)_{i\in\mathbb{N}}$ where $\RV{D}_i:\Omega\to D$ and $\RV{Y}_i:\Omega\to Y$.
\end{definition}

Given a sequence of variables $(\RV{D}_i,\RV{Y}_i)_{i\in \mathbb{N}}$ where the ``inputs'' are $\RV{D}:=(\RV{D}_i)_{i\in\mathbb{N}}$ and the ``outputs'' are $\RV{Y}=(\RV{Y}_i)_{i\in\mathbb{N}}$, say the inputs are data-independent if $\RV{D}_i\CI^e_{\prob{P}_C} \RV{Y}_{[i-1]}|(\RV{D}_{[i-1]},\RV{C})$ for all $i\in\mathbb{N}$. This could model an experiment where it may be possible to choose different inputs $\RV{D}$, but all the inputs are determined before the outputs $\RV{Y}$ are known.

Theorem \ref{th:data_ind_CC} applies Theorem \ref{th:ciid_rep_kernel} to the case of a sequential input-output model where $\prob{P}_C^{\RV{Y}|\RV{D}}$ is causally contractible. It shows that these are precisely the models $(\prob{P}_C,\RV{D},\RV{Y})$ with data-independent inputs, conditionally independent and identical response functions $\prob{P}_C^{\RV{Y}_i|\RV{D}_i\RV{H}}$ and data-independent hypotheses.

The a requirement that all values of $D$ appear infinitely often allows us to define the hypothesis $\RV{H}$ as a function of the observed variables. Without it, it's possible to define a latent $\RV{H}$, but it is not generally a function of observed variables.

\begin{theorem}[Data-independent causal contractibility]\label{th:data_ind_CC}
Given a sequential input-output model $(\prob{P}_C,\RV{D},\RV{Y})$ on $(\Omega,\sigalg{F})$ with $D$ countable, suppose $\RV{Y}\CI^e_{\prob{P}_C} \RV{C}|\RV{D}$ and, letting $E\subset D^{\mathbb{N}}$ be the set of all sequences $d$ such that each $x\in D$ appears in $d$ infinitely often, suppose also that $\prob{P}_\alpha^{\RV{D}}(E)=1$ for all $\alpha\in C$. Then $\prob{P}_C^{\RV{Y}|\RV{D}}$ is causally contractible if and only if there is some directing random measure $\RV{H}:\Omega\to H$ such that $\prob{P}_C^{\RV{Y}_i|\RV{D}_i\RV{H}}=\prob{P}_C^{\RV{Y}_j|\RV{D}_j\RV{H}}$ for all $i,j\in \mathbb{N}$, $\RV{Y}_{i}\CI^e_{\prob{P}_C} (\RV{Y}_{[i-1]},\RV{D}_{[i-1},\RV{C})|(\RV{D}_i,\RV{H})$, $\RV{D}_i\CI^e_{\prob{P}_C} \RV{Y}_{[i-1]}|(\RV{D}_{[i-1]},\RV{C})$ and $\RV{H}\CI_{\prob{P}_C} (\RV{D},\RV{C})$.
\end{theorem}

\begin{proof}
If:
First, construct the variable $\RV{Y}^D$. Let $\#_{\RV{D}_k=j}^i$ be $1$ if $\RV{D}_k=j$ and it is exactly the $i$th coordinate of $\RV{D}$ that does so, and $0$ otherwise. Then for $i\in \mathbb{N}$ and $j\in D$
\begin{align}
    \RV{Y}^D_{ij} = \sum_{k=1}^{\infty} \#_{\RV{D}_k=j}^i \RV{Y}_k
\end{align}
That is, (roughly speaking) the $(i,j)$-th coordinate of $\RV{Y}^D$ is equal to the coordinate $\RV{Y}_k$ for which the corresponding $\RV{D}_k= j$ and there have been $i-1$ preceding instances where a coordinate of $\RV{D}_l$ has been equal to $j$ ($l<k$). Note that $\RV{Y}^D_{ij}$ is equal to some $\RV{Y}_k$ with probability 1 by the assumption that all values of $D$ occur infinitely often almost surely.

Note that $\#_{\RV{D}_k=j}^i$ and $\#_{\RV{D}_l=j}^i$ are mutually exclusive for $k\neq l$ and, furthermore, $\sum_{j\in D}\sum_{i\in \mathbb{N}} \#_{\RV{D}_k=j}^i=1$ almost surely (that is, there is some $i,j$ such that $\RV{D}_k$ is the $i$th occurrence of $j$). Define $\RV{R}_k:\omega \mapsto \argmax_{i\in\mathbb{N},j\in D} \#_{\RV{D}_k=j}^i(\omega)$ and $\RV{R}:k\mapsto \RV{R}_k$. $\RV{R}$ is almost surely bijective and 
\begin{align}
    \RV{Y}^D&:= (\RV{Y}^D_{ij})_{i\in \mathbb{N},j\in D}\\
    &= (\RV{Y}_{\RV{R}^{-1}(i,j)})_{i\in \mathbb{N},j\in D}\\
    &:= \RV{Y}_{\RV{R}^{-1}}
\end{align}

By construction, $\RV{D}_{\RV{R}^{-1}(i,j)}=j$ almost surely; that is, $\RV{D}_{\RV{R}^{-1}}$ is single-valued. In particular, it is almost surely equal to $e:=(e_{ij})_{i\in\mathbb{N},j\in D}$ such that $e_{ij}=j$. Hence
\begin{align}
    \prob{P}_\alpha^{\RV{Y}^D|\RV{D}_{\RV{R}^{-1}}}(A|d)&\overset{\prob{P}_C}{\cong} \prob{P}_C^{\RV{Y}_{\RV{R}^{-1}}|\RV{D}_{\RV{R}^{-1}}}(A|d)\\
    &\overset{\prob{P}_C}{\cong} \prob{P}_C^{\RV{Y}_{\RV{R}^{-1}}|\RV{D}_{\RV{R}^{-1}}}(A|e)\label{eq:yd_is_indep}\\
    &=: \prob{P}_C^{\RV{Y}^D}\label{eq:yd_dist}
\end{align}
for any $d\in D^{\mathbb{N}}$. Equation \ref{eq:yd_is_indep} implies $\RV{Y}^D\CI (\RV{D},\RV{C})$, hence the possibility of defining $\prob{P}_C^{\RV{Y}^D}$.

Now,
\begin{align}
    \prob{P}^{\RV{Y}_{\RV{R}^{-1}}|\RV{D}_{\RV{R}^{-1}}}_\alpha(A|d) &= \int_R \prob{P}_\alpha^{\RV{Y}_\rho|\RV{D}_{\rho}}(A|d)\prob{P}_\alpha^{\RV{R}^{-1}}(\mathrm{d}\rho)\label{eq:need_ccont}\\
\end{align}
For each $\rho$, define $\rho^n:\mathbb{N}\to \mathbb{N}$ as the finite permutation that agrees with $\rho$ on the first $n$ indices and is the identity otherwise. By causal contractibility, for $n\in \mathbb{N}$
\begin{align}
    \prob{P}^{\RV{Y}_{\rho^n([n])}|\RV{D}_{\rho^n([n])}} &= \prob{P}^{\RV{Y}_{\rho([n])}|\RV{D}_{\rho([n])}}\\
    &= \prob{P}^{\RV{Y}_{[n]}|\RV{D}_{[n]}}
\end{align}
By Lemma \ref{lem:infinitely_extended_kernels}, it must therefore be the case that
\begin{align}
    \prob{P}^{\RV{Y}|\RV{D}} = \prob{P}^{\RV{Y}_{\rho}|\RV{D}_{\rho}}
\end{align}
Then from Equation \ref{eq:need_ccont}
\begin{align}
    \prob{P}^{\RV{Y}_{\RV{R}^{-1}}|\RV{D}_{\RV{R}^{-1}}}_\alpha(A|d) &\overset{\prob{P}_C}{\cong} \int_R \prob{P}_\alpha^{\RV{Y}_\rho|\RV{D}_{\rho}}(A|d)\prob{P}_\alpha^{\RV{R}^{-1}}(\mathrm{d}\rho)\\
    &\overset{\prob{P}_C}{\cong} \int_R \prob{P}_C^{\RV{Y}|\RV{D}}(A|d)\prob{P}_\alpha^{\RV{R}^{-1}}(\mathrm{d}\rho)\\
    &\overset{\prob{P}_C}{\cong} \prob{P}_C^{\RV{Y}|\RV{D}}(A|d)\label{eq:rotated_conditional}
\end{align}
 for all $i,j\in \mathbb{N}$. Then by Equation \ref{eq:yd_dist} and Equation \ref{eq:rotated_conditional}
\begin{align}
    \prob{P}_C^{\RV{Y}^D}(A) &= \prob{P}_C^{\RV{Y}|\RV{D}}(A|e)\label{eq:rel_bet_y_yd}
\end{align}

From Equation \ref{eq:rel_bet_y_yd} and causal contractibility of $\prob{P}_C^{\RV{Y}|\RV{D}}(A|e)$, take some $d\in D^{\mathbb{N}}$ and
\begin{align}
    (\prob{P}_C^{\RV{Y}^D}\otimes \mathrm{id}_D)\kernel{F}_{ev}(A|d) &= \prob{P}_C^{(\RV{Y}^D_{d_i i})_{\mathbb{N}}}\\
    &=\prob{P}_C^{(\RV{Y}_{d_i i})_{\mathbb{N}}|\RV{D}}(A|e)\\
    &= \prob{P}_C^{\RV{Y}|\RV{D}}(A|(e_{d_i i})_{i\in \mathbb{N}})\\
    &= \prob{P}_C^{\RV{Y}|\RV{D}}(A|d)
\end{align}

Hence by application of Lemma \ref{lem:extabl_to_respf}, taking $H=\mathcal{M}_1(Y^{D\times\mathbb{N}})$, there is some $\RV{H}:\RV{Y}^D\to H$ such that, defining $\prob{P}^{\RV{H}}_C:=\prob{P}_C^{\RV{Y}^D}\kernel{F}_{\RV{H}}$,
\begin{align}
    \prob{P}_C^{\RV{Y}|\RV{D}} &= \tikzfig{do_model_representation}
\end{align}

It remains to be shown that $\kernel{L}$ is a version of $\prob{P}_C^{\RV{Y}_i|\RV{D}_i\RV{H}}$ for all $i\in \mathbb{N}$, $\RV{D}_i\CI^e_{\prob{P}_C} \RV{Y}_{[i-1]}|(\RV{D}_{[i-1]},\RV{C})$ and $\RV{Y}_i\CI^e_{\prob{P}_C} (\RV{Y}_{[i-1]},\RV{D}_{[i-1]},C)|(\RV{D}_i,\RV{H})$.

\todo[inline]{I actually need the joint distribution of $(\RV{H},\RV{Y})$}

For the independence $\RV{Y}_i\CI^e_{\prob{P}_C} (\RV{Y}_{[i-1]},\RV{D}_{[i-1]},C)|(\RV{D}_i,\RV{H})$, note that
\begin{align}
     \prob{P}_C^{\RV{Y}_{<i}|\RV{H}\RV{X}_{<i}\RV{X}_i} &= \tikzfig{independence_inductive_base_0}\\
     &= \tikzfig{independence_inductive_base}
\end{align}
hence $\RV{Y}_{<i}\CI^e_{\prob{P}_C} (\RV{X}_i,\RV{C})|(\RV{H},\RV{X}_{<i})$

Then
\begin{align}
    \prob{P}_C^{\RV{Y}_i\RV{Y}_{<i}|\RV{H}\RV{D}_i\RV{D}_{<i}} &= \tikzfig{independence_inductive}\\
    \implies \prob{P}_C^{\RV{Y}_i|\RV{H}\RV{D}_i\RV{D}_{<i}} &\overset{\prob{P}_C}{\cong} \tikzfig{independence_inductive_last}
\end{align}
by Theorem \ref{th:higher_order_conditionals}. Hence $\RV{Y}_i\CI^e_{\prob{P}_C} (\RV{X}_{<i},\RV{Y}_{<i},\RV{C})|(\RV{H},\RV{X}_i)$.

Only if:
By assumption, for all $i\in \mathbb{N}$
\begin{align}
    \prob{P}_C^{\RV{Y}_i|\RV{H}\RV{X}_{[i]}\RV{Y}_{<i}} &\overset{\prob{P}_C}{\cong} \text{del}_{X^{i-1}\times Y^{i-1}}\otimes \prob{P}_C^{\RV{Y}_1|\RV{H}\RV{X}_1}
\end{align}

Thus for all $n\in \mathbb{N}$ by repeated application of Theorem \ref{th:higher_order_conditionals}
\begin{align}
    \prob{P}_C^{\RV{Y}_{[n]}|\RV{H}\RV{X}_{[n]}} &\overset{\prob{P}_C}{\cong} \tikzfig{do_model_representation_finite}
\end{align}
thus by Lemma \ref{lem:infinitely_extended_kernels}
\begin{align}
    \prob{P}_C^{\RV{Y}_{\mathbb{N}}|\RV{H}\RV{X}_{\mathbb{N}}} &\overset{\prob{P}_C}{\cong} \tikzfig{do_model_representation_noprior}
\end{align}
and, because $\RV{H}\CI_{\prob{P}_C}^e(\RV{X},\RV{C})$
\begin{align}
    \prob{P}_C^{\RV{Y}_{\mathbb{N}}|\RV{X}_{\mathbb{N}}} &\overset{\prob{P}_C}{\cong} \tikzfig{do_model_representation}
\end{align}
causal contractibility follows from Theorem \ref{th:ciid_rep_kernel}.
\end{proof}

A consequence of Theorem \ref{th:equal_of_condits} applied to just-do models $\prob{P}_C$ with causally contractible $\prob{P}_C^{\RV{Y}|\RV{D}}$ is that, for any $A,B\subset\mathbb{N}$ with $|A|=|B|$, $\prob{P}_C^{\RV{Y}_A|\RV{D}_A}=\prob{P}_C^{\RV{Y}_B|\RV{D}_B}$. A further consequence is the interchangeability of conditioning data -- for any $i\in \mathbb{N}$, $\prob{P}_C^{\RV{Y}_i|\RV{D}_i\RV{Y}_A\RV{D}_A}=\prob{P}_C^{\RV{Y}_i|\RV{D}_i\RV{Y}_B\RV{D}_B}$.

\begin{theorem}[Equality of contracted conditionals]\label{th:equal_of_reduced_condits}
A sequential just-do model $(\prob{P}_C,\RV{D},\RV{Y})$ with $\prob{P}_C^{\RV{Y}|\RV{D}}$ causally contractible satisfies, for any $A,B\subset \mathbb{N}$ with $|A|=|B|$
\begin{align}
    \prob{P}_C^{\RV{Y}_A|\RV{D}_A} \overset{\prob{P}_C}{\cong} \prob{P}_C^{\RV{Y}_B|\RV{D}_B}
\end{align}
\end{theorem}

\begin{proof}
Only if:
For any $A,B\subset \mathbb{N}$, let $\text{swap}_{B\leftrightarrow A,D}:D^{\mathbb{N}}\kto D^{\mathbb{N}}$ be the transposiiton of $B$ with $A$ indices and $\text{swap}_{B\leftrightarrow A,Y}:Y^{\mathbb{N}}\kto Y^{\mathbb{N}}$ be the same defined on $Y$. By Theorem \ref{th:equal_of_condits}
\begin{align}
    \prob{P}_C^{\RV{Y}_A|\RV{D}_A}\otimes \text{del}_{D^{\mathbb{N}}} &=  \prob{P}_C^{\RV{Y}|\RV{D}} \text{marg}_A\\
     &= \text{swap}_{A\leftrightarrow[n],D} \prob{P}_C^{\RV{Y}_{[n]}|\RV{D}}\\
    &= \prob{P}_C^{\RV{Y}_{[n]}|\RV{D}_{[n]}}\otimes \text{del}_{D^{\mathbb{N}}}\\
    &= \text{swap}_{N\leftrightarrow[n],D} \prob{P}_C^{\RV{Y}_{[n]}|\RV{D}}\\
    &= \prob{P}_C^{\RV{Y}_B|\RV{D}_B}\otimes \text{del}_{D^{\mathbb{N}}}
\end{align}
\end{proof}

\section{Combs}\label{sec:def_combs}

\todo[inline]{I think this really fits better in Chapter 2}

Combs generalise conditional probabilities in this sense: given a conditional distribution and a marginal distribution of the right type, joining them together (with the semidirect product\ref{def:copyproduct}) I get a marginal distribution of a different type. Define ``1-combs'' as conditional probabilities and ``0-combs'' as conditional distributions. Then the previous observation can be restated as: given a 1-comb and a 0-comb of the right type,  joining them together yields a 0-comb of a different type. Higher order combs generalise this: given an $n$-comb and an $n-1$-comb of the right type, joining them yields an $n-1$ comb.

Joining combs uses an ``insert'' operation (Definition \ref{def:insert_discrete}).  A graphical depiction of this operation gives some intuition for why it is called ``insert'':
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_{1}\RV{D}_2\RV{Y}_2|\RV{D}_1}&=\text{insert}(\prob{P}_\alpha^{\RV{D}_2|\RV{D}_1\RV{Y}_1},\prob{P}_C^{\RV{Y_{[2]}}\combbreak\RV{D}_{[2]}})\\
    &= \tikzfig{comb_insert_complicated}\label{eq:comb_insert_complicated}\\
    &= \tikzfig{comb_insert_gettingsimpler}\\
    &= \tikzfig{comb_insert_simple}\label{eq:comb_insert_simple}
\end{align}
While Equation \ref{eq:comb_insert_complicated} is a well-formed string diagram in the category of Markov kernels, Equation \ref{eq:comb_insert_simple} is not. In the case that all the underlying sets are discrete, Equation \ref{eq:comb_insert_simple} can be defined using an extended string diagram notation appropriate for the category of real-valued matrices \citep{jacobs_causal_2019}, though we do not introduce this extension here.

Two different notations are used to describe combs. There is an ``extended'' notation which explicitly references the ``gaps'' and the ``conditionals'' that form a comb. For example, the comb $\prob{P}_C^{\RV{Y_{[2]}}\combbreak\RV{D}_{[2]}})$ above could be written $\prob{P}_C^{\RV{Y}_{2}|\RV{D}_2\combgap \RV{Y}_1|\RV{D}_1}$. Alternatively, given two sequences $(\RV{Y}_i)_{i\in [n]}$ and $(\RV{D}_i)_{i\in [n]}$ and a comb that consists of a sequence of conditionals of corresponding indices interspersed by gaps $\prob{P}_C^{...\combgap \RV{Y}_{2}|\RV{D}_2\combgap \RV{Y}_1|\RV{D}_1}$, we compress this to $\prob{P}_C^{\RV{Y}_{[n]}\combbreak \RV{D}_{[n]}}$.

Formal definitions of combs and both notations follow. As with conditional probabilities, a \emph{uniform} $n$-comb $\prob{P}_C^{\RV{Y}_{[n]}\combbreak \RV{X}_{[n]}}$ is a Markov kernel that satisfies the definition of an $n$-comb for each $\alpha\in C$.

\begin{definition}[2-comb]\label{def:2-comb}
Given a probability space $(\prob{P},\Omega,\sigalg{F})$ with variables $\RV{W}$, $\RV{X}$, $\RV{Y}$ and $\RV{Z}$, the 2-comb $\prob{P}^{\RV{Z}|\RV{Y}\combgap\RV{X}|\RV{W}}$ is defined as the Markov kernel given by
\begin{align}
    \prob{P}^{\RV{Z}|\RV{Y}\combgap\RV{X}|\RV{W}} := \tikzfig{2_comb_special_notation}
\end{align}
\end{definition}

\begin{definition}[$n$-Comb]\label{def:uniform_comb}
Given a probability space $(\prob{P},\Omega,\sigalg{F})$ with variables $\RV{Y}_i:\Omega\to Y$, $\RV{D}_i:\Omega\to D$ for $i\in [n]$ and uniform conditional probabilities $\{\RV{P}^{\RV{Y}_i|\RV{D}_{[i]}\RV{Y}_{[i-1]}}|i\in [n]\}$, the uniform $n$-comb $\prob{P}^{\RV{Y}_{[n]}\combbreak \RV{D}_{[n]}}:D^n\kto Y^n$ is the Markov kernel given by the recursive definition
\begin{align}
    \prob{P}^{\RV{Y}_{1}\combbreak \RV{D}_{1}} &= \prob{P}^{\RV{Y}_1|\RV{D}_1}\\
    \prob{P}^{\RV{Y}_{[m]}\combbreak \RV{D}_{[m]}} &= \po\tikzfig{comb_inductive}
\end{align}
\end{definition}

\begin{definition}[$\mathbb{N}$-comb]
Given a probability space $(\prob{P},\Omega,\sigalg{F})$ with variables $\RV{Y}_i:\Omega\to Y$ and $\RV{D}_i:\Omega\to D$ for $i\in \mathbb{N}$ and uniform conditional probabilities $\{\RV{P}^{\RV{Y}_i|\RV{D}_{[i]}\RV{Y}_{[i-1]}}|i\in \mathbb{N}\}$, the uniform $\mathbb{N}$-comb $\prob{P}^{\RV{Y}_{\mathbb{N}}\combbreak \RV{D}_{\mathbb{N}}}:D^\mathbb{N}\kto Y^\mathbb{N}$ is the Markov kernel such that for all $n\in \mathbb{N}$
\begin{align}
    \prob{P}^{\RV{Y}_{\mathbb{N}}\combbreak \RV{D}_{\mathbb{N}}}(\mathrm{id}_{Y^{n}}\otimes \mathrm{del}_{Y^{\mathbb{N}}}) &= \prob{P}^{\RV{Y}_{[n]}\combbreak \RV{D}_{[n]}}\otimes \mathrm{del}_{Y^{\mathbb{N}}}
\end{align}
\end{definition}

\begin{theorem}[Existence of $\mathbb{N}$-combs]
Given a probability set $\prob{P}$ with variables $\RV{Y}_i:\Omega\to Y$ and $\RV{D}_i:\Omega\to D$ for $i\in \mathbb{N}$ and conditional probabilities $\{\RV{P}^{\RV{Y}_i|\RV{D}_{[i]}\RV{Y}_{[i-1]}}|i\in \mathbb{N}\}$, a uniform $\mathbb{N}$-comb $\prob{P}^{\RV{Y}_{\mathbb{N}}\combbreak \RV{D}_{\mathbb{N}}}:D^\mathbb{N}\kto Y^\mathbb{N}$ exists.
\end{theorem}

\begin{proof}
For each $n\in \mathbb{N}$ $m<n$, we have
\begin{align}
    \prob{P}^{\RV{Y}_{[n]}\combbreak \RV{D}_{[n]}}(\mathrm{id}_{Y^{n-m}})\otimes \mathrm{del}_{Y^m}) &= \prob{P}^{\RV{Y}_{[n-m]}\combbreak \RV{D}_{[n-m]}}\otimes \mathrm{del}_{Y^m}
\end{align}

Therefore the existence of $\prob{P}^{\RV{Y}_{\mathbb{N}}\combbreak \RV{D}_{\mathbb{N}}}$ is a consequence of Lemma \ref{lem:infinitely_extended_kernels}.
\end{proof}

For discrete sets, the insert operation has a compact definition, and this will be sufficient for our purposes.

\begin{definition}[Comb insert - discrete]\label{def:insert_discrete}
Given an $n$-comb $\prob{P}_\alpha^{\RV{Y}_{[n]}\combbreak \RV{D}_{[n]}}$ and an $n-1$ comb $\prob{P}_\alpha^{\RV{D}_{[2,n]}|\RV{Y}_{[n-1]}}$, $(D,\sigalg{D})$ and $(Y,\sigalg{Y})$ discrete, for all $y_i\in Y$ and $d_i\in D$
\begin{align}
    \mathrm{insert}(\prob{P}_\alpha^{\RV{D}_{[2,n]}\combbreak \RV{Y}_{[n-1]}},\prob{P}_\alpha^{\RV{Y}_{[n]}\combbreak \RV{D}_{[n]}})(y_{[n]},d_{[2,n]}|d_1) &= \prob{P}_\alpha^{\RV{Y}_{[n]}\combbreak \RV{D}_{[n]}}(y_n|d_{[2,n]},d_1)\prob{P}_\alpha^{\RV{D}_{[1,n]}\combbreak\RV{Y}_{[n-1]}}(d_{[2,n]}|y_{[n-1]})
\end{align}
\end{definition}

\subsection{Aside: combs are the output of the ``fix'' operation}

There is a relationship between combs and the ``fix'' operation defined in \citet{richardson_nested_2017}. In particular, suppose we have a probability $\prob{P}_\alpha$ and a comb $\prob{P}_\alpha^{\RV{Y}_{[2]}|\RV{D}_{[2]}}$. Then (assuming discrete sets)
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_{[2]}\combbreak \RV{D}_{[2]}}(y_1,y_2|d_1,d_2) &= \prob{P}_\alpha^{\RV{Y}_1|\RV{D}_1}(y_1|d_1)\prob{P}_\alpha^{\RV{Y}_2|\RV{D}_2}(y_2|d_2)\\
    &= \frac{\prob{P}_\alpha^{\RV{Y}_1|\RV{D}_1}(y_1|d_1)\prob{P}_\alpha^{\RV{D}_2|\RV{Y}_1\RV{D}_1}(d_2|y_1,d_1)\prob{P}_\alpha^{\RV{Y}_2|\RV{D}_2}(y_2|d_2)}{\prob{P}_\alpha^{\RV{D}_2|\RV{Y}_1\RV{D}_1}(d_2|y_1,d_1)}\\
    &= \frac{\prob{P}_\alpha^{\RV{Y}_{[2]}\RV{D}_2|\RV{D}_1}(y_1,y_2,d_2|d_1)}{\prob{P}_\alpha^{\RV{D}_2|\RV{Y}_1\RV{D}_1}(d_2|y_1,d_1)}
\end{align}
That is (at least in this case), the result of ``division by a conditional probability'' used in the fix operation is a comb. We speculate that the output of the fix operation is, in general, an $n$-comb, but we have not proven this.

\section[Varying inputs]{Conditionally independent and identical response functions with varying inputs}\label{sec:nonu_input}


\subsubsection{Examples}\label{sec:examples}

Purely passive observations can be modeled with a probability set $\prob{P}_C$ where $|\prob{P}_C|=1$. In this case, a model that is exchangeable over the sequence of pairs $(\RV{D}_i,\RV{Y}_i)_{i\in \mathbb{N}}$ has $\prob{P}_C^{\RV{Y}|\RV{D}}$ causally contractible. This follows from the fact that

\begin{align}
    \prob{P}_C^{\RV{YD}} &= \tikzfig{do_model_rep_onechoice_combined}
\end{align}
and so
\begin{align}
    \tikzfig{do_model_rep_onechoice}
\end{align}
is a version of $\prob{P}_C^{\RV{Y}|\RV{D}}$.

Instead of passive observations only, a model might feature a subsequence of passive observations and a subsequence of active interventions. Say the passive observations are $(\RV{D},\RV{Y})_{i\in\mathbb{N}}$ and the active interventions are $(\RV{E},\RV{Z})_{i\in \mathbb{N}}$. By the previous argument, $\prob{P}_C^{\RV{Y}|\RV{D}}$ is causally contractible. We might further assume that $\prob{P}_C^{\RV{YZ}|\RV{DE}}$ is causally contractible -- that is, there is a independent and identical response function $\prob{P}_C^{\RV{Z}_i|\RV{E}_i\RV{H}}$ equal to $\prob{P}_C^{\RV{Y}_i|\RV{D}_i\RV{H}}$.

One consequence of this is ``observational imitation'': any choice $\alpha$ that makes $\prob{P}_\alpha^{\RV{D}\RV{E}}$ exchangeable also makes $\prob{P}_\alpha^{\RV{YZ}}$ exchangeable. That is, if for some permutation $\mathrm{swap}_\rho$
\begin{align}
    \prob{P}_\alpha^{\RV{DE}}\mathrm{swap}_\rho &= \prob{P}_\alpha^{\RV{DE}}
\end{align}
then by commutativity of exchange
\begin{align}
    \prob{P}_\alpha^{\RV{YZ}} &= \prob{P}_\alpha^{\RV{DE}} \prob{P}_C^{\RV{YZ}|\RV{DE}}\\
    &=  \prob{P}_\alpha^{\RV{DE}}\mathrm{swap}_\rho \prob{P}_C^{\RV{YZ}|\RV{DE}}\\
    &= \prob{P}_\alpha^{\RV{DE}} \prob{P}_C^{\RV{YZ}|\RV{DE}}\mathrm{swap}_\rho\\
    &= \prob{P}_C^{\RV{YZ}|\RV{DE}}\mathrm{swap}_\rho
\end{align}

However, the assumption that $\prob{P}_C^{\RV{YZ}|\RV{DE}}$ is causally contractible seems unreasonable in most situations. One implication of this assumption is (by Theorem \ref{th:equal_of_condits}):
\begin{align}
    \prob{P}^{\RV{Y}\RV{Z}_i|\RV{D}\RV{E}_i}_C &= \prob{P}^{\RV{Z}|\RV{E}}\\
    \implies \prob{P}^{\RV{Z}_i|\RV{E}_i\RV{D}\RV{Y}}_C &= \prob{P}^{\RV{Z}_{i}|\RV{E}_i\RV{E}_{\{i\}^C}\RV{Z}_{\{i\}^C}}\label{eq:interchangeability}
\end{align}
That is, the model must yield the same result when conditioned on either the observational results, or the results of other active interventions. It is rare to assume \emph{a priori} that observational and experimental data are equally informative. Such a conclusion could be drawn \emph{after} reviewing both sequences of data, see for example \citet{eckles_bias_2021}, or it might be rejected \citet{gordon_comparison_2018,gordon_close_2022}.

\begin{example}[Backdoor adjustment]\label{ex:backdoor}
If a sequential just-do model $(\prob{P}_C,(\RV{D},\RV{X}),\RV{Y})$ has $\prob{P}_C^{\RV{Y}|\RV{DX}}$ causally contractible as well as:
\begin{itemize}
    \item $\RV{X}_{i}\CI^e_{\prob{P}_C}\RV{D}_{i}C|\RV{H}$ ($\RV{X}_i$ is extended independent of $\RV{D}_i$ conditional on $\RV{H}$)
    \item $\prob{P}_C^{\RV{X}_{i}|\RV{H}}\cong \prob{P}_C^{\RV{X}_{1}|\RV{H}}$ (the distribution of $\RV{X}$ is exchangeable)
 \end{itemize}
Then the model exhibits a kind of ``backdoor adjustment'' \citet[Chap. 1]{pearl_causality:_2009}. Specifically
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_{i}|\RV{D}_{i}\RV{H}}(A|d,h) &= \int_X \prob{P}_\alpha^{\RV{Y}_{i}|\RV{X}_{i}\RV{D}_{i}\RV{H}}(A|d,x,h)\prob{P}_\alpha^{\RV{X}_{i}|\RV{D}_{i}\RV{H}}(\mathrm{d}x|d,h)\\
    &= \int_X \prob{P}_C^{\RV{Y}_{1}|\RV{X}_{1}\RV{D}_{1}\RV{H}}(A|d,x,h)\prob{P}_C^{\RV{X}_{i}|\RV{H}}(\mathrm{d}x|h)\\
    &= \int_X \prob{P}_C^{\RV{Y}_{1}|\RV{X}_{1}\RV{D}_{1}\RV{H}}(A|d,x,h)\prob{P}_C^{\RV{X}_{1}|\RV{H}}(\mathrm{d}x|h)\label{eq:backdoor}
\end{align}
\end{example}


Equation \ref{eq:backdoor} is identical to the backdoor adjustment formula for an intervention on $\RV{D}_1$ targeting $\RV{Y}_1$ where $\RV{X}_1$ is a common cause of both.

\subsubsection{Example: body mass index}

Given a sequential just-do model $(\prob{P}_C,(\RV{B},\RV{I}),\RV{Y})$ with $\RV{B}:=(\RV{B}_i)_{i\in M}$ representing body mass index of individual $\RV{I}_i$ and $\RV{Y}:=(\RV{Y}_i)_{i\in M}$ representing health outcomes of interest for the same individual, \citet{hernan_does_2008} noted that there are multiple different choices that can influence an individual's body mass index $\RV{B}_i$ in the same way. Thus $\RV{YI}\CI^e_{\prob{P}_C} \RV{C}|\RV{B}$ might generally be rejected, and so there may be no uniform conditional $\prob{P}_C^{\RV{Y}|\RV{IB}}$. In this case, $\prob{P}_C^{\RV{Y}|\RV{IB}}$ cannot be causally contractible because it doesn't exist.

Suppose instead a model $(\prob{P}_C,(\RV{D},\RV{I}),(\RV{B},\RV{Y}))$ is given, with $\RV{D}=(\RV{D}_i)_{i\in M}$ representing ``decisions'', appropriately fine-grained to satisfy
\begin{align}
    &\RV{YBI}\CI^e_{\prob{P}_C} \RV{C}|\RV{D}\\
    &\RV{YBI}\CI^e_{\prob{P}_C} \RV{D}|\RV{C}
\end{align}
and $\prob{P}_C^{\RV{YB}|\RV{ID}}$ causally contractible. Then by Theorem \ref{th:cc_ind_treat} $\prob{P}_C^{\RV{Y}|\RV{BD}}$ is also causally contractible. In general, there may be some $U\subset H$ such that for any $h\in U$ 
\begin{align}
    \prob{P}_C^{\RV{Y}_i|\RV{B}_i\RV{D}_i\RV{H}}(y|b,d,h) &= \prob{P}_C^{\RV{Y}_i|\RV{B}_i\RV{H}}(y|b,h)\label{eq:conditional_conditional_independence}
\end{align}
then, \emph{conditioning on }$\RV{H}\in U$, the resulting $\prob{P}_{C,\RV{H}\in U}^{\RV{Y}|\RV{B}}$ is causally contractible.
\todo[inline]{Defining conditioning}
So it may be possible to derive the fact that there is a independent and identical response conditional $\prob{P}_{C,\RV{H}\in U}^{\RV{Y}_i|\RV{H}\RV{B}_i}$ if $\RV{H}\in U$ is implied by available data, even if it is not assumed outright.

\section[Data-dependent inputs]{Conditionally independent and identical response functions with data-dependent inputs}\label{sec:data_dependent}

The results of the previous section concern ``just-do'' models where actions have not dependence on previous data. Decision problems of interest actually have actions that depend on data -- what's really wanted are ``see-do'' models of some variety (see Definition \ref{def:see_do_model}). Here, Theorem \ref{th:data_ind_CC} is generalised to sequential see-do models with the use of \emph{probability combs}.

To begin with an example, consider a probability set $(\prob{P}_C,\RV{D},\RV{Y})$ with $\RV{D}:=(\RV{D}_i)_{i\in\mathbb{N}}$ and $\RV{Y}:=(\RV{Y}_i)_{i\in\mathbb{N}}$ as usual, and take a subsequence $(\RV{D}_i,\RV{Y}_i)_{i\in [2]}$ of length 2. Suppose $\prob{P}_C$ features independent and identical response conditionals in the sense that the following holds
\begin{align}
    \RV{Y}_i&\CI^e_{\prob{P}_C} (\RV{Y}_{<i},\RV{D}_{<i},\RV{C})|\RV{H}\RV{D}_i&\forall i\in \mathbb{N}\\
    \land \RV{H} &\CI^e_{\prob{P}_C} \RV{D} C\\
    \land \prob{P}_C^{\RV{Y}_i|\RV{H}\RV{D}_i} &= \prob{P}_C^{\RV{Y}_0|\RV{H}\RV{D}_0} & \forall i\in \mathbb{N}
\end{align}

Then, for arbitrary $\alpha\in C$
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_{[2]}} &= \tikzfig{response_conditional_comb}
\end{align}
note that $\RV{D}_2$ depends on $\RV{Y}_1$ and $\RV{D}_1$. Instead of multiplying by a distribution over $(\RV{D}_1,\RV{D}_2)$, $\prob{P}_\alpha^{\RV{D}_2|\RV{Y}_1\RV{D}_1}$ has been ``inserted'' between the response conditionals $\prob{P}_C^{\RV{Y}_1|\RV{D}_1\RV{H}}$ and $\prob{P}_C^{\RV{Y}_2|\RV{D}_2\RV{H}}$. A comb is a Markov kernel that yields a probability distribution when another Markov kernel of appropriate type is inserted in this manner.

Given $\prob{P}_C^{\RV{Y}_1|\RV{D}_1\RV{H}}$ and $\prob{P}_C^{\RV{Y}_2|\RV{D}_2\RV{H}}$, define the comb
\begin{align}
    \prob{P}_C^{\RV{Y}_{[2]}\combbreak \RV{D}_{[2]}} := \tikzfig{causally_contractible_comb}
\end{align}
then $\prob{P}_C^{\RV{Y}_{[2]}\combbreak \RV{D}_{[2]}}$ is causally contractible. $\prob{P}_C^{\RV{Y}_{[2]}\combbreak \RV{D}_{[2]}}$ is \emph{not} a uniform conditional probability; in general 
\begin{align}
    \prob{P}_\alpha^{\RV{D}_1\RV{D}_2} \prob{P}_C^{\RV{Y}_{[2]}\combbreak \RV{D}_{[2]}}\neq \prob{P}_\alpha^{\RV{Y}_1\RV{Y}_2}
\end{align}



\subsection{Response conditionals in models with data dependent actions}\label{sec:data_dependent_representation}

Theorem \ref{th:response_hdep} generalises Theorem \ref{th:data_ind_CC} to models $(\prob{P}_C,\RV{D},\RV{Y})$ with data-dependent actions, where instead the assumption that the uniform comb $\prob{P}_C^{\RV{Y}\combbreak \RV{D}}$ is causally contractible replaces the assumption that the conditional probability $\prob{P}_C^{\RV{Y}| \RV{D}}$ is causally contractible.

\begin{definition}[Sequential see-do model]
A \emph{sequential see-do model} is a triple $(\prob{P}_C,\RV{D},\RV{Y})$ where $\prob{P}_C$ is a probability set on $(\Omega,\sigalg{F})$, $\RV{D}$ is a sequence of ``inputs'' $\RV{D}:=(\RV{D}_i)_{i\in\mathbb{N}}$ and $\RV{Y}$ is a corresponding sequence of ``outputs'' $\RV{Y}=(\RV{Y}_i)_{i\in\mathbb{N}}$ where $\RV{D}_i:\Omega\to D$ and $\RV{Y}_i:\Omega\to Y$ and $\RV{Y}_i\CI^e_{\prob{P}_C} C|(\RV{D}_{[i]},\RV{Y}_{<i})$.
\end{definition}

\begin{theorem}[]\label{th:response_hdep}
Given a sequential see-do model $(\prob{P}_C',\RV{D}',\RV{Y}')$ on $(\Omega,\sigalg{F})$, then $\prob{P}_C^{\prime \RV{Y}'\combbreak \RV{D}'}$ is causally contractible if and only if there is a latent extension $\prob{P}_C$ of $\prob{P}_C'$ to $(\Omega\times H,\sigalg{F}\otimes\sigalg{Y}^{D\times\mathbb{N}})$ with hypothesis $\RV{H}:\Omega\times H\to H$ such that $\RV{Y}_i\CI^e_{\prob{P}_C'} (\RV{Y}_{<i},\RV{X}_{<i},C)|(\RV{X}_i,\RV{H})$ and $\prob{P}_C^{\RV{Y}_i|\RV{X}_i\RV{H}}=\prob{P}_C^{\RV{Y}_j|\RV{X}_j\RV{H}}$ for all $i,j\in \mathbb{N}$ and $\RV{H}\CI_{\prob{P}_C} (\RV{X},\RV{C})$.
\end{theorem}

\begin{proof}
If:
By assumption, there is some $\kernel{L}:H\times D\kto Y$ such that
\begin{align}
    \prob{P}_C^{\RV{Y}_i|\RV{H}\RV{D}_i} &= \kernel{L}
\end{align}
and $\RV{Y}_i\CI^e_{\prob{P}_C} (\RV{Y}_{<i},\RV{D}_{<i})|(\RV{D}_i,\RV{H})$. Thus
\begin{align}
    \prob{P}_C^{\RV{Y}_i|\RV{H}\RV{D}_i\RV{Y}_{<i}\RV{D}_{<i}} &= \kernel{L}\otimes \text{erase}_{Y^{i-1}\times D^{i-1}}
\end{align}
and so
\begin{align}
    \prob{P}_C^{\RV{Y}\combbreak \RV{D}} &= \tikzfig{do_model_representation}\label{eq:comb_representation_w_CI}
\end{align}
and so by Theorem \ref{th:ciid_rep_kernel}, $\prob{P}_C^{\RV{Y}\combbreak \RV{D}}$ is causally contractible.

Only if:
First, define the extension $\prob{P}_C$. From Theorem \ref{th:ciid_rep_kernel} and causal contractibility of $\prob{P}_C^{\prime \RV{Y}'\combbreak \RV{D}'}$ there is some $H$, $\mu\in \Delta(H)$ and $\kernel{L}:H\times D\kto Y$ such that
\begin{align}
    \prob{P}_C^{\prime \RV{Y}'\combbreak\RV{D}'} &= \tikzfig{do_model_representation_mu}
\end{align}
thus, by the definition of the comb insert operation
\begin{align}
    \prob{P}_\alpha^{\prime\RV{D}'_{[n]} \RV{Y}'_{[n]}} &= \prob{P}_\alpha^{\RV{D}_1}\odot \text{insert}(\prob{P}_\alpha^{\prime \RV{D}'_{[2,n]}\combbreak\RV{Y}'_{[n-1]}}, \prob{P}_C^{\prime \RV{Y}'_{[n]}\combbreak\RV{D}'_{[n]}}) 
\end{align}
Let
\begin{align}
    \prob{P}_C^{\RV{Y}_i|\RV{H}\RV{D}_i} &= \kernel{L}\label{eq:identical_response_assumption}
\end{align}
and let $\RV{Y}_i\CI^e_{\prob{P}_C} (\RV{Y}_{<i},\RV{D}_{<i})|(\RV{D}_i,\RV{H})$, and for all $\alpha$ set $\prob{P}_\alpha^{\RV{W}|\RV{DY}}=\prob{P}_\alpha^{\prime \RV{W}'|\RV{D'Y'}}$ for all $\RV{W}':\Omega\to W$ and $\prob{P}_\alpha^{\RV{D}_i|\RV{Y}_{<i}\RV{D}_{<i}}=\prob{P}_\alpha^{\prime \RV{D}_i'|\RV{Y}_{<i}'\RV{D}_{<i}''}$.

It remains to be shown that $\prob{P}_\alpha^{\RV{DY}}=\prob{P}_\alpha^{\prime \RV{DY}}$.

By Equation \ref{eq:identical_response_assumption} and $\RV{Y}_i\CI^e_{\prob{P}_C} (\RV{Y}_{<i},\RV{D}_{<i})|(\RV{D}_i,\RV{H})$, it follows (for identical reasons as Equation \ref{eq:comb_representation_w_CI}) that
\begin{align}
    \prob{P}_C^{\RV{Y}\combbreak \RV{D}} &= \tikzfig{do_model_representation}\\
    &= \tikzfig{do_model_representation_mu}\\
    &= \prob{P}_C^{\prime \RV{Y}'\combbreak\RV{D}'}
\end{align}

And so for all $n\in \mathbb{N}$
\begin{align}
    \prob{P}_\alpha^{\RV{D}_{[n]} \RV{Y}_{[n]}} &=  \prob{P}_\alpha^{\RV{D}_1}\odot \text{insert}(\prob{P}_\alpha^{ \RV{D}_{[2,n]}\combbreak\RV{Y}_{[n-1]}}, \prob{P}_C^{\prime \RV{Y}_{[n]}\combbreak\RV{D}_{[n]}}) \\
    &= \prob{P}_\alpha^{\RV{D}_1}\odot \text{insert}(\prob{P}_\alpha^{\prime \RV{D}'_{[2,n]}\combbreak\RV{Y}'_{[n-1]}}, \prob{P}_C^{\prime \RV{Y}'_{[n]}\combbreak\RV{D}'_{[n]}}) \\
    &= \prob{P}_\alpha^{\prime\RV{D}'_{[n]} \RV{Y}'_{[n]}}
\end{align}
\end{proof}

In contrast to the data-independent case where causal contractibility of $\prob{P}_C^{\RV{Y}|\RV{X}}$ implies the equivalence of all subsequence conditionals $\prob{P}_C^{\RV{Y}_A|\RV{X}_A}$ for all equally sized $A\subset\mathbb{N}$, a causally contractible comb $\prob{P}_C^{\RV{Y}\combbreak \RV{D}}$ does not generally imply that subsequence combs $\prob{P}_C^{\RV{Y}_A\combbreak \RV{D}_A}$ and $\prob{P}_C^{\RV{Y}_B\combbreak \RV{D}_B}$ are equivalent with $|A|=|B|$.



\section{Assessing decision problems for exchange commutativity}

Exchange commutativity is a condition that, if it holds, allows a decision maker to use the map $D\kto Y$ calculated from relative frequencies to determine the optimal course of action. The question is: when should a decision maker consider this assumption reasonable?, confronted with a decision problem actually adopt a causally contractible model $\prob{P}_C$ to help them make their decision? This is not an easy question for several reasons. Two of these are:
\begin{itemize}
    \item The kind of symmetry required by exchange commutativity seems to us much harder to intuit than the kind of symmetry required by regular exchangeability
    \item The conditions of exchange commutativity and locality must hold for each choice in $C$
\end{itemize}

``Ordinary'' exchangeability is often considered to be appropriate when modelling a measurement procedure that consists of a sequence of indistinguishable sub-procedures. A common example is a sequence of coin flips -- there is (usually) no reason to consider any coin flip to differ in any important way from any other. Thus, one can reason, swapping the labels of the coin flips yields a measurement procedure that is effectively identical. It follows that the model should be unchanged under a permutation of the variables representing the sequence of flips -- that is, it should be exchangeable\footnote{As \citet[pg. 461]{walley_statistical_1991} points out, the conclusion of exchangeability also requires the assumption that the measurement procedure should be modeled with a single probability distribution, which is an assumption that is being made in this chapter}. The basic judgement call is then: the subprocedures for each coin flip are effectively identical.

Exchange commutativity requires a different kind of judgement. A common causal inference example features a decision procedure that yields a sequence of (treatment, outcome). Exchange commutativity asks us to compare the original procedure with an arbitrary procedure that shuffles the pairs. Then, \emph{given any fixed vector of treatment values}, the resulting pair of procedures must be effectively indistinguishable. Full causal contractibility adds the requirement that, comparing two procedures of this type and restricting our attention to a subsequence of outcomes, we can ignore any differences between treatment vectors that do not correspond to the subsequence of interest.

This is not particularly easy to think about! \citet{greenland_identifiability_1986} mention the condition that the treatments of different patients could be swapped without changing the distribution over outcomes. This can be interpreted as saying: given two choices that induce deterministic treatment vectors, if the vector induced by the first is a permutation of the vector induced by the second, the resulting distributions of outcomes (appropriately permuted) should be identical. This is a consequence of exchange commutativity, but it is not equivalent: treatments (or ``inputs'') may not be deterministic for all choices, in which case it's not clear what ``swapping treatments'' means. If it's a hypothetical action that swaps treatments (see the discussion at the end of \ref{sec:whats_the_point}), it seems that some theory is needed to say what equivalence under such hypothetical actions imply for the actual choices to be evaluated.

A further complication is due to the fact that, by necessity, a probability set $\prob{P}_C$ models a measurement procedure for each of a set of choices $C$. Someone constructing a model $\prob{P}_C$ to help them deal with decision problem may want to reason that their state of knowledge after selecting some choice $\alpha\in C$ is the same as their state of knowledge when they are constructing $\prob{P}_C$. That is, they don't want to worry about whether their choice ``depends on anything''. The fact that they don't want to worry about this doesn't mean that they don't have to! The theory of probability sets is formal, and it can be augmented with decision rules to yield a formal theory of making decisions, but the correspondence between $\prob{P}_C$ and the ``real things that constitute the decision problem'' is a judgement call, and it is possible to make poor calls. Example \ref{ex:confounding} is an example illustrating this. There are ways to deal with actions that ``depend on things'', see for example \citet{gallow_causal_2020}'s discussion of ``managing the news'', but the question of constructing appropriate models seems hard enough without the extra complication.

Individual-level causal contractibility is an attempt to specify a method for model construction that involves judgements that are (mostly) easier to think about than regular causal contractibility and that may sometimes yield regular causal contractibility as a result (Theorem \ref{th:cc_ind_treat}). Notably, the assumption of individual-level causal contractibility can, under certain conditions, imply that a model is causally contractible conditional on an ``unobserved'' variable, analogous to the familiar assumption of hidden confounding. 

\subsection{Causal contractibility can be undermined by a lack of control}\label{sec:assessing}

It is much easier to construct a model $\prob{P}_C$ if the choice $\RV{C}$ doesn't ``depend on'' anything. However, this property is not guaranteed, as Example \ref{ex:confounding} shows -- the model need to be specified in the right way for the right kind of problem. 

\begin{example}[Confounded choices]\label{ex:confounding}
We set this up in terms of an ``analyst'' and an ``administrator'' not because it's necessary for the example but because it can help make it easier to understand. The analyst's job is to construct a model $\prob{P}_C$, evaluate different options $\alpha\in C$ and offer advice regarding the choice. The administrator's job is to actually choose some $\alpha\in C$ satisfying the analyst's advice and to carry out the associated procedure.

This separation of concerns gives the administrator a degree of freedom in their choice: they can potentially choose $\alpha$ with access to information that the analyst lacks.

In particular, suppose $\RV{Y}:=(\RV{Y}_i)_{i\in \mathbb{N}}$, $\RV{D}:=(\RV{D}_i)_{i\in \mathbb{N}}$, $\RV{U}=(\RV{U}_i)_{i\in\mathbb{N}}$ and the set of choices $C=[0,1]^{\mathbb{N}}$ is a length $\mathbb{N}$ sequence of probability distributions in $\Delta(\{0,1\})$. The analyst, based on their knowledge of the experiment, constructs the model $\prob{P}_C$ where $\prob{P}_C^{\RV{Y}_i|\RV{U}_i\RV{D}_i}(1|\cdot,\cdot)$ is given by:
\begin{center}
\begin{tabular}{ c | c | c }
  & $\RV{D}_i=0$ & $\RV{D}_i=1$ \\\hline 
 $\RV{U}_i=0$ & 0 & 0 \\ \hline 
 $\RV{U}_i=1$ & 1 & 1   
\end{tabular}
\end{center}
and 
\begin{align}
    \prob{P}_C^{\RV{U}_i}(\{u\}) &= 0.5
\end{align}
and
\begin{align}
    \prob{P}_\alpha^{\RV{D}_i}(1) &= \alpha_i
\end{align}
where $\alpha=(\alpha_i)_{i\in\mathbb{N}}$ and the triples $(\RV{D}_i,\RV{U}_i,\RV{Y}_i)$ are mutually independent given $\RV{C}$. From the analyst's point of view, $\prob{P}_C^{\RV{Y}|\RV{UD}}$ is causally contractible, and $\RV{U}_i$ is identically uniform for all $i$. 

The analyst recommends any $\alpha$ such that $\lim_{n\to\infty} \sum_i^n \frac{\alpha_i}{n} = 0.5$ (acknowledging that, in this contrived example, there's no obvious reason to do so). Suppose that the administrator operates by the following rule: \emph{first} they observe the binary result of $\proc{U}_i$, then they choose $\alpha_i$ equal to whatever they saw with an $\epsilon$ sized step towards $0.5$. That is, if they see $\proc{U}_i\yields 1$, they choose $\alpha_i=1-\epsilon$.

Then, from the analyst's point of view, $\alpha_i=1-\epsilon\implies \prob{P}_{1-\epsilon}^{\RV{Y}_i|\RV{D}_i}(1|d) = 1$ for all $i$ and $\alpha_j=\epsilon\implies \prob{P}_{1-\epsilon}^{\RV{Y}_j|\RV{D}_j}(1|d) = 0$ for all $j$. This means that $\prob{P}_C^{\RV{Y}|\RV{D}}$ is not exchange commutative. The administrator's rule for choosing $\alpha$ means that, though the analyst does not know the outcome of $\proc{U}$, they know what it would be for any $\alpha$.
\end{example}

Example \ref{ex:confounding} features a situation where it is arguable whether or not the analyst was applying $\prob{P}_C$ to a ``decision problem''. In particular, they did not offer a particular choice $\alpha$ as a result of their analysis, but instead offered a subset of $C$. For example, the analyst might require that . Example \ref{ex:confounding} in fact satisfies this requirement.

The original justification for having a set of choices $C$ is that $C$ is the set of things that, after deliberation aided by the model $\prob{P}_C$, the decision maker might select. Example \ref{ex:confounding} does not satisfy this understanding of the meaning of the set $C$ -- the analyst aided by $\prob{P}_C$ selects nothing or, in the previous paragraph, selects a subset of $C$ rather than an element of $C$. Thus this example suggests that one should be cautious about using a probability set $\prob{P}_C$ to evaluate choices without choosing anything definite based on this evaluation.

\citet{kasy_why_2016} argues that ``randomised controlled trials are not needed for causal identifiability, only controlled trials'', and suggests that experiments should sometimes be designed with deterministic assignments of patients to treatment and control groups. From one point of view, if $\prob{P}_C^{\RV{Y}|\RV{D}}$ is causally contractible, then causal contractibility holds whether the choice $\alpha$ yields a deterministic or nondeterministic distribution over $\RV{D}$. If the same model $\prob{P}_C$ is used to design a `trial' (say, for the first $n$ $(\RV{D}_i,\RV{Y}_i)$ pairs) and to select optimal actions for the pairs after this, there is no obvious reason to choose the first $n$ actions according to a random number generator -- one may be able to get better coverage of the actions of interest by choosing deterministically. Indeed, as \citet{kasy_why_2016} points out, if random choices are mixtures of deterministic choices, then the random choices can be at best as good as some deterministic choice (in his paper, Kasy considers situations with covariates, which we ignore for simplicity). 

However, a typical trial involves an experimenter conducting the trial (which comprises the procedure for ``the first $n$ pairs'') and publishing a paper summarising their findings. A reader may then read the paper and use the results to make decisions (which comprises the procedure for ``the pairs after the first $n$''). It is the reader who is in the position of needing a model $\prob{P}_C$ accounting for both the experimental data and the data that arises as a consequence of her actions. In this case, the reader is in a position somewhat like the ``analyst'' in Example \ref{ex:confounding}, while the experimenter is the ``administrator'' from this example. Fixing the proportion of treatments in each class (like in Example \ref{ex:confounding}) leaves the experimenter with many degrees of freedom to select deterministic sequences of actions. On the other hand, if the experimenter may only select a target proportion of treatments, and must then assign treatments randomly given this target, they have much less freedom to influence the consequences of the experiment. An experimenter with many degrees of freedom might well be trusted to make choices in a manner that is transparent to any readers, but this does require an additional assumption on the part of the readers.