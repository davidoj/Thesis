%!TEX root = ../main.tex

\chapter[Repeatable decision problems]{Models of repeatable decision problems}\label{ch:evaluating_decisions}

Chapter \ref{ch:tech_prereq} introduced probability sets as generic tools for causal modelling, while Chapter \ref{ch:2p_statmodels} examined how probability set models can be used in decision problems and Section \ref{sec:cons_to_sdp} in particular introduced \emph{see-do} models, which featured four variables representing observations, consequences, choices and hypotheses. A decision maker wants to pick choices that promote desirable consequences, and in this chapter, we investigate how they can use observations to inform their views of which choices go with which consequences.

A distinguishing feature of decision making is the need to compare the consequences of multiple different options (see Section \ref{sec:assumptions}). In the setup we consider in this work, a decision maker is interested in making a single choice. After making their choice, they can observe the consequences of the option they chose, but they can only speculate about the consequences of all of the other options they had available. Thus, instead of observing an entire (stochastic) response function that maps choices to consequences, which is what they used to make the decision, they only observe the function's output for the particular choice they made. For example, if the decision maker is a person considering taking a medicine to help with a headache they can either take the medicine, in which case they never find out what would have happened if they didn't take it, or they could avoid the medicine and never learn the consequences of taking it.

A simple case to consider where the decision maker can learn a function mapping their choices to consequences is when they face a choice that is, in the appropriate sense, repeated. Specifically, we suppose that there is a fixed response function that maps ``inputs'' to ``outputs'' and the decision maker has multiple opportunities to pick an input and observe the outputs. In this case, the decision maker can learn the entire function by trying each possible input a number of times. If the person previously discussed frequently has headaches, then whenever they have a headache they might sometimes take the medicine and sometimes avoid it. Under the assumption that the function that maps inputs (medicine) to outputs (headaches) is repeated, they can infer that the way their headaches respond to medicine in the future will be the same as the way they have responded to it in the past.

However, it's not entirely clear what this assumption -- that the decision maker's choice determines inputs to a fixed response function -- actually means. The headache-prone individual cannot examine the source code of the universe and find that some fixed function is called every time they have a headache and take (or avoid) medication for it. Existing causal inference frameworks rely on some version of this assumption to licence inference from a sequence of observations to the consequences of an action. The possibility to identify causal effects given blocked backdoor paths in the structural intervention framework \citep[Ch. ~1]{pearl_causality:_2009} depends on the fact that the distribution of the effect variable conditional on the cause and the variables blocking the backdoor paths is unchanged after intervention. The assumption of conditional ignorability \citep{rubin_causal_2005} similarly implies that the distribution of the output conditional on the input and the covariates does not depend on which counterfactual is ``chosen''. This point is further discussed in Chapter \ref{ch:other_causal_frameworks}.

In comparison with the IID assumption, the assumption of repeated response functions is less often appropriate. Consider our headache-prone individual once more. For argument's sake, they might reasonably assume that their daily history of medication and subsequent headaches can be modeled by an IID sequence of pairs of variables $(\RV{X}_i,\RV{Y}_i)$ where $\RV{X}_i$ represents whether they took medication on day $i$ and $\RV{Y}_i$ the severity of their subsequent headache. The distribution of this sequence $\prob{P}$ will also induce a conditional distribution $\prob{P}^{\RV{Y}_1|\RV{X}_1}$ which is a response map from medication to outcomes. However, this is \emph{not} the stochastic map that this individual should use to help them make a decision today. Supposing that in the past they only took the medication when they had a headache, then most of the days on which they took no medication were days on which they had no headache to begin with; in this case, the conditional distribution $\prob{P}^{\RV{Y}_1|\RV{X}_1}$ may well indicate that they are much more likely to have headaches on days where they took the medication, even if the medication is in reality quite effective at relieving their pain.

This story describes a standard case of confounding -- this person's experience of headaches after taking medication is confounded by whether or not they had a headache before taking it. Confounding is ubiquitous in situations in which people are trying to use data to inform choices, and is one of the major reasons for the aphorism ``correlation does not imply causation''.

In summary, the assumption of repeated response functions is often a critical assumption for causal inference (like the assumption of IID variables in classical statistics). However, it is hard to make a positive case for this assumption and in many common cases it is clearly violated. The purpose of this chapter is to explore this issue in detail: when, and to what degree is the assumption at least reasonable, even if not justified in any absolute sense. In this chapter, we present results that facilitate an alternative interpretation of this assumption, which (to some extent) addresses the second point -- that it's not clear exactly what justifies the assumption of repeated response functions -- though rather than offering new practical justifications for the assumption of repeated response functions, the perspective we offer mainly reinforces the widely held view that this assumption is mostly inappropriate\footnote{It's very easy to find statements like ``this assumption seems unreasonably strong, but we have to make it if we want to work anything out'' -- see, for example, \citet[~pg. 11]{saarela_role_2020}, \citet[~pg. 579]{hernan_estimating_2006} or \citet[~pg. 40]{pearl_causality:_2009}. One can also find many criticisms of inappropriate use of this assumption, see for example \citet{muller_causal_2015} or \citet{berk_what_2010}.}.

What we show is analogous to a well-known result of Bruno De Finetti for conditionally independent and identically distributed sequences of observations. De Finetti considered models where observations were given by repetitions of identical but unknown probability distributions, where we consider input-output pairs given by repetitions of identical but unknown stochastic functions. De Finetti showed that this structural assumption was equivalent to an assumption that the measurement procedure obeyed a certain symmetry. In particular, the assumption of conditionally independent and identically distributed sequences was appropriate precisely when the measurement procedure in question was, for the purposes of modelling, identical to any measurement procedure that proceeded in the same fashion but permuted the indices to which each observation was assigned\footnote{Note that this result does not apply in a non-Bayesian setting where we use sets of probability distributions rather than a single probability distribution to model observations. In this setting, the symmetry over measurement procedures described here does not imply the structural results of independent and identically distributed variables, see \citet[pg. 463]{walley_statistical_1991}. We consider the ``Bayesian'' setting here of a single stochastic function because it is simpler.}.

In this chapter, we examine symmetries of this sort. The key equivalence we show can be roughly stated in the following form: given a model of a sequence of input-output pairs, these pairs can be related by repeated response functions if and only if the distribution of finite sequences of outputs conditioned on a corresponding sequence of inputs \emph{and} an infinite history of other input-output pairs is unchanged under arbitrary permutations.

The motivation for deriving this result was, in part, to consider alternative justifications for the assumption of repeated response functions. However, this result is, in our view, mostly negative. Our result implies, for example:
\begin{itemize}
    \item Suppose we have sequence of input-output pairs from a well-conducted experiment and a similar sequence from passive observation, and want to predict a held-out experimental output; the assumption of repeatable response functions implies that the experimental and observational data are interchangeable for this purpose 
    \item Suppose we have sequence of input-output pairs from a well-conducted experiment, and are interested in predicting the consequences of our own plans under consideration; the assumption of repeatable response functions implies that this problem is essentially the same as predicting held-out experimental outputs
\end{itemize}

In many situations, we expect that both of these implications are not acceptable. In fact, little domain expertise seems to be required to recognise that the different problems discussed are \emph{not} essentially the same. Whether the experiment is testing medical treatments, educational interventions or software modifications -- in all of these circumstances, one doesn't need deep domain knowledge to know that data generated in different contexts is usually not interchangeable.

This is not to say that the assumption of repeated response functions is never acceptable, but that the required symmetry places some strict limits on the cases when it is. In this chapter we use the example of a ``multi-armed bandit'', where the assumption is justified by the fact that the experiment is repeatedly interacting with a machine that is known to implement a fixed input-output function. A/B testing, where a developer randomly chooses which version of a page is served to users for some time, and deterministically picks the best page thereafter plausibly satisfies the second symmetry above -- serving page version ``B'' because you've decided it's the best and serving page version ``B'' because you're continuing the experiment do seem like two situations that call for the same predictive model (at least, if there is good reason to neglect potential interactions between versions that load at different times).

Thus, the major practical conclusion we draw from these results is that the assumption of repeated response functions is usually too strong, at least when applied to observed variables. We do want to use data to help make decisions, so we're motivated to find assumptions that allow us to do this that are weaker than that of repeated response functions. In Chapter \ref{ch:other_causal_frameworks} we present two existing solutions to this problem, as well as introducing the weaker assumption of \emph{precedented responses}.

This chapter also has a preliminary investigation into repeated response functions in the case of data-dependent models, where inputs are allows to depend arbitrarily on any of the previous inputs and outputs. This is a generalisation of the standard causal inference setting where actions taken and consequences experienced after the data is reviewed do not appear in the model. In this setting, we consider \emph{probability combs} which are a kind of generalised conditional probability introduced by \citet{chiribella_quantum_2008} and applied to causal models by \citet{jacobs_causal_2019}. We show that data-dependent models with repeated stochastic functions feature probability comb symmetries. 

\subsection{Chapter outline}

This chapter introduces sequences of \emph{conditionally independent and identical response functions} (CIIR sequences), a precise term for what we refer to above as ``repeated response functions''. The key theorem in this chapter, Theorem \ref{th:ciid_rep_kernel}, relates the assumption of conditionally independent and identical response functions to a kind of symmetry which we call \emph{IO contractibility}. A model with data-independent actions features conditionally independent and identical response functions if and only if it is IO contractible. Theorem \ref{th:response_is_cc_hdep} introduces a more general notion of IO contractibility and relaxes the data-independent assumption, but comes with some different side conditions.

Section \ref{sec:prev_work} surveys previous work, particularly related to symmetries of causal models. Section \ref{sec:response_functions} defines and explains the idea of conditionally independent and identical response functions. Section \ref{sec:ccontracibility} defines IO contractibility, as well as setting out key definitions, lemmas and the proof of Theorem \ref{th:ciid_rep_kernel}. Section \ref{sec:symmetries_discussion} presents a collection of examples that illustrate various features of models that are (or are not) IO contractible. Section \ref{sec:data_dependent} extends the work from Section \ref{sec:ccontracibility} to models where inputs can be data-dependent. The extension is dense and retreads a lot of ground already covered in a slightly different way, but Section \ref{sec:def_combs} introduces the notion of a comb, which is an extension of a conditional probability, that has applications in areas of causal inference beyond what is covered in this chapter, and this subsection stands on its own. Finally, some concluding remarks are in Section \ref{sec:discussion}.


\section{Previous work on causal symmetries}\label{sec:prev_work}

\cite{de_finetti_foresight_1992} introduced two key ideas to probability modelling: first, he established an equivalence between exchangeable sequences and conditionally independent and identically distributed sequences, and secondly he proposed that we can deduce symmetries of probability models from informal idea that measurement procedures differing only by label permutations are essentially identical. De Finetti's technical result has been extended in many ways, including to finite sequences \citep{kerns_definettis_2006,diaconis_finite_1980} and for partially exchangeable arrays \citep{aldous_representations_1981}. A comprehensive overview of results is presented in \citet{kallenberg_probabilistic_2005}. A result from classical statistics that is particularly similar to the result presented in this chapter is the notion of ``partial exchangeability'' from \citet{diaconis_recent_1988}.

The application of similar ideas to causal models has received some attention, though comparatively little in comparison. \citet{lindley_role_1981} discussed models consisting of a sequence of exchangeable observations along with ``one more observation'', a structure that is similar to the models with observations and consequences discussed in section \ref{pgph:two_kinds}. Lindley discussed the application of this model to questions of causation, but did not explore this deeply due to the perceived difficulty of finding a satisfactory definition of causation. \citet{rubin_causal_2005}'s overview of causal inference with potential outcomes along with the text \citet{imbens_causal_2015} made use of the assumption of exchangeable potential outcomes to prove several identification results. \citet{saarela_role_2020}, used structural causal models to propose \emph{conditional exchangeability}, defined as the exchangeability of the non-intervened causal parents of a target variable under intervention on some of its parents. Sareela et. al. suggested that this could be interpreted as a symmetry of an experiment involving administering treatments to patients with respect to exchanging the patients in the experiment. In fact, many authors have posited causal notions of exchangeability that involve swapping people or experimental units involved in an experiment: \citet{hernan_estimating_2006,hernan_beyond_2012,greenland_identifiability_1986,banerjee_chapter_2017,dawid_decision-theoretic_2020} all discuss assumptions of this type. Section \ref{sec:ilevel_ccontract} discusses the notion of ``swapping individuals'' in more detail.

A stronger symmetry assumption than commutativity of exchange, which is comparable to the symmetries discussed above, is the assumption of \emph{IO contractibility} (Definition \ref{def:caus_cont}), which adds the assumption of \emph{locality}. This additional assumption has similarities to the stable unit treatment distribution assumption (SUTDA) in \citet{dawid_decision-theoretic_2020}, and the stable unit treatment value assumption (SUTVA) in \citep{rubin_causal_2005}:
\begin{blockquote}
(SUTVA) comprises two sub-assumptions. First, it assumes that \emph{there is no interference between units (Cox 1958)}; that is, neither $Y_i(1)$ nor $Y_i(0)$ is affected by what action any other unit received. Second, it assumes that \emph{there are no hidden versions of treatments}; no matter how unit $i$ received treatment $1$, the outcome that would be observed would be $Y_i(1)$ and similarly for treatment $0$.
\end{blockquote}

There are two subtle caveats to existing causal treatments of symmetries. First, the kind of symmetry originally considered by De Finetti and by subsequent work in classical statistics involved probability models that were unchanged under permutations of a sequence of random variables (or under some other transformation). By contrast, the causal treatments usually consider models where the ``true'' interventional distributions (for example, \citet{saarela_role_2020}) or the ``true'' conditional distributions of potential outcomes (for example, \citet{hernan_estimating_2006}) are unchanged under some transformation. As we clarify in this chapter, this amounts to the claim that the \emph{in the limit of infinite conditioning data}, the distribution of an output conditional on an input is unchanged by the transformations in question. Other features of the model might be substantially changed by the transformation.

The second subtle point is the nature of the transformations themselves. The kind of tranformation envisioned in De Finetti's original result is of the following form: suppose you conduct some measurement procedure and write down the results in a table of values. These results are bound to random variables on the basis of their position in the table. Consider an alternative measurement procedure: we do exactly as before, but write the same numbers in different positions in the table. We are asked to accept that, if we have a good probability model of the first measurement procedure, and this model is unchanged by permutation of the random variables, then it is also a good probability model for the second procedure. This seems pretty plausible -- intuitively, permuting a sequence of random variables seems to accomplish the same thing as permuting the measurement results that these random variables bind to -- and its plausibility doesn't depend on the details of the measurement procedure in question.

On the other hand, the kind of transformation envisioned in causal versions of exchangeability are of the following nature: suppose you conduct a medical trial that involves administering treatment to a number of patients, and withholding treatment from a number of other patients. Now, consider an alternative procedure: first, you shuffle some patients between the ``treatment administered'' group and the ``withheld'' group, then you proceed as before. First, this is not a generic setup! Not all decision problems involve patients that can be shuffled. Secondly, it is not altogether clear that this transformation of the measurement procedure corresponds to a permutation of random variables. Here, we are not merely changing the order in which results are written into a table at the end of the experiment, but altering a seemingly more substantive aspect of the manner in which the experiment is carried out.

In this chapter we discuss \emph{commutativity of exchange}, which is a symmetry of a conditional distribution to permutations of pairs of random variables. This can be understood in terms of the first kind of measurement procedure transformation: in particular, that the appropriate conditional distribution to model the procedure is unchanged by changing the order in which pairs of measurement results are written down. In the following chapter, Section \ref{sec:ilevel_ccontract}, we consider modelling transformations like ``shuffling patients in a medical experiment''.

\section[Response functions]{Conditionally independent and identical response functions}\label{sec:response_functions}

Suppose a decision maker is implementing a decision procedure where they'll make a choice and subsequently receive a sequence of paired values $(\proc{D}_i,\proc{Y}_i)$, with their objective depending on the output values yielded by $\proc{Y}_i$s only. Usually the $\proc{D}_i$s, which we call ``inputs'', are under the decision maker's control to some extent, but this might not always be the case. For example, perhaps the first $m$ pairs come from data collected by someone else, where the decision maker has no control over inputs, and the next $n$ depend on their own actions where they have complete control over the inputs.

Suppose the decision maker uses a probability set $\prob{P}_C$ to model such a procedure, and variables $(\RV{D}_i,\RV{Y}_i)$ are associated with the inputs and outputs. There are two different relationships between $\RV{D}_i$ and $\RV{Y}_i$ that might be of interest to the decision maker:
\begin{itemize}
    \item For some choice $\alpha$, $j>m$ and some fixed value of $\RV{D}_j$, what are the \emph{likely consequences} with regard to $\RV{Y}_j$?
    \item For some choice $\alpha$, all $i\leq m$ with some fixed value of $\RV{D}_i$, what is the \emph{relative frequency} of different values of $\RV{Y}_i$?
\end{itemize}
The first is what the decision maker wants to know in order to make a good decision, and the second is something they can learn from the data before taking any actions. In particular, if the decision maker has a good reason to think that the two relationships should be (approximately) the same \emph{and} be independent of the decision maker's overall choice $\text{id}_C$, then they may reduce the overall problem of choosing $\text{id}_C$ to the problem of influencing the inputs under their control $\RV{D}_j$ for $j>m$ toward values that have been associated to with favourable consequences according to the past data.

The conditional independence of consequence $\RV{Y}_i$ from the choice $\text{id}_C$ given the input $\RV{D}_i$ is important for this reduction; otherwise the decision maker needs to consider how $\RV{Y}_i$ depends on $\text{id}_C$ as well as $\RV{D}_i$. However, this independence is not required for the results in this chapter, and so we do not assume it. More generally, the results presented here do not show any particular method is appropriate for making decisions, and additional assumptions may be needed for that purpose.

In this chapter, we are interested in models $\prob{P}_C$ where the probabilistic relationship between each $\RV{D}_i$ and the corresponding $\RV{Y}_i$ is unknown but identical for all indices $i$. To model this, we introduce a hypothesis $\RV{H}$ that represents this unknown relationship, and assert that the distribution of $\RV{Y}_i$ given $(\RV{D}_i,\RV{H})$ is identical for all $i$, independent of all data prior to $i$.

\begin{definition}[Conditionally independent and identical response functions]\label{def:cii_rf}
A probability set $\prob{P}_C$ on $(\Omega,\sigalg{F})$ with variables $\RV{Y}:=(\RV{Y}_i)_{i\in \mathbb{N}}$ and $\RV{D}:=(\RV{D}_i)_{i\in\mathbb{N}}$ has \emph{independent and identical response functions conditional on} $\RV{H}$ if for all $i$, $\RV{Y}_i\CI^e_{\prob{P}_C} (\RV{D}_{[1,i)},\RV{Y}_{[1,i)})|(\RV{H},\text{id}_C)$ and $\prob{P}_\alpha^{\RV{Y}_i|\RV{D}_i\RV{H}}=\prob{P}_\alpha^{\RV{Y}_j|\RV{D}_j\RV{H}}$ for all $i,j$.
\end{definition}

We only require outputs $\RV{Y}_i$ to be independent of \emph{previous} inputs and outputs, conditional on $\RV{H}$ and $\RV{D}_i$. If $\RV{D}_i$ is selected based on previous data, then in general there may be relationships between $\RV{D}_j$ and $\RV{Y}_i$ for $j>i$ even after conditioning on $\RV{D}_i$ and $\RV{H}$ (e.g. $\RV{D}_j$ is chosen deterministically equal to $\RV{Y}_i$ for some $j>i$). However, for much of this chapter, we will focus on the simpler case where inputs are \emph{weakly data-independent}, which means that conditional on $\RV{H}$, the $\RV{Y}_i$ are also independent of future inputs. This allows for a kind of ``pseudo-dependence'' on past data, where inputs may be chosen as if an oracle told the decision maker the value of the usually unknown response function $\RV{H}$, but not further depending on any particular previous data values. We explore relaxing this assumption in Section \ref{sec:data_dependent}, although this work is only preliminary.

We show that for weakly data-independent models with conditionally independent and identical response functions, there is some variable $\RV{W}$ such that the conditional probability $\prob{P}_C^{\RV{Y}|\RV{WD}}$ is IO contractible. On the other hand, for data-dependent models, we instead require the \emph{comb} (Section \ref{sec:def_combs}) $\prob{P}_C^{\RV{Y}\combbreak \RV{D}|\RV{W}}$ IO contractible for some $\RV{W}$.

\section[Symmetries]{Symmetries of sequential conditional probabilities}\label{sec:ccontracibility}

In this section we define key technical terms, including symmetries of conditional probabilities, and prove the technical results IO contractibility and eventually prove the key theorems \ref{th:ciid_rep_kernel} and \ref{th:infinite_condition_swaps}.

We introduce two basic symmetries: \emph{exchange commutativity} and \emph{locality}. The first says that permutations of a sequence of input-output pairs leaves a conditional probability unchanged, while the second says that the probability of an output does not depend on the value of any non-corresponding inputs. Note that the dependence that is ruled out by locality may be ``physical'' -- for example, herd immunity makes each person's likelihood of infection depend on the vaccination/recovery status of the rest of the population -- or merely ``epistemic'', where, for example, many people choosing to eat at one restaurant instead of a neighbouring one is evidence that the first serves better food that can be obtained without ever sampling the food from either.

The assumptions of exchange commutativity and locality together make input-output contractibility, or IO contractibility for short. IO contractibility is equivalent to the condition that the conditional probabilities of every equally sized subsequence are equal.

Graphical notation can offer an intuitive picture of these two assumptions. In the simplified case of a sequence of length 2 (that is, $\kernel{K}:X^2\kto Y^2$), exchange commutativity for two inputs and outputs is given by the following equality:
\begin{align}
    \tikzfig{commutativity_of_exchange}
\end{align}
swapping the inputs is equivalent to applying the same swap to the outputs. Locality is given by the following pair of equalities:
\begin{align}
    \tikzfig{cons_locality_1}\\
    \tikzfig{cons_locality_2}
\end{align}
and expresses the idea that the outputs are independent of the non-corresponding input, conditional on the corresponding input.

The definitions follow.

Call a model $\prob{P}_C$ with sequential outputs $\RV{Y}$ and a corresponding sequence of inputs $\RV{D}$ a ``sequential input-output model''.

\begin{definition}[Sequential input-output model]\label{def:seq_io}
A \emph{sequential input-output model} is a triple $((\prob{P}_\cdot,(\Omega,\sigalg{F}),(C,\sigalg{C})),\RV{D},\RV{Y})$ where $(\prob{P}_\cdot,(\Omega,\sigalg{F}),(C,\sigalg{C}))$ is a decision model, $\RV{D}$ is a sequence of ``inputs'' $\RV{D}:=(\RV{D}_i)_{i\in\mathbb{N}}$ and $\RV{Y}$ is a corresponding sequence of ``outputs'' $\RV{Y}=(\RV{Y}_i)_{i\in\mathbb{N}}$ where $\RV{D}_i:\Omega\to D$ and $\RV{Y}_i:\Omega\to Y$.
\end{definition}

\begin{notation}
We use the shorthand $(\prob{P}_C,\RV{D},\RV{Y})$ to refer to a sequential input-output model $((\prob{P}_\cdot,(\Omega,\sigalg{F}),(C,\sigalg{C})),\RV{D},\RV{Y})$, with $(\Omega,\sigalg{F})$ implicit.
\end{notation}

Locality holds with respect to some auxiliary variable $\RV{W}$ when an output $i$ is independent of future inputs, conditioned on the corresponding input $i$ and $\RV{W}$.

\begin{definition}[Locality]\label{def:caus_cont}
Given a sequential input-output model $(\prob{P}_C,\RV{D},\RV{Y})$ along with some $\RV{W}:\Omega\to W$, for $\alpha\in C$ we say $\prob{P}_\alpha^{\RV{Y}|\RV{WD}}$ is \emph{local} over $\RV{W}$ if for all $\alpha\in C$, $n\in \mathbb{N}$
\begin{align}
    \tikzfig{local_lhs} &= \tikzfig{local_rhs}\\
    &\iff\\
    \prob{P}_\alpha^{\RV{Y}|\RV{WD}}(\bigtimes_{i\in [n]} A_i\times Y^{\mathbb{N}}|w, x_{[n]},x_{[n]^{\complement}}) &= \prob{P}_C^{\RV{Y}_{[n]}|\RV{WD}_{[n]}}(\bigtimes_{i\in [n]} A_i|w,x_{[n]}) \\&  \forall A_i\in \sigalg{Y},(x_{[n]},x_{[n]^{\complement}})\in\mathbb{N},w\in W
\end{align}
That is, $\RV{Y}_i\CI^e_{\prob{P}_C} \RV{X}_{(i,\infty)}|(\RV{W},\RV{X}_i,\text{id}_C)$.
\end{definition}

Exchange commutativity holds with respect to some auxiliary variable $\RV{W}$ when swapping input, output pairs doesn't alter the conditional distribution of outputs given inputs. 

\begin{notation}
Given a sequence $\RV{Y}:=(\RV{Y}_i)_{i\in \mathbb{N}}$ and a permutation $\rho:\mathbb{N}\to\mathbb{N}$, the permuted sequence $\RV{Y}_{\rho}$ is defined to be $(\RV{Y}_{\rho(i)})_{i\in\mathbb{N}}$.
\end{notation}

\begin{definition}[Exchange commutativity]\label{def:caus_exch}
Given a sequential input-output model $(\prob{P}_C,\RV{D},\RV{Y})$ along with some $\RV{W}:\Omega\to W$, $\alpha\in C$ we say $\prob{P}_\alpha^{\RV{Y}|\RV{WD}}$ \emph{commutes with exchange} over $\RV{W}$ if for all finite permutations $\rho:\mathbb{N}\to\mathbb{N}$
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_\rho|\RV{WD}_\rho} &=  \prob{P}_\alpha^{\RV{Y}|\RV{WD}}
\end{align}
\end{definition}

IO contractibility is the conjunction of both previous assumptions.

\begin{definition}[IO contractibility]\label{def:ccontract}
Given a sequential input-output model $(\prob{P}_C,\RV{D},\RV{Y})$ along with some $\RV{W}:\Omega\to W$, $\prob{P}_\alpha^{\RV{Y}|\RV{WD}}$ is \emph{IO contractible} over $\RV{W}$ if it is local and commutes with exchange.
\end{definition}

If $\prob{P}_\alpha^{\RV{Y}|\RV{WD}}$ IO contractible over $\RV{W}$, then for any subsequences $A,B\subset\mathbb{N}$ with $|A|=|B|$, we have $\prob{P}_\alpha^{\RV{Y}_A|\RV{WD}_A}=\prob{P}_\alpha^{\RV{Y}_B|\RV{WD}_B}$. In fact, Theorem \ref{th:equal_of_condits} shows a stronger condition: if we take $\prob{P}_\alpha^{\RV{Y}|\RV{WD}}$ and multiply it by $\text{del}_{A^{\complement}}$ which erases all the indices in $A^{\complement}$, we get the same result as multiplying $\prob{P}_\alpha^{\RV{Y}|\RV{WD}}$ by $\text{del}_{B^{\complement}}$. The fact that arbitrary contractions of the sequence yield the same result motivates the name \emph{IO contractibility}. 

\begin{theorem}[Equality of equally sized contractions]\label{th:equal_of_condits}
Given a sequential input-output model $(\prob{P}_C,\RV{D},\RV{Y})$ and some $\RV{W}$, $\prob{P}_\alpha^{\RV{Y}|\RV{WD}}$ is IO contractible over $\RV{W}$ if and only if for all subsequences $A,B\subset \mathbb{N}^{|A|}$ and for every $\alpha$
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_A|\RV{WD}_{A,\mathbb{N}\setminus A}} &= \prob{P}_\alpha^{\RV{Y}_B|\RV{WD}_{B,\mathbb{N}\setminus B}}\\
    &= \prob{P}_\alpha^{\RV{Y}_A|\RV{WD}_A}\otimes \text{del}_{D^{|\mathbb{N}\setminus A|}}
\end{align}
\end{theorem}

\begin{proof}
Appendix \ref{sec:io_contract_proof}
\end{proof}

Theorem \ref{th:no_implication} shows that neither locality nor exchange commutativity is implied by the other.

\begin{theorem}\label{th:no_implication}
Exchange commutativity does not imply locality or vice versa.
\end{theorem}

\begin{proof}
We prove the claim by way of presenting counterexamples.

First, a model that exhibits exchange commutativity but not locality. Suppose $D=Y=\{0,1\}$ and $\prob{P}_C^{\RV{Y}|\RV{D}}:D^{\mathbb{N}}\kto Y^{\mathbb{N}}$ is given by
\begin{align}
    \prob{P}_C^{\RV{Y}|\RV{D}}(\bigtimes_{i\in\mathbb{N}} A_i |(d_i)_{i\in\mathbb{N}}) &= \prod_{i\in \mathbb{N}} \delta_{\lim_{n\to\infty} \sum_{i\in\mathbb{N}} \frac{d_i}{n}}(A_i)
\end{align}
for some sequence $(d_i)_{i\in\mathbb{N}}$ such that this limit exists. Then for any finite permutation $\rho$
\begin{align}
    \prob{P}_C^{\RV{Y}_\rho|\RV{D}_\rho}(\bigtimes_{i\in\mathbb{N}} A_i |(d_i)_{i\in\mathbb{N}}) &= \prod_{i\in \mathbb{N}} \delta_{\lim_{n\to\infty} \sum_{i\in\mathbb{N}} \frac{d_{\rho^{-1}(i)}}{n}}(A_{\rho^{-1}(i)})\\
    &= \prob{P}_C^{\RV{Y}|\RV{D}}(\bigtimes_{i\in\mathbb{N}} A_i |(d_i)_{i\in\mathbb{N}})
\end{align}
so $(\prob{P}_C,\RV{D},\RV{Y})$ commutes with exchange, but
\begin{align}
    \prob{P}_C^{\RV{Y}_1|\RV{D}}(A_1 |0,1,1,1....) &= \delta_1(A_1)\\
    \prob{P}_C^{\RV{Y}_1|\RV{D}}(A_1 |0,0,0,0....) &= \delta_0(A_1)
\end{align}
so $(\prob{P}_C,\RV{D},\RV{Y})$ is not local.

Next, a model that satisfies locality but does not commute with exchange. Suppose again $D=Y=\{0,1\}$ and $\prob{P}_C^{\RV{Y}|\RV{D}}:D^{\mathbb{N}}\kto Y^{\mathbb{N}}$ is given by
\begin{align}
    \prob{P}_C^{\RV{Y}|\RV{D}}(\bigtimes_{i\in\mathbb{N}} A_i |(d_i)_{i\in\mathbb{N}}) &= \prod_{i\in \mathbb{N}} \delta_i(A_i)
\end{align}
then
\begin{align}
    \prob{P}_C^{\RV{Y}_\rho|\RV{D}_\rho}(\bigtimes_{i\in\mathbb{N}} A_i |(d_i)_{i\in\mathbb{N}}) &= \prod_{i\in \mathbb{N}} \delta_i(A_{\rho^{-1}(i)})\\
    &\neq \prod_{i\in \mathbb{N}} \delta_i(A_{i})\\
    =\prob{P}_C^{\RV{Y}|\RV{D}}(\bigtimes_{i\in\mathbb{N}} A_i |(d_i)_{i\in\mathbb{N}})
\end{align}
so $(\prob{P}_C,\RV{D},\RV{Y})$ does not commute with exchange but for all $n$
\begin{align}
    \prob{P}_C^{\RV{Y}_{[n]}|\RV{D}}(\bigtimes_{i\in[n]} A_i |(d_i)_{i\in\mathbb{N}}) &= \prod_{i\in [n]} \delta_i(A_{\rho^{-1}(i)})\\
    &= \prob{P}_C^{\RV{Y}_{[n]}|\RV{D}}(\bigtimes_{i\in[n]} A_i |(0)_{i\in\mathbb{N}})
\end{align}
so $(\prob{P}_C,\RV{D},\RV{Y})$ is local.
\end{proof}

Theorem \ref{th:no_implication} presents abstract counterexamples to show that the assumptions of exchange commutativity and locality are independent. For some more practical examples, a model of the treatment of several patients who are known to have different illnesses might satisfy consequence locality but not exchange commutativity. Patient B's treatment can be assumed not to affect patient A, but the same results would not be expected from giving patient A's treatment to patient B as from giving patient A's treatment to patient A.

A model of strategic behaviour could satisfy exchange commutativity but not locality. Suppose a decision maker is observing people playing a game where they press a red or green button, and (for reasons mysterious to the decision maker), receive a payout randomly of 0 or \$100. The decision maker might reason that the results should be the same no matter who presses a button, but also that people will be more likely to press the red button if the red button tends to give a higher payout. In this case, the decision maker's prediction for the payout of the $i$th attempt given the red button has been pressed will be higher if the proportion of red button presses in the entire dataset is higher. There are other reasons why exchange commutativity might hold but not locality -- \citet{dawid_causal_2000} offers the alternative example of herd immunity in vaccination campaigns. In this case, the overall proportion of the population vaccinated will affect the disease prevalence over and above an individual's vaccination status.

Although locality could be described as an assumption that there is no interference between inputs and outputs of different indices, it actually allows for some models with certain kinds of interference between inputs and non-corresponding outputs. For example: consider an experiment where I first flip a coin and record the results of this flip as the outcome $\RV{Y}_1$ of ``step 1''. Subsequently, I can either copy the outcome from step 1 to the result for ``step 2'' (this is the input $\RV{D}_1=0$), or flip a second coin use this as the input for step 2 (this is the input $\RV{D}_1=1$). $\RV{D}_2$ is an arbitrary single-valued variable. Then for all $d_1, d_2$
\begin{align}
    \prob{P}^{\RV{Y}_1|\RV{D}}(y_1|d_1,d_2) &= 0.5\\
    \prob{P}^{\RV{Y}_2|\RV{D}}(y_2|d_1,d_2) &= 0.5
\end{align}
Thus the marginal distribution of both experiments in isolation is $\text{Bernoulli}(0.5)$ no matter what choices I make, but the input $\RV{D}_1$ affects the joint distribution of the results of both steps, which is not ruled out by locality.

\subsection[Representation of IO contractible models]{Representation of IO contractible models}\label{sec:rep_theorem}

In this section, we prove the main Theorem \ref{th:ciid_rep_kernel}, which shows that a sequence of inputs and corresponding outputs features conditionally independent and identical responses if and only if the conditional distribution of the outputs given the inputs is IO contractible over some variable $\RV{W}$.

We make use of a number of concepts in the following work: models with sequences of inputs and outputs, ``tabulated'' representations of conditional probabilities and ``hypotheses'' or ``directing measures'' defined as the limit of relative frequencies. These are all defined below.

\begin{definition}[Count of input values]\label{def:count_of_inputs}
Given a sequential input-output model $(\prob{P}_C,\RV{D},\RV{Y})$ on $(\Omega,\sigalg{F})$ with countable $D$, $\#_{j}^k$ is the variable
\begin{align}
    \#_{j}^k := \sum_{i=1}^{k-1} \llbracket \RV{D}_i = j \rrbracket
\end{align}
In particular, $\#_{j}^k$ is equal to the number of times $\RV{D}_i=j$ over all $i<k$.
\end{definition}

\begin{definition}[Tabulated conditional distribution]\label{def:tab_cd}
Given a sequential input-output model $(\prob{P}_C,\RV{D},\RV{Y})$ on $(\Omega,\sigalg{F})$, define the tabulated conditional distribution $\RV{Y}^D:\Omega\to Y^{\mathbb{N}\times D}$ by
\begin{align}
    \RV{Y}^D_{ij} = \sum_{k=1}^{\infty} \llbracket \#_j^k = i-1\rrbracket \llbracket \RV{D}_k = j \rrbracket \RV{Y}_k
\end{align}
That is, the $(i,j)$-th coordinate of $\RV{Y}^D(\omega)$ is equal to the coordinate $\RV{Y}_k(\omega)$ for which the corresponding $\RV{D}_k(\omega)$ is the $i$th instance of the value $j$ in the sequence $(\RV{D}_1(\omega),\RV{D}_2(\omega),...)$, or 0 if there are fewer than $i$ instances of $j$ in this sequence.
\end{definition}

\begin{definition}[Measurable set of probability distributions]
Given a measurable set $(\Omega,\sigalg{F})$, the measurable set of distributions on $\Omega$, $\mathcal{M}_1(\Omega)$, is the set of all probability distributions on $\Omega$ equipped with the coarsest $\sigma$-algebra such that the evaluation maps $\eta_B:\nu\mapsto \nu(B)$ are measurable for all $B\in \sigalg{F}$.
\end{definition}

We define the \emph{directing random measure} of a sequence of variables as the map from a set to the limit of normalised partial sums of indicator functions over that set, where that limit exists. We refer to directing random measures with the letter $\RV{H}$ by default, and treat it like a hypothesis -- under appropriate conditions, the directing random measure $\RV{H}$ is almost surely equal to the distribution of any variable in the sequence conditional on $\RV{H}$. We also define $\RV{H}$ in the case that the relevant limit does not exist for completeness, although we are only interested in cases where the limit does exist. Definition \ref{def:dir_rand_meas} reduces to the definition of a directing random measure given in \citet{kallenberg_basic_2005} when we consider a probability space instead of a probability set.

\begin{definition}[Directing random measure]\label{def:dir_rand_meas}
Given a probability set $(\prob{P}_C,\Omega,\sigalg{F})$ and a sequence $\RV{X}:=(\RV{X}_i)_{i\in\mathbb{N}}$, the directing random measure of $\RV{X}$ written $\RV{H}:\Omega\to \mathcal{M}_1(X)$ is the function
\begin{align}
    \RV{H} := A \mapsto \begin{cases}
    \lim_{n\to \infty}\frac{1}{n} \sum_{i=1}^{\infty} \mathds{1}_{A}(\RV{X}_{i}) & \text{this limit exists for all }\alpha\in C\\
    \llbracket A = X \rrbracket &\text{otherwise}
    \end{cases} 
\end{align}
\end{definition}

Given two variable sequences $(\RV{D},\RV{Y})$, which we call the inputs and outputs respectively, we define the \emph{directing random conditional} as the directing random measure of the ``tabulated conditional'' $\RV{Y}^D$, interpreted as a sequence of column vectors $((\RV{Y}^D_{1j})_{j\in D},(\RV{Y}^D_{2j})_{j\in D},...)$.

\begin{definition}[Directing random conditional]\label{def:dir_rand_cond}
Given a sequential input-output model $(\prob{P}_C,\RV{D},\RV{Y})$, we will say the directing random measure $\RV{H}:\Omega\to \mathcal{M}_1(Y^D)$ is the function
\begin{align}
    \RV{H} := \bigtimes_{j\in D} A_j \mapsto \begin{cases}
    \lim_{n\to \infty}\frac{1}{n} \sum_{i=1}^{\infty} \prod_{j\in D} \mathds{1}_{A_j}(\RV{Y}^D_{ij}) & \text{this limit exists}\\
    \llbracket \bigtimes_{j\in D} A_j = Y^D \rrbracket &\text{otherwise}
    \end{cases} 
\end{align}
\end{definition}

We say a model satisfies data-independence when future inputs are independent of outputs conditional on past inputs and the directing measure $\RV{H}$. This makes the analysis much easier, but it rules out the analysis of decision rules that depend in some non-symmetric way on the input-output sequence (or on finite subsequences).

\begin{definition}[Data-independent]\label{def:weak_di}
A sequential input-output model $(\prob{P}_C,\RV{D},\RV{Y})$ is weakly data-independent if $\RV{Y}_i\CI^e_{\prob{P}_C} \RV{D}_{(i,\infty]}|(\RV{H},\RV{D}_{[1,i]},\text{id}_C)$.
\end{definition}

A finite permutation of rows is a function that independently permutes a finite number of elements in each row of a table. A special case of such a function is one that swaps entire columns (that is, a permutation of rows that applies the same permutation to each row).

\begin{definition}[Permutation of rows]
Given a sequence of indices $(i,j)_{i\in \mathbb{N},j\in D}$ a finite permutation of rows is a function $\eta:\mathbb{N}\times D\to \mathbb{N}\times D$ such that for each $j\in D$, $\eta_j:=\eta(\cdot,j)$ is a finite permutation $\mathbb{N}\to \mathbb{N}$ and $\eta(i,j)=(\eta_j(i),j)$.
\end{definition}

Lemma \ref{th:table_rep_kernel} shows that an IO contractible conditional distribution can be represented as the product of a column exchangeable probability distribution and a ``lookup function'' or ``switch''. This lookup function is also used in the representation of potential outcomes models (see, for example, \citet{rubin_causal_2005}), but interpreting such a model as potential outcomes requires additional assumptions we don't make here. By representing a conditional probability as an exchangeable regular probability distribution, we can apply De Finetti's, which is a key step in proving the main result of Theorem \ref{th:ciid_rep_kernel}.

To prove Lemma \ref{th:table_rep_kernel}, we assume that the set of input sequences in which each value appears infinitely often has measure 1 for every option in $C$. Without this assumption, the tabulated conditional $\RV{Y}^D$ cannot be a function of the inputs $\RV{D}$ and outputs $\RV{Y}$ as there would be some values of the inputs which would not be seen often enough. Without this assumption, $\RV{Y}^D$ may not be uniquely defined. We call this side condition \emph{infinite support}.

\begin{definition}[Infinite support]
Given a sequential input-output model $(\prob{P}_C,\RV{D},\RV{Y})$ with $D$ countable if, letting $E\subset D^{\mathbb{N}}$ be the set of all sequences for which each $j\in D$ occurs infinitely often, $\prob{P}_\alpha^{\RV{D}|\RV{W}}(E|w)=1$ for all $\alpha,w$, then we say $\RV{D}$ is \emph{infinitely supported over }$\RV{W}$.
\end{definition}


\begin{lemma}\label{th:table_rep_kernel}
Suppose a sequential input-output model $(\prob{P}_C,\RV{D},\RV{Y})$ is given with $D$ countable and $\RV{D}$ infinitely supported over $\RV{W}$. Then for some $\RV{W}$, $\alpha$, $\prob{P}_\alpha^{\RV{Y}|\RV{WD}}$ is IO contractible if and only if
\begin{align}
    \prob{P}_\alpha^{\RV{Y}|\RV{WD}} &= \tikzfig{lookup_representation_kernel}\label{eq:lup_rep_kernel}\\
    &\iff\\
    \prob{P}_\alpha^{\RV{Y}|\RV{WD}}(\bigtimes_{i\in \mathbb{N}}A_i|w,(d_i)_{i\in \mathbb{N}}) &= \prob{P}_\alpha^{(\RV{Y}^D_{i d_i})_{i\in\mathbb{N}}|\RV{W}}(\bigtimes_{i\in \mathbb{N}}A_i|w)&\forall A_i\in \sigalg{Y}^{D}, w\in W, d_i\in D
\end{align}
Where $\prob{F}_{\text{lu}}$ is the Markov kernel associated with the lookup map
\begin{align}
    \text{lu}:X^\mathbb{N}\times Y^{\mathbb{N}\times D}&\to Y\\
    ((x_i)_\mathbb{N},(y_{ij})_{i,j\in \mathbb{N}\times D})&\mapsto (y_{i d_i})_{i\in \mathbb{N}}
\end{align}
and for any finite permutation of rows $\eta:\mathbb{N}\times D\to \mathbb{N}\times D$
\begin{align}
    \prob{P}_\alpha^{(\RV{Y}^D_{ij})_{\mathbb{N}\times D}|\RV{W}}&= \prob{P}_\alpha^{(\RV{Y}^D_{\eta(i,j)})_{\mathbb{N}\times D}|\RV{W}}\label{eq:col_exch}
\end{align}
\end{lemma}

\begin{proof}
Only if: We define a random invertible function $\RV{R}:\Omega\times \mathbb{N}\to \mathbb{N}\times {D}$ that reorders the indicies so that, for $i\in \mathbb{N},j\in D$, $\RV{D}_{\RV{R}^{-1}(i,j)}=j$ almost surely. We then use IO contractibility to show that $\prob{P}_\alpha^{\RV{Y}|\RV{D}}(\cdot|d)$ is equal to the distribution of the elements of $\RV{Y}^D$ selected according to $d\in D^{\mathbb{N}}$.

If: We construct a conditional probability according to Definition \ref{def:tab_cd} and verify that it satisfies IO contractibility.

The full proof can be found in Appendix \ref{sec:io_contract_models}.
\end{proof}

As a consequence of Lemma \ref{th:table_rep_kernel} along with De Finetti's representation theorem, we can say that given $(\prob{P}_C,\RV{D},\RV{Y})$ IO contractible, conditioning on $\RV{H}$ renders the columns of $\RV{Y}^D$ independent and identically distributed.

\begin{lemma}\label{lem:ciid_yd}
Suppose a sequential input-output model $(\prob{P}_C,\RV{D},\RV{Y})$ is given with $D$ countable, $\RV{D}$ infinitely supported over $\RV{W}$ and for some $\RV{W}$, $\prob{P}_\alpha^{\RV{Y}|\RV{WD}}$ is IO contractible for all $\alpha$. Then, letting $\RV{H}$ be the directing random conditional of $(\prob{P}_C,\RV{D},\RV{Y})$ (Definition \ref{def:dir_rand_cond}) and $\RV{Y}^D_{iD}:=(\RV{Y}^D_{ij})_{j\in D}$, we have for all $i\in\mathbb{N}$, $\RV{Y}^D_{iD}\CI^e_{\prob{P}_C} (\RV{Y}^D_{\mathbb{N}\setminus\{i\}D},\RV{W}) | (\RV{H},\text{id}_C)$ and $\prob{P}_\alpha^{\RV{Y}^D_{iD}} = \prob{P}_\alpha^{\RV{Y}^D_{kD}}$ and
\begin{align}
    \prob{P}_\alpha^{\RV{Y}^D_{iD}|\RV{H}}(A|\nu) \overset{\prob{P}_\alpha}{\cong} \nu(A)
\end{align}
\end{lemma}

\begin{proof}
This follows directly from applying De Finetti's representation theorem to $\RV{Y}^D$, see Appendix \ref{sec:io_contract_models}.
\end{proof}

If the conditions of Lemma \ref{th:table_rep_kernel} are satisfied, we do not need the full sequence of pairs $(\RV{D},\RV{Y})$ to calculate $\RV{H}$; any subsequence $A\subset\mathbb{N}$ that satisfies the condition that $\RV{D}_A$ is infinitely supported over $\RV{W}$ is sufficient.

\begin{theorem}\label{th:any_infinite_sequence}
Suppose a sequential input-output model $(\prob{P}_C,\RV{D},\RV{Y})$ is given with $D$ countable,  $\RV{D}$ infinitely supported over $\RV{W}$ and for some $\RV{W}$, $\prob{P}_\alpha^{\RV{Y}|\RV{WD}}$ is IO contractible for all $\alpha$. Consider an infinite set $A\subset \mathbb{N}$, and let $\RV{D}_A:=(\RV{D}_i)_{i\in A}$ and $\RV{Y}_A:=(\RV{Y}_i)_{i\in A}$ such that $\RV{D}_A$ is also infinitely supported over $\RV{W}$. Then $\RV{H}_A$, the directing random conditional of $(\prob{P}_C,\RV{D}_A,\RV{Y}_A)$ is almost surely equal to $\RV{H}$, the directing random conditional of $(\prob{P}_C,\RV{D},\RV{Y})$.
\end{theorem}

\begin{proof}
The strategy we pursue is to show that an arbitrary subsequence of $(\RV{D}_i,\RV{Y}_i)$ pairs induces a random contraction of the rows of $\RV{Y}^D$. Then we show that the contracted version of $\RV{Y}^D$ has the same distribution as the original, and consequently the normalised partial sums converge to the same limit.

The proof is in Appendix \ref{sec:io_contract_models}.
\end{proof}

The following is a technical lemma that will be used in Theorem \ref{th:ciid_rep_kernel}.

\begin{lemma}\label{lem:hw_interchange}
Suppose a sequential input-output model $(\prob{P}_C,\RV{D},\RV{Y})$ is given with $D$ countable, $\RV{D}$ infinitely supported over $\RV{W}$, for some $\RV{W}$, $\prob{P}_\alpha^{\RV{Y}|\RV{WD}}$ is IO contractible for all $\alpha$ and for all $\alpha$
\begin{align}
    \prob{P}_\alpha^{\RV{Y}|\RV{WD}} &= \tikzfig{lookup_representation_kernel}\label{eq:lup_rep_kernel_2}
\end{align}
then $\RV{Y}\CI^e_{\prob{P}_C} \RV{W} | (\RV{H},\RV{D},\text{id}_C)$ and for all $\alpha$
\begin{align}
    \prob{P}_{\alpha}^{\RV{Y}|\RV{HD}} &= \tikzfig{lookup_representation_kernel_h}
\end{align}
\end{lemma}

\begin{proof}
We show that the function that maps the variables $\RV{Y}$ and $\RV{D}$ to $\RV{H}$ also maps $\RV{Y}^D$ and the constant $e\in D^{\mathbb{N}}$ to $\RV{H}'$ with $\RV{H}'\overset{\prob{P}_C}{\cong} \RV{H}$, and the result follows from disintegration along with a conditional independence given by Lemma \ref{th:table_rep_kernel}.

The full proof is in Appendix \ref{sec:ciid_rep_proof}.
\end{proof}

Theorem \ref{th:ciid_rep_kernel} is the main result of this section. It shows that sequential input-output model $(\prob{P}_C,\RV{D},\RV{Y})$ is IO contractible over some $\RV{W}$ if and only if there is some hypothesis $\RV{H}$ such that the $\RV{Y}_i$s are related to the $\RV{D}_i$s by conditionally independent and identical response functions (subject to a support assumption).

In the following theorem, property (2) is equivalent to the conjunction of conditionally independent and identical response functions (Def \ref{def:cii_rf}) and weak data-independence (Def \ref{def:weak_di}).

\begin{theorem}[Representation of IO contractible models]\label{th:ciid_rep_kernel}
Suppose a sequential input-output model $(\prob{P}_C,\RV{D},\RV{Y})$ with sample space $(\Omega,\sigalg{F})$ is given with $D$ countable and $\RV{D}$ infinitely supported over $\RV{W}$. Then the following are equivalent:
\begin{enumerate}
    \item There is some $\RV{W}$ such that $\prob{P}_\alpha^{\RV{Y}|\RV{WD}}$ is IO contractible for all $\alpha$
    \item For all $i$, $\RV{Y}_i\CI^e_{\prob{P}_C} (\RV{Y}_{\neq i},\RV{D}_{\neq i},\text{id}_C)|(\RV{H},\RV{D}_i)$ and for all $i,j, \alpha$ $$\prob{P}_\alpha^{\RV{Y}_i|\RV{H}\RV{D}_i}=\prob{P}_\alpha^{\RV{Y}_j|\RV{H}\RV{D}_j}$$
    \item There is some $\kernel{L}:H\times X\kto Y$ such that for all $\alpha$, $$\prob{P}_\alpha^{\RV{Y}|\RV{HD}}= \tikzfig{do_model_representation_conditional}$$
\end{enumerate}
\end{theorem}

\begin{proof}
(1)$\implies$(3):
We apply Lemma \ref{th:table_rep_kernel} followed by Lemma \ref{lem:ciid_yd} followed by Lemma \ref{lem:hw_interchange}.


(3)$\implies$ (2):
We verify that the required conditional independences hold assuming (3).

(2)$\implies$ (1):
We show that, assuming (2), then $\prob{P}_\alpha^{\RV{Y}|\RV{WD}}$ is IO contractible over $\RV{W}$ for all $\alpha$.

See Appendix \ref{sec:data_independent_proofs} for the full proof.
\end{proof}

As a consequence of Theorem \ref{th:ciid_rep_kernel}, if a sequence of input and output pairs features independent and identical responses conditional on some arbitrary variable, then we can without loss of generality consider the conditioning variable to be the directing random conditional defined over the same sequence of input-output pairs.

\begin{corollary}\label{lem:ci_drc}
If a sequential input-output model $(\prob{P}_C,\RV{D},\RV{Y})$ has independent and identical response functions conditional on some variable $\RV{G}$ and $\RV{D}$ has infinite support over $\RV{W}$, then letting $\RV{H}$ be the directing random conditional with respect to inputs $\RV{D}$ and outputs $\RV{Y}$, it follows that for for all $i$, $\RV{Y}_i\CI^e_{\prob{P}_C} \RV{G}|(\RV{D}_i,\RV{H},\text{id}_C)$ and for all $\alpha, i, j$, $\prob{P}_\alpha^{\RV{Y}_i|\RV{D}_i\RV{H}}=\prob{P}_\alpha^{\RV{Y}_j|\RV{D}_j\RV{H}}$.
\end{corollary}

\begin{proof}
We have $\prob{P}_\alpha^{\RV{Y}|\RV{G}\RV{D}}$ is IO contractible over $\RV{G}$. The conclusion follows by applying Theorem \ref{th:ciid_rep_kernel}.
\end{proof}

\subsection[Symmetries of CIIR sequences]{Symmetries of sequences with conditionally independent and identical responses}\label{sec:data_independent_actions}

Theorem \ref{th:ciid_rep_kernel} says that a data independent sequential input-output model $(\prob{P}_C,\RV{D},\RV{Y})$ features conditionally independent and identical response functions $\prob{P}_\alpha^{\RV{Y}_i|\RV{HD}_i}$ for all $\alpha$ if and only if there is some $\RV{W}$ such that $\prob{P}_\alpha^{\RV{Y}|\RV{WD}}$ is IO contractible over $\RV{W}$ for all $\alpha$. The variable $\RV{W}$ is something of a nuisance; rather than thinking only about whether IO contractibility holds, we must consider whether there's \emph{any} variable that licenses the assumption of IO contractibility.

A simple special case to consider is when $\RV{W}$ is single valued -- that is, when $\prob{P}_\alpha^{\RV{Y}|\RV{D}}$ is IO contractible. As Theorem \ref{th:data_ind_CC} shows, this corresponds to the CIIR sequence models where the inputs $\RV{D}$ are unconditionally data-independent and independent of the hypothesis $\RV{H}$. We can also consider the case where $(\prob{P}_C, \RV{D},\RV{Y})$ is only exchange commutative over $*$. This corresponds to models where the inputs $\RV{D}$ are data-independent and the hypothesis $\RV{H}$ depends on a symmetric function of the inputs $\RV{D}$ (under some side conditions).

More generally, we can observe that by Lemma \ref{lem:ci_drc}, any sequence of inputs and outputs with conditionally independent and identical responses with infinitely supported inputs must be causally contractible over $\RV{H}$. Furthermore, by Theorem \ref{th:any_infinite_sequence}, we can take $\RV{H}$ to be a function of any sequence of inputs and outputs for which the inputs have infinite support. Together, these imply IO contractibility over arbitrary subsequences with infinite support. This observation is the basis of Theorem \ref{th:infinite_condition_swaps}. Applying Theorem \ref{th:equal_of_condits} allows us to state this implication in a manner that we feel is more intuitive: informally speaking, if we want to predict any output from its corresponding input and some infinitely supported subsequence of data, then it doesn't matter which output we pick or which subsequence of data we pick, the problem remains essentially the same in every case.

\begin{theorem}[Data-independent IO contractibility]\label{th:data_ind_CC}
Suppose a sequential input-output model $(\prob{P}_C,\RV{D},\RV{Y})$ with sample space $(\Omega,\sigalg{F})$ is given with $D$ countable and, letting $E\subset D^{\mathbb{N}}$ be the set of all sequences for which each $j\in D$ occurs infinitely often, $\prob{P}_\alpha^{\RV{D}}(E)=1$ for all $\alpha$. Then the following are equivalent:
\begin{enumerate}
    \item $\prob{P}_\alpha^{\RV{Y}|\RV{D}}$ is IO contractible for all $\alpha$
    \item For all $i$, $\RV{Y}_i\CI^e_{\prob{P}_C} (\RV{Y}_{\neq i},\RV{D}_{\neq i},\text{id}_C)|(\RV{H},\RV{D}_i)$, for all $i,j,\alpha$ $$\prob{P}_\alpha^{\RV{Y}_i|\RV{H}\RV{D}_i}=\prob{P}_\alpha^{\RV{Y}_j|\RV{H}\RV{D}_j}$$, $\RV{H}\CI^e_{\prob{P}_C} \RV{D}|\text{id}_C$ and for all $i$ $\RV{D}_i\CI^e_{\prob{P}_C} \RV{D}_{(i,\infty]}) | (\RV{D}_{[1,i)},\text{id}_C)$
    \item There is some $\kernel{L}:H\times X\kto Y$ such that for all $\alpha$, $$\prob{P}_\alpha^{\RV{YH}|\RV{D}}= \tikzfig{do_model_representation_with_h}$$
\end{enumerate}
\end{theorem}

\begin{proof}
See Appendix \ref{sec:data_independent_proofs}.
\end{proof}

While $\prob{P}_C^{\RV{Y}|\RV{D}}$ exchange commutative is not necessarily IO contractible, exchange commutativity of this conditional implies IO contractibility over the directing random conditional $\RV{H}$, and thus is sufficient for conditionally independent and identical responses.

\begin{theorem}\label{lem:exch_prod_ciid}
If $\prob{P}_C^{\RV{Y}|\RV{D}}$ is exchange commutative, and for each $\alpha$ $\prob{P}_\alpha^{\RV{D}}$ is absolutely continuous with respect to some exchangeable distribution $\prob{Q}_\alpha^{\RV{D}}$ in $\Delta(D^{\mathbb{N}})$ with directing random measure $\RV{F}$ and $\RV{D}$ infinitely supported over $\RV{F}$ with respect to $\prob{Q}_\alpha$ , then $\prob{P}_\alpha^{\RV{Y}|\RV{HD}}$ is IO contractible, where $\RV{H}$ is the directing random conditional for $\prob{P}_\alpha^{\RV{Y}|\RV{D}}$.
\end{theorem}

\begin{proof}
We show that there is an exchangeable distribution for which the relevant conditional automatically satisfies IO contractibility and is almost surely equal to $\prob{P}_\alpha^{\RV{Y}|\RV{GD}}$ for some $\RV{G}$.
\end{proof}

\begin{corollary}\label{th:ciid_rep_kernel_nolocal}
If $(\prob{P}_C,\RV{D},\RV{Y})$ is exchange commutative over $*$, and for each $\alpha$ $\prob{P}_\alpha^{\RV{D}}$ is absolutely continuous with respect to some exchangeable distribution in $\Delta(D^{\mathbb{N}})$ then
\begin{align}
    \prob{P}_\alpha^{\RV{Y}|\RV{HD}}= \tikzfig{do_model_representation_conditional}
\end{align}
\end{corollary}

\begin{proof}
By Theorem \ref{lem:exch_prod_ciid}, $\prob{P}_\alpha^{\RV{Y}|\RV{WD}}$ is IO contractible over some $\RV{W}$ for all $\alpha$, so the result follows immediately from Theorem \ref{th:ciid_rep_kernel}.
\end{proof}

Theorem \ref{th:infinite_condition_swaps} shows that the interchangeability of infinitely supported subsequences of data is a necessary condition for CIIR sequences. We only show sufficiency under the assumption that the input-output sequence is exchangeably dominated, and it is an open question if sufficiency holds in the general case.

\begin{theorem}\label{th:infinite_condition_swaps}
A data-independent sequential input-output model $(\prob{P}_C,\RV{D},\RV{Y})$ features conditionally independent and identical response functions $\prob{P}_\alpha^{\RV{Y}_i|\RV{D}_i\RV{G}}$ with $\RV{D}$ infinitely supported over $\RV{G}$ only if for any sets $A,B\subset \mathbb{N}$ such that $\RV{D}_A$ and $\RV{D}_B$ are also infinitely supported over $\RV{G}$ and any $i,j\in \mathbb{N}$ such that $i\not\in A$, $j\not\in B$, $$\prob{P}_\alpha^{\RV{Y}_i|\RV{D}_i\RV{Y}_A,\RV{D}_A}=\prob{P}_\alpha^{\RV{Y}_j|\RV{D}_j|RV{Y}_B\RV{D}_B}$$.  If in addition each $\prob{P}_\alpha^{\RV{YD}}$ is dominated by some $\prob{Q}_\alpha$ such that $\prob{Q}_\alpha^{\RV{Y}\RV{D}}$ is exchangeable, then the reverse implication also holds.
\end{theorem}

\begin{proof}
See Appendix \ref{sec:data_independent_proofs}.
\end{proof}

\section[Discussion]{Discussion}\label{sec:symmetries_discussion}

\subsection{Simple symmetries vs strategic behaviour}

The previous section established a number of symmetries of input-output models that either imply, or are equivalent to (under some side conditions) conditionally independent and identical responses. Theorem \ref{th:ciid_rep_kernel} shows that for weakly data-independent models, conditionally independent and identical responses is equivalent to IO contractibility over the directing random conditional $\RV{H}$. Where $\prob{P}_\alpha^{\RV{YD}}$ is dominated by an exchangeable measure for every $\alpha$, Theorem \ref{th:infinite_condition_swaps} establishes the alternative condition that, loosely speaking, the conditional distribution of any output given the corresponding input and an infinite sequence of additional input-output pairs is identical.

These general results both establish a symmetry of the conditional distribution of outputs after conditioning on some ``long run limit'' (either $\RV{H}$ or an infinite sequence of input-output pairs). This makes the results less tidy than the classic result for conditionally independent and identically distributed sequences, which required only that the distribution of the sequence be symmetric to permutation, and no conditioning on long-run limits.

We can consider simpler versions of exchange commutativity or IO contractibility that omit the conditioning on the long-run limit. This is where we have exchange commutativity (or IO contractibility) over the trivial variable $\RV{W}=*$ in Definitions \ref{def:caus_exch} and \ref{def:ccontract} respectively. Theorem \ref{th:ciid_rep_kernel} and Corollary \ref{th:ciid_rep_kernel_nolocal} respectively establish that these simpler symmetries are sufficient for conditionally independent and identical responses. We present a few examples to show that these simpler symmetries are not necessary for this property, however. The basic idea in these examples is that, even with conditionally independent and identical responses, inputs could be chosen strategically and different inputs could be chosen according to different strategies.

\paragraph{Example 1: purely passive observation}\label{pgph:passive_strategic}

Purely passive observations can be modeled with a single-element probability set $\prob{P}_C$ where $|\prob{P}_C|=1$. In this case, a model that is exchangeable over the sequence of pairs $\RV{YD}:=(\RV{D}_i,\RV{Y}_i)_{i\in \mathbb{N}}$ has $(\prob{P}_C, \RV{D},\RV{Y})$ exchange commutative over $*$. This follows from the fact that
\begin{align}{}
    \prob{P}_C^{\RV{YD}} &= \prob{P}_C^{(\RV{Y}\RV{D})_\rho}\\
    \implies \prob{P}_C^{\RV{Y}|\RV{D}} &= \prob{P}_C^{\RV{Y}_\rho|\RV{D}_\rho}
\end{align}
thus by Corollary \ref{th:ciid_rep_kernel_nolocal}, $(\prob{P}_C, \RV{D},\RV{Y})$ features conditionally independent and identical response functions. Note that $\prob{P}_C^{\RV{Y}|\RV{D}}$ is not necessarily IO contractible. Suppose there is a machine with two arms $D=\{0,1\}$, one of which pays out \$100 and the other that pays out nothing. A decision maker (DM) doesn't know which is which, but the DM watches a sequence of people operate the machine who almost all do know which one is good. The DM is sure that they all want the money, and that they will pull the good arm $1-\epsilon$ of the time independent of every other trial. Set the hypotheses $\RV{H}$ to ``0 is good'' and ``1 is good'' (which we'll just refer to as $\{0,1\}$), with 50\% probability on each initially. Then
\begin{align}
    \prob{P}_C^{\RV{Y}_2|\RV{D}_2}(100|1) &= \sum_{0,1} \prob{P}_C^{\RV{Y}_2|\RV{D}_2\RV{H}}(100|1,0)\prob{P}_C^{\RV{H}|\RV{D}_2}(0|1) + \prob{P}_C^{\RV{Y}_2|\RV{D}_2\RV{H}}(100|1,1)\prob{P}_C^{\RV{H}|\RV{D}_2}(1|1)\\
    &= 1-\epsilon
\end{align}
but
\begin{align}
    \prob{P}_C^{\RV{Y}_2|\RV{D}_1\RV{D}_2}(100|0,1) &= \sum_{0,1} \prob{P}_C^{\RV{Y}_2|\RV{D}_1\RV{D}_2\RV{H}}(100|0,1,0)\prob{P}_C^{\RV{H}|\RV{D}_1\RV{D}_2}(0|0,1) + \prob{P}_C^{\RV{Y}_2|\RV{D}_1\RV{D}_2\RV{H}}(100|0,1,1)\prob{P}_C^{\RV{H}|\RV{D}_1\RV{D}_2}(1|0,1)\\
    &= 0.5
\end{align}

\paragraph{Example 2: all inputs chosen by the decision maker}

Consider the previous example, except instead of watching knowledgeable operators, the DM will pull each lever themselves, and they will decide in advance on the sequence of pulls. We suppose that the DM's model reflects precisely their knowledge of $\RV{H}$ when the choose the sequence $\RV{D}$, and so $\RV{H}$ has no dependence on $\RV{D}$.
\begin{align}
        \prob{P}_C^{\RV{Y}_2|\RV{D}_2}(100|1) &= \sum_{0,1} \prob{P}_C^{\RV{Y}_2|\RV{D}_2\RV{H}}(100|1,0)\prob{P}_C^{\RV{H}}(0) + \prob{P}_C^{\RV{Y}_2|\RV{D}_2\RV{H}}(100|1,1)\prob{P}_C^{\RV{H}}(1)\\
        &= 0.5\\
        \prob{P}_C^{\RV{Y}_2|\RV{D}_1\RV{D}_2}(100|0,1) &= \sum_{0,1} \prob{P}_C^{\RV{Y}_2|\RV{D}_1\RV{D}_2\RV{H}}(100|0,1,0)\prob{P}_C^{\RV{H}}(0) + \prob{P}_C^{\RV{Y}_2|\RV{D}_1\RV{D}_2\RV{H}}(100|0,1,1)\prob{P}_C^{\RV{H}}(1)\\
        &= 0.5
\end{align}
so here the decision maker has adopted a model where $\prob{P}_C^{\RV{Y}|\RV{D}}$ is IO contractible.

\paragraph{Example 3: mixing strategies}\label{pgph:two_kinds}

A decision maker might be in the position of having both observational and experimental data. Modify the machine from the previous example so that the good lever pays out \$100 $0.5+\epsilon$ of the time, and the bad lever pays out $0.5-\epsilon$ of the time and (as before) the DM's prior probability that each lever is the good one is $0.5$. Suppose the DM from the previous examples observes a sequence of strangers operating the machine, the results associated with the sequence of pairs $(\RV{D}_i,\RV{Y}_i)_{i\in\mathbb{N}}$, and also operates the machine themselves according to a plan fixed in advance, the results associated with the sequence of pairs $(\RV{E}_i,\RV{Z}_i)_{i\in \mathbb{N}}$. 

If, in this situation, the DM were to adopt a model $(\prob{P}_C,(\RV{D},\RV{E}),(\RV{Y},\RV{Z}))$ such that $\prob{P}_\alpha^{\RV{YZ}|\RV{DE}}$ is IO contractible over $*$ for all $\alpha$, understanding $(\RV{D},\RV{E},\RV{Y},\RV{Z})$ to be a single sequence of pairs, then Theorem \ref{th:equal_of_condits} implies, for some $n\in\mathbb{N}$ and any choice of actions by the DM $\alpha$,
\begin{align}
    \prob{P}_\alpha^{\RV{Z}_i|\RV{E}_i\RV{D}_{[n]}\RV{Y}_{[n]}} &= \prob{P}_\alpha^{\RV{Y}_i|\RV{D}_i\RV{E}_{[n]}\RV{Z}_{[n]}}
\end{align}
That is, there is a symmetry between predicting the consequences of one of the DM's inputs from the DMs passive observations and predicting the outputs of one of the passive observations from the DM's input-output pairs. However, this might not be appropriate - while the DM is ignorant about which lever is better, the others who operate the machine might not be. If the DM supposes that the strangers are knowledgeable regarding the better lever, then he will take the stranger's having chosen a certain lever as evidence that that lever is the better one, while he will not treat his own choice of lever in the same way. Thus, for example,
\begin{align}
    \prob{P}_\alpha^{\RV{Z}_i|\RV{E}_i\RV{D}_{[2]}\RV{Y}_{[2]}}(100|1,1,1,0,100) > \prob{P}_\alpha^{\RV{Y}_i|\RV{E}_i\RV{E}_{[2]}\RV{Z}_{[2]}}(100|1,1,1,0,100)
\end{align}
In this case, the DMs model is not even exchange commutative over $*$.

\subsection{Implications of IO contractibility}\label{sec:imp_iocont}

Theorem \ref{th:infinite_condition_swaps} establishes a necessary condition for conditionally independent and identical response functions: the conditional distributions of every output given the corresponding input and a suitable infinite sequence of other input-output pairs are identical. The following two examples substantiate the claims made at the beginning of this chapter: that conditionally independent and identical response functions imply, under appropriate conditions, that experimental and observational data is interchangeable and that experimental data predicts the outcomes of a decision maker's choices just as well as it predicts held out experimental outputs.

We are of the view that it is not simply ``hard to know'' when this condition is reasonable -- in fact, it's often easy to know that it is unreasonable. One might respond that we might still accept that the condition is close to holding, and in this case it may often be possible to make good decisions by reasoning as if it holds precisely. However, this begs the question: in what sense is it ``close'' to holding? In other words, if we want to relax this assumption, what do we relax it to?

A key question is thus: how do we formulate weaker assumptions that are more widely acceptable than the assumption of conditionally independent and identical response functions? This is explored in Chapter \ref{ch:other_causal_frameworks}.

\paragraph{Example 4: experimental and observational data}

Suppose we have two sequences of binary pairs $((\RV{D},\RV{X}),\RV{Y}):=((\RV{D}_i,\RV{X}_i),\RV{Y}_i)_{i\in \mathbb{N}}$ the $\RV{D}_i$s represent whether patient $i$ was given a particular medicine. The $\RV{D}_i$s were assigned uniformly according to some source of randomness for even $i\geq 2$, while what exactly determined the $\RV{D}_j$ for odd $j$ is not known and is likely to have involved patient or doctor discretion. The $\RV{X}_i$s are covariates, and the $\RV{Y}_i$s record binarized outcomes of the treatment. $\RV{D}_0$ is up to the decision maker, set deterministically according to $\alpha\in 0,1$. Within both the even and the odd indices of $\RV{D}$ both options are taken infinitely often with probability 1.

According to Theorem \ref{th:infinite_condition_swaps}, the assumption of conditionally independent and identical responses applied to $((\RV{D},\RV{X}),\RV{Y})$ implies
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_0|\RV{D}_0\RV{X}_0\RV{D}_{\text{odds}}\RV{X}_{\text{odds}}\RV{Y}_{\text{odds}}}&=\prob{P}_\alpha^{\RV{Y}_0|\RV{D}_0\RV{D}_{\text{evens}\setminus\{0\}}\RV{X}_{\text{evens}\setminus\{0\}}\RV{Y}_{\text{evens}\setminus\{0\}}}\\
    &=\prob{P}_\alpha^{\RV{Y}_2|\RV{D}_2\RV{X}_2\RV{X}_{\text{evens}\setminus\{0,2\}}\RV{Y}_{\text{evens}\setminus\{0,2\}}}\\
    &=\prob{P}_\alpha^{\RV{Y}_2|\RV{D}_2\RV{X}_2\RV{X}_{\text{odds}}\RV{Y}_{\text{odds}}}
\end{align}

That is, under this assumption, four problems are deemed identical:
\begin{itemize}
    \item Predicting a held-out experimental outcome from the experimental data
    \item Predicting a held-out experimental outcome from the observational data
    \item Predicting the outcome of the decision maker's input from the experimental data
    \item Predicting the outcome of the decision maker's input from the observational data
\end{itemize}

But the proposition that these problems are \emph{identical} is hard to swallow: despite the obvious differences in the procedures used to obtain the various sequences of pairs, such an assumption nevertheless holds that these differences cannot possibly lead to any differences between the problems discussed.

In practice, when both experimental and observational data are available, they are \emph{not} assumed to be interchangeable in this sense -- in fact, the question of how well the observational data predicts experimental outputs is one of substantial interest \citet{eckles_bias_2021,gordon_comparison_2018,gordon_close_2022}.

\paragraph{Example 5: Backdoor adjustment}

The ``backdoor adjustment'' formula is a fundamental tool for many kinds of causal inference. This is a short example to show the conditions under which it's applicable, stated in terms of IO contractibility. Suppose a sequential input-output model $(\prob{P}_C,(\RV{D},\RV{X}),\RV{Y})$ where $(\prob{P}_\cdot^{\RV{Y}|\RV{WDX}}$ is IO contractible, and:
\begin{itemize}
    \item $i>n\implies \RV{X}_{i}\CI^e_{\prob{P}_C}\RV{D}_{i}|(\RV{H},\text{id}_C)$
    \item $\prob{P}_\alpha^{\RV{X}_{i}|\RV{H}}\cong \prob{P}_\alpha^{\RV{X}_{1}|\RV{H}}$ for all $\alpha$
 \end{itemize}
Then the model exhibits a kind of ``backdoor adjustment''. Specifically, for $i>n$
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_{i}|\RV{D}_{i}\RV{H}}(A|d,h) &= \int_X \prob{P}_\alpha^{\RV{Y}_{i}|\RV{X}_{i}\RV{D}_{i}\RV{H}}(A|d,x,h)\prob{P}_\alpha^{\RV{X}_{i}|\RV{D}_{i}\RV{H}}(\mathrm{d}x|d,h)\\
    &= \int_X \prob{P}_\alpha^{\RV{Y}_{1}|\RV{X}_{1}\RV{D}_{1}\RV{H}}(A|d,x,h)\prob{P}_\alpha^{\RV{X}_{i}|\RV{H}}(\mathrm{d}x|h)\\
    &= \int_X \prob{P}_\alpha^{\RV{Y}_{1}|\RV{X}_{1}\RV{D}_{1}\RV{H}}(A|d,x,h)\prob{P}_\alpha^{\RV{X}_{1}|\RV{H}}(\mathrm{d}x|h)\label{eq:backdoor}
\end{align}

Equation \eqref{eq:backdoor} is identical to the backdoor adjustment formula \citep[Chap. 1]{pearl_causality:_2009} for an intervention on $\RV{D}_1$ targeting $\RV{Y}_1$ where $\RV{X}_1$ is a common cause of both.

\subsection[No causes in, no causes out]{Causal assumptions and distinguishing inputs from outputs}\label{sec:nci_nco}

Example 1 above shows that the assumption of a CIIR sequence applies to an exchangeable probability model with no real alternative options or anything else ``causal''. As a result, one might wonder whether the assumption of CIIR sequences really is a ``causal'' assumption (we have no clear idea what does or does not constitute a causal assumption here, we're merely noting that an assumption that applies to regular probability distributions seems like it might not fit the bill). The apparent claim that CIIR sequences nonetheless allow us to draw causal conclusions might them (seemingly) run afoul of Cartwright's adage ``no causes in, no causes out'' \citep{cartwright_no_1994}.

We have also supposed, in the background at least, that a relation between the chosen option $\alpha$ and the distribution of (some) inputs $\prob{P}_\alpha^{\RV{D}}$ was known in advance. This seems much more like a causal assumption -- it really doesn't make sense to try to apply this assumption to a probability model with no option set.

There is no obvious reason why the problem has to be this way, but it is certainly intuitively appealing to think about ``inputs'' that we already know how to control and ``outputs'' that we want to learn how to control from the given data. In fact, it is perhaps this intuitive appeal that led us to name the two sequences ``inputs'' and ``outputs'' respectively.

Given an assumption of this type, there seems to be a distinction we can draw between inputs and outputs beyond an arbitrary convention. In particular, this assumption suggests that that $\RV{D}_i$ is distinguished from $\RV{Y}_i$ by the fact that we know how to control $\RV{D}_i$ \emph{before} we've seen any data, but in the best case we will only know how to control $\RV{Y}_i$ \emph{after} reviewing the data. This distinction does not obviously always correspond to a ``causal'' direction. For example, we might have a clearer idea of how to prompt someone to go and exercise than we have for how to help them feel energetic, yet it's plausible that (under normal circumstances) feeling energetic causes people to engage in exercise.

Nevertheless, prior knowledge of this type suggests that the inputs $\RV{D}_i$ are susceptible to influence by at least one person who doesn't know exactly how the outputs depend on the inputs -- namely, the decision maker. We will investigate in the following chapter a situation where prior knowledge like this (though not this exactly) may facilitate a conclusion of a CIIR input-output from a weaker starting assumption.

\section[Data-dependent inputs]{Conditionally independent and identical response functions with data-dependent inputs}\label{sec:data_dependent}

The results of the previous section concern data independent models; in these models, inputs cannot depend on previous data except via an ``oracle'' on the hypothesis $\RV{H}$. This differs from the ``see-do'' models discussed in the previous chapter, where some inputs can depend in arbitrary ways on the sequence of data preceding them. Intuitively, we might expect that something similar to Theorems \ref{th:ciid_rep_kernel} and \ref{th:infinite_condition_swaps} might hold. However, the situation is complicated by the fact that we can no longer arbitrarily shuffle the order of the input-output sequence; if future inputs depend on the past data, then we could have, for example, $\RV{D}_2$ deterministically equal to $\RV{Y}_1$ which would, in general, may $\RV{Y}_1$ not independent of $\RV{D}_2$ conditional on the hypothesis $\RV{H}$. Theorem \ref{th:ciid_rep_kernel_k} is similar to Theorem \ref{th:ciid_rep_kernel} except it applies to this more general setting of data-dependent inputs. However, it also doesn't lend itself to any easy interpretation that we are aware of. We consider the work in this section to be preliminary.

\begin{example}\label{ex:insertion}
Consider an input-output model $(\prob{P}_C,\RV{D},\RV{Y})$ with $\RV{D}:=(\RV{D}_i)_{i\in\mathbb{N}}$ and $\RV{Y}:=(\RV{Y}_i)_{i\in\mathbb{N}}$ as usual, and take a subsequence $(\RV{D}_i,\RV{Y}_i)_{i\in [2]}$ of length 2. Suppose $\prob{P}_C$ features conditionally independent and identical response functions -- that is, the following holds for some hypothesis $\RV{G}$:
\begin{align}
    \RV{Y}_i&\CI^e_{\prob{P}_C} (\RV{Y}_{<i},\RV{D}_{<i},\text{id}_C)|\RV{G}\RV{D}_i&\forall i\in \mathbb{N}\\
    \land \prob{P}_\alpha^{\RV{Y}_i|\RV{H}\RV{D}_i} &= \prob{P}_\alpha^{\RV{Y}_0|\RV{G}\RV{D}_0} & \forall \alpha\in C, i\in \mathbb{N}
\end{align}

and, for simplicity, assume $\RV{G} \CI^e_{\prob{P}_C} (\RV{D}, \text{id}_C)$ also.

Then, for arbitrary $\alpha\in C$
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_{[2]}} &= \tikzfig{response_conditional_comb}
\end{align}
note that $\RV{D}_2$ depends on $\RV{Y}_1$ and $\RV{D}_1$. $\prob{P}_\alpha^{\RV{D}_2|\RV{Y}_1\RV{D}_1}$ has been ``inserted'' between the response conditionals $\prob{P}_C^{\RV{Y}_1|\RV{D}_1\RV{H}}$ and $\prob{P}_C^{\RV{Y}_2|\RV{D}_2\RV{H}}$.

Given $\prob{P}_C^{\RV{Y}_1|\RV{D}_1\RV{H}}$ and $\prob{P}_C^{\RV{Y}_2|\RV{D}_2\RV{H}}$, define the comb
\begin{align}
    \prob{P}_C^{\RV{Y}_{[2]}\combbreak \RV{D}_{[2]}} := \tikzfig{causally_contractible_comb}
\end{align}
then $\prob{P}_C^{\RV{Y}_{[2]}\combbreak \RV{D}_{[2]}}$ is IO contractible. $\prob{P}_C^{\RV{Y}_{[2]}\combbreak \RV{D}_{[2]}}$ is \emph{not} a uniform conditional probability; in general 
\begin{align}
    \prob{P}_\alpha^{\RV{D}_1\RV{D}_2} \prob{P}_C^{\RV{Y}_{[2]}\combbreak \RV{D}_{[2]}}\neq \prob{P}_\alpha^{\RV{Y}_1\RV{Y}_2}
\end{align}
\end{example}

\subsection{Combs}\label{sec:def_combs}

Combs are a generalisation of conditional distributions that support the ``insert'' operation that appears in Example \ref{ex:insertion}. Speaking very roughly, where conditional distributions are Markov kernels missing a part ``on the left'', combs are Markov kernels that may be missing one or more parts ``in the middle''. If we provide a conditional distribution with its missing part -- that is, if we compute the semidirect product of the conditional distribution and the appropriate marginal -- we get the joint distribution of the variables that appear on the left and right sides of the conditional distribution. Similarly, if we ``insert'' all of the missing parts into a comb, we get the joint distribution of all of the incoming and outgoing variables that appear in the comb.

A graphical depiction of the ``insert'' operation gives some intuition for why it is called ``insert'':
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_{1}\RV{D}_2\RV{Y}_2|\RV{D}_1}&=\text{insert}(\prob{P}_\alpha^{\RV{D}_2|\RV{D}_1\RV{Y}_1},\prob{P}_C^{\RV{Y_{[2]}}\combbreak\RV{D}_{[2]}})\\
    &= \tikzfig{comb_insert_complicated}\label{eq:comb_insert_complicated}\\
    &= \tikzfig{comb_insert_gettingsimpler}\\
    &= \tikzfig{comb_insert_simple}\label{eq:comb_insert_simple}
\end{align}
While Equation \eqref{eq:comb_insert_complicated} is a well-formed string diagram in the category of Markov kernels, Equation \eqref{eq:comb_insert_simple} is not. In the case that all the underlying sets are discrete, Equation \eqref{eq:comb_insert_simple} can be defined using an extended string diagram notation appropriate for the category of real-valued matrices \citep{jacobs_causal_2019}, though we do not introduce this extension here.

Formal definitions of combs follow. As with conditional probabilities, a \emph{uniform} $n$-comb $\prob{P}_C^{\RV{Y}_{[n]}\combbreak \RV{X}_{[n]}}$ is a Markov kernel that satisfies the definition of an $n$-comb for each $\alpha\in C$.

\begin{definition}[$n$-Comb]\label{def:uniform_comb}
Given a probability space $(\prob{P},\Omega,\sigalg{F})$ with variables $\RV{Y}_i:\Omega\to Y$, $\RV{D}_i:\Omega\to D$ for $i\in [n]$ and $\RV{W}:\Omega\to W$, the uniform $n$-comb $\prob{P}^{\RV{Y}_{[n]}\combbreak \RV{D}_{[n]}|\RV{W}}:W\times D^n\kto Y^n$ is the Markov kernel given by the recursive definition
\begin{align}
    \prob{P}^{\RV{Y}_{1}\combbreak \RV{D}_{1}|\RV{W}} &= \prob{P}^{\RV{Y}_1|\RV{D}_1\RV{W}}\\
    \prob{P}^{\RV{Y}_{[m]}\combbreak \RV{D}_{[m]}|\RV{W}} &= \tikzfig{comb_inductive}
\end{align}
\end{definition}

\begin{definition}[$\mathbb{N}$-comb]
Given a probability space $(\prob{P},\Omega,\sigalg{F})$ with variables $\RV{Y}_i:\Omega\to Y$ and $\RV{D}_i:\Omega\to D$, for $i\in \mathbb{N}$ and $\RV{W}:\Omega\to W$, the $\mathbb{N}$-comb $\prob{P}^{\RV{Y}_{\mathbb{N}}\combbreak \RV{D}_{\mathbb{N}}|\RV{W}}:W\times D^\mathbb{N}\kto Y^\mathbb{N}$ is the Markov kernel such that for all $n\in \mathbb{N}$
\begin{align}
    \prob{P}^{\RV{Y}_{\mathbb{N}}\combbreak \RV{D}_{\mathbb{N}}|\RV{W}}[\mathrm{id}_{Y^{n}}\otimes \mathrm{del}_{Y^{\mathbb{N}}}] &= \prob{P}^{\RV{Y}_{[n]}\combbreak \RV{D}_{[n]}|\RV{W}}\otimes \mathrm{del}_{Y^{\mathbb{N}}}
\end{align}
\end{definition}

\begin{theorem}[Existence of $\mathbb{N}$-combs]
Given a probability set $\prob{P}$ with variables $\RV{Y}_i:\Omega\to Y$ and $\RV{D}_i:\Omega\to D$ for $i\in \mathbb{N}$ and $\RV{W}:\Omega\to W$, $D,Y,W$ standard measurable, a uniform $\mathbb{N}$-comb $\prob{P}^{\RV{Y}_{\mathbb{N}}\combbreak \RV{D}_{\mathbb{N}}|\RV{W}}:W\times D^\mathbb{N}\kto Y^\mathbb{N}$ exists.
\end{theorem}

\begin{proof}
For each $n\in \mathbb{N}$ $m<n$, we have
\begin{align}
    \prob{P}^{\RV{Y}_{[n]}\combbreak \RV{D}_{[n]}|\RV{W}}[\mathrm{id}_{Y^{n-m}}\otimes \mathrm{del}_{Y^m}] &= \prob{P}^{\RV{Y}_{[n-m]}\combbreak \RV{D}_{[n-m]}}\otimes \mathrm{del}_{Y^m}
\end{align}
and each $m$ and $n$ comb exists because the requisite conditional probabilities exist. Therefore the existence of $\prob{P}^{\RV{Y}_{\mathbb{N}}\combbreak \RV{D}_{\mathbb{N}}}$ is a consequence of Lemma \ref{lem:infinitely_extended_kernels}.
\end{proof}

For discrete sets, the insert operation has a compact definition:

\begin{definition}[Comb insert - discrete]\label{def:insert_discrete}
Given an $n$-comb $\prob{P}_\alpha^{\RV{Y}_{[n]}\combbreak \RV{D}_{[n]}}$ and an $n-1$ comb $\prob{P}_\alpha^{\RV{D}_{[2,n]}\combbreak \RV{Y}_{[n-1]}|\RV{D}_1}$ with $(D,\sigalg{D})$ and $(Y,\sigalg{Y})$ discrete, for all $y_i\in Y$ and $d_i\in D$
\begin{align}
    &\mathrm{insert}(\prob{P}_\alpha^{\RV{D}_{[2,n]}\combbreak \RV{Y}_{[n-1]}|\RV{D}_1},\prob{P}_\alpha^{\RV{Y}_{[n]}\combbreak \RV{D}_{[n]}})(y_{[n]},d_{[2,n]}|d_1)\\
     &= \prob{P}_\alpha^{\RV{Y}_{[n]}\combbreak \RV{D}_{[n]}}(y_{[n]}|d_{[n]})\prob{P}_\alpha^{\RV{D}_{[2,n]}\combbreak \RV{Y}_{[n-1]}|\RV{D}_1}(d_{[n]}|d_1,y_{[n-1]})
\end{align}
\end{definition}

Inserting a comb into a comb (of appropriate dimensions) yields a conditional probability.

\begin{theorem}
Given an $n$-comb $\prob{P}_\alpha^{\RV{Y}_{[n]}\combbreak \RV{D}_{[n]}}$ and an $n-1$ comb $\prob{P}_\alpha^{\RV{D}_{[2,n]}\combbreak \RV{Y}_{[n-1]}|\RV{D}_1}$, $(D,\sigalg{D})$ and $(Y,\sigalg{Y})$ discrete,
\begin{align}
    &\mathrm{insert}(\prob{P}_\alpha^{\RV{D}_{[2,n]}\combbreak \RV{Y}_{[n-1]}|\RV{D}_1},\prob{P}_\alpha^{\RV{Y}_{[n]}\combbreak \RV{D}_{[n]}})\\
     &= \prob{P}_\alpha^{\RV{Y}_{[n]}\RV{D}_{[2,n]}|\RV{D}_1}
\end{align}
\end{theorem}

\begin{proof}
Take $\RV{Y}_[0]=\RV{D}_{n+1}=*$, and
\begin{align}
    &\mathrm{insert}(\prob{P}_\alpha^{\RV{D}_{[2,n]}\combbreak \RV{Y}_{[n-1]}|\RV{D}_1},\prob{P}_\alpha^{\RV{Y}_{[n]}\combbreak \RV{D}_{[n]}})(y_{[n]},d_{[2,n]}|d_1)\\
     &= \prob{P}_\alpha^{\RV{Y}_{[n]}\combbreak \RV{D}_{[n]}}(y_{[n]}|d_{[n]})\prob{P}_\alpha^{\RV{D}_{[n]}\combbreak\RV{Y}_{[n-1]}|\RV{D}_1}(d_{[2,n]}|d_1,y_{[n-1]})\\
    &= \prod_{i=1}^n \prob{P}_\alpha^{\RV{Y}_{[i]}|\RV{D}_{[i]}\RV{Y}_{[i-1]}}(y_i|d_{[i]},y_{[i-1]})\prob{P}_\alpha^{\RV{D}_{i+1}|\RV{D}_{[i]}\RV{Y}_{[i-1]}}(d_i|d_{[i-1]},y_{[i-1]})\\
    &= \prob{P}^{\RV{Y}_{[n]}\RV{D}_{[2,n]}}(y_{[n]},d_{[n]}|d_1)
\end{align}
    
\end{proof}

\subsubsection{Aside: combs are the output of the ``fix'' operation}

There is a relationship between combs and the ``fix'' operation defined in \citet{richardson_nested_2017}. In particular, suppose we have a probability $\prob{P}_\alpha$ and a comb $\prob{P}_\alpha^{\RV{Y}_{[2]}|\RV{D}_{[2]}}$. Then (assuming discrete sets)
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_{[2]}\combbreak \RV{D}_{[2]}}(y_1,y_2|d_1,d_2) &= \prob{P}_\alpha^{\RV{Y}_1|\RV{D}_1}(y_1|d_1)\prob{P}_\alpha^{\RV{Y}_2|\RV{D}_2}(y_2|d_2)\\
    &= \frac{\prob{P}_\alpha^{\RV{Y}_1|\RV{D}_1}(y_1|d_1)\prob{P}_\alpha^{\RV{D}_2|\RV{Y}_1\RV{D}_1}(d_2|y_1,d_1)\prob{P}_\alpha^{\RV{Y}_2|\RV{D}_2}(y_2|d_2)}{\prob{P}_\alpha^{\RV{D}_2|\RV{Y}_1\RV{D}_1}(d_2|y_1,d_1)}\\
    &= \frac{\prob{P}_\alpha^{\RV{Y}_{[2]}\RV{D}_2|\RV{D}_1}(y_1,y_2,d_2|d_1)}{\prob{P}_\alpha^{\RV{D}_2|\RV{Y}_1\RV{D}_1}(d_2|y_1,d_1)}
\end{align}
That is (at least in this case), the result of ``division by a conditional probability'' used in the fix operation is a comb. We speculate that the output of the fix operation is, in general, an $n$-comb, but we have not proven this.


\subsection[Representation of data-dependent inputs]{Representation of models with data dependent inputs}\label{sec:data_dependent_representation}

If we want to specify a ``see-do'' model where the input $\RV{D}_i$ might depend on inputs and outputs with indices lower than $i$, it might be substantially easier to talk about the comb $\prob{P}_\alpha^{\RV{Y}\combbreak \RV{D}}$ than about the conditional probability $\prob{P}_\alpha^{\RV{Y}|\RV{D}}$. The latter will have to account for possible dependence between outputs $\RV{Y}_i$ and \emph{future} inputs $\RV{D}_j$, which may not be straightforward, while by construction specification of the comb only requires the dependence of $\RV{Y}_i$ on past inputs and outputs.

The definitions of IO contractibility (Section \ref{sec:ccontracibility}) don't apply directly to the case of combs, because (for example) 
\begin{align}
    \text{Swap}_{\rho} \prob{P}_C^{\RV{Y}\combbreak \RV{D}} \text{Swap}_{\rho-1} \neq \prob{P}_C^{\RV{Y}_{\rho}\combbreak \RV{D}_\rho}
\end{align}
 
We can generalise IO contractibility to a notion that applies to generic Markov kernels, and do so in Section \ref{sec:ccontracibile_kernel}. The downside of this is that it's no longer easy to talk about what the transformations mean in terms of equalities of conditional distributions of variables -- nor indeed, in terms of probability comb equalities, because unlike a conditional distribution, the product of a probability comb and a swap map is not necessarily a probability comb itself. In any case, Theorem \ref{th:response_is_cc_hdep} is an analogue of Theorem \ref{th:ciid_rep_kernel} for the case of a data-dependent model. There are two crucial differences between these theorems. First, while Theorem \ref{th:ciid_rep_kernel} constructs the hypothesis $\RV{H}$ as a function of the given variables, Theorem \ref{th:response_is_cc_hdep} extends the sample space to construct the corresponding hypothesis $\RV{G}$. If the ``given variables'' are observable, this means that $\RV{G}$ is not necessarily able to be constructed from observables.

Secondly, Theorem \ref{th:response_is_cc_hdep} considers the only unconditional IO contractibility, without the ``auxiliary'' variable $\RV{W}$. As a result it, it is restricted to the special case where $\RV{G}\CI_{\prob{P}_C} (\RV{X},\text{id}_C)$.

\subsubsection[IO contractible Markov kernels]{IO contractible Markov kernels - definitions and explanation}\label{sec:ccontracibile_kernel}

The following definitions mirror the definitions Section \ref{sec:ccontracibility}, except they are stated in terms of kernel products instead of variables. This is so that they can be applied to combs, instead of limited to conditional probabilities.

\begin{definition}[kernel locality]\label{def:caus_cont_k}
A Markov kernel $\kernel{K}:X^{\mathbb{N}}\kto Y^{\mathbb{N}}$ is \emph{local} if for all $n\in \mathbb{N}$, $A_i\in \sigalg{Y}$, $(x_{[n]},x_{[n]^{\complement}})\in\mathbb{N}$ there exists $\kernel{L}:X^n\kto Y^n$ such that
\begin{align}
    \tikzfig{local_lhs} &= \tikzfig{local_rhs}\\
    &\iff\\
    \kernel{K}(\bigtimes_{i\in [n]} A_i\times Y^{\mathbb{N}}|x_{[n]},x_{[n]^{\complement}}) &= \kernel{L}(\bigtimes_{i\in [n]} A_i|x_{[n]})
\end{align}
\end{definition}

\begin{definition}[kernel exchange commutativity]\label{def:caus_exch_k}
A Markov kernel $\kernel{K}:X^{\mathbb{N}}\kto Y^{\mathbb{N}}$ \emph{commutes with exchange} if for all finite permutations $\rho:\mathbb{N}\to\mathbb{N}$, $A_i\in \sigalg{Y}$, $(x_{[n]},x_{[n]^{\complement}})\in\mathbb{N}$
\begin{align}
    \kernel{K}\mathrm{swap}_{\rho,Y} &=  \mathrm{swap}_{\rho,X} \kernel{K}\\
    &\iff\\
    \kernel{K}(\bigtimes_{i\in\mathbb{N}} A_{\rho(i)}|(x_i)_{i\in {\mathbb{N}}}) &= \kernel{K}(\bigtimes_{i\in\mathbb{N}} A_{i}|(x_{\rho(i)})_{i\in {\mathbb{N}}})
\end{align}
\end{definition}

IO contractibility is the conjunction of both assumptions.
\begin{definition}[kernel IO contractibility]
A Markov kernel $\kernel{K}:X^{\mathbb{N}}\kto Y^{\mathbb{N}}$ is \emph{IO contractible} if it is local and commutes with exchange.
\end{definition}

\subsubsection[Representation of IO contractible kernels]{Representation of IO contractible Markov kernels}

The main theorem is proved in this section. Much of the work parallels work already done in Section \ref{sec:ccontracibility}.

Theorem \ref{th:equal_of_condits_k} is similar to Theorem \ref{th:equal_of_condits}, except it is stated in terms of transformations of a Markov kernel instead of in terms of conditional probabilities of variables.

\begin{definition}[Marginalisation map]
Given a sequence $\RV{Y}:=(\RV{Y}_i)_{i\in\mathbb{N}}$ and $A\subset \mathbb{N}$, $\text{marg}_A$ is the Markov kernel associated with the function $\RV{Y}\mapsto (\RV{Y}_i)_{i\in A}$.
\end{definition}

\begin{theorem}[Equality of equally sized contractions]\label{th:equal_of_condits_k}
A Markov kernel $\kernel{K}:X^{\mathbb{N}}\kto Y^{\mathbb{N}}$ is \emph{IO contractible} if and only if for every $n\in \mathbb{N}$ and every $A\subset\mathbb{N}$ there exists some $\kernel{L}:X^n\kto Y^n$ such that
\begin{align}
    \kernel{K} \text{marg}_A &= \text{swap}_{A\rightarrow [n]} \kernel{L}\otimes \text{del}_{X^{\mathbb{N}}}
\end{align}
\end{theorem}

\begin{proof}
Appendix \ref{sec:data_dep_proofs}
\end{proof}

Lemma \ref{th:table_rep_kernel_k} is similar to Lemma \ref{th:table_rep_kernel}, except the latter uses a variable $\RV{Y}^D$ explicitly defined on the sample space, while Lemma \ref{th:table_rep_kernel_k} simply says an appropriate probability distribution exists, but may not be the distribution of any variable on the given sample space.

\begin{lemma}\label{th:table_rep_kernel_k}
A Markov kernel $\kernel{K}:X^{\mathbb{N}}\kto Y^{\mathbb{N}}$ is IO contractible if and only if there exists a column exchangeable probability distribution $\mu \in \Delta(Y^{|X|\times \mathbb{N}})$ such that
\begin{align}
    \kernel{K} &= \tikzfig{lookup_representation_kernel}\label{eq:lup_rep_kernel_k}\\
    &\iff\\
    \kernel{K}(\bigtimes_{i\in \mathbb{N}} A_i|(x_i)_{i\in \mathbb{N}}) &= \mu (\bigtimes_{i\in \mathbb{N}} Y^{x_i-1}\times A\times Y^{|X|-(x_{i}+1)})\forall A_i\in \sigalg{Y}
\end{align}
Where $\prob{F}_{\text{lu}}$ is the Markov kernel associated with the lookup map
\begin{align}
    \text{lu}:X^\mathbb{N}\times Y^{\mathbb{N}\times D}&\to Y\\
    ((x_i)_\mathbb{N},(y_{ij})_{i,j\in \mathbb{N}\times D})&\mapsto (y_{i d_i})_{i\in \mathbb{N}}
\end{align}
\end{lemma}

\begin{proof}
Appendix \ref{sec:data_dep_proofs}.
\end{proof}


\begin{lemma}[Exchangeable table to response functions]\label{lem:extabl_to_respf_k}
Given $\kernel{K}:X^{\mathbb{N}}\kto Y^{\mathbb{N}}$, $X$ and $Y$ standard measurable, if
\begin{align}
    \kernel{K} &= \tikzfig{lookup_representation_kernel_k}
\end{align}
for $\mu\in \Delta(Y^{X\times\mathbb{N}})$ column exchangeable, then defining $(H,\sigalg{H}):=\mathcal{M}_1(Y^{X\times\mathbb{N}})$ there is some $\RV{H}:Y^{X\times\mathbb{N}}\to H$ and $\kernel{L}:H\times X\kto Y$ such that
\begin{align}
    \kernel{K} &= \tikzfig{do_model_representation_kernel_k}
\end{align}
\end{lemma}

\begin{proof}
Appendix \ref{sec:data_dep_proofs}.
\end{proof}

Theorem \ref{th:ciid_rep_kernel_k} is similar to Theorem \ref{th:ciid_rep_kernel}, but it is stated without the use of variables.  It shows that a IO contractible Markov kernel $X^{\mathbb{N}}\kto Y^{\mathbb{N}}$ is representable as a ``prior'' $\mu\in \Delta(H)$ and a ``parallel product'' of Markov kernels $H\times X\kto Y$, which the response conditionals.

\begin{theorem}\label{th:ciid_rep_kernel_k}
Given a kernel $\kernel{K}:X^{\mathbb{N}}\kto Y^{\mathbb{N}}$, let $(H,\sigalg{H}):=\mathcal{M}_1(Y^X)$ be the set of probability distributions on $(Y^X,\sigalg{Y}^X)$. $\kernel{K}$ is IO contractible if and only if there is some $\mu\in \Delta(H)$ and $\kernel{L}:H\times X\kto Y$ such that 
\begin{align}
    \kernel{K} &= \tikzfig{do_model_representation_kernel}\\
    &\iff\\
    \kernel{K}(\bigtimes_{i\in\mathbb{N}}A_i|(x_i)_{i\in\mathbb{N}}) &= \int_H \prod_{i\in\mathbb{N}} \kernel{L}(A_i|h,x_i)\mu(\mathrm{d}h)
\end{align}
\end{theorem}

\begin{proof}
Appendix \ref{sec:data_dep_proofs}.
\end{proof}

Theorem \ref{th:response_is_cc_hdep} introduces a latent hypothesis variable $\RV{G}$ to allow a statement of Theorem \ref{th:ciid_rep_kernel_k} in terms of conditional probabilities instead of ``anonymous'' Markov kernels.

\begin{theorem}\label{th:response_is_cc_hdep}
Given a sequential input-output model $(\prob{P}_C',\RV{D}',\RV{Y}')$ on $(\Omega,\sigalg{F})$, then $\prob{P}_C^{\prime \RV{Y}'\combbreak \RV{D}'}$ is IO contractible if and only if there is a latent extension $\prob{P}_C$ of $\prob{P}_C'$ to $(\Omega\times H,\sigalg{F}\otimes\sigalg{Y}^{D\times\mathbb{N}})$ with projection map $\RV{G}:\Omega\times G\to G$ such that $\RV{Y}_i\CI^e_{\prob{P}_C'} (\RV{Y}_{<i},\RV{X}_{<i},C)|(\RV{X}_i,\RV{G})$ and $\prob{P}_C^{\RV{Y}_i|\RV{X}_i\RV{G}}=\prob{P}_C^{\RV{Y}_j|\RV{X}_j\RV{G}}$ for all $i,j\in \mathbb{N}$ and $\RV{G}\CI_{\prob{P}_C} (\RV{X},\text{id}_C)$.
\end{theorem}

\begin{proof}
Appendix \ref{sec:data_dep_proofs}.
\end{proof}

\section{Discussion}\label{sec:discussion}

The work in this chapter is motivated by the aim of better understanding the assumption of repeated response functions. We show that this assumption implies symmetries that are often unreasonable in typical causal inference problems. In particular, causal inference is often interested in drawing lessons form data generated in one context in order to exercise control in a context that is usually substantially different -- not the least that, in the latter context, some aspects of the outcomes are under the decision maker's control. However, the assumption of repeated response functions implies that this shift in context makes no difference at all in terms of what we can learn from the data in the long run (though, as we point out in Section \ref{sec:symmetries_discussion} Example 3, the shift is allowed to make a difference in the short run).

For this reason, we don't think that assuming repeatable response functions is a viable starting point for analysis of causal inference problems. This point is perhaps not news to many people who have engaged with this question in much depth. Instead, we want to consider weaker assumptions that are more broadly acceptable, and perhaps we could speculate that repeated response functions arise as a limiting case of an appropriately weaker assumption. In Chapter \ref{ch:other_causal_frameworks}, we explore a few candidates for such weaker assumptions.

While this chapter also includes a discussion of data-dependent models (Theorem \ref{th:response_is_cc_hdep}), this work has not yet offered any easy-to-interpret equivalences between repeated response functions and model symmetries. Notably, combs play a key role in this analysis as well as the analysis of the seemingly unrelated question of identification of marginal graphical models.


% Open question:

% IO contractibility of $\prob{P}_C^{\RV{Y}|\RV{D}}$ might be too strong for many purposes. Theorem \ref{th:ciid_rep_kernel} implies that, whenever $(\prob{P}_C, \RV{D},\RV{Y})$ has conditionally independent and identical response functions, IO contractibility over \emph{some} $\RV{W}$ must hold -- in the worst case over $\RV{H}$. $\RV{H}$ itself is a function of the limiting relative frequencies of $(\RV{D}_i,\RV{Y}_i)$ pairs.  We pose the following as an open question:
% \begin{itemize}
%     \item If $\prob{P}_\cdot^{\RV{Y}|\RV{HD}}$ is IO contractible, when is $\prob{P}_\cdot^{\RV{Y}_{(n,\infty)}|(\RV{D}_i,\RV{Y}_i)_{[n]}\RV{D}_{(n,\infty)}}$ approximately IO contractible?
% \end{itemize}
% In particular, if $\prob{P}_\cdot^{\RV{Y}_{(n,\infty)}|(\RV{D}_i,\RV{Y}_i)_{[n]}\RV{D}_{(n,\infty)}}$ is approximately IO contractible for sufficiently large $n$ then Equation \eqref{eq:interchangeability_of_conditioning} may hold approximately whenever $|A|$ is sufficiently large.


% \section{Assessing decision problems for exchange commutativity}

% Exchange commutativity is a condition that, if it holds, allows a decision maker to use the map $D\kto Y$ calculated from relative frequencies to determine the optimal course of action. The question is: when should a decision maker consider this assumption reasonable?, confronted with a decision problem actually adopt a IO contractible model $\prob{P}_C$ to help them make their decision? This is not an easy question for several reasons. Two of these are:
% \begin{itemize}
%     \item The kind of symmetry required by exchange commutativity seems to us much harder to intuit than the kind of symmetry required by regular exchangeability
%     \item The conditions of exchange commutativity and locality must hold for each choice in $C$
% \end{itemize}

% ``Ordinary'' exchangeability is often considered to be appropriate when modelling a measurement procedure that consists of a sequence of indistinguishable sub-procedures. A common example is a sequence of coin flips -- there is (usually) no reason to consider any coin flip to differ in any important way from any other. Thus, one can reason, swapping the labels of the coin flips yields a measurement procedure that is effectively identical. It follows that the model should be unchanged under a permutation of the variables representing the sequence of flips -- that is, it should be exchangeable\footnote{As \citet[pg. 461]{walley_statistical_1991} points out, the conclusion of exchangeability also requires the assumption that the measurement procedure should be modeled with a single probability distribution, which is an assumption that is being made in this chapter}. The basic judgement call is then: the subprocedures for each coin flip are effectively identical.

% Exchange commutativity requires a different kind of judgement. A common causal inference example features a decision procedure that yields a sequence of (treatment, outcome). Exchange commutativity asks us to compare the original procedure with an arbitrary procedure that shuffles the pairs. Then, \emph{given any fixed vector of treatment values}, the resulting pair of procedures must be effectively indistinguishable. Full IO contractibility adds the requirement that, comparing two procedures of this type and restricting our attention to a subsequence of outcomes, we can ignore any differences between treatment vectors that do not correspond to the subsequence of interest.

% This is not particularly easy to think about! \citet{greenland_identifiability_1986} mention the condition that the treatments of different patients could be swapped without changing the distribution over outcomes. This can be interpreted as saying: given two choices that induce deterministic treatment vectors, if the vector induced by the first is a permutation of the vector induced by the second, the resulting distributions of outcomes (appropriately permuted) should be identical. This is a consequence of exchange commutativity, but it is not equivalent: treatments (or ``inputs'') may not be deterministic for all choices, in which case it's not clear what ``swapping treatments'' means. If it's a hypothetical action that swaps treatments (see the discussion at the end of \ref{sec:whats_the_point}), it seems that some theory is needed to say what equivalence under such hypothetical actions imply for the actual choices to be evaluated.

% A further complication is due to the fact that, by necessity, a probability set $\prob{P}_C$ models a measurement procedure for each of a set of choices $C$. Someone constructing a model $\prob{P}_C$ to help them deal with decision problem may want to reason that their state of knowledge after selecting some choice $\alpha\in C$ is the same as their state of knowledge when they are constructing $\prob{P}_C$. That is, they don't want to worry about whether their choice ``depends on anything''. The fact that they don't want to worry about this doesn't mean that they don't have to! The theory of probability sets is formal, and it can be augmented with decision rules to yield a formal theory of making decisions, but the correspondence between $\prob{P}_C$ and the ``real things that constitute the decision problem'' is a judgement call, and it is possible to make poor calls. Example \ref{ex:confounding} is an example illustrating this. There are ways to deal with actions that ``depend on things'', see for example \citet{gallow_causal_2020}'s discussion of ``managing the news'', but the question of constructing appropriate models seems hard enough without the extra complication.

% Individual-level IO contractibility is an attempt to specify a method for model construction that involves judgements that are (mostly) easier to think about than regular IO contractibility and that may sometimes yield regular IO contractibility as a result (Theorem \ref{th:cc_ind_treat}). Notably, the assumption of individual-level IO contractibility can, under certain conditions, imply that a model is IO contractible conditional on an ``unobserved'' variable, analogous to the familiar assumption of hidden confounding. 