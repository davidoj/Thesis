
%!TEX root = ../main.tex

\chapter{Models with choices and consequences}\label{ch:2p_statmodels}

Decision models are used to model \emph{decision problems}. In such problems, three things are given: a set of options (one of which must be chosen), a set of consequences and a means of judging which consequences are more desirable than others. The role of the decision model is to associate each option with a prediction of the consequences of choosing this option. We have already suggested that the type of a decision model is a map from the option set to probability distributions over the consequence set. In this chapter, we examine the literature on decision theory, and find that it offers essentially the same prescription for the type of a decision model.

In practice, a lot of empirical causal analysis is concerned with problems a step removed from choosing among options. Often, the purpose of causal analysis is to support other decision makers deliberating on a course of action, rather than to recommend an action directly. These are still problems involving a choice among options, but the procedure by which the choice is made is somewhat opaque.  We consider problems where the analyst makes the choice because it is an important class of problems in its own right, and may serve as a useful idealisation of more opaque decision problems. In Section \ref{sec:whats_the_point}, we point out that many works in the causal inference literature regard problems of decision making or control as a particularly important class of problem. While some authors have held that counterfactual questions are a more general class of causal problem, the extra assumptions needed for counterfactual models are not required by decision makers, and we regard the extra complications of counterfactual models as mostly beyond the scope of this thesis.

In Section \ref{sec:modelling_decision_problems}, we discuss in more detail what we take to be a prototypical decision problem and the need for some kind of ``relation'' (broadly understood) between options and consequences. This relation must be able to represent uncertainty somehow, and with this in mind we offer two elaborations on the definition of decision models given in the previous chapter. The first of these may be loosely considered a ``Bayesian'' decision model and the second a ``non-Bayesian'' decision model.

The structure of the decision models we propose relies on some apparently arbitrary choices -- for example, the choice to use probability to represent uncertainty. The literature on decision theory has offered a number of axiomatisations of ``rational choice'' or ``coherent preferences'' that aim to put such choices on a clearer footing. Section \ref{sec:how_represent_conseqeunces} provides an overview of four major decision theories along along with (where applicable) their axiomatisations. These are \emph{Savage decision theory}, \emph{Jeffrey decision theory} (or evidential decision theory), Lewis' \emph{causal decision theory} and \emph{statistical decision theory}. While there are significant controversies surrounding the question of how decision problems should be modeled, there is substantial (though not perfect) agreement between the different theories on the type of model a decision maker uses to evaluate their options, and this type is typically a Bayesian decision model.

Section \ref{sec:how_represent_conseqeunces} explores in particular detail the connections between \emph{statistical decision theory} \citep{wald_statistical_1950} and decision models. We demonstrate a close connection between a certain family of decision models and the classical statistical notion of the \emph{risk} of a decision rule. This family of decision models is defined by a particular conditional independence structure which we find is shared by the kinds decision models that we construct to represent potential outcomes and causal Bayesian networks in Chapter \ref{ch:other_causal_frameworks}. That is, we show that the conditional independence structure of the decision models underlying classical statistical decision theory is the same as the structure of the decision models underlying modern approaches to causal inference.

\section{What is the point of causal inference?}\label{sec:whats_the_point}

\citet{pearl_book_2018} argue that causal reasoning frameworks should be understood by the kinds of questions that they may be able to answer. They classify causal questions into three types, which they claim form a hierarchy or a ``ladder''. That is: questions of type $m$ are also questions of type $n$ for $m<n$. The question types are \citep{barenboim_foundations_2020}:
\begin{enumerate}
    \item \emph{Associational}: ``questions about relationships and predictions''; formally defined as queries that can be answered by a single probability distribution
    \item \emph{Interventional}: ``questions about the consequences of interventions''; formally defined as queries that can be answered by a causal Bayesian network (CBN)
    \item \emph{Counterfactual}: ``questions concerning imagining alternate worlds''; formally defined as queries that can be answered by a structural causal model (SCM)
\end{enumerate}

Models that address decision problems are concerned primarily with consequences of choices, which seems to place them at the second level of this ladder. Given that this thesis is concerned with foundational questions in causal inference and that counterfactual questions are, according to this ladder, a more general kind of causal question, one might ask why we only focus on questions at level 2. 

There are a few reasons for focusing on level 2 questions as the primary motivation for a theory of causal inference. First, decision problems are a particularly important subset of causal inference problems. Within the causal inference literature, ``interventional'' questions and interpretations are much more prominent than strictly counterfactual questions. For example, \citet{rubin_causal_2005} points out that causal inference often informs a decision maker by providing ``scientific knowledge'', but does not make recommendations by itself. \citep{imbens_causal_2015} introduce causal inference as the study of ``outcomes of manipulations'' and \citep{spirtes_causation_1993} highlight the universal relevance of understanding how to control certain outcomes, while further arguing that clarifying commonsense ideas of causation is also an important aim of causal inference. \citet{hernan_whatif_2020} present causal knowledge as critical for assessing the consequences of actions. Secondly, as discussed in Chapter \ref{ch:introduction} and will be further discussed in Chapter \ref{ch:other_causal_frameworks}, a key feature of both causal models and decision problems is the fact that they come with a set of ``alternative possibilities under consideration''. These possibilities might be interventions, counterfactual proposals or options. In decision problems specifically, the set of options is a natural candidate for this set of ``possibilities under consideration'' which -- by assumption -- is always available. Unless we restrict our attention to the less prevalent class of questions that are directly about counterfactual possibilities, we do not automatically get a set of ``possible alternatives'' in the counterfactual setting. The usual solution is to specify counterfactual possibilities by convention, but one of the aims of this work is to show how conventional causal constructions can be derived from broadly applicable assumptions and, as such, we want to avoid adopting such conventions as a basic assumption.

Furthermore, while we only consider how to apply decision models to decision problems, they may also be applicable to counterfactual problems. Speculatively, we may consider counterfactual queries to be decision problems with fanciful options. Consider an informal decision problem and a counterfactual query addressing similar material:
\begin{itemize}
    \item Decision problem: I want my headache to go away. If I take Aspirin, will it do so?
    \item Counterfactual query: I wish I didn't have headache. If I had taken the Aspirin, would I still have it?
\end{itemize}
If I haven't taken aspirin, then there's nothing I can actually choose to do to make it so that I had. However, if I imagine that I did have some fanciful option available that accomplished this -- such as a time machine which could return me to my state two hours ago with the intention of taking aspirin -- then the structure of the two questions seems rather similar. Both ask: if I take the option, what will the consequence be? Of course, it's hard to say what makes a correct answer to the second question, but this is a feature of counterfactual questions in general.

None of this discussion is intended to suggest that understanding how to model counterfactual problems is not a useful endeavour nor is it a trivial extension of modelling regular decision problems. Our project is to fashion alternative foundations for causal modelling, and to keep the task to a manageable size we have focused on decision problems as they are much more common.

\section{Modelling decision problems}\label{sec:modelling_decision_problems}

People who need to make decisions might (and often do) make them with no mathematical reasoning at all. However, this work is concerned with making decisions supported by mathematical reasoning, and this requires a mathematical representation of the decision making problem. We suppose that a decision maker finds themselves in the following kind of situation:
\begin{enumerate}
    \item They are contemplating a collection of different options and must choose one of them (which may include ``do nothing'')
    \item They know what could possibly happen in the future, and prefer some of these possibilities to others
\end{enumerate}
Such a decision maker could choose to mathematically represent their set of options with an option set $C$ and the set of possible things that could happen in the future with $\Omega$. They could also choose to represent their preferences over future possibilities with a scoring or utility function $u$. This means that their preferences must form a total order on the set of future possibilities, and we accept this assumption.

The set $\Omega$ here does not necessarily correspond to the possible results of a measurement procedure $\Psi$, as we discussed in Section \ref{sec:mprocs}, which reflects the fact that decision makers might have preferences about the values of things that are never measured. However, in this thesis we only consider the simpler family of problems where the consequences of interest are all observed.

This decision maker faces the problem that they can only choose from among their options, but they only have the ability to evaluate different future possibilities. What they need is some means of relating the options $C$ and the possibilities $\Omega$, which will allow them to consider the different future possibilities brought about by choices among their options, and thereby to also determine which of their options are preferred. In general, they won't know exactly which future will be brought about by any of their choices, and so whatever method they use to relate $C$ to $\Omega$ must allow them to represent uncertainty about this relationship. 

We can consider two kinds of model (though these are far from exhaustive!):
\begin{itemize}
    \item The decision maker considers the consequences of each option to be uncertain, and uses probability alone to represent this uncertainty; their model is a Markov kernel $C\kto \Omega$
    \item The decision maker considers the consequences of each option to be probabilistically uncertain, and furthermore considers the the appropriate Markov kernel to lie in some set $H$, the correct choice being non-probabilistically uncertain; their model is a map $C\times H\kto \Omega$
\end{itemize}
The first kind of model enables the decision maker to use the principle of expected utility to induce a total order on their set of options, while the second kind of model in conjunction with expected utility induces only a preorder on the decision maker's option set, and some further decision rule is generally needed to select a ``best'' option given this structure.

A decision maker might also contemplate a decision model with no probabilistic uncertainty at all, such as a binary relation between $C$ and $\Omega$, although they might be substantially hamstrung by neglecting any means of representing graduations of uncertainty. Alternatively, they might consider a model that associates each option with an \emph{imprecise probability} over the consequences \citep{walley_statistical_1991}. Whether or not this is equivalent to the second kind of model is an open question.

There have been a number of attempts to show that a rational reasoner or decision maker \emph{must} use probability to represent uncertain knowledge. For example, \citet{de_finetti_foresight_1992,horvitz_framework_1986} propose a number of principles they claim coherent reasoning under uncertainty must follow, and demonstrate that a reasoner who follows these principles must be able to represent their uncertainty with a probability distribution. These principles have, in turn, been criticised \citep{halpern_counter_1999}. Our question -- how to represent decision models -- has been more directly addressed in the literature on decision theory which is surveyed in Section \ref{sec:how_represent_conseqeunces}. 

While we believe the project of axiomatising model choices is valuable because it helps decision makers to understand what commitments they are making when they adopt a certain type of model to aid their deliberation, we think it is likely to be very difficult to sustain an argument that a decision maker \emph{must} commit to any given set of principles and do not make such an argument here.

\subsection{Formal definitions}\label{sec:probability_set_models}

We suppose that we are given a few basic ingredients: a set of choices $C$ equipped with an algebra $\sigalg{C}$, a set of consequences $\Omega$ with an algebra of events $\sigalg{F}$ and a utility function $u:\Omega\to \mathbb{R}$. We call these ingredients a ``decision problem''.

\begin{definition}[Decision problem]\label{def:dec_prob}
A decision problem is a triple $((C,\sigalg{C}),(\Omega,\sigalg{F}),u)$ consisting of a measurable set $(C,\sigalg{C})$ of choices, $(\Omega,\sigalg{F})$ consequences and a measurable utility function $u:\Omega\to \mathbb{R}$.
\end{definition}

Our task is to find a \emph{model} that relates choice $C$ to consequences $\Omega$. We assume two forms of model -- a ``Bayesian'' model, which associates each choice with a unique probability distribution, and a ``non-Bayesian'' model that consists of a set of Bayesian models.

\begin{definition}[Bayesian decision model]\label{def:ch_only}
Given a decision problem $((C,\sigalg{C}),(\Omega,\sigalg{F}),u)$, a \emph{Bayesian decision model} is a triple $(\prob{P}_\cdot,(\Omega,\sigalg{F}),(C,\sigalg{C}))$ where $\prob{P}_\cdot:C\kto \Omega$ is a Markov kernel.
\end{definition}

\begin{definition}[non-Bayesian decision model]\label{def:ch_and_hyp}
Given a decision problem $((C,\sigalg{C}),(\Omega,\sigalg{F}),u)$, a \emph{non-Bayesian decision model} is a triple $(\prob{P}_\cdot,(\Omega,\sigalg{F}),(C\times H,\sigalg{C}\otimes\sigalg{H}))$ where $H$ is a set of \emph{hypotheses} and $\prob{P}_\cdot:C\times H\kto \Omega$ is a Markov kernel.
\end{definition}

By convention, we use $\prob{P}_\cdot$ with the subscript $\cdot$ to denote the map from options to consequences, subscripts $\prob{P}_\alpha$ refer to the model evaluated at $\alpha\in C$ (or in $C\times H$) and the subscript $\prob{P}_{A}$ refers to the probability set formed by the image of $A\subset C$ of $A$ under the model.

\section{Theories of decision making}\label{sec:how_represent_conseqeunces}

The question of how decision problems ought to be represented has received substantial attention. We survey a number of key theories from this literature, and point out connections with our scheme:
\begin{itemize}
    \item Every theory surveyed proposes that choices are evaluated by way of a probabilistic map from choices to consequences, along with some measure of the desirability of consequences
    \item Most theories have some analogue of hypotheses (Definition \ref{def:ch_and_hyp}, see also Chapter \ref{ch:evaluating_decisions})
    \item Most theories have some notion of a ``prior'' over hypotheses
\end{itemize}

Statistical Decision Theory (SDT), introduced by \citet{wald_statistical_1950}, further proves a \emph{complete class theorem}, which shows that, under some conditions, choices that are admissible (Definition \ref{def:admissible_decision}) are also optimal with respect to some prior over hypotheses. That is, any admissible decision under a choices and hypotheses model can be rationalised as a decision under a choices only model with some prior (though, importantly, this \emph{doesn't} establish that proposing a prior is always the appropriate way to go about making a decision). We show that SDT corresponds to a particular class of decision models involving action and hypothesis variables (Definition \ref{def:ci_see_do_model}) combined with the principle of expected utility maximisation. With this in hand, we show that the complete class theorem applies to this class of decision models, and extend it to a slightly broader class.

The following discussion will often make reference to \emph{complete preference relations}. A complete preference relation is a relation $\succ,\prec,\sim$ on a set $A$ such that for any $a,b,c$ in $A$ we have:
\begin{itemize}
    \item Exactly one of $a\succ b$, $a\prec b$, $a\sim b$ holds
    \item $(a\succ b)\iff(b\prec a)$
    \item $a\succ b$ and $b\succ c$ implies $a\succ c$
    \item $\sim$ is an equivalence relation
\end{itemize}
In short, it is a strict total order without antisymmetry ($a$ and $b$ can be equally preferred even if they are not in fact equal).

This definition is meant to correspond to the common sense idea of having preferences over some set of things, where $\succ$ can be read as ``strictly better than'', $\prec$ read as ``strictly worse than'' and $\sim$ read as ``as good as''. Given any two things from the set, I can say which one I prefer, or if I prefer neither (and all of these are mutually exclusive). If I prefer $a$ to $a'$ then I think $a'$ is worse than $a$. Furthermore, if I prefer $a$ to $a'$ and $a'$ to $a''$ then I prefer $a$ to $a''$.

Define $a\preceq b$ to mean $a\prec b$ or $a \sim b$.

\subsection{von Neumann-Morgenstern utility}

\citet{von_neumann_theory_1944} (henceforth abbreviated to vNM) proved that when the \emph{vNM axioms} hold (not defined here; see the original reference or \citet{steele_decision_2020}), an agent's preferences between ``lotteries'' (probability distributions in $\Delta(\Omega)$ for some $(\Omega,\sigalg{F})$) can be represented as the comparison of the expected value under each lottery of a utility function $u$ unique up to affine transformation. That is, for lotteries $\prob{P}_\alpha$ and $\prob{P}_{\alpha'}$, there exists some $u:\Omega\to \mathbb{R}$ unique up to affine transformation such that $\mathbb{E}_{\prob{P}_\alpha}[u]> \mathbb{E}_{\prob{P}_{\alpha'}}[u]$ if and only if $\prob{P}_{\alpha} \succ \prob{P}_{\alpha'}$.

In vNM theory, the set of lotteries is is the set of all probability measures on $(\Omega,\sigalg{F})$. Thus von Neumann-Morgenstern theorem gives conditions under which preferences \emph{over distributions of consequences} can be represented using expected utility. If a decision problem were given such that the set of available choices was in 1-to-1 correspondence with the set of probability distributions in $\Delta(\Omega)$, then the vNM theory provides conditions on the preference relation such that, if these conditions are satisfied, the preference relation can be represented by some utility function on the set of consequences. Typically, the set of choices is not in 1-to-1 correspondence with probability distributions in $\Delta(\Omega)$. Indeed, the starting point of this work is that the relation between choices and consequences is not always obvious, and this situation might be improved by a better understanding of models that relate the two.

\subsection{Savage decision theory}

Savage's decision theory distinguishes \emph{acts} $C$, \emph{consequences} $\Omega$ and \emph{states} $(S,\sigalg{S})$ \citep{savage_foundations_1954}. In our framework, acts are similar to choices, consequences to consequences and states are similar to hypotheses. Unlike vNM theory, the mapping from acts to consequences is not assumed to be given at the outset. Instead, each act is assumed to induce a known mapping from each state to an element of the set of consequences. His theorem conditions under which, given such a map from acts and states to consequences, a preference relation over acts can be represented by a ``prior'' over states and a utility function $u:\Omega\to \mathbb{R}$ in combination with the principle of expected utility. As Theorem \ref{th:sav_pmap} shows, the prior over states induces a probabilistic map from choices to consequences that, in combination with the utility, is sufficient to evaluate the desirability of the choices.

We have said that acts are similar to choices and states are similar to hypotheses in our framework -- but there are differences. We've taken the set of choices to be the set of all the things that the decision maker might choose once they've finished considering their problem. In Savage's theory, like ours, the decision maker has a preference relation over the set of acts. Unlike our theory, however, the set of acts is precisely the set of all functions from states to consequences, which is usually much bigger than the set of all the things the decision maker might actually choose to do. This could be considered a requirement of extendability: given a non-Bayesian model $(\prob{P}_{C\times H},\Omega,C\times H)$, we might consider it a Savage decision model if there is some $(\prob{Q}_{C'\times H},\Omega,C'\times H)$ where $C'$ is defined as the convex closure of the set of all deterministic functions $H\kto \Omega$ and $\alpha\in C$ implies $\prob{Q}_\alpha = \prob{P}_\alpha$ and the Savage axioms hold (see Appendix \ref{sec:savage_axioms}). 

The reason why Savage's theory has such a rich set of options is because the derivation needs to deduce a preference relation over consequences from the preference relation over the options (in contrast, we simply assume preferences over consequences are available at the outset). All a decision maker actually needs is the ordering on the options that they're actually considering, and this might be compatible with many orderings of consequences. Sufficiently enriching the set of options can restrict this to a unique relation. We don't know if there are cases of decision problems where this extendability requirement introduces difficulties. 

\begin{definition}[Elements of a Savage decision problem]
A \emph{Savage decision problem} features a measurable set of states $(S,\sigalg{S})$, a set of consequences $(\Omega,\sigalg{F})$ and a set of acts $C$ such that $|C|=\Omega^S$ and a measurable evaluation function $T:S\times C\to \Omega$ such that for any $f:S\to \Omega$ there exists $c\in C$ such that $T(\cdot,c)=f$.
\end{definition}

Theorem \ref{th:savage_representation} is Savage's representation theorem. The Savage axioms aren't investigated in detail in this work, but for the reader's convenience they're given in Appendix \ref{sec:savage_axioms}.

\begin{theorem}\label{th:savage_representation}
Given any Savage decision problem $(S,\Omega,C,T)$ with a preference relation $(\prec,\sim)$ on $C$ that satisfies the \emph{Savage axioms}, there exists a unique probability distribution $\mu\in\Delta(\sigalg{S})$ and a utility $u:\Omega\to \mathbb{R}$ unique up to affine transformation such that
\begin{align}
    \alpha\preceq \alpha' &\iff \int_S u(T(s,\alpha))\mu(\mathrm{d}s) \leq \int_S u(T(s,\alpha'))\mu(\mathrm{d}s)&\forall \alpha,\alpha'\in C
\end{align}
\end{theorem}

\begin{proof}
\citet{savage_foundations_1954}
\end{proof}

Savage's setup implies the existence of a unique probabilistic function $C\kto \Omega$ representing the ``probabilistic consequences'' of each choice.

\begin{theorem}\label{th:sav_pmap}
Given any Savage decision problem $(S,\Omega,C,T)$ with a preference relation $(\prec,\sim)$ on $C$ that satisfies the Savage axioms, and a $\sigma$-algebra $\sigalg{F}$ on $\Omega$ such that $T$ is measurable, there is a probabilistic function $\prob{P}_{\cdot}:C\kto \Omega$ and a utility $u:\Omega\to \mathbb{R}$ unique up to affine transformation such that
\begin{align}
    \alpha\preceq \alpha' &\iff \int_\Omega u(f)\prob{P}_\alpha(\mathrm{d}f) \leq \int_\Omega u(f)\prob{P}_{\alpha'}(\mathrm{d}f)&\forall \alpha,\alpha'\in C
\end{align}
\end{theorem}

\begin{proof}
Define $\prob{P}_\cdot:C\kto \Omega$ by
\begin{align}
    \prob{P}_\alpha(A) &:= \mu (T_\alpha^{-1}(A))&\forall A\in \sigalg{F}
\end{align}
where $\RV{T}_\alpha:S\to F$ is the function $s\mapsto T(s,\alpha)$. $\prob{P}_\alpha$ is the pushforward of $T_\alpha$ under $\mu$.

Then 
\begin{align}
    \int_\Omega u(f)\prob{P}_\alpha(\mathrm{d}f) &= \int_S u \circ T_\alpha (s)\mu(\mathrm{d}s)\\
    &= \int_S u(T(s,\alpha))\mu(\mathrm{d}s)
\end{align}
\end{proof}

\subsection{Jeffrey's decision theory}

Jeffrey's decision theory is an alternative to Savage's that starts from a different set of assumptions. One of the key differences is in what is assumed at the outset: where Savage assumes a set of states $S$, acts $C$ and consequences $\Omega$, Jeffrey's theory only considers a single space $\underline{\sigalg{F}}$, which is a complete atomless boolean algebra. Elements of $\underline{\sigalg{F}}$ are said to be propositions. We note that $\underline{\sigalg{F}}$ cannot be understood as the set of events with respect to a finite measurement procedure (Section \ref{sec:interp_of_dms}). The collection of finite propositions regarding the results of some finite measurement procedure followed by flipping an infinite number of coins could perhaps be represented by a complete atomless boolean algebra. The theory is set out in \citet{jeffrey_logic_1990}, and the key representation theorem proved in \citet{bolker_functions_1966}.

Recall that our fundamental problem is relating a set $C$ of things we can choose to a set $F$ of things we can compare. Jeffrey's theory uses a different strategy to accomplish this than Savages'; where Savage identifies a set of acts $C$ with all functions $S\to F$ and proposes axioms that constrain a preference relation on $C$, Jeffrey assumes that choices are elements of the algebra $\underline{\sigalg{F}}$, accompanied by other propositions that do not correspond precisely to choices. Jeffrey's axioms pertain to a preference relation on $\underline{\sigalg{F}}$, and preferences over choices are given by the restriction of the preference relation to $C$. In common with Savage's theory, the preference relation is assumed to be available over a much richer set than the set of choices actually under consideration.

Complete atomless boolean algebras are somewhat different to standard measurable $\sigma$-algebras. The $\sigma$-algebra $(\mathbb{R},\sigalg{B}(\mathbb{R}))$ is a complete Boolean algebra when identifying $\land$ with $\cap$, $\lor$ with $\cup$, $0$ with $\emptyset$ and $1$ with $\mathbb{R}$, but it has atoms: any singleton $\{x\}$ has only the subsets $\emptyset$ and $\{x\}$. An example of a complete atomless boolean algebra can be constructed from the set of Lebesgue measurable sets on $[0,1]$ with any two sets that differ by a set of measure zero identified \citet{bolker_simultaneous_1967}.

\begin{definition}[Complete atomless boolean algebra]\label{def:c_atom_ba}
A boolean algebra $\underline{\sigalg{F}}$ is a tuple $(A,\land,\lor,\neg,0,1)$ such that, for all $a,b,c\in A$:
\begin{itemize}
    \item $(a\lor b)\lor c = a\lor (b\lor c)$ and $(a\land b)\land c = a\land (b\land c)$
    \item $a\lor b = b\lor a$ and $a\land b = b\land a$
    \item $a\lor (a\land b) = a$ and $a\land(a\lor b) = a$
    \item $a\lor 0 = a$ and $a\land 1=a$
    \item $a\lor(b\land c) = (a\lor b) \land (a\lor c)$ and $a\land(b\lor c) = (a\land b) \lor (a\land c)$
    \item $a\lor \neg a = 1$ and $a\land \neg a = 0$
\end{itemize}
say $a\leq b$ exactly when $a\lor b = b$. A boolean algebra is atomless if for any $b$ there is some $a\neq 0$ such that $a\leq b$. A boolean algebra is complete if for every $B\subset A$, there is some $c$ such that $c$ is an upper bound of $B$ and for all upper bounds $c'$  of $B$ $c\leq c'$. 
\end{definition}

The Bolker axioms are also not analysed deeply in this work, but for the reader's convenience they can be found in Appendix \ref{sec:bolker_axioms}).

\begin{theorem}\label{th:bolker_jeffrey}
Suppose there is a complete atomless Boolean algebra $\underline{\sigalg{F}}$ with a preference relation $\preceq$. If $\preceq$ satisfies the \emph{Bolker axioms} then there exists a desirability function $\text{des}:\underline{\sigalg{F}}\to\mathbb{R}$ and a probability distribution $\mu\in \Delta(\underline{\sigalg{F}})$ such that for $A,B\in \underline{\sigalg{F}}$ and finite partition $D_1,...,D_n\in \underline{\sigalg{F}}$:
\begin{align}
    (A \preceq B) \iff \sum_{i}^n \text{des}(D_i) \mu(D_i|A) \leq \sum_{i}^n \text{des}(D_i) \mu(D_i|B) \label{eq:ev_dec_theory}
\end{align}
where $\mu(D_i|A):=\frac{\mu(A\cap D_i)}{\mu(A)}$ for $\mu(A)>0$, undefined otherwise.
\end{theorem}

\begin{proof}
\citet{bolker_functions_1966})
\end{proof}

As mentioned, in Jeffrey's theory the \emph{choices} under consideration $C$ are assumed to be some subset of $\underline{\sigalg{F}}$. Thus we can deduce from a Jeffrey model a function $C\to \Delta(\underline{\sigalg{F}})$ that ``represents the consequences of choices'' in the sense of Theorem \ref{th:jeffrey_with_choices}.

\begin{theorem}\label{th:jeffrey_with_choices}
Suppose there is a complete atomless Boolean algebra $\underline{\sigalg{F}}$ with a preference relation $\preceq$ that satisfies the Bolker axioms and set of choices $C \subset \underline{\sigalg{F}}$. Then there is a function $\prob{P}_\cdot:C\to \Delta(\underline{\sigalg{F}})$ such that for any $\alpha,\alpha'\in C$ and finite partition $D_1,...,D_n\in \underline{\sigalg{F}}$:
\begin{align}
    \alpha \preceq \alpha'\iff \sum_{i}^n \text{des}(D_i) \prob{P}_\alpha(D_i) \leq \sum_{i}^n \text{des}(D_i) \prob{P}_{\alpha'}(D_i)\label{eq:ev_with_choices}
\end{align}
Where $\mu$ and $\mathrm{des}$ are as in Theorem \ref{th:bolker_jeffrey}
\end{theorem}

\begin{proof}
Define $\prob{P}_\cdot$ by $\alpha\mapsto \mu(\cdot|\alpha)$. Then Equation \eqref{eq:ev_with_choices} follows from Equation \eqref{eq:ev_dec_theory}.
\end{proof}

\subsection{Causal decision theory}

Causal decision theory was developed after both Jeffrey's and Savage's theory. A number of authors \citet{lewis_causal_1981,skyrms_causal_1982} felt that Jeffrey's theory erred by treating the consequences of a choice as an ``ordinary conditional probability''. \citet{lewis_causal_1981} suggested that causal decision theory can be used to evaluate choices when we are given a set $\Omega$ of consequences over which preferences are known, a set $C$ of choices and a set $H$ of dependency hypotheses (the letters have been changed to match usage in this work; in the original the consequences were called $S$, the choices $A$ and the dependency hypotheses $H$). Choices are then evaluated according to the causal decision rule. We have taken the liberty to state Lewis' rule in the language of the present work.

\begin{definition}[Causal decision rule]
Given a set $C$ of choices, sample space $(\Omega,\sigalg{F})$, variables $\RV{H}:\Omega\to H$ (the \emph{dependency hypothesis}) and $\RV{S}:\Omega\to S$ (the \emph{consequence}) and a utility $u:\Omega\to \mathbb{R}$, the \emph{causal utility} of a choice $\alpha\in C$ is given by
\begin{align}
    U(\alpha) := \int_S \int_H u(s) \prob{P}_\alpha^{\RV{S}|\RV{H}}(\mathrm{d}s|h) \prob{P}_C^{\RV{H}}(\mathrm{d}h)\label{eq:lewis_cdt}
\end{align}
For some probabilistic function $\prob{P}_\cdot:C\kto \Omega$ where $\prob{P}_C^{\RV{H}}$ exists.
\end{definition}

The reasons why Lewis wanted to introduce dependency hypothesis and modify Jeffrey's rule to Equation \eqref{eq:lewis_cdt} are controversial and do not come up in this work. However, causal decision theory is still relevant to this work in two ways: firstly, once again is a probabilistic function $\prob{P}_\cdot:C\kto \Omega$. Secondly, causal decision theory introduces the notion of the dependency hypothesis $\RV{H}$. The dependency hypothesis is similar to the state in Savage's theory, however Lewis does not require a deterministic map from dependency hypotheses to consequences, nor does he require a choice to correspond to every possible function from dependency hypotheses to states.

Dependency hypotheses are an important idea in causal reasoning. Lewis' decision rule connects the theory of probability sets with \emph{statistical decision theory}, as Section \ref{sec:sdt} will show. Chapter \ref{ch:evaluating_decisions} goes into considerable detail concerning the question of when probability sets support certain types of dependency hypotheses. While they are typically not explicitly represented in common frameworks for causal inference, Chapter \ref{ch:other_causal_frameworks} discusses how dependency hypotheses are often implicit in these approaches, and shows how they can be made explicit.

\subsection{Statistical decision theory}\label{sec:sdt}

Statistical decision theory (SDT), created by \citet{wald_statistical_1950}, predates all of the decision theories discussed above. Savage's theory appears to have developed in part to explain some features of SDT \citet{savage_theory_1951}, and Jeffrey's theory and subsequent causal decision theories were in turn influenced by Savage's decision theory. While the later decision theories were concerned with articulating why their theory fit the role of a theory for rational decision under uncertainty, Wald focused much more on the mathematical formalism and solutions to statistical problems. Statistical decision theory introduced many fundamental ideas that have since entered the ``water supply'' of machine learning theory, such as \emph{decision rules} and \emph{risk} as a measure of the quality of a decision rule.

In contrast to the later decision theories, SDT has no explicit representation of the ``consequences'' of a decision. Rather, it is assumed that a loss function is given that maps decisions and hypotheses directly to a loss, which is a kind of desirability score similar to a utility (although it is minimised rather than maximised). The following definitions are all standard to SDT.

\begin{definition}[Statistical decision problem]
A statistical decision problem (SDP) is a tuple $(X, H, D, l, \prob{P}_\cdot)$ where $(X,\sigalg{X})$ is a set of outcomes, $(H,\sigalg{H})$ is a set of hypotheses, $(D,\sigalg{D})$ is a set of decisions, $l:D\times H\to \mathbb{R}$ is a loss function and $\prob{P}_\cdot:H\kto X$ is a Markov kernel from hypotheses to to outcomes.
\end{definition}

Statistical decision theory is concerned with the selection of \emph{decision rules}, rather than the selection of decisions directly. A decision rule maps observations to decisions, and may be deterministic or stochastic.

\begin{definition}[Decision rule]
Given a statistical decision problem $(X, H, D, l, \prob{P}_\cdot)$, a decision rule is a Markov kernel $\kernel{D}_\alpha:\Omega\kto D$.
\end{definition}

Because decision rules in SDT play the role of what we call \emph{choices}, we denote the set of all available decision rules by $C$. A further feature of SDT that is unlike the later decision theories is that SDT does not offer a single rule for assessing the desirability of any choice in $C$. Instead, it offers a definition of the risk, which assesses the desirability of a choice \emph{relative to a particular hypothesis}. The risk function completely characterises the problem of choosing a decision function. Two different rules are for turning this ``intermediate assessment'' into a final assessment of the available choices - Bayes optimality and minimax optimality. Bayes optimality reuquires a prior over hypotheses, while minimax optimality does not.

\begin{definition}[SDP Risk]\label{def:risk}
Given a statistical decision problem $(X, H, D, l, \prob{P}_\cdot)$ and decision functions $C$, the \emph{risk} functional $R:C\times H\to \mathbb{R}$ is defined by
\begin{align}
    R(\kernel{D}_\alpha,h):= \int_X \int_D l(d,h) \kernel{D}_\alpha(\mathrm{d}d|f)\kernel{P}_h(\mathrm{d}f)
\end{align}
\end{definition}

It is possible to find risk functions in problems that aren't SDPs. The definitions of Bayes and Minimax optimality still apply to risk functions obtained in other manners. Thus Bayes optimality and minimax optimality are defined in terms of risk functions in general, not SDP risk functions.

\begin{definition}[Bayes risk]
Given decision functions $C$, hypotheses $(H,\sigalg{H})$, risk $R:C\times H\to \mathbb{R}$ and prior $\mu\in \Delta(H)$, the $\mu$-\emph{Bayes risk} is
\begin{align}
    R_\mu(\kernel{D}_\alpha) &:= \int_{H} R(\kernel{D}_\alpha,h)\mu(\mathrm{d}h)
\end{align}
\end{definition}

\begin{definition}[Bayes optimal]
Given decision functions $C$, hypotheses $(H,\sigalg{H})$, risk $R:C\times H\to \mathbb{R}$ and prior $\mu\in \Delta(H)$, $\alpha\in C$ is $\mu$-Bayes optimal if
\begin{align}
    R_\mu(\kernel{D}_\alpha) &= \inf_{\alpha'\in C} R_{\mu}(\kernel{D}_{\alpha'})
\end{align}
\end{definition}

\begin{definition}[Minimax optimal]
Given decision functions $C$, hypotheses $(H,\sigalg{H})$, risk $R:C\times H\to \mathbb{R}$, a \emph{minimax decision function} is any decision function $\kernel{D}_\alpha$ satisfying
\begin{align}
    \sup_{h\in H}  R(\kernel{D}_\alpha,h) &= \inf_{\alpha' in C} \sup_{h\in H} R(\kernel{D}_{\alpha'},h)
\end{align}
\end{definition}

\subsubsection{From consequences to statistical decision problems}\label{sec:cons_to_sdp}

In this section, we relate our new work to the standard formulation of SDT presented above.

Statistical decision theory ignores the notion of general consequences of choices; the only ``consequence'' in the theory is the loss incurred by a particular decision under a particular hypothesis. The kinds of probability set models studied here probabilistically map decisions to consequences, and the set of consequences is understood to have a utility function to allow for assessment of the desirability of different choices via the principle of expected utility. Not every probability set induces a statistical decision problem in this manner. A family of models that does are those involving \emph{action}, \emph{observation} and \emph{hypothesis} variables. Observation variables are independent of the ``choice'', while action variables are independent of the hypothesis given the choice.
\begin{definition}[Statistical decision model]\label{def:see_do_model}
A probability set model of a statistical decision problem, or a statistical decision model for short, is a tuple $(\prob{P}_{C\times H}, \RV{X},\RV{Y},\RV{A})$ where $\prob{P}_{C\times H}$ is a probability set indexed by elements of $C\times H$ on $(\Omega,\sigalg{F})$. $\RV{X}:\Omega\to X$ are \emph{observations}, $\RV{Y}:\Omega\to Y$ are \emph{consequences} and $\RV{A}:\Omega\to A$ are \emph{actions}. $\prob{P}_{C\times H}$ must observe the following conditional independences:
\begin{align}
    \RV{X}&\CI^e_{\prob{P}_{C\times H}} \RV{C}|\RV{H}\quad\text{observations independent of choice}\\
    \RV{A}&\CI^e_{\prob{P}_{C\times H}} \RV{H}|\RV{C}\quad\text{actions independent of hypothesis given choice}
\end{align}
where $\RV[C]:C\times H\to C$ and $\RV{H}:C\times H\to H$ are the respective projections (refer to Definition \ref{def:eci_orig}, which form a complimentary pair required for extended conditional independence with respect to $\prob{P}_{C\times H}$.
\end{definition}
\begin{definition}[Conditionally independent statistical decision model]\label{def:ci_see_do_model}
A conditionally independent statistical decision model is a statistical decision model $(\prob{P}_{C\times H}, \RV{X},\RV{Y},\RV{A})$ where the following additional conditional independence holds:
\begin{align}
    \RV{Y}&\CI^e_{\prob{P}_{C\times H}} (\RV{X,C})|(\RV{A},\RV{H})
\end{align}
That is, consequences are independent of the observations and the choice given the actions and the hypothesis.
\end{definition}

If we are given a statistical decision model and a utility function is available depending on the consequence $\RV{Y}$ only, we can identify the loss with the negative expected utility conditional on a particular decision and hypothesis.
\begin{definition}[Induced loss]
Given a statistical decision model $(\prob{P}_{C\times H}, \RV{X},\RV{Y},\RV{A})$ and a utility $u:Y\to \mathbb{R}$, the induced loss $l:A\times H\to \mathbb{R}$ is defined as
\begin{align}
    l(a,h)&:= -\int_Y u(y) \prob{P}_{C\times\{h\}}^{\RV{Y}|\RV{A}}(\mathrm{d}y|a)
\end{align}
\end{definition}
where the uniform conditional $\prob{P}_{C\times\{h\}}^{\RV{Y}|\RV{A}}$'s existence is guaranteed by $\RV{Y}\CI^e_{\prob{P}_{C\times H}} (\RV{X,C})|(\RV{A},\RV{H})$.

A statistical decision model induces a set of decision functions: for each $\alpha\in C$, there is an associated probability distribution $\prob{P}_\alpha^{\RV{A}|\RV{X}}$. Using the above definition of loss, the expected loss of a decision function in a conditionally independent statistical decision model induces a risk function identical to the SDP risk.
\begin{theorem}[Induced SDP risk]\label{th:ind_risk}
Given a conditionally independent statistical decision model $(\prob{P}_{C\times H}, \RV{X},\RV{Y},\RV{A})$ along with a utility $u:Y\to \mathbb{R}$, the expected utility for each choice $\alpha\in C$ and hypothesis $h\in H$ is equal to the negative SDP risk of the associated decision rule $\prob{P}_\alpha^{\RV{A}|\RV{X}}$ and hypothesis $h$.
\begin{align}
    \prob{P}_{\alpha,h}^{\RV{Y}}u &= -R(\prob{P}_{\{\alpha\}\times H}^{\RV{A}|\RV{X}},h)
\end{align}
\end{theorem}

\begin{proof}
The expected utility given $\alpha$ and $h$ is
\begin{align}
    \int_Y u(y)\prob{P}_{\alpha,h}^{\RV{Y}}(\mathrm{d}y) &= \int_Y  \int_A \int_{X} u(y)\prob{P}_{\alpha,h}^{\RV{Y}|\RV{AX}}(\mathrm{d}y|a,x)\prob{P}_{\alpha,h}^{\RV{A}|\RV{X}}(\mathrm{d}a|x)\prob{P}_{\alpha,h}^{\RV{X}}(\mathrm{d}x) \\
    &= \int_X  \int_A \int_{Y} u(y) \prob{P}_{\alpha,h}^{\RV{Y}|\RV{A}}(\mathrm{d}y|a)\prob{P}_{\alpha,h}^{\RV{A}|\RV{X}}(\mathrm{d}a|x)\prob{P}_{\alpha,h}^{\RV{X}}(\mathrm{d}x)\label{eq:because_of_ci}\\
    &= \int_{X} \int_A \int_Y u(y) \prob{P}_{C\times\{h\}}^{\RV{Y}|\RV{A}}(\mathrm{d}y|a)\prob{P}_{\{\alpha\}\times H}^{\RV{A}|\RV{X}}(\mathrm{d}a|x)\prob{P}_{C\times\{h\}}^{\RV{X}}(\mathrm{d}x)\\
     &= -\int_A\int_X l(d,h)\prob{P}_{\{\alpha\}\times H}^{\RV{A}|\RV{X}}(\mathrm{d}a|x)\prob{P}_{C\times\{h\}}^{\RV{X}}(\mathrm{d}x)\\
    &= -R(\prob{P}_{\{\alpha\}\times H}^{\RV{A}|\RV{X}},h)
\end{align}
where Equation \eqref{eq:because_of_ci} follows from $\RV{Y}\CI^e_{\prob{P}_{C\times H}} (\RV{X,C})|(\RV{A},\RV{H})$, the uniform conditional $\prob{P}_{\{\alpha\}\times H}^{\RV{A}|\RV{X}}$ exists due to $\RV{A}\CI^e_{\prob{P}_{C\times H}} \RV{H}|\RV{C}$ and the uniform conditional $\prob{P}_{C\times\{h\}}^{\RV{X}}$ exists due to $\RV{X}\CI^e_{\prob{P}_{C\times H}} \RV{C}|\RV{H}$.
\end{proof}

Theorem \ref{th:ind_risk} does \emph{not} hold if we have have a utility that depends on $\RV{X}$ even after conditioning on $\RV{A}$ and $\RV{H}$. The form of the loss function in SDT forces no direct dependence on observations. The generic ``decision model risk'' (Definition \ref{def:see_do_risk}) provides a notion of risk for the more general case, while Theorem \ref{th:ind_risk} shows it reduces to SDP risk in the case of conditionally independent statistical decision models models with a utility that depends only on the consequences $\RV{Y}$.
\begin{definition}[Decision model risk]\label{def:see_do_risk}
Given a statistical decision model $(\prob{P}_{C\times H}, \RV{X},\RV{Y},\RV{A})$ along with a utility $u:X\times Y\to \mathbb{R}$, the \emph{decision problem risk} $R:C\times H\to \mathbb{R}$ is given by
\begin{align}
    R(\alpha,h) &:= -\prob{P}_{\alpha,h}^{\RV{XY}}u&\forall \alpha\in C,h\in H
\end{align}
\end{definition}

Section \ref{sec:modelling_decision_problems} noted that two types of probability set model are considered: probability sets $\prob{P}_C$ indexed by choices alone, and probability sets $\prob{P}_{C\times H}$ jointly indexed by choices and hypotheses. Statistical decision models are an instance of the second kind, jointly indexed by choices and hypotheses. Bayesian statistical decision models are of the former type, indexed by choices alone. A statistical decision model $(\prob{P}_{C\times H}, \RV{X},\RV{Y},\RV{A})$ and a prior over hypotheses $\mu\in \Delta(H)$ can be combined to form a Bayesian statistical decision model, and under the right conditions the risk of the Bayesian model reduces to the Bayes risk of the original statistical decision model.

\begin{definition}[Bayesian statistical decision model]
A Bayesian statistical decision model is a tuple $(\prob{P}_{C}, \RV{X},\RV{Y},\RV{D},\RV{H})$ where $\prob{P}_C$ is a probability set on $(\Omega,\sigalg{F})$, $\RV{X}:\Omega\to X$ are the observations, $\RV{Y}:\Omega\to Y$ are the consequences, $\RV{A}:\Omega\to A$ are the decisions and $\RV{H}:\Omega\to H$ is the hypothesis. $\prob{P}_C$ must observe the following conditional independences:
\begin{align}
    \RV{X}&\CI^e_{\prob{P}_{C}} \text{id}_C|\RV{H}\\
    \RV{A}&\CI^e_{\prob{P}_{C}} \RV{H}|\text{id}_C\\
    \RV{H}&\CI^e_{\prob{P}_C} \text{id}_C
\end{align}
\end{definition}

\begin{definition}[Induced Bayesian tastical decision model]
Given a statistical decision model $(\prob{P}_{C\times H}, \RV{X},\RV{Y},\RV{A},\RV{H})$ on $(\Omega,\sigalg{F})$ and a prior $\mu\in \Delta(H)$, the induced Bayesian statistical decision model $\prob{P}_C$ on $(\Omega\times H,\sigalg{F}\otimes \sigalg{H})$ is
\begin{align}
    \prob{P}_C(B\times D) &= \int_{D} \prob{P}_{C\times \{h\}}(B)\mu(\mathrm{d}h)&\forall B\in \sigalg{F}, D\in\sigalg{H}
\end{align}
\end{definition}

\begin{theorem}[Induced SDP Bayes risk]
Given a conditionally independent statistical decision model $(\prob{P}_{C}, \RV{X},\RV{Y},\RV{A},\RV{H})$ along with a consequence-dependent utility $u:Y\to \mathbb{R}$ and a prior $\mu\in \Delta(H)$, the expected utility for each choice $\alpha\in C$ under the induced Bayesian statistical decision model is equal to the negative $\mu$-Bayes risk of that decision rule.
\end{theorem}

\begin{proof}
First, note that $h\mapsto \prob{P}_{C\times \{h\}}^{\RV{Y}|\RV{XA}}$ is a version of $\prob{P}_C^{\RV{Y}|\RV{XA}}$ and hence $\RV{Y}\CI^e_{\prob{P}_C} (\RV{X},\text{id}_C)|(\RV{H},\RV{A})$, a property it inherits from the underlying statistical decision model.

Also, note that $\prob{P}_C^{\RV{H}}=\mu$, by construction.

The expected utility of $\alpha\in C$ is 
\begin{align}
    \prob{P}_\alpha^{\RV{Y}} u &= \int_Y u(y) \prob{P}_{\alpha}^{\RV{Y}}(\mathrm{d}y) \\
    &= \int_Y  \int_A \int_{X} \int_H u(y)\prob{P}_{\alpha}^{\RV{Y}|\RV{AXH}}(\mathrm{d}y|a,x,h)\prob{P}_{\alpha}^{\RV{A}|\RV{XH}}(\mathrm{d}a|x,h)\prob{P}_{\alpha}^{\RV{X}|\RV{H}}(\mathrm{d}x|h)\prob{P}_\alpha^{\RV{H}}(\mathrm{d}h) \\
    &= \int_X  \int_A \int_{Y} \int_H u(y) \prob{P}_{\alpha}^{\RV{Y}|\RV{AH}}(\mathrm{d}y|a,h)\prob{P}_{\alpha}^{\RV{A}|\RV{X}}(\mathrm{d}a|x)\prob{P}_{\alpha}^{\RV{X}|\RV{H}}(\mathrm{d}x|h)\prob{P}_\alpha^{\RV{H}}(\mathrm{d}h)\\
    &=  \int_X  \int_A \int_{Y} \int_H u(y) \prob{P}_{C}^{\RV{Y}|\RV{AH}}(\mathrm{d}y|a,h)\prob{P}_{\alpha}^{\RV{A}|\RV{X}}(\mathrm{d}a|x)\prob{P}_{C}^{\RV{X}|\RV{H}}(\mathrm{d}x|h)\mu(\mathrm{d}h)\\
     &= -\int_A\int_X\int_H l(a,h)\prob{P}_{\alpha}^{\RV{A}|\RV{X}}(\mathrm{d}a|x)\prob{P}_{C}^{\RV{X}|\RV{H}}(\mathrm{d}x|h)\mu(\mathrm{d}h)\\
    &= -\int_H R(\prob{P}_{\alpha}^{\RV{A}|\RV{X}},h)\mu(\mathrm{d}h)\\
    &= -R_{\mu}(\prob{P}_{\alpha}^{\RV{A}|\RV{X}})
\end{align}
\end{proof}

\subsubsection{Complete class theorem}\label{sec:cc_theorem}

The \emph{complete class theorem} is a key theorem of classical SDT that establishes, under certain conditions, any \emph{admissible} decision rule (Definition \ref{def:admissible_decision}) for a statistical decision model $\prob{P}_{C\times H}$ with a utility $u$ must minimise the Bayes risk for a Bayesian model constructed from $\prob{P}_{C\times H}$ and some prior over hypotheses $\mu\in \Delta(H)$. This can be interpreted in a similar way to the decision theoretic representation discussed above: if you accept that the relevant assumptions apply to the decision problem at hand, then there is a Bayesian statistical decision model along with $u$ that captures the important features of this problem. The assumptions are that a statistical decision model $\prob{P}_{C\times H}$ with a utility $u$ that satisfies the relevant conditions is available, and that the principle used to evaluate decision rules should yield an admissible decision rule (though it may also be desired to satisfy other properties as well).

If more is required of the decision rule than merely admissibility, then the complete class theorem does not prove that it is easy to find any Bayesian model that will yield rules satisfying these requirements. It also does not prove that a Bayesian approach is helpful for finding a ``correct'' decision rule according to some vague notion of ``correct''.

We have shown in Theorem \ref{th:ind_risk} that conditionally independent statistical decision models induce statistical decision problems. However, the complete class theorem iteself (Theorem \ref{th:complete_class}) depends only on the risk function induced by a decision making model. In particular, the complete class theorem can also apply to general statistical decision models, without the assumption of conditional independence, which we show in Example \ref{ex:cc_sdt} and \ref{ex:cc_nonsdt}.

\begin{definition}[Risk function]
Given a set of choice $C$ and a set of hypotheses $H$, a risk function is a map $R:C\times H\to \mathbb{R}$.
\end{definition}

If the second set $H$ were, instead of hypotheses about nature, a set of options available to a second player playing a game, then a ``risk function'' defines a two-player zero-sum game \citet{toutenburg_ferguson_1967}.

\begin{definition}[Admissible choice]\label{def:admissible_decision}
Given a risk function $R:C\times H\to \mathbb{R}$, a choice $\alpha\in C$ dominates a choice $\alpha'\in C$ if for all $h\in H$, $R(\alpha,h)\leq R(\alpha',h)$ and for at least on $h^*$, $R(\alpha,h)<R(\alpha,h^*)$. An \emph{admissible choice} is a choice $\alpha\in C$ such that there is no $\alpha'\in C$ dominating $\alpha$.
\end{definition}

\begin{definition}[Complete class]\label{th:complete_class}
A \emph{complete class} is any $B\subset C$ such that, for any $\alpha'\not \in B$ there is some $\alpha\in B$ that dominates $\alpha'$. A \emph{minimal complete class} is a complete class $B$ such that no proper subset of $B$ is complete
\end{definition}

\begin{theorem}
If a minimal complete class $B\subset C$ exists then $B$ is the set consisting of all the admissible decision rules.
\end{theorem}

\begin{proof}
See \citet[Theorem 2.1]{toutenburg_ferguson_1967}
\end{proof}

\begin{definition}[Risk set]
Given a finite set of hypotheses $H$, a set of choices $C$ and a risk function $R:C\times H\to \mathbb{R}$, the risk set is the subset of $\mathbb{R}^{|H|}$ given by
\begin{align}
    S := \{(R(\alpha,h))_{h\in H}|\alpha\in C\}
\end{align}
\end{definition}

\begin{theorem}[Complete class theorem]
Given a risk function $R:C\times H\to \mathbb{R}$, if the risk set S is convex, bounded from below and closed downwards, and H is finite, then the set of Bayes optimal choices is a minimal complete class.
\end{theorem}

\begin{proof}
See \citet[~Theorem 2.10.2]{toutenburg_ferguson_1967}
\end{proof}

Two examples of the application of the complete class theorem will be presented (Examples \ref{ex:cc_sdt} and \ref{ex:cc_nonsdt}). In order to explain them, we need a few lemmas.

\begin{lemma}\label{lem:convex_closed}
Given $H$ and $C$ both finite and a risk function $R:C\times H\to \mathbb{R}$ and an associated probability set $\prob{P}_C$ on $(\Omega,\sigalg{F})$, $\Omega$ finite, if the function
\begin{align}
    \prob{P}_{\alpha,h}^{\RV{A}|\RV{X}}\mapsto R(\alpha,h)
\end{align}
is linear and 
\begin{align}
    Q:= ((\prob{P}_{\alpha,h}^{\RV{A}|\RV{X}})_{h\in H})_{\alpha\in C}
\end{align}
is convex closed, then the risk set $S$ is convex closed.
\end{lemma}

\begin{proof}
By linearity of 
\begin{align}
    \prob{P}_{\alpha,h}^{\RV{A}|\RV{X}}\mapsto R(\alpha,h)
\end{align}
we also have linearity of 
\begin{align}
    (\prob{P}_{\alpha,h}^{\RV{A}|\RV{X}})_{h\in H}\mapsto (R(\alpha,h))_{h\in H}
\end{align}
Furthermore, $Q$ is bounded when viewed as an element of $\mathbb{R}^{\Omega\times H\times C}$, and so $S$ is the linear image of a compact convex set, and is therefore also compact convex.
\end{proof}

\begin{lemma}\label{lem:linear}
For a statistical decision model $(\prob{P}_{C\times H}, \RV{X},\RV{Y},\RV{A},\RV{H})$ with utility $u:X\times Y\to \mathbb{R}$, the map
\begin{align}
    \prob{P}_{\alpha,h}^{\RV{A}|\RV{X}}\mapsto R(\alpha,h)
\end{align}
is linear.
\end{lemma}

\begin{proof}
By definition,
\begin{align}
    R(\alpha,h) &= - \prob{P}_{\alpha,h}^{\RV{XY}} u\\
    &= -\prob{P}_{C\times\{h\}}^{\RV{X}} \odot \prob{P}_{\alpha\times h}^{\RV{A}|\RV{X}} \odot \prob{P}_{C\times\{h\}}^{\RV{Y}|\RV{AX}} u
\end{align}
Which is a composition of kernel products involving $\prob{P}_{\alpha\times H}^{\RV{A}|\RV{X}}$, and kernel products are linear, hence this function is linear.
\end{proof}

The preceding theorem does \emph{not} hold for a utility defined on $\Omega$ rather than on $X\times Y$. In this case we have instead
\begin{align}
    -\prob{P}_{C\times\{h\}}^{\RV{X}} \odot \prob{P}_{\alpha\times h}^{\RV{A}|\RV{X}} \odot \prob{P}_{\alpha,h}^{\Omega|\RV{AX}} u
\end{align}
where $\alpha$ appears twice on the right hand side, rendering the map nonlinear.

\begin{lemma}\label{lem:all_kernels_is_convex_hull}
For finite $X$ and $A$, the set of all Markov kernels $X\kto A$ is convex closed.
\end{lemma}

\begin{proof}
From \citet{blackwell_theory_1979}, the set of all Markov kernels $X\kto A$ is the convex hull of the set of all deterministic Markov kernels $X\kto A$. There are a finite number of deterministic Markov kernels, and so the convex hull of this set is closed.
\end{proof}

\begin{example}\label{ex:cc_sdt}
Suppose we have a conditionally independent statistical decision model $(\prob{P}_{C}, \RV{X},\RV{Y},\RV{A},\RV{H})$ along with a bounded utility $u:Y\to \mathbb{R}$ where $H,A,X$ and $Y$ are all finite, and $\{\prob{P}_\alpha^{\RV{A}|\RV{X}}|\alpha\in C\}$ is the set of all Markov kernels $X\kto A$. Then the risk set is convex and closed downwards, and so the set of Bayes optimal choices is exactly the set of admissible choices.

The boundedness of the risk set $S$ follows from the boundedness of the utility $u$; if $u$ is bounded above by $k$, then $S$ is bounded below in every dimension by $-k$.

The fact that $S$ is convex and closed follows from Lemmas \ref{lem:convex_closed}, \ref{lem:linear} and \ref{lem:all_kernels_is_convex_hull}.
\end{example}

\begin{example}\label{ex:cc_nonsdt}
As before, but suppose we have the statistical decision model is not conditionally independent. Because none of the lemmas \ref{lem:convex_closed}, \ref{lem:linear} and \ref{lem:all_kernels_is_convex_hull} made use of the conditional independence assumption, the risk set is still convex and closed downwards and so the set of Bayes optimal choices is also exactly the set of admissible choices.
\end{example}

% \subsection{How should decision making models be constructed?}\label{sec:how_to_construct}

% The decision theories proposed by Jeffrey and Lewis differ in the kind of space in which they're defined and in the decision rule proposed. However, the reasons given by Lewis (and others) for proposing ``causal'' decision theory relate to their views about how decision models should be constructed.

% Consider a decision maker (DM) faced with a machine with two buttons, one red and one green, that costs \$51 to operate. The DM has observed a large number of people operate the machine previously, half have pushed the red button and half have pushed the green button. No matter which button was pressed, 60\% of the time the operator received \$100 from the machine, and 30\% of the time they received nothing. The DM is contemplating whether to insert \$51 and push the red button or the green button. Consider a second person, a forecaster, who has observed all of the machine operators so far, and wants to predict what the DM will decide to do.

% The decision maker constructs a see-do model (Definition \ref{def:see_do_model}) with observations being the previous operators, and hypothesizes that each button is associated with a payout probability in $[0,1]$ that might vary operator to operator. The consequences are payouts (0 or \$100) less the \$10 cost of operation if he chooses to play. The forecaster constructs a joint probability distribution over the entire sequence of operators and payouts, and, reasoning that every operator-machine pair is essentially the same, hypothesizes that each button is associated with a fixed payout probability $[0,1]$ for each button, and a probability of pressing the red button in $[0,1]$. Having seen a large number of people already, the forecaster concludes each button has a payout probability of 70\%, and each operator has a probability of pressing the red button of 50\%.

% The question is: does the decision maker adopt a model that agrees with the forecaster, such that he concludes that no matter which button he presses, the payout probability will be 70\% (in agreement with the forecaster), or does the decision maker adopt some model that induces a different map from choices to consequences?

% In this case, the decision maker may well want to adopt a different model. To the forecaster, the operator-machine pairs are indistinguishable in that the forecaster is no more knowledgeable about the internal state of the machine at any point, and also no more knowledgeable about the internal state of the operator's head at each point. Thus it makes little difference to them whether each button implements a random method to pay out 60\% of the time no matter what, or if each button has a deterministic payout at each episode and 20\% of operators know exactly what it is while the other 80\% choose one button at random.

% To the decision maker, however, some additional information is available. In particular, the decision maker knows they do not have any information that might tell them how the correct button differs in this instance to the correct button in any other instance. The decision maker does \emph{not} know if all of the other operators also lacked such information. It is relevant to the decision maker whether the button pays out according to some fixed probabilistic procedure each time, or if the payout is deterministically associated with a particular button each episode, and sometimes the operators know which button is correct, but sometimes they do not. In the first case, the decision make could assign a 60\% chance to each button paying out, while in the second case they might choose a different value (perhaps 50\%).

% So far, everyone agrees that the distribution over consequences the decision maker associated with each button need not agree with the forecaster's conditional distribution. The disagreement is in extreme cases. Suppose everything is as above, except the payout is seen to have been received 100\% of the time. The decision maker might reason that, while \emph{some} of the other operators might have access to additional information, it is unlikely that \emph{all} of them differ in this respect, and so the DM might then conclude that they should regard the probability of both buttons paying out as close to 100\%.

% Suppose additionally that it is proven to the decision maker that the payout associated with each button is known with certainty to an oracle, and furthermore that the oracle knows that in each instance one button pays out \$100 and the other pays out nothing. In this case, it is typically argued that the decision maker should 

%  Furthermore, this oracle has a good reason for why 100\% of the previous operators received a payout: the oracle 

%   that problem is to press one of two buttons,. They have some data, some choices they want to compare, and some possible consequences over which they have a utility defined. Consider a second person whose job is to forecast the choice of the decision maker, and the consequences that arise after their choice. Suppose also that the forecaster and the decision maker are given the same data ? Second, there is the problem faced by the forecaster: given the available data, what choice is the decision maker likely to arrive at, and what consequences are likely to occur as a result? All agree that the decision maker employs a probabilistic map from choices to consequences to help make the decision, and all agree that, if the forecaster expresses their forecast as a joint probability distribution, it is possible to derive a conditional distribution of consequences given choices, which is also a probabilistic map from choices to consequences (see Definition \ref{def:disint}). The disagreement is about exactly when the decision maker's consequence map should agree with the forecaster's conditional distribution.

% We don't offer a solution to this question. What we do in Chapters \ref{ch:evaluating_decisions} and \ref{ch:other_causal_frameworks} is to relate a certain notion of ``causal effect'' to a kind of symmetry of decision-making models. Causal effects are typically associated with the decision maker's perspective, while symmetry is a tool that forecasters may use to construct probabilistic models. 

\section{Conclusion}

We define ``decision making models'' as maps from a set of choices $C$ to distributions over a set of consequences $\Omega$. We suppose that decision making models are accompanied by a utility function that rates the desirability of each consequence, though we do not often explicitly consider the utility function. This general scheme is common to many theories of decision making.

We distinguish decision making models with choices only from decision making models with choices and consequences. The former are ``Bayesian'' models, with the consequences of each choice given by a unique probability distribution, while the latter are ``non-Bayesian''. Bayesian models with an expected utility induce a complete order on the choices -- each choice is either better, worse or just the same as another choice. On the other hand, non-Bayesian models induce a partial order, with admissible choices being better than inadmissible choices, but pairs of admissible choices are not known to be indifferent. The complete class theorem shows that any rule for selecting from the admissible choices can be rationalised as a rule for selecting Bayes-optimal choices with respect to \emph{some} prior, and we show that this theorem applies to statistical decision models equipped with a utility function if the set of hypotheses is finite and the induced risk function is convex and downward closed.

We introduce variables and measurement procedures as our understanding of how models correspond to ``real world decision problems''. Measurement procedures are typically in the background, and we don't explicitly discuss them. However, when we talk about ``observed variables'', we mean that there is a measurement procedure in the background, and an observed variable is a partial result of this procedure.