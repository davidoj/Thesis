%!TEX root = ../main.tex

\chapter{Causal modelling with decision models}\label{ch:other_causal_frameworks}

In previous chapters we've proposed a general type for decision models and examined a particular assumption -- the assumption of CIIR sequences -- that allows one to predict future consequences of actions based on previously observed data. We noted that this assumption is often unreasonable. In this chapter, we consider the problem of constructing more broadly applicable decision models that license some kind of causal inference. 

Our approach is first to examine what kinds of decision models are induced by causal Bayesian networks and potential outcomes models, two widely used frameworks for causal modelling. We then work backwards to propose two assumptions applicable to decision models based on regularities imposed by these frameworks. These assumptions are \emph{precedent} and \emph{individual level CIIR sequences} respectively.

When we try to work out what kind of decision models are expressed by causal Bayesian networks and potential outcomes, there are small gaps that need to be filled in with additional assumptions. Causal Bayesian networks are ``rolled up'' and an assumption must be made about how exactly to ``unroll'' it to a sequential model. Some of the additional assumptions that we regard to be required to ensure that the resulting unrolled model is faithful to the original intent of the rolled-up model are nontrivial. Potential outcomes models, on the other hand, do not offer a notion of ``options'', and a judgement must be made about how a potential outcomes model induces a collection of options and their associated consequences. These gaps are minor, but they do reflect the different commitments of the different approaches. These differences are summarised in the table below:
\begin{center}
\begin{tabular}{ |c|p{5cm}|p{5cm}| } 
 \hline
  & Must & Optionally  \\
 \hline
 Potential outcomes & Represent potential outcomes of every ``output'' variable & Represent consequences of choosing different options \\ 
 Causal Bayesian networks & Represent consequences of choosing different options for ``generic variables'', options are interventions & Specify exactly which variables are affected by the choice \\
 Decision models & Represent consequences of choosing different options & Options are interventions, represent potential outcomes of output variables\\
 \hline
\end{tabular}
\end{center}
Section \ref{sec:how_to_cbn} sets out the kinds of decision models induced by causal Bayesian networks, and Section \ref{sec:potential_outcomes} sets out the decision models induced by potential outcomes.

The assumption of \emph{precedent}, explored in Section \ref{sec:precedent}, is motivated by the general idea that in some cases a decision maker might guess that every option they have available has already been tried by someone, somewhere, though they do not know exactly when. On its own, it doesn't allow a decision maker to conclude very much (although it does allow them to surmise that, given enough data, the consequences of their choice must have occurred at least once). However, in conjunction with the assumption of \emph{generic relationships between conditionals}, it allows a decision maker to conclude from a conditional independence that a certain sequence of variable pairs features conditionally independent and identical responses. This result is similar in spirit to the principle underlying causal inference by invariant prediction \citep{peters_causal_2016}, which identifies causal parents on the basis of groups of variables that render a target variable conditionally independent of an ``environment'' variable. However, we offer substantially different assumptions -- instead of an assumption of generic relationships between conditionals, invariant prediction places restrictions on the kinds of ``interventions'' that can happen in each environment. The assumption of generic relationships between conditionals is itself similar to assumptions that have been used to justify inferences of causal direction \citep{meek_strong_1995,lemeire_replacing_2013}. Our result suggests that the assumption of precedent might facilitate understanding of the relationship between this notion of causal direction and invariance based causal inference without appealing to any notion of intervention. This is explored further in Section \ref{sec:precedent}

We also investigate alternative justifications of the assumption of CIIR sequences motivated by these causal modelling frameworks. In the previous chapter, we focussed on the equivalence of certain prediction problems. In Section \ref{sec:justifying} we consider that, under some conditions, CIIR sequences can be alternatively justified by considering how different options might induce symmetric consequences. We also consider how might treat potential outcomes as ``pseudo-observable'' variables that represent certain things that we are practically unable to measure, but that we nonetheless might impose constraints on as if they were observed variables with certain properties.

% Individual-level CIIR sequences are a weaker version of the assumption of CIIR sequences explored in Section \ref{sec:ilevel_ccontract}. Here, we suppose each element of the sequence has a unique identifier. Importantly, it must be possible for any element of the sequence to receive any identifier, which rules out (for example) identifying the first element with ``1'' and the second element with ``2'' and so forth. We then make the assumption that we can identify a CIIR sequence of (input, identifier)-output pairs -- where, we stress, \emph{the identifier is one of the ``inputs''}. This rules out the assumption that the sequence of (identifier, input) pairs has infinite support -- in fact, by assumption, each such pair can appear only once. 

% The assumption of individual level CIIR sequences implies that each possible value of the identifier is attached to a ``fixed but unknown'' response function. One can therefore view the assumption of individual level CIIR sequences as similar to the assumption of potential outcomes, where the ``potential outcomes'' are bound to the identifier variable. We also offer an alternative interpretation of this assumption as a kind of ``order independence''. In particular, we argue that this assumption may be reasonable if an investigator believes that the following experiments are essentially the same:
% \begin{itemize}
%     \item Run the experiment as normal
%     \item Run the experiment as normal to the point of determining the vector of (input, identifier) pairs but not the outputs, shuffle the vector of (input, identifier) pairs without altering which input goes with which identifier, then continue the experiment as normal 
% \end{itemize}
% That is, \emph{once we know how inputs and identifiers correspond}, the outputs do not further depend on the order in which they appear.

% In Theorem \ref{th:cc_ind_treat} we prove that, given the assumption of individual level CIIR sequences, an experiment in which we further accept that the identifiers themselves are exchangeable in the appropriate sense \emph{and} inputs are completely under the decision maker's control, then we also conclude that the input-output sequence (excluding the identifiers) is also related by a sequence of conditinally independent and identical responses. We argue that a number of previously informal arguments for ``interchangeability of patients'' implying \emph{ignorability} in potential outcomes models are captured by Theorem \ref{th:cc_ind_treat}.

\section{Causal Bayesian networks}\label{sec:how_to_cbn}

Causal Bayesian networks are a family of structural interventional models. In the form presented here, they provide a ``hard intervention'' operation, which offers a rule for transforming a joint probability distribution $\prob{P}^{\RV{V}}$ over a sequence of variables $\RV{V}$ according to a directed acyclic graph $\graph{G}$ with nodes $\node{V}$ corresponding 1-to-1 to the variables in $\RV{V}$. There are many variations of causal Bayesian networks with more general classes of intervention \citep{yang_characterizing_2018}.

Structural Causal Models (SCMs) are an alternative class of structural interventional models. Such models distinguish a set of noises $\RV{U}$ from observed variables $\RV{V}$, and provide a collection of functions relating the observed variables to the noises and other observed variables. The noises are allowed to be stochastic. Functional relationships can be interpreted as deterministic conditional probabilities, and it is in principle possible to formulate SCMs using our approach. However SCMs are often extended in ways that causal Bayesian networks are not, for example with counterfactual operations \citep{barenboim_foundations_2020}, or to include cyclic causal relationships \citep{bongers_theoretical_2016,forre_causal_2020}, and because addressing these extensions in detail is beyond the scope of this work, we focus on causal Bayesian networks.

In order to represent causal Bayesian networks as decision models, we have to make two generalisations. First, the variables that appear in a causal Bayesian network are ``generic variables''. This is different to the ``observed random variables'' discussed in Section \ref{sec:rvs_mps} which are associated with measurement procedures. Generic variables are instead associated with a \emph{type} of measurement procedure. We can have a measurement procedure to measure \emph{your} height, and a type of measurement procedure that measures people's height. A sequence of measurements of different people's heights could be associated with a single type of measurement procedure. To make predictions about observed random variables, we need to generalise causal Bayesian networks to sequential models -- this is the first generalisation.

Causal Bayesian networks are specified by a directed acyclic graph $\graph{G}$ and a probability distribution $\prob{P}$. However, implicit in their use is the fact that the probability distribution $\prob{P}$ and maybe even the graph $\graph{G}$ (or parts of it) should be learned from the given data. A decision model makes the dependence of the model on the data explicit via a hypothesis variable $\RV{H}$. A causal Bayesian network might be thought of as a representation of a hypothesis, rather than a decision model in its own right. Constructing decision models with hypotheses corresponding to causal Bayesian networks is the second generalisation we make.

\subsection{Definition of a Causal Bayesian Network}\label{sec:def_cbn}

We follow the definition of a Causal Bayesian Network on \citet[page ~23-24]{pearl_causality:_2009}. There are a couple of technical differences: we require that interventional models are a measurable map from interventions to probability distributions, and we assume that there is a common sample space for every interventional distribution. There are also some non-technical differences: the notation is adapted for compatibility with the rest of the work in this thesis, and we separate the definition into two parts for clarity (Definitions \ref{def:interventional} and \ref{def:CBN}). These changes don't make a meaningful difference to the content of the theory.

An interventional model is a \emph{Causal Bayesian Network} with respect to a directed acyclic graph if it satisfies a number of compatibility requirements. The following definitions are standard, and reproduced here for convenience. The definitions here are terse, readers should refer to \citet[chap. ~1]{pearl_causality:_2009} for a more intuitive explanation.

\begin{definition}[Directed graph]\label{def:dir_graph}
A directed graph $\graph{G}=(\node{V},\node{E})$ is a set of nodes $\node{V}$ and edges, which are ordered pairs of nodes $\node{E}\subset \node{V}\times\node{V}$. Nodes are written using the font $\node{V}$.
\end{definition}

The parents of a target node are all nodes with an edge ending at the target node.

\begin{definition}[Parents]
Given a directed graph $\graph{G}=(\node{V},\node{E})$ and $\node{V}_i\subset \node{V}$, the parents of $\node{V}_i$ are $\PA{\graph{G}}{\node{V}_i}:=\{\node{V}_j\in \node{V}|(\node{V}_j,\node{V}_i)\in\node{E}\}$.
\end{definition}

We offer a recursive definition of \emph{descendants}.

\begin{definition}[Non-descendants]
Given a directed graph $\graph{G}=(\node{V},\node{E})$, a descendant of $\node{V}_i$ is a node $\node{V}_j$ such that $\node{V}_i$ is a parent of $\RV{V}_j$, or there is some parent $\node{V}_k$ of $\node{V}_j$ that is a descendant of $\node{V}_i$. Any node that is a not a descendant of $\node{V}_i$ is a non-descendant of $\node{V}_i$, and the set of non-descendants of $\node{V}_i$ is denoted $\mathrm{ND}({\node{V}_i})$
\end{definition}

A path is a sequence of edges such that the $i$th edge and the $i+1$th edge share exactly one node.

\begin{definition}[Path]
Given a directed graph $\graph{G}=(\node{V},\node{E})$, a path is a sequence of edges $(E_i)_{i\in A}$ (where $A$ is either $[n]$ or $\mathbb{N}$) such that for any $i$, $E_i$ and $E_{i+1}$ share exactly one node.
\end{definition}

A directed path is a sequence of edges such that the end of the $i$th edge is the beginning of the $i+1$th edge.

\begin{definition}[Directed path]
Given a directed graph $\graph{G}=(\node{V},\node{E})$, a directed path is a sequence of edges $(E_i)_{i\in A}$ (where $A$ is either $[n]$ or $\mathbb{N}$) such that for any $i$, $E_i=(\node{V}_k,\node{V}_l)$ implies $E_{i+1}=(\node{V}_l,\node{V}_m)$ for some $\node{V}_m\in \node{V}$.
\end{definition}

In an acyclic graph, directed paths never reach to the same node more than once.

\begin{definition}[Directed acyclic graph]
A directed graph $\graph{G}=(\node{V},\node{E})$ is acyclic if, for every path, each node appears at most once. Directed acyclic graph is abbreviated to ``DAG''.
\end{definition}

d-separation is a key property of directed acyclic graphs for defining causal Bayesian networks. It is defined with respect to undirected paths.

\begin{definition}[Blocked path]
Given a DAG $\graph{G}=(\node{V},\node{E})$, a path $p$ is blocked by $\node{V}_A\subset\node{V}$ iff
\begin{enumerate}
    \item $(\node{V}_i,\node{V}_j)\in p$ and $(\node{V}_j,\node{V}_k)\in p$ for all $\node{V}_j\in \node{V}_A$
    \item $(\node{V}_j,\node{V}_i)\in p$ and $(\node{V}_j,\node{V}_k)\in p$ for all $\node{V}_j\in \node{V}_A$
    \item $(\node{V}_i,\node{V}_j)\in p$ and $(\node{V}_k,\node{V}_j)\in p$ for all $\node{V}_j\cup \DE(\node{V}_j)\cap \node{V}_A=\emptyset$
\end{enumerate}
\end{definition}

\begin{definition}[d-separation]
Given a DAG $\graph{G}=(\node{V},\node{E})$, $\node{V}_A$ is $d$-separated from $\node{V}_B$ by $\node{V}_C$ (all subsets of \node{V}) if $\node{V}_C$ blocks every path starting at $\node{V}_A$ and ending at $\node{V}_B$. This is written $\node{V}_A\perp_{\mathcal{G}} \node{V}_B | \node{V}_C$.
\end{definition}

\begin{definition}[Variable-node association]
Given a graph $\graph{G}=(\node{V},\node{E})$ and a sequence of variables $\RV{V}_A:=(\RV{V}_i)_{i\in A}$, if $|A|=|\node{V}|$ we can associate a variable with each node of the graph with an invertible map $m:\{\RV{V}_i|i\in A\}\to \node{V}$ that sends $\RV{V}_i\mapsto \node{V})_i$. By convention, we give associated variables and nodes corresponding indices, and graphical operations are defined on variables through $m$, i.e. $\mathrm{Pa}(\RV{V}_i):=m(\mathrm{Pa}(m^{-1}(\RV{V}_i)))$.
\end{definition}

\begin{definition}[Compatibility]\label{def:compat}
Given a measurable space $(\Omega,\sigalg{F})$, a Markov kernel $\prob{P}_\cdot:C\kto \Omega$ and a sequence of variables $(\RV{V}_i)_{i\in A}$ with $\RV{V}_i:\Omega\to V_i$ and a DAG $\mathcal{G}$ with nodes $\{\node{V}_i\}_{i\in A}$ and the variable-node association $m$, $\prob{P}_\cdot$ is compatible with $\mathcal{G}$ relative to $m$ if for all $I,J,K\subset A$, $\node{V}_I\perp_{\mathcal{G}} \node{V}_J | \node{V}_K$ implies $\RV{V}_I\CI^e_{\prob{P}_\cdot} \RV{V}_J | (\RV{V}_K,\text{id}_C)$.
\end{definition}

The following definition is reproduced from \citet{pearl_causality:_2009} with the differences previously mentioned: notation has been matched to ours, the interventional model is assumed to be measurable and the interventional distributions are assumed to be defined on a common sample space.

\begin{definition}[Interventional model]\label{def:interventional}
An interventional model is a tuple $(\prob{P}_C,\Omega,(\RV{V}_A)$ where $(\Omega,\sigalg{F})$ is a measurable space,  $\RV{V}:=(\RV{V}_i)_{i\in A}$, $A\subset\mathbb{N}$ a sequence of variables with $\RV{V}_i:\Omega\to V_i$, and where the option set $C$ given by
\begin{align}
    C:=\{\mathrm{do}_{\emptyset}\}\cup \{(\mathrm{do}_B,v_B)|B\subset A,v_B\in \mathrm{Range}(V_B)\}
\end{align}
That is, we take every subsequence $\RV{V}_B$ of $\RV{V}_A$ and for each subsequence add to $C$ every element of the range of $\RV{V}_B$, each labeled with the symbol $\mathrm{do}_B$.
\end{definition}

\begin{definition}[Causal Bayesian network]\label{def:CBN}
Given an interventional model $(\prob{P}_C,\Omega,\RV{V}_A)$ and a directed acyclic graph $\graph{G}$ with nodes $\node{V}$, $(\prob{P}_C,\Omega,\RV{V}_A,\graph{G})$  is a \emph{causal Bayesian network} with respect the node-variable association $m$ if:
\begin{enumerate}
    \item $\prob{P}_\cdot$ is compatible with $\graph{G}$ with respect to $m$
    \item $B\neq \emptyset \implies \prob{P}_{(\mathrm{do}_{B},v_B)}^{\RV{V}_B} = \delta_{v_B}$
    \item $\prob{P}_{(\mathrm{do}_{B},v_B)}^{\RV{V}_i|\mathrm{Pa}(\RV{V}_i)}\overset{\prob{P}_{(\mathrm{do}_{B},v_B)}}{\cong}\prob{P}_{\mathrm{do}_{\emptyset}}^{\RV{V}_i|\mathrm{Pa}(\RV{V}_i)}$ for all $i\not\in B$
\end{enumerate}
\end{definition}

This definition of a causal Bayesian network is agnostic about how one actually goes about constructing a model of this type. As a practical matter, if in the course of trying to to construct a causal Bayesian network one selects a set of variables $\RV{V}_A$ because they are convenient or for some other reason that's not downstream of the chosen modelling, one has to be careful the chosen variables $\RV{V}_A$ are ``interventionally compatible''. In particular, we require $v_{\mathrm{ND}(\RV{V}_i)}\mapsto \delta_{v_i}$ be a $\RV{V}_i|\mathrm{ND}(\RV{V}_i)$-valid conditional for all $i$ (Definition \ref{def:valid_conditional_prob}, or else the probability set for some interventions on $\RV{V}_i$ may be empty. For a contrived example, the sequence $(\RV{V}_i, 2*\RV{V}_i)$ is not interventionally compatible, as at least one variable must be a non-descendant of the other, but it is not possible to set the value of one independently of the other. See also the discussion of body mass index in Example \ref{ex:invalidity} for a real world example arising from a failure to perform this check.

For continuously valued variables, the fact that this definition offers the ability to pick a version of the conditional probability for each intervention is problematic. Suppose $\node{V}_i$ is a parent of $\node{V}_j$, and the associated variable $\RV{V}_i$ is continuously valued and $\prob{P}_{\mathrm{do}_{\emptyset}}^{\RV{V}_i}(\{v_i\})=0$ for all singletons $v_i\in V_i$. Then for every intervention $\mathrm{do}_{\{i\}}(v_i)$, we can choose a version of $\prob{P}_{\mathrm{do}_{\emptyset}}^{\RV{V}_j|\RV{V}_i}$ that takes an arbitrary value at the point $v_i$ (because this point has measure 0), so property (3) is satisfied trivially. Some additional consistency condition seems to be required for this case, but we do not explore what it is here.

The freedom to choose versions of the conditional distributions where the ``passive'' distribution has no support might actually be a feature that distinguishes causal Bayesian networks from SCMs, but we don't investigate this point in detail.

\subsection{Unrolled causal Bayesian networks}\label{sec:unrolling}

Given a probability space $(\prob{P},\Omega,\sigalg{F})$ and an independent and identically distributed (IID) sequence $\RV{X}:=(\RV{X}_i)_{i\in [n]}$, it is common to ``roll up'' the joint distribution $\prob{P}^{\RV{X}}\in\Delta(X^n)$ to a single representative distribution $\prob{P}^{\RV{X}_0}\in \Delta(X)$ and say something like ``the $\RV{X}_i$ are IID according to $\prob{P}^{\RV{X}_0}$''. Because of the IID assumption, the full joint distribution $\prob{P}^{\RV{X}}\in\Delta(X^n)$ can be unambiguously reconstructed from a statement like this.

A causal Bayesian network is similarly a rolled-up representation of a model of some sequence of variables. Unlike an IID sequence, it isn't completely unambiguous how to unroll it, though the ambiguity doesn't seem to be especially problematic for ordinary causal Bayesian networks. In Definition \ref{def:unr_CBN}, we propose a canonical form of an unrolled causal Bayesian network. 

\begin{definition}[Unrolled causal Bayesian network]\label{def:unr_CBN}
Given an interventional model $(\prob{P}_\cdot,C,\Omega,\RV{V}_{A\times \mathbb{N}})$ and a directed acyclic graph $\graph{G}$ with nodes $\node{V}_A$, $(\prob{P}_\cdot,C,\Omega,\RV{V}_{A\times \mathbb{N}})$  is an \emph{unrolled causal Bayesian network} with respect to the node-variable association maps $m_j:\RV{V}_{ij}\mapsto \node{V}_i$ if, for all $j,k\in \mathbb{N}$:
\begin{itemize}
    \item [1*] $\prob{P}_\cdot^{\RV{V}_{Aj}}$ is compatible with $\graph{G}$ with respect to $m_j$ for all $j\in \mathbb{N}$
    \item [2*] $\pi_j(\alpha) = (\mathrm{do}_{B_j},v_{B_j})$ and $B_j\neq \emptyset$ implies $\prob{P}_{(\mathrm{do}_{B_j},v_{B_j})}^{\RV{V}_{Bj}} = \delta_{v_{B_j}}$
    \item [3*] If $\pi_j(\alpha) = (\mathrm{do}_{B_j},v_{B_j})$, $\pi_k(\alpha) = (\mathrm{do}_{B_k},v_{B_k})$ and $i\not \in B_j\cup B_k$ then $\prob{P}_{\alpha}^{\RV{V}_{ij}|\mathrm{Pa}(\RV{V}_{ij})}\overset{\prob{P}_{\alpha}}{\cong}\prob{P}_{\mathrm{do}_{\emptyset}}{\RV{V}_{ik}|\mathrm{Pa}(\RV{V}_{ik})}$
    \item [4*] $\pi_j(\alpha)=\pi_j(\alpha')$ implies $\prob{P}_\alpha^{\RV{V}_{Aj}}= \prob{P}_{\alpha'}^{\RV{V_{Aj}}}$
    \item [5*] $\RV{V}_{Aj}\CI^e_{\prob{P}_{C}} \RV{V}_{A \mathbb{N}\setminus\{j\}} |\text{id}_C$
\end{itemize}
\end{definition}

\subsubsection[Unrolled CBNs]{Explaining the definition of unrolled causal Bayesian networks}

Suppose we have a causal Bayesian network $(\prob{P}_C,\Omega,\RV{V}_A)$ that we want to extend to a sequence of variables $\RV{V}:= (\RV{V}_{ij})_{i\in A,j\in \mathbb{N}}$. We need to propose a set of interventions for the unrolled model, and ensure that we have appropriate analogues of all of the causal Bayesian network compatibility conditions that hold for each element of the sequence. There is a little ambiguity in the choice of an extended set of interventions (though it may not be problematic in practice). Unrolled causal Bayesian networks have not been defined in the literature, and there are other formal ambiguities -- for example, we could omit condition 4*, but omitting this condition would allow for unrolled models that roll up to different causal Bayesian networks depending on the precise extended intervention chosen, which clashes with our understanding of what a causal Bayesian network is ``supposed'' to model.

First, we extend the set $C$ to be the set of sequences of interventions 
\begin{align}
\{(\mathrm{do}_{B_j j}(v_B))_{j\in \mathbb{N}}|\forall j: B_j \subset A,v_{B_j}\in \mathrm{Range}(\RV{V}_{B_j})\}
\end{align}
i.e. $C$ now consists of all sequences of separate interventions to each subsequence of variables $\RV{V}_{Aj}:=(\RV{V}_{ij})_{i\in A}$, understood to refer to variables arising from a particular iteration of the decision procedure. 

This specification of interventions in an unrolled causal Bayesian network differs slightly from the method explored in \citet{lattimore_causal_2019} which unrolls a causal Bayesian network to a length 2 sequence and forces the intervention on the first element of the sequence to be the passive intervention. This difference seems like it would usually be unproblematic insofar as the specification of which elements of the sequence might be influenced by the choices of the decision maker may often be pretty obvious.

Given a graph $\graph{G}=(\node{V},\node{E})$, we now have a collection of variable-node association maps $m_j:\{\RV{V}_{ij}|i\in A\}\to \node{V}$ such that $m_j(\RV{V}_{ij})=\node{V}_i$.

We now need to specify how variables in an unrolled causal Bayesian network are distributed, given some sequence of interventions. By analogy with the original case of IID variables, we conclude that the $\RV{V}_{Aj}:=(\RV{V}_{ij})_{i\in A}$ are mutually independent given any particular sequence of interventions. Furthermore, Definition \ref{def:CBN} constrains the distribution of each variable given a particular sequence of interventions from $C$. For a sequence of interventions $\alpha\in C$, let $\pi_j(\alpha)$ be the $j$th intervention in the sequence. One might posit the following analogue of condition (3): 
\begin{itemize}
    \item [3'] $\pi_j(\alpha)=(\mathrm{do}_{B_j},v_{B_j})$ implies $\prob{P}_{\alpha}^{\RV{V}_{ij}|\mathrm{Pa}(\RV{V}_{ij})}\overset{\prob{P}_{\alpha}}{\cong}\prob{P}_{\mathrm{do}_{\emptyset}}^{\RV{V}_{i1}|\mathrm{Pa}(\RV{V}_{i1})}$ for all $i\not\in B$
\end{itemize}
Where $\mathrm{do}_{\emptyset}^n$ is a sequence of $n$ $\mathrm{do}_{\emptyset}$ interventions. This is a combination of an assumption that variables in the sequence are conditionally independent and identically distributed given appropriate interventions and condition (3) from Definition \ref{def:CBN}. However, it's not quite satisfactory. Take $B_j:=\mathrm{Pa}(\RV{V}_{ij})$, and suppose $\prob{P}_{\mathrm{do}_{\emptyset}^n}^{\RV{B}_{1}}(\{x\})=0$. Then (3') would be satisfied by a model for which
\begin{align}
    \prob{P}_{(\mathrm{do}_{B_1},x,\mathrm{do}_{B_2},x)}^{\RV{V}_{i1}|\RV{V}_{B_1}}(U|x) &= \delta_0(U)\\
    \prob{P}_{(\mathrm{do}_{B_1},x,\mathrm{do}_{B_2},x)}^{\RV{V}_{i2}|\RV{V}_{B_2}}(U|x) &= \delta_1(U)
\end{align}
that is, if the empty intervention is unsupported over some element of the range of a variable, then (3') allows models that assign different consequences to repetitions of the same intervention on this variable, if those interventions force the variable into the region that originally had no support.

We propose instead a restricted assumption of identical response functions: for any pair $\RV{V}_{ij}$ and $\RV{V}_{ik}$, unless $i$ is intervened on by $\pi_j(\alpha)$ and not intervened on by $\pi_{k}(\alpha)$, then then the conditional probability of $\RV{V}_{ij}$ given its parents is almost surely equal (with respect to $\prob{P}_\alpha$) to the conditional probability of $\RV{V}_{ik}$ given its parents. This is condition 3*.

In order to be able to ``roll up'' a sequence of interventions, we also require that the response to the $j$th intervention does not depend on any of the interventions other than the $j$th. If this were not the case, then even if the restricted assumption of identical response functions were satisfied, different sequences of interventions might ``roll up'' to different interventional models. In particular, consider $\prob{P}_{\mathrm{do}_{\emptyset}^n}^{\RV{B}_{1}}(\{x\})=0$ again, $\RV{B}_j$ as before. Then we might have, consistently with 1*-3*,
\begin{align}
    \prob{P}_{(\mathrm{do}_{B_1},x,\mathrm{do}_{B_2},y)}^{\RV{V}_{i1}|\RV{V}_{B_1}}(U|x) &= \delta_0(U)\\
    \prob{P}_{(\mathrm{do}_{B_1},x,\mathrm{do}_{B_2},y')}^{\RV{V}_{i1}|\RV{V}_{B_1}}(U|x) &= \delta_1(U)
\end{align}
There is some freedom in choosing the conditional distribution of $\RV{V}_i$ given its parents because it has no support under the passive intervention, and without 4* this freedom allows us to make different choices when we intervene in different ways on unrelated elements of the sequence.

Condition 5* is the requirement that observations are mutually independent.

\subsection{Causal Bayesian networks with uncertain joint distributions}

Condition 3* of Definition \ref{def:unr_CBN} establishes that in a causal Bayesian network we can identify some input and output sequences with identical responses, with exactly which ones we can identify depending on the interventions chosen. In Chapter \ref{ch:evaluating_decisions}, we considered response functions that were identical \emph{conditional on some hypothesis} $\RV{H}$. The joint distribution $\prob{P}$ that appears in the previous definition of a causal Bayesian network is implicitly understood to be unknown. We can make this explicit by adding a hypothesis $\RV{T}$ and requiring that the causal Bayesian network properties hold for each value of $\RV{T}$. We could have $\RV{T}$ index both joint distributions and directed graphs, but here we will consider the case where the graph is known in advance and only the joint distribution is not.

\begin{definition}[Uncertain unrolled causal Bayesian network]\label{def:unc_unr_cbn}
Given an interventional model $(\prob{P}_\cdot,C,\Omega,(\RV{V}_{ij})_{i\in A,j\in \mathbb{N}})$ and a directed acyclic graph $\graph{G}$, $(\prob{P}_\cdot,C,\Omega,(\RV{V}_{ij})_{i\in A,j\in \mathbb{N}},\RV{T},\graph{G})$ is an \emph{uncertain unrolled causal Bayesian network} with respect to some hypothesis $\RV{T}:\Omega\to T$ if for each $h\in H$, defining $\prob{P}_{\cdot,t}:= \alpha\mapsto \prob{P}_\alpha^{\mathrm{id}_{\Omega}|\RV{T}}(\cdot|t)$, $(\prob{P}_{\cdot,t},C,\Omega,(\RV{V}_i)_{i\in A},\graph{G})$ is an unrolled causal Bayesian network.
\end{definition}

Recalling the discussion in Section \ref{sec:probability_set_models}, Definition \ref{def:unc_unr_cbn} associates each intervention with a unique probability distribution. One could suggest that uncertain unrolled causal Bayesian networks should therefore be called ``Bayesian causal Bayesian networks''. We could also consider models with a non-stochastic hypothesis $\RV{H}$ which we might call ``non-Bayesian causal Bayesian networks''.

An uncertain unrolled causal Bayesian network \emph{almost} features a number of CIIR input-output sequences. Due to 3*, such a model features conditionally independent and identical response functions with inputs $\mathrm{Pa}(\RV{V}_i)$ and outputs $\RV{V}_i$ wherever $\alpha$ consists of a sequence of interventions none of which target $\RV{V}_{ij}$ for any $j$. This leads us to the key result of this section: considering a subset of the interventions in $C$, an uncertain unrolled causal Bayesian network is IO contractible (with respect to some parameter $\RV{H}$) by application of Theorem \ref{th:ciid_rep_kernel}.

\begin{theorem}[IO contractibility of CBNs]\label{th:causal_contractibility_cbn}
Given an uncertain unrolled causal Bayesian network $(\prob{P}_\cdot,C,\Omega,(\RV{V}_{ij})_{i\in A,j\in \mathbb{N}},\RV{H},\graph{G})$, take $C'\subset C$ to be sequences of interventions that, for some $i\in A$, do not target a particular $\RV{V}_{ij}$ for any $j\in \mathbb{N}$ and ensure every sequence $\RV{V}_{j\mathbb{N}}$ has infinite support. Then $\RV{V}_{i\mathbb{N}}\CI^e_{\prob{P}_{C'}}\text{id}_C|(\RV{H}_i,\mathrm{Pa}(\RV{V}_{i \mathbb{N}}))$ and $\prob{P}_C^{\RV{V}_{i\mathbb{N}}|\RV{H}_i\mathrm{Pa}(\RV{V}_{i \mathbb{N}})}$ is IO contractible over $\RV{H}_i$ where $\RV{H}_i$ is the directing random conditional with respect to $(\prob{P}_C,\mathrm{Pa}(\RV{V}_{i \mathbb{N}}),\RV{V}_{i\mathbb{N}})$. 
\end{theorem}

\begin{proof}
Appendix \ref{sec:cbn_proofs}.
\end{proof}

\subsection[Probabilistic Graphical Models]{Probabilistic Graphical Models}

\citet{lattimore_replacing_2019,lattimore_causal_2019} have demonstrated how to ``unroll'' causal Bayesian networks into what they call ``Probabilistic Graphical Models''. Their construction is very similar to ours, as we will show here, and for readers interested in how identifiability results from regular causal Bayesian networks translate to our scheme, Lattimore and Rohde's work provides several examples.

Precisely, a probabilistic graphical model is a map $\prob{P}_\cdot$ from the set of single-node interventions $C$ to probability distributions $\prob{P}_\alpha$ defined on $(\Omega,\sigalg{F})$. A probabilistic graphical model is typically associated with a causal Bayesian network $(\prob{Q}_\cdot,C,\Omega',(\RV{V}_i)_{i\in A},\graph{G})$ where, for each $\RV{V}_i:\Omega\to V_i$ in the original causal Bayesian network, two variables $\RV{V}_i$ and $\RV{V}_i^*$ are defined on $(\Omega,\sigalg{F})$. 

The probabilistic graphical model also adds a ``parameter'' $\RV{W}_i$ for each variable pair $(\RV{V}_i,\RV{V}_i^*)$ such that, taking $C'$ to be interventions not targeting $\RV{V}_i^*$, for any $\alpha\in C'$, $\prob{P}_\alpha^{\RV{V}_i|\RV{W}_i\mathrm{Pa}(\RV{V}_i)}=\prob{P}_\alpha^{\RV{V}_i^*|\RV{W}_i\mathrm{Pa}(\RV{V}^*_i)}$ and $\RV{V}_i\CI^e_{\prob{P}_{C'}} (\RV{V}^*_A,\text{id}_C)|(\RV{W}_i)$ (where parents are assessed relative to the graph $\graph{G}$). $\RV{W}_i$ serves precisely the same role as $\RV{H}_i$ in Theorem \ref{th:causal_contractibility_cbn}, except it is not defined in terms of the directing random conditional.

A depiction of probabilistic graphical models and uncertain unrolled causal Bayesian networks using string diagrams gives some intuition regarding the structure of these different types of models, as well as some of the ``off-page'' assumptions of ordinary causal Bayesian networks.

Here is the original graph $\graph{G}$ associated with $(\prob{Q}_\cdot,C,\Omega',(\RV{V}_i)_{i\in A},\graph{G})$:

\begin{align}
    \tikzfig{cbn_example_cgm}
\end{align}

Here is the probabilistic graphical model associated with the intervention $(\mathrm{do}_2,v_2)$

\begin{align}
    &\phantom{=} \prob{P}_\alpha^{\RV{V}\RV{V}^*}\\
    &=\tikzfig{cgm}
\end{align}

and here is the uncertain unrolled CBN associated with the restricted set of interventions $C'$ that consists of, for each element of the sequence, either the empty intervention or some intervention targeting $\RV{V}_2$

\begin{align}
    &\phantom{=}\prob{P}_\alpha^{\RV{V}_{[1,3][n]}}\\
     &=\tikzfig{cbn_unrolled_example}
\end{align}

where 

\begin{align}
    \prob{P}_\alpha^{\RV{V}_{2i}|\RV{V}_{1i}\RV{H}} &= \begin{cases}
        \delta_{v} & \pi_i(\alpha)=(\mathrm{do}_2,v)\\
        \prob{P}_{mathrm{do}_{\emptyset}}^{\RV{V}_{2i}|\RV{V}_{1i}\RV{H}} & \text{otherwise}
    \end{cases}
\end{align}

\subsection{Unobserved confounders and precedent}\label{sec:precedent}

As we have pointed out, the assumption of CIIR sequences is often unreasonable. Causal Bayesian networks ``almost'' make this assumption with respect to inputs $\text{Pa}(\RV{V}_{i\mathbb{N}})$ and outputs $\RV{V}_{i\mathbb{N}}$. One of the ways that this appraoch gets around the fact that the assumption is unreasonable is to assume that some elements of $\text{Pa}(\RV{V}_{i\mathbb{N}})$ are unobserved. In this case, the ``interchangeable conditioning sequences'' are never actually observed, and so the question of whether or not they are interchangeable is moot.

We are limiting our attention to data-independent models (recall Definition \ref{def:weak_di}), which means that unobservability of some variable does not have any implications within the model -- only that it isn't attached to a measurement procedure. If we were considering a data-dependent variation of causal Bayesian networks, the fact that $\RV{V}_{1\mathbb{N}}$ is unobserved may have implications within the model -- for example, that input $\RV{D}_i$ may not depend on $\RV{V}_{1j}$ for any $j$.

Unobserved variables in the set of parents of a particular variable of interest may be called \emph{unobserved confounders}\footnote{To be a confounder, we also require an observed parent to be a descendant of the unobserved parent, but this detail isn't important for this discussion}. Suppose we have an uncertain unrolled causal Bayesian network $(\prob{P}_\cdot,C,\Omega,(\RV{V}_{ij})_{i\in [3],j\in \mathbb{N}},\RV{H},\graph{G})$ where the graph $\graph{G}$ is as follows:
\begin{align}
    \tikzfig{cbn_example_cgm}
\end{align}
and we consider the subset $C'\subset C$ of interventions that are either empty or target $\RV{V}_2$ only. We note that Theorem \ref{th:causal_contractibility_cbn} implies that $\prob{P}_C^{\RV{V}_{3\mathbb{N}}|\RV{H}_3\RV{V}_{1\mathbb{N}}\RV{V}_{2[n]}}$ is IO contractible, but not $\prob{P}_C^{\RV{V}_{3\mathbb{N}}|\RV{H}_3\RV{V}_{2\mathbb{N}}}$. We declare that $\RV{V}_{1[n]}$ is not observed -- that is, it is not associated with a measurement procedure.

We observe in Theorem \ref{th:condit_exchange} that the IO contractibility of $\prob{P}_C^{\RV{V}_{3\mathbb{N}}|\RV{H}\RV{V}_{1\mathbb{N}}\RV{V}_{2\mathbb{N}}}$ implies that $\prob{P}_C^{\RV{V}_{3\mathbb{N}}|\RV{H}\RV{V}_{2\mathbb{N}}}$ is unchanged by any swaps that leave the value of the full vector $\RV{V}_{1\mathbb{N}}$ unchanged. That is, the assumption of an unobserved confounder in this case implies a kind of ``partial IO contractibility''.

\begin{theorem}\label{th:condit_exchange}
Given $(\prob{P}_\cdot,C,\Omega,(\RV{V}_{ij})_{i\in [3],j\in \mathbb{N}},\RV{H},\graph{G})$ with $\prob{P}_C^{\RV{V}_{3\mathbb{N}}|\RV{H}_3\RV{V}_{1\mathbb{N}}\RV{V}_{2\mathbb{N}}}$ IO contractible over $\RV{H}_3$, $V_i$ discrete for all $i\in [3]$ and $(\RV{V}_{1\mathbb{N}},\RV{V}_{2\mathbb{N}})$ infinitely supported over $\RV{H}_3$, let $\RV{Q}:\Omega\to \mathbb{N}^\mathbb{N}$ be a random finite permutation of $\mathbb{N}$ such that $\RV{V}_{1\mathbb{N}}\overset{\prob{P}_C}{\cong}\RV{V}_{1\RV{Q}(\mathbb{N})}$. Then
\begin{align}
    \prob{P}_C^{\RV{V}_{3\mathbb{N}}|\RV{H}_3\RV{V}_{2\mathbb{N}}} &\overset{\prob{P}_C}{\cong} \prob{P}_C^{\RV{V}_{3\RV{Q}(\mathbb{N})}|\RV{H}_3\RV{V}_{2\RV{Q}(\mathbb{N})}}
\end{align}
\end{theorem}

\begin{proof}
By IO contractibility of $\prob{P}_C^{\RV{V}_{3\mathbb{N}}|\RV{H}\RV{V}_{1\mathbb{N}}\RV{V}_{2\mathbb{N}}}$ over $\RV{H}_3$
\begin{align}
    \prob{P}_C^{\RV{V}_{3\mathbb{N}}|\RV{H}_3\RV{V}_{1\mathbb{N}}\RV{V}_{2\mathbb{N}}} &= \prob{P}_C^{\RV{V}_{3\RV{Q}(\mathbb{N})}|\RV{H}_3\RV{V}_{1\RV{Q}(\mathbb{N})}\RV{V}_{2\RV{Q}(\mathbb{N})}}\\
    &= \prob{P}_C^{\RV{V}_{3\RV{Q}(\mathbb{N})}|\RV{H}_3\RV{V}_{1\mathbb{N}}\RV{V}_{2\RV{Q}(\mathbb{N})}}
\end{align}
Thus
\begin{align}
    &\prob{P}_C^{\RV{V}_{3\mathbb{N}}|\RV{H}\RV{V}_{2\mathbb{N}}}\\
     &= \tikzfig{contractible_over_hidden}\\
     &= \tikzfig{contractible_over_hidden_2}\\
     &= \tikzfig{contractible_over_hidden_3}\\
     &= \prob{P}_C^{\RV{V}_{3\RV{Q}(\mathbb{N})}|\RV{H}\RV{V}_{2\RV{Q}(\mathbb{N})}}
\end{align}
\end{proof}

Theorem \ref{th:condit_exchange} says that IO contractibility of $\prob{P}_C^{\RV{V}_{3\mathbb{N}}|\RV{H}\RV{V}_{1\mathbb{N}}\RV{V}_{2\mathbb{N}}}$ implies the ``exchange commutativity'' of $\prob{P}_C^{\RV{V}_{3\mathbb{N}}|\RV{H}\RV{V}_{2\mathbb{N}}}$ with respect to the random permutation $\RV{Q}$ of the indices of $\mathbb{N}$ -- in fact, for all such random permutations that preserve $\RV{V}_{1\mathbb{N}}$. Note that $\RV{Q}$ generally cannot be deduced from observations, because $\RV{V}_{1\mathbb{N}}$ cannot be deduced from observations. That is, $\RV{V}_{1\mathbb{N}}$ determines some subset of indices that are interchangeable, even though not \emph{all} indices are interchangeable. 

Suppose $\RV{V}_{[3]n}$ is the result of some intervention targeting $\RV{V}_{2n}$, and $\RV{V}_{[3]\{n\}^\complement}$ are all the result of passive observation. We can interpret Theorem \ref{th:condit_exchange} as expressing the idea that there is some (unknown) random subset of $\{n\}^\complement$ that is ``just like'' the intervened sample $\RV{V}_{[3]n}$. We could say that this random subset is a \emph{precedent} for the intervention.

Thus, using causal Bayesian networks we can express the idea that some intervention has precedent in the observed data, and indeed such an assumption appears to be implicit in the notion of unobserved parents in a causal Bayesian network. This raises the question: do we \emph{need} causal Bayesian networks to express this notion? The sentiment of this assumption long predates formal theories of causation. In fact, we can find it in the Bible:
\begin{quote}
Everything that happens has happened before; nothing is new, nothing under the sun. Ecclesiastes 1:9, Contemporary English Version
\end{quote}

We offer a simple definition of precedent. The idea is that we're given some sequence $\RV{Z}:=(\RV{Z}_i)_{i\in\mathbb{N}}$ and our choice influences $\RV{Z}_n$ and, no matter what we do, there is some subsequence $\RV{Z}_A$ of $\RV{Z}_{\{n\}^\complement}$ such that $\RV{Z}_{A\cup\{n\}}$ is exchangeable ($A$ may, in general, depend on the option chosen). We propose the following properties to precisely characterise precedent:

\begin{definition}[Precedent]\label{def:preemption}
A decision model $(\prob{P}_\cdot,(\Omega,\sigalg{F}),(C,\sigalg{C}))$ along with a sequence of variables $\RV{Z}:=(\RV{Z}_i)_{i\in\mathbb{N}}$ is $n$-\emph{precedented} if
\begin{enumerate}
    \item The sample space can be extended with a discrete variable $\RV{T}:=(\RV{T}_i)_{i\in\mathbb{N}}$ such that for every $\alpha\in C$, each $\RV{T}_i$ indexes an exchangeable subsequence to which the corresponding $\RV{Z}_i$ belongs; i.e. $\prob{P}_\alpha^{\RV{Z}|\RV{T}}$ is exchange commutative for all $\alpha\in C$ (over $*$)
    \item Neither $\RV{Z}_{\{n\}^\complement}$ nor $\RV{T}_{\{n\}^\complement}$ depend on the choice $\alpha$; $(\RV{Z}_{\{n\}^\complement},\RV{T}_{\{n\}^\complement} )\CI^e_{\prob{P}_C} \text{id}_C$
    \item $(\RV{Z}_i,\RV{T}_i)_{i\in\{n\}^\complement}$ is exchangeable
    \item $\RV{T}$ is infinitely supported over to the $(\prob{P}_C,\RV{T},\RV{Z})$ directing conditional $\RV{H}$
    \item $\prob{P}_\alpha^{\RV{T}_n}(t)>0$ for $t\in T$
\end{enumerate}
\end{definition}
Property (1) guarantees that for every available choice, $\RV{Z}$ can be partitioned into exchangeable subsequences, and we identify which exchangeable subsequence $\RV{Z}_i$ belongs to with $\RV{T}_i$. Property (2) identifies $(\RV{Z}_{\{n\}^\complement},\RV{T}_{\{n\}^\complement})$ as passive observations that the decision maker can't affect. Property (3) assumes for convenience that these observations are exchangeable, a common assumption to make of observations. Properties (4) and (5) are convenient and (as far as we can tell) do no harm.

From this, it follows that, for $\RV{H}$ the directing random conditional of $(\prob{P}_C,\RV{T},\RV{Z})$, $\prob{P}_C^{\RV{Z}|\RV{HT}}$ is IO contractible.

\begin{theorem}[IO contractibility of precedented decision models]
Given $(\prob{P}_\cdot,(\Omega,\sigalg{F}),(C,\sigalg{C}))$, if $\RV{Z}:=(\RV{Z}_i)_{i\in\mathbb{N}}$ is $n$-precedented with indexing variable $\RV{T}$, then taking $\RV{H}$ to be the directing random conditional of $(\prob{P}_C,\RV{T},\RV{Z})$, $\RV{Z}\CI^e_{\prob{P}_C} \text{id}_C | (\RV{H},\RV{T})$ and $\prob{P}_C^{\RV{Z}|\RV{HT}}$ is IO contractible.
\end{theorem}

\begin{proof}
By assumption, $\prob{P}_\alpha^{\RV{Z}|\RV{T}}$ is exchange commutative. In addition, also by assumption, $\prob{P}_\alpha^{\RV{T}_{\{n\}^\complement}}$ is exchangeable. But then $\prob{P}_\alpha^{\RV{T}}$ is exchangeably dominated, and the result follows from Theorem \ref{lem:exch_prod_ciid}.
\end{proof}

Thus precedent implies ``garden variety'' IO contractibility of $\prob{P}_C^{\RV{Z}|\RV{HT}}$. Apart from the stronger assumptions about observations, the only difference to the discussion of IO contractibility in Chapter \ref{ch:evaluating_decisions} is in our interpretation; we don't assume that $\RV{T}$ is observed, and we don't assume that we know anything in advance about how $\RV{T}$ responds to $\alpha$.

The assumption of precedent as given in Definition \ref{def:preemption} can, under some side conditions, yield nontrivial conclusions, as we show in Theorem \ref{th:det_obs_to_cons}. Before we state this theorem, it is worth going through an example first.

\begin{example}\label{ex:doctor_precedent}
Suppose we have a collection of doctors who each see a series of patients, offer a treatment $\RV{X}_i$ and report their results $\RV{Y}_i$. Each doctor may decide whether or not to prescribe based on any number of unobserved factors, and may offer additional unrecorded treatments, vary in their bedside manner and so forth, and these decisions could be stochastic or deterministic. The decision maker is \emph{also} a doctor, and is reviewing the data contained in the sequences $(\RV{Z}_i,\RV{X}_i,\RV{Y}_i)_{i\in [n]}$, where $\RV{Z}_i$ identifies the doctor involved in the $i$th treatment interaction. The decision maker supposes that whatever overall treatment plan the decision maker will adopted (which could and probably does also involve features not listed in this set of variables), the same thing has probably been done at least sometimes by some of these other doctors -- that is, their treatment protocol is precedented in the data. Because the other doctors have some variation in their treatment behaviour, it stands to reason that different doctors making the same prescription decisions should see different results \emph{if the different unobserved treatment plans actually lead to different results}. Conversely, if there is \emph{no} variation in results different doctors obtain, then whether or not treatment occurred is the \emph{only} important feature of any treatment plan.

This story might fail if the doctors all knew exactly the long-run probabilistic outcomes of different treatment plans and coordinated with one another to mask any variation they induced. For example, doctor 1 picks a medium effectiveness unobserved plan 100\% of the time, while doctor 2 picks a highly effective unobserved plan 50\% of the time and a low effectiveness unobserved plan 50\% of the time, leading to the same distribution over outcomes. Our decision maker also has to assume that there is some ``randomness'' in each doctor's choice of unobserved treatment plan so that they cannot precisely coordinate in this manner. In particular, one doctor's hidden treatment plan cannot be a deterministic function of any other doctor's hidden treatment plan and the ``true'' effectiveness of the treatment plans.
\end{example}


\begin{notation}[Matrix notation]
Given a sequential input-output model $(\prob{P}_C,\RV{D},\RV{Y})$ with $D,Y$ discrete, the directing random conditional $\RV{H}$ takes values in the set of Markov kernels $D\kto Y$, which can be identified with a subset of matrices in $\mathbb{R}^{|D|\times |Y|}$. We can therefore refer to elements of $\RV{H}$ as matrix elements $(\RV{H}_d^y)_{d\in D,y\in Y}$ with $\sum_{y\in Y} \RV{H}_d^y \overset{\prob{P}_C}{\cong} 1$ for all $d$, and $\RV{H}_d^y\overset{\prob{P}_\alpha}{\cong}\prob{P}_\alpha^{\RV{Y}|\RV{H}\RV{D}}(y|h,d)$.
\end{notation}

\begin{theorem}\label{th:det_obs_to_cons}
Suppose we have a probability set $\prob{P}_C$ on $(\Omega,\sigalg{F})$ with variables $\RV{Y}:=(\RV{Y}_i)_{i\in \mathbb{N}}$, $\RV{X}:=(\RV{X}_i)_{i\in \mathbb{N}}$ and $\RV{Z}_i:= (\RV{Z}_i)_{i\in \mathbb{N}}$, with $X,Y,Z$ discrete and $n$-precedented with indexing variable $\RV{T}$. Let $\RV{G}$ be the directing random conditional of $(\RV{P}_C,\RV{T},\RV{Z})$ and $\RV{H}$ the directing random conditional of $(\prob{P}_C,(\RV{T},\RV{Z}),(\RV{X},\RV{Y}))$. Suppose there is some $N\in\sigalg{H}$ such that $\prob{P}_\alpha^{\RV{H}}(N)>0$ for all $\alpha$, and define the probability conditioned on $\RV{H}\in N$:
\begin{align}
prob{P}_{\alpha,\RV{H}\in N} (A):= \frac{\prob{P}_\alpha^{\text{id}_\Omega \RV{H}}(A\times N)}{\prob{P}_\alpha^{\RV{H}}(N)}
\end{align}

Suppose for all $i$, $\RV{Y}_i\CI^e_{\prob{P}_{C,\RV{H}\in N}} \RV{Z}_i|(\RV{H},\RV{X}_i)$, and for all $t\in T$ and $z,z'\in Z$,
\begin{align}
    \prob{P}_{\alpha,\RV{H}\in N}^{\RV{G}^d_{z}|\RV{H}\RV{G}^d_{z'}}(\cdot|h,g^d_{z'}) \ll U_{[0,1]}\label{eq:lebesgue_dom}
\end{align}
where $U_{[0,1]}$ is the uniform probability measure on $([0,1],\mathcal{B}([0,1]))$.

Then $\prob{P}_{\alpha,\RV{H}\in N}^{\RV{Y}|\RV{X}\RV{H}}$ is IO contractible over $\RV{H}$.
\end{theorem}

\begin{proof}
We show that the assumption of conditional independence imposes a polynomial constraint on $\RV{G}^d_z$ which is nontrivial unless $\RV{Y}_i\CI^e_{\prob{P}_{C,\RV{H}\in\mathbb{N}}} (\RV{Z}_i,\RV{T}_i,\text{id}_C)|(\RV{H},\RV{X}_i)$, and hence the solution set $S$ for this constraint is empty when this conditional independence does not hold.

Full proof in Appendix \ref{sec:proof_precedent}.
\end{proof}

Theorem \ref{th:det_obs_to_cons} provides some support for the kind of inference explained in Example \ref{ex:doctor_precedent}. We offer the assumption of precedent as a formalisation of the idea that ``whatever the decision maker does, it's been done before''. If the decision maker can conclude that the hypothesis $\RV{H}$ lies in a set $N$ that implies the outcome is independent of the doctor identifiers $\RV{Z}_i$ given the treatment decisions $\RV{X}_i$, and further believes that within the set $N$, there is nevertheless zero probability that doctors coordinate precisely enough to mask any variation due to unobserved differences in treatment protocol, then the decision maker can conclude that the observed treatment-outcome map is identical to the treatment-outcome map they should use to assess the decision of whether or not to treat.

A substantial shortcoming of Theorem \ref{th:det_obs_to_cons} is the fact that, if the distribution of $\RV{H}$ is dominated by the uniform measure on $[0,1]^{|T||Z||X||Y||}$, then $\RV{H}\in N$ will also be a set of measure 0, as the conditional independence condition required by $N$ is itself a nontrivial polynomial restriction on values of $\RV{H}$. Thus assuming some continuous distribution of $\RV{H}$ does not satisfy the conditions required by the theorem.

This is reminiscent of problems encountered by other conditional independence based approaches to causal inference. In particular, we draw conclusions from conditional independence -- a measure 0 event under a continuous distribution of $\RV{H}$ -- but also from the assumption that \emph{other} measure 0 events under continuous distributions can be neglected. One way that this has been addressed is to treat approximate conditional independence as if it was conditional independence \citep{zhang_strong_2003}.

The appropriate strengthening of the assumptions in Example \ref{ex:doctor_precedent} seems to require not only that the other doctor's actions aren't perfectly coordinated, but that we can further place a limit on how precisely they might be approximately coordinated. In this case, insufficient variation in overall outcomes between doctors might still license the conclusion that the unobserved treatment variations \emph{probably} don't have \emph{much} influence. Strengthening the required conditions like this has substantial practical implications; our decision maker is interested in $\RV{Y}$ presumably because they are interested in influencing $\RV{Y}$. However, for the same reason, all of the other doctors may well be interested in influencing $\RV{Y}$ in exactly the same way. Thus, while it's unlikely they can all coordinate \emph{perfectly}, they may well achieve a good approximation of coordination, which, unless the decision maker has a very large amount of data, might be enough to prevent them from drawing conclusions from an apparent lack of variation in outcomes between doctors.

Technically, we could consider an $\epsilon$-enlargement of the set of solutions $S$ constructed in the proof of Theorem \ref{th:det_obs_to_cons}, and place an upper limit on the probability of parameters falling into this set. It may then be possible to conclude a similar result from a suitably precise approximation of the assumed conditional independence, but demonstrating this remains an open question.

Assumptions similar to Eq. \eqref{eq:lebesgue_dom} appear in two other capacities in the causal inference literature. First, in justifications for \emph{causal faithfulness} it is assumed that, if we have variables ordered according to their structural causal relationships then an assumption of this type holds \citet{meek_strong_1995}. Secondly, according to the principle of \emph{independence of conditionals}, researchers have proposed a number of measures of ``genericity'' that are maximised when conditionals are computed in a manner compatible with their structural causal relationships \citep{lemeire_replacing_2013}. The common thread here is that an assumption like Eq. \eqref{eq:lebesgue_dom} is considered to be implied by the direction of causal relationships in a structural model. Our result is somewhat different -- we start with the assumption of a generic relationship between conditionals, rather than a causal direction, and draw a conclusion of identical responses. These approaches might be combined -- under the right conditions, assuming or inferring a causal direction might be sufficient to apply Theorem \ref{th:det_obs_to_cons}, though exactly when such reasoning is valid is an open question.

\section{Potential Outcomes models}\label{sec:potential_outcomes}

Potential outcomes is another popular framework for modelling causal problems. There are two key differences between the potential outcomes approach and the causal Bayesian network approach. Potential outcomes models are sequential and they feature no notion of ``intervention''. A third difference relates to the possibility of expressing ``counterfactual'' statements, (although this difference may be contingent on the particular manner we use to unroll a causal Bayesian network).

To formulate a decision making model from a potential outcomes model, we do have to make a judgement about what the ``options'' are (CBNs provide the notion of ``intervention'' for this role), but we do not need to make any judgements about how to unroll a potential outcomes model. For the following, we rely on \citet{rubin_causal_2005} for the definition of a potential outcomes model.

Our definition of potential outcomes is formally similar to the tabulated conditional distribution (Definition \ref{def:tab_cd}), but not quite identical: in particular, for any $d\in D$, a potential outcomes model holds that $\prob{P}_\alpha^{\RV{Y}_i|\RV{Y}^D_i\RV{D}_i}(y^d|y^D,d)=1$, but this is generally false for the tabulated conditionals in Definition \ref{def:tab_cd}.

\begin{definition}[Potential outcomes]\label{def:potential_outcomes}
Given $(\prob{P}_C,\Omega,\sigalg{F})$ and, for some $i$, variables $\RV{D}_i:\Omega\to D$ ($D$ denumerable), $\RV{Y}_i:\Omega\to Y$ and $\RV{Y}^D_i:\Omega\to Y^D$, $\RV{Y}^D_i$ is a vector of \emph{potential outcomes} with respect to $\RV{D}_i$ for all $\alpha$
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_i|\RV{Y}^D_i\RV{D}_i} &= \tikzfig{lookup_representation_single}
\end{align}
Where $\prob{F}_{\text{lus}}$ is the Markov kernel associated with the single-shot lookup map
\begin{align}
    \text{lus}:Y^D\times D &\to Y\\
    (d,(y_{i})_{i\in D})&\mapsto y_{d}
\end{align}
\end{definition}

Note that $|D|$ copies of $\RV{Y}_i$ $(\RV{Y}_i,\RV{Y}_i,\RV{Y}_i,...\RV{Y}_i)$ always satisfies Definition \ref{def:potential_outcomes}. This definition is not the sole constraint on potential outcomes, but the additional constraints come from what we want them to model, and are therefore not able to be formally stated.

A ``potential outcomes model'' is simply some probabilistic model with potential outcomes. Traditionally, potential outcomes models did not feature any set of options, and so are modeled by a single probability distribution. That is, a ``traditional'' potential outcomes model is a probability space $(\prob{P},\Omega,\sigalg{F})$ rather than a probability function, but our definition also allows for decision models with potential outcomes.

\begin{definition}[Potential outcomes model]
$(\prob{P}_C,\Omega,\sigalg{F})$ is a potential outcomes model with respect to $\RV{Y}^D:=(\RV{Y}^D_i)_{i\in A}$, $\RV{Y}:=(\RV{Y}_i)_{i\in A}$ and $(\RV{D}_i)_{i\in A}$ if $\RV{Y}^D_i$ is a vector of potential outcomes with respect to $\RV{D}_i$ and $\RV{Y}_i$ for all $i\in A$.
\end{definition}

\begin{theorem}
A potential outcomes model $(\prob{P}_C,\Omega,\sigalg{F})$ with respect to $\RV{D}_i:\Omega\to D$, $\RV{Y}_i:\Omega\to Y$ and $\RV{Y}^D_i:\Omega\to Y^D$, $\RV{Y}^D_i$ features the conditional independence $\RV{Y}\CI_{\prob{P}_C}^e\text{id}_C|(\RV{D},\RV{Y}^D)$, and $\prob{P}_C^{\RV{Y}|\RV{Y}^D\RV{D}}$ is IO contractible (with respect to $*$).
\end{theorem}

\begin{proof}
IO contractibility of  follows from the fact that $\RV{Y}_i$ is deterministic given $\RV{Y}^D_i$ and $\RV{D}_i$, and thus $\RV{Y}_i\CI^e_{\prob{P}_C} (\RV{D}_{\{i\}^{\complement}},\RV{Y}_{\{i\}^{\complement}},\text{id}_C)|(\RV{Y}^D_{i},\RV{D}_i)$. Furthermore, for all $i,j,\alpha$
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_i|\RV{Y}^D_i\RV{D}_i} &= \prob{P}_\alpha^{\RV{Y}_j|\RV{Y}^D_j\RV{D}_j}
\end{align}
hence for all $\alpha$, $(\RV{D},\RV{Y})$ is a CIIR sequence of inputs and outputs, and hence $\prob{P}_\alpha^{\RV{Y}|\RV{Y}^D\RV{D}}$ is IO contractible for all $\alpha$.

Furthermore, from Definition \ref{def:potential_outcomes}, $\prob{P}_\alpha^{\RV{Y}_i|\RV{Y}^D_i\RV{D}_i}$ is the same for all $\alpha\in C$, and by the argument above,
\begin{align}
    \prob{P}_C^{\RV{Y}_i|\RV{Y}^D_i\RV{D}_i\RV{Y}^D_{\{i\}^{\complement}}\RV{D}_{\{i\}^{\complement}}} &= \prob{P}_C^{\RV{Y}_i|\RV{Y}^D_i\RV{D}_i}\otimes \mathrm{Del}_{Y^{|D\times A\setminus\{i\}|}\times D^{|A|}}
\end{align}
hence
\begin{align}
    \prob{P}_C^{\RV{Y}|\RV{Y}^D\RV{D}} &= \bigotimes_{i\in A} \prob{P}_C^{\RV{Y}_1|\RV{Y}^D_1\RV{D}_1}
\end{align}
hence $\RV{Y}\CI_{\prob{P}_C}^e\text{id}_C|(\RV{D},\RV{Y}^D)$.
\end{proof}

A key theorem of potential outcomes is that, if $\RV{D}$ is ``strongly ignorable'' with respect to $\RV{Y}^D$, then the average treatment effect is identified. ``Strong ignorability'' here means that the probability $\prob{P}_\alpha^{\RV{D}_i}(d)>0$ for each $d$ and for each choice $\alpha$ the inputs $\RV{D}$ are independent of the potential outcomes $\RV{Y}^D$ given the covariates and the choice. We reproduce this theorem in terms of IO contractibility. Note that Theorem \ref{th:potential_outcomes_identifiability} applies to our generalisation of potential outcomes to decision models, not only to probability distributions featuring potential outcomes.

\begin{theorem}[Potential outcomes identifiability]\label{th:potential_outcomes_identifiability}
Suppose $(\prob{P}_C,\Omega,\sigalg{F})$ is a potential outcomes model with respect to $\RV{Y}^D:=(\RV{Y}^D_i)_{i\in \mathbb{N}}$, $\RV{Y}:=(\RV{Y}_i)_{i\in \mathbb{N}}$ and $\RV{D}:=(\RV{D}_i)_{i\in \mathbb{N}}$, and further suppose there is some $\RV{X}:= (\RV{X}_i)_{i\in \mathbb{N}}$ such that $\prob{P}_\alpha^{\RV{Y}^D\RV{X}}$ is exchangeable for all $\alpha$, $\RV{D} \CI^e_{\prob{P}_C} \RV{Y}^D | (\RV{X},\RV{Y},\text{id}_C)$, $(\RV{D},\RV{X})$ is infinitely supported and for each $\alpha$ $\prob{P}_\alpha^{\RV{DX}}$ is absolutely continuous with respect to some exchangeable distribution in $\Delta((D\times X)^{\mathbb{N}})$. Then there is some $\RV{W}$ such that for all $\alpha$ $\prob{P}_\alpha^{\RV{Y}|\RV{WXD}}$ is IO contractible over $\RV{W}$.
\end{theorem}

\begin{proof}
By exchangeability of $\prob{P}_\alpha^{\RV{Y}^D\RV{X}}$, $\prob{P}_\alpha^{\RV{Y}^D|\RV{X}}$ commutes with exchange. Because $\RV{Y}$ is deterministic given $\RV{D}$ and $\RV{Y}^D$, $\RV{Y}\CI^e_{\prob{P}_C} (\RV{X},\text{id}_C)|(\RV{Y}^D,\RV{D})$ Thus, for some finite permutation $\rho$, by IO contractibility of $\prob{P}_C^{\RV{Y}|\RV{Y}^D\RV{D}}$
\begin{align}
    \prob{P}_\alpha^{\RV{Y}|\RV{XD}} &= \tikzfig{potential_outcomes_ccontract1}\\
    &= \tikzfig{potential_outcomes_ccontract2}\\
    &= \tikzfig{potential_outcomes_ccontract3}
\end{align}
IO contractibility of $\prob{P}_\alpha^{\RV{Y}|\RV{WXD}}$ over some $\RV{W}$ follows from Theorem \ref{lem:exch_prod_ciid}.
\end{proof}

\subsection{Justifying identifiability assumptions}\label{sec:justifying}

In the previous chapter we investigated some justifications for IO contractibility (which, roughly speaking, is the same as ``identifiability'' in other causal modelling frameworks). Theorem \ref{th:potential_outcomes_identifiability} offers an alternative collection of assumptions and yields a similar conclusion. An obvious question to ask is what the justifications for these assumptions are, and how they relate to the justifications in the previous chapter.

We think the justifications given for these assumptions are typically quite different to those discussed in Section \ref{sec:imp_iocont}. We focus on the two assumptions of exchangeability of potential outcomes ($\prob{P}_\alpha^{\RV{Y}^D\RV{X}}$ is exchangeable in Theorem \ref{th:potential_outcomes_identifiability}), and ``ignorability'' ($\RV{D} \CI^e_{\prob{P}_C} \RV{Y}^D | (\RV{X},\RV{Y},\text{id}_C)$ in Theorem \ref{th:potential_outcomes_identifiability}). Two means of justifying these assumptions involve considering changes to an experiment that one believes would yield identical outcome distributions, or treating the potential outcomes as if they were variables that we could measure in principle, and asking what properties these variables might have.

\subsubsection{Different actions that yield the same outcome distribution}\label{sec:exch_peop}

\citet{greenland_identifiability_1986} explain an assumption akin to the conjunction of the exchangeability of potential outcomes and ignorability:
\begin{quote}
    Equivalence of response type may be thought of in terms of exchangeability of individuals: if the exposure states of the two individuals had been exchanged, the same data distribution would have resulted.
\end{quote}
This assumption is evocative of the assumption of commutativity of exchange (Definition \ref{def:caus_exch}). The latter \emph{seems} to say something like ``swapping the inputs leaves the outputs unchanged up to permutation''. However, strictly speaking all it asserts is that a conditional probability is unchanged under swaps of the labels, and says nothing about swapping ``individuals''.

One way we could interpret Greenland and Robins' assumption (and by no means do we think this interpretation is obligatory) is to suppose that the decision maker's option set $C$ genuinely affords them the opportunity to swap treatments. We might consider, more generally, that there is some $\alpha,\alpha'$ such that for some finite permutation $\rho$
\begin{align}
    \prob{P}_\alpha^{\RV{D}} = \prob{P}_{\alpha'}^{\RV{D}_\rho}
\end{align}
and, furthermore, these permuted inputs lead to permuted outputs
\begin{align}
    \prob{P}_\alpha^{\RV{D}}\odot \prob{P}_\alpha^{\RV{Y}|\RV{D}} &= \prob{P}_{\alpha'}^{\RV{D}_\rho}\odot \prob{P}_{\alpha'}^{\RV{Y}_\rho|\RV{D}_\rho}\label{eq:swapped_decisions}
\end{align}
\emph{This} condition is actually independent of commutativity of exchange. For simplicity we'll consider deterministic examples. First, suppose $\prob{P}_\alpha^{\RV{Y}|\RV{D}}$ maps everything to an infinite sequence of $1$s, while $\prob{P}_{\alpha'}^{\RV{Y}|\RV{D}}$ maps everything to an infinite sequence of $2$s. Both are clearly exchange commutative, but, if the marginals of $\RV{D}$ were appropriately related, would not satisfy Equation \eqref{eq:swapped_decisions}.

On the other hand, suppose $\prob{P}_\alpha^{\RV{Y}_1|\RV{D}_1}$ sends $d_1\mapsto -d_1$ and $\prob{P}_\alpha^{\RV{Y}_{\mathbb{N}\setminus\{1\}}|\RV{D}_{\mathbb{N}\setminus\{1\}}}$ sends $d_{\mathbb{N}\setminus\{1\}} \mapsto d_{\mathbb{N}\setminus\{1\}}$. On the other hand $\prob{P}_{\alpha'}^{\RV{Y}_2|\RV{D}_2}$ sends $d_2\mapsto -d_2$ and $\prob{P}_{\alpha'}^{\RV{Y}_{\mathbb{N}\setminus\{2\}}|\RV{D}_{\mathbb{N}\setminus\{2\}}}$ sends $d_{\mathbb{N}\setminus\{2\}} \mapsto d_{\mathbb{N}\setminus\{2\}}$. If $\prob{P}_\alpha^{\RV{D}_1\RV{D}_{\mathbb{N}\setminus\{1\}}}=\prob{P}_{\alpha'}^{\RV{D}_2\RV{D}_{\mathbb{N}\setminus\{2\}}}$ then Equation \eqref{eq:swapped_decisions} will be satisfied but exchange commutativity generally will not.

It's hard to think of a practical example of the first possibility, but for the second possibility we can imagine an experiment where we can either let the first patient choose their treatment and compel the second patient, or vise-versa. Up to permutation, both yield the same joint distribution of treatments and outcomes, but this joint distribution will generally not be exchangeable. Note that Greenland and Robins do not consider stochastic swaps of individuals' treatments, so this objection doesn't apply to their original scenario. Having said that, if stochastically influencing treatment is all we can actually do, then their original assumption requires some alternative interpretation to the one we have provided here.

The two conditions under discussion coincide if we additionally assume $\RV{Y}\CI^e_{\prob{P}_C} \text{id}_C |\RV{D}$. In that case, we have
\begin{align}
    \prob{P}_\alpha^{\RV{Y}|\RV{D}} &= \prob{P}_{\alpha'}^{\RV{Y}|\RV{D}}
\end{align}
which, under a support condition together with Eq. \eqref{eq:swapped_decisions} implies
\begin{align}
    \prob{P}_\alpha^{\RV{Y}|\RV{D}} &= \prob{P}_{\alpha}^{\RV{Y}_\rho|\RV{D}_\rho}
\end{align}
The independence $\RV{Y}\CI^e_{\prob{P}_C} \text{id}_C |\RV{D}$ allows us to conclude exchange commutativity from the symmetries between the consequences brought about by different options.

\subsubsection{Potential outcomes as ``pseudo-observables''}\label{sec:exch_po}

\citet{rubin_causal_2005} writes in defense of the exchangeability of potential outcomes:
\begin{quote}
    Here there are $N$ units, which are physical objects at particular points in time (e.g., plots of land, individual people, one person at repeated points in time)[...]the indexing of the units is, by definition, a random permutation of $1,..., N$, and thus any distribution on the science must be row-exchangeable
\end{quote}
We interpret Rubin to mean something like this:
\begin{enumerate}
    \item The properties of ``unit $i$'' -- some real-world thing -- determine the values of all of the variables indexed by $i$, and the potential outcomes (``the science'') of unit $i$ is one of its properties
    \item The assignment of indices to units was either literally performed by some physically random process, or at least if it were the model in question would be exactly the same
    \item Thus we should adopt a model that is invariant to the operation of shuffling all of the indices assigned to the units, or equivalently invariant to the operation of shuffling the indices assigned to sets of variables
\end{enumerate}
This argument doesn't quite go through. Recall how, in Section \ref{sec:nci_nco}, we mentioned that a decision maker may often know the impact of their choice on the inputs $\RV{D}$ before examining the data. Note also in Definition \ref{def:unr_CBN} (and it's descendants) that a CBN model permits us to choose interventions that have different effects on variables of the same type with different indices. Perhaps a decision maker knows a priori how to fix the value of $\RV{D}_5$, but has no influence over $\RV{D}_{[4]}$, or (essentially equivalently) they can intervene on $\RV{D}_5$ but not $\RV{D}_{[4]}$. Any model with this property is not invariant to swapping the index $5$ with any of the prior indices. Similarly, if a decision maker can control the entire sequence $\RV{D}_{[5]}$, they may in general choose different distributions to set each $\RV{D}_i$ to, which will again make the model change under permutations of $\RV{D}_{[5]}$.

One could replace claim 3 with something like:
\begin{enumerate}
    \item[3'] The distribution of potential outcomes is the same no matter what choice the decision maker makes
    \item[4] The distribution of variables that are insensitive to the decision maker's choices should be invariant to the operation of shuffling indices
\end{enumerate}
To the extent that potential outcomes model counterfactual relationships, 3' is a strong claim about the nature of counterfactuals. It would be surprising if such a relation between counterfactual quantities and the consequences of making choices were true unless counterfactuals were defined in such a manner as to make it true.

Proposition 4 together with 2 is suggestive of a two-step process of assessing symmetry in decision models: first, we identify some apparent symmetry of the decision procedure in the real world, and second we consider this symmetry to imply exchangeability only of those variables associated with features of the world that do not change in response to our choices.

% \section{Individual-level response functions}\label{sec:ilevel_ccontract}

% Exchangeability of potential outcomes, a key assumption in Theorem \ref{th:potential_outcomes_identifiability}, is hard to explain in terms of symmetries of experiments. Given some experiment producing a sequence of pairs $(\RV{D}_i,\RV{Y}_i)_{i\in\mathbb{N}}$, say where $\RV{D}_i$s are treatment administrations and $\RV{Y}_i$s are health outcomes, there's no obvious generic way to design a related experiment whose model is the same as the original except with potential outcomes $\RV{Y}^D_i$ interchanged. This is in sharp contrast to the assumption of exchangeability of observed outcomes - say, instead of the potential outcomes being exchangeable, we hold that the pairs $(\RV{D}_i,\RV{Y}_i)$ are exchangeable in the original experiment. Then we commit ourselves to the proposition that an alternative experiment which proceeds exactly as the first except, before being ``committed to memory'', the experimental results are interchanged should be modeled exactly as the first.

% One could propose that exchangeability of potential outcomes in our example experiment corresponds to an \emph{exchangeability of patients}; perhaps, if we believe the model should be unchanged after we swap the order in which patients are seen, then we should accept that the model has exchangeable potential outcomes. However, it certainly does not follow from a potential outcomes model that exchangeability of patients implies exchangeability potential outcomes. If each patient were assumed to have a fixed but unknown vector of potential outcomes that is unchanged by the swapping operation, then swapping patients does indeed correspond to swapping potential outcomes. On the other hand, if we instead suppose that each index is associated with a fixed vector of potential outcomes regardless of any exchanges of patients that might be made, we could swap patients without altering the model and yet the potential outcomes would not in general be exchangeable.

% We formalise the idea of ``potential outcomes attached to individuals'' as \emph{individual-level response functions}. We offer a formal definition of the assumption of individual-level response functions, but like exchangeability of potential outcomes it is difficult to understand fundamentally what this assumption entails, or what it might be motivated by. Nevertheless, it does allow us to separate the assumption of ``exchangeable potential outcomes'' into the assumption of individual level response functions and the assumption of exchangeability of individuals. We also use this notion to prove Theorem \ref{th:cc_ind_treat}. At a high level, it plays a similar role to Theorem \ref{th:potential_outcomes_identifiability}: it seems to justify causal identifiability in certain kinds of controlled experiments. However, the content of the two theorems is very different. While Theorem \ref{th:potential_outcomes_identifiability} concerns independence of the inputs and potential outcomes along with exchangeability of the potential outcomes, Theorem \ref{th:cc_ind_treat} says (informally) if:
% \begin{itemize}
%     \item There are individual-level response functions
%     \item Individuals can be swapped without meaningfully altering the experiment
%     \item Inputs are deterministically controlled by the choice
%     \item There is only one choice for each value of the inputs
% \end{itemize}
% then the model is also IO contractible with respect to the inputs and the outputs only (ignoring the individual identifiers). In our view, this comes closer to a set of assumptions that are directly appliccable to a controlled experiment than those in Theorem \ref{th:potential_outcomes_identifiability}, and reflects \citet{kasy_why_2016}'s dictum that, for the identifiability of causal effects, a ``controlled experiment'' is sufficient.

% \subsection{References to individual-level IO contractibility}

% The role of individuals has often been mentioned in literature on causal inference. For example, \citet{greenland_identifiability_1986} explain
% \begin{quote}
%     Equivalence of response type may be thought of in terms of exchangeability of individuals: if the exposure states of the two individuals had been exchanged, the same data distribution would have resulted.
% \end{quote}
% Here, the idea of ``exchangeable individuals'' plays a role in the author's reasoning about model construction, but ``individuals'' are not actually referenced by the resulting model, and ``exchanging individuals'' does not correspond to a model transformation.

% \citet{dawid_decision-theoretic_2020} suggests (with some qualifications) that ``post-treatment exchangeability'' for a decision problem regarding taking aspirin to treat a headache may be acceptable if the data are from
% \begin{quote}
%     A group of individuals whom I can regard, in an intuitive sense, as similar to myself, with headaches similar to my own.
% \end{quote}
% As in the previous work, the similarity of individuals involved in an experiment is raised when justifying particular model constructions, but the individuals are not referenced by the model.

% \citet[pg. 98]{pearl_causality:_2009} writes
% \begin{quote}
%     Although the term unit in the potential-outcome literature normally stands for the identity of a specific individual in a population, a unit may also be thought of as the set of attributes that characterize that individual, the experimental conditions under study, the time of day, and so on  all of which are represented as components of the vector $u$ in structural modeling.
% \end{quote}
% Once again, the idea of an individual (or a particular set of conditions) is raised in the context of explaining modelling choices. Unlike the previous authors, Pearl introduces a vector $u$ to stand for the ``unit''. However, he subsequently assumes that $u$ is a sequence of \emph{independent samples} from some distribution. This seems to contradict an important feature of ``individuals'' or ``units'': individuals are typically supposed to be unique, a property that will usually not be satisfied by independently sampling from some distribution (at least, as long as the distribution is discrete).

% \subsection{Unique identifiers}

% A sequence of \emph{unique identifiers} is a vector of finite or infinite length such that no two coordinates are equal. We are interested in models that assign positive probability to any particular coordinate having any particular value. This is straightforward in the finite case. In the infinite case, note that a vector of $|\mathbb{N}|$ unique values with an arbitrary entry $k$ in the $j$th coordinate can be obtained by starting with $(i)_{i\in \mathbb{N}}$ and then transposing $j$ with $k$. More generally, we consider infinite length sequences of unique identifiers to be elements of the set of finite permutations $\mathbb{N}\to\mathbb{N}$.

% \begin{definition}[Measurable space of unique identifiers]
% The measurable space of unique identifiers $(I,\sigalg{I})$ is the set $I$ of finite permutations $\mathbb{N}\to \mathbb{N}$ with the discrete $\sigma$-algebra $\sigalg{I}$.
% \end{definition}

% The set $I$ is countable, as it is the countable union of finite subsets (i.e. the permutations that leave all but the first $n$ numbers unchanged for all $n$).

% \begin{definition}[Unique identifier]
% Given a sample space $(\Omega,\sigalg{F})$, a \emph{sequence of unique identifiers} $\sigalg{I}:\Omega\to I$ is a variable taking values in $I$.
% \end{definition}

% The values of each coordinate of sequence of unique identifiers is just called an identifier (for obvious reasons, we don't call it an identity).

% \begin{definition}[Identifiers]
% Given $\RV{I}$, define the $i$-th \emph{identifier} $\RV{I}_i:=\mathrm{ev}(i,\RV{I})$, where $\mathrm{ev}:\mathbb{N}\times I\to \mathbb{N}$ is the evaluation map $(i,f)\mapsto f(i)$.
% \end{definition}

% For \emph{any} sample space $(\Omega,\sigalg{F})$ we can define a trivial $\sigalg{I}$ that maps every $\omega\in\Omega$ to $(1,2,3,....)=:(\mathbb{N})$. In this case, the identifiers are all known by the modeller at the outset. Using this sequence of identifiers renders exchange commutativity trivial.

% \begin{example}\label{eq:il_exchc}
% Given a sequential input-output model $(\prob{P}_C, (\RV{D},\RV{I}),\RV{Y})$ where $\RV{I}$ is the identifier variable $\omega\mapsto (\mathbb{N})$, $\prob{P}_\alpha^{\RV{Y}|\RV{DI}}$ commutes with exchange.

% This is because for any permutation $\rho:\mathbb{N}\to\mathbb{N}$ except the identity, $\prob{P}_\alpha^{\RV{Y}|\RV{DI}}$ and $\text{Swap}_{\rho}\prob{P}_\alpha^{\RV{Y}|\RV{DI}}$ will have no common support; the first will be supported on $\RV{I}\yields (\mathbb{N})$ only, and the second only on $\RV{I}\yields \rho(\mathbb{N})$.
% \end{example}

% We are particularly interested in models where exchange commutativity is not trivial, so we focus on the case where each identifier $\RV{I}_i$ has some non-zero probability of taking any value in $\mathbb{N}$. 

% % \subsection{Individual-level IO contractibility and unobserved confounding}

% % Our first result is that some models with individual-level IO contractibility can be seen as models with unobserved confounding. A model $\prob{P}_C$ with individual-level IO contractibility features a IO contractible $\prob{P}_C^{\RV{Y}|\RV{ID}}$ for a sequence of outputs $\RV{Y}$, inputs $\RV{D}$ and individual identifiers $\RV{I}$. A model with unobserved confounding features IO contractible $\prob{P}_C^{\RV{Y}|\RV{UD}}$ where $\RV{Y}$ and $\RV{D}$ are as before and $\RV{U}$ is an ``unobserved confounder''. They key difference between $\RV{I}$ and $\RV{U}$ is that the individual identifier for each observation is unique, while unobserved variables (typically) have $|U|<N$ where $N$ is the number of observations.

% \subsection[Identification]{Identification with individual-level response functions}

% The key result of this section is Theorem \ref{th:cc_ind_treat}, which establishes sufficient conditions that, in conjunction with the key assumption of \emph{individual-level response functions}, yield a conclusion of observable CIIR sequences. Individual-level response functions is the assumption that, given a sequential input-output model $(\prob{P}_C, (\RV{D},\RV{I}),\RV{Y})$, there is some $\RV{J}$ such that $\prob{P}_C^{\RV{Y}_i|\RV{D}_i\RV{I}_i\RV{J}}$ are mutually independent and identical responses for all $\alpha$ and $i$. $\RV{J}$, unlike the directing random conditional $\RV{H}$ (Definition \ref{def:dir_rand_cond}), is not necessarily a function of the inputs and outputs, as we do not have infinitely supported inputs.

% We could consider $\RV{J}$ to be a variable assigning a stochastic response for each individual independent of the order of treatment or the treatment of any other individual. That is, $\RV{J}$ could be considered a specification of stochastic potential outcomes ``bound to the identifiers''. There is also an alternative interpretation of this assumption, which is suggested by its graphical representation. In particular, the exchange commutativity of $\prob{P}_C^{\RV{Y}|\RV{J}\RV{I}\RV{D}}$ implies

% \begin{align}
%  &\phantom{=} \tikzfig{indiv_ciir}\\
%  &= \tikzfig{indiv_ciir_swapped}
% \end{align}

% We give the following informal interpretation to this diagram: there is essentially no difference between ``sampling'' $(\RV{I},\RV{D})$ and running the experiment as normal, or ``sampling'' $(\RV{I},\RV{D})$ according to the same procedure, jointly shuffling the sequence $(\RV{I},\RV{D})$, and then continuing the experiment as normal according to the shuffled sequence. Along similar lines, we can justify locality if we accept there's no difference regarding the first $n$ outputs $\RV{Y}_{[n]}$ between  ``sampling'' $(\RV{I},\RV{D})$ and continuing as normal, or ``sampling'' $(\RV{I},\RV{D})$ and scrambling the inputs and identifiers after the $n$th $(\RV{I}_i,\RV{D}_i)_{i\in (n,\infty)}$. That is, this assumption might be justified if we accept that
% \begin{itemize}
%     \item The order of (input, identifier) pairs does not matter once the correspondence between inputs and identifiers has been determined
%     \item The output $\RV{Y}_i$ does not depend on inputs and identifiers with index not equal to $i$
% \end{itemize}

% As an example, suppose we have a medical treatment experiment with binary treatments $\RV{D}_i$ and binary outcomes $\RV{Y}_i$ and patient identifiers $\RV{I}_i$. $\RV{D}_1$ is determined by random assignment and $\RV{D}_2$ is determined according to the patient's free choice. It is not known in advance which patient will appear first and which will appear second, and the patient identifiers are not informative to the decision maker. Because of the different assignment mechanisms
% \begin{align}
%     \prob{P}_\alpha^{\RV{Y}_1|\RV{I}_1\RV{D}_1} \neq \prob{P}_\alpha^{\RV{Y}_2|\RV{I}_2\RV{D}_2}
% \end{align}
% hence $\prob{P}_\alpha^{\RV{Y}|\RV{ID}}$ is not exchange commutative. However, because the only difference is the assignment mechanism, the decision maker considers switching the assignment mechanisms to yield essentially the same experiment, modulo the order of the outputs. That is, the experiment where $\RV{D}_2$ is determined by random assignment and $\RV{D}_1$ is determined by free patient choice is essentially the same as the first experiment with the ordering of outputs switched.

% The idea underpinning Theorem \ref{th:cc_ind_treat} is that if this assumption holds as well as the assumption that identifiers can be switched independently of inputs, then the observed inputs and outputs $(\RV{D},\RV{Y})$ also form a CIIR sequence. Furthermore, we can conclude that identifiers can be switched independently of inputs if we accept that identifiers are not informative on their own and that inputs are completely under our control.

% Lemma \ref{lem:ind_to_cc} shows that individual level CIIR sequences along with a particular conditional independence of the identifiers implies inputs and outputs also form a CIIR sequence. Theorem \ref{th:ind} uses this to show that individual level CIIR sequences along with outputs independent of identifiers and complete control over inputs yields the same result. Both results do \emph{not} require that $\RV{I}$ be a sequence of unique identifiers. The reason why we are interested mainly in the case where $\RV{I}$ is a sequence of unique identifiers is that the key assumption $\RV{Y}\CI^e_{\prob{P}_C} \RV{I}|\text{id}_C$ is less compelling in the case where $\RV{I}$ is a non-unique sequence of labels. In particular, it implies that the conditional probability of $(\RV{Y}_1,\RV{Y}_2)$ given $(\RV{I}_1=1,\RV{I}_2=1)$ is exactly the same as the conditional probability of $(\RV{Y}_1,\RV{Y}_2)$ given $(\RV{I}_i=1,\RV{I}_2=2)$; observations associated with the same labels are no more relevant to each other that observations associated with different labels.

% Finally, Theorem \ref{th:cc_ind_treat} replaces the assumption that outputs are independent of identifiers in Theorem \ref{th:ind} with a permutability of identifiers assumption, and in this case the result only holds in the case that $\RV{I}$ is a sequence of unique identifiers.

% \begin{lemma}\label{lem:ind_to_cc}
% Given sequential input-output model $(\prob{P}_C,(\RV{D},\RV{I}),\RV{Y})$ with $\prob{P}_\alpha^{\RV{Y}|\RV{WDI}}$ IO contractible over $\RV{W}$, if $\RV{Y}\CI_{\prob{P}_C}^e (\RV{I},\text{id}_C)|(\RV{W},\RV{D})$ and for any $j\in I$, $\sum_{\alpha\in C} \prob{P}_\alpha^{\RV{I}_i}(j)>0$, then there is some $\RV{W}'$ such that $\prob{P}_\alpha^{\RV{Y}|\RV{W}'\RV{D}}$ is also IO contractible over $\RV{W}$.
% \end{lemma}

% \begin{proof}
% Appendix \ref{app:il_ciir}.
% \end{proof}

% \begin{theorem}\label{th:ind}
% Given a sequential input-output model $(\prob{P}_C,(\RV{D},\RV{I}),\RV{Y})$ on $(\Omega,\sigalg{F})$ with $Y$ standard measurable and $C$ countable, if there is some $\RV{J}$ such that for each $\alpha$
% \begin{align}
%     \prob{P}_\alpha^{\RV{Y}_i|\RV{J}\RV{I}_i\RV{D}_i} &= \prob{P}_\alpha^{\RV{Y}_i|\RV{J}\RV{I}_i\RV{D}_i} &\forall i,j\in \mathbb{N}\\
%     \RV{Y}_i&\CI^e_{\prob{P}_C} (\RV{I}_{\{i\}^{\complement}},\RV{D}_{\{i\}^{\complement}})|(\RV{J},\RV{I}_i,\RV{D}_i)
% \end{align}
% and
% \begin{align}
%     &\RV{Y}\CI^e_{\prob{P}_C} \RV{I} | \text{id}_C\\
%     &\RV{YIJ}\CI^e_{\prob{P}_C} \RV{D}|\text{id}_C\\
%     &\RV{YIJ}\CI^e_{\prob{P}_C} \text{id}_C|\RV{D}\\
%     &\forall i,j\in \mathbb{N}: \sum_{\alpha\in C} \prob{P}_\alpha^{\RV{I}_i}(j)>0
% \end{align}
% then $\prob{P}_C^{\RV{Y}|\RV{JD}}$ is IO contractible over $\RV{J}$.
% \end{theorem}

% \begin{proof}
% Appendix \ref{app:il_ciir}.
% \end{proof}

% Lemma \ref{lem:exch_to_ind} is used to apply Theorem \ref{th:ind} to models where $\RV{I}$ is a sequence of unique identifiers. Only in this case, exchangeability of the unique identifiers implies the identifiers are independent of the outcomes $\RV{Y}$.

% \begin{lemma}\label{lem:exch_to_ind}
% Given any probability set $\prob{P}_C$ where $\RV{Y}\CI_{\prob{P}_C}^e \text{id}_C|(\RV{D},\RV{I})$ and $\RV{I}:\Omega\to I$ is an infinite sequence of unique identifiers, if for each finite permutation $\rho:\mathbb{N}\to \mathbb{N}$
% \begin{align}
%     \prob{P}_\alpha^{\RV{Y}|\RV{I}} &= (\text{Swap}_{\rho(I)})\prob{P}_\alpha^{\RV{Y}|\RV{I}}
% \end{align}
% then $\RV{Y}\CI_{\prob{P}_C}^e \RV{I}|\text{id}_C$.
% \end{lemma}

% \begin{proof}
% By definition of the set $I$ of finite permutations, for every $\rho\in I$, $B\in\sigalg{Y}^{\mathbb{N}}$, $d\in D^{\mathbb{N}}$ there is a finite permutation $\rho^{-1}\in I$ such that $\rho\circ\rho^{-1}=\text{id}_{\mathbb{N}}$. Then
% \begin{align}
%     \prob{P}_\alpha^{\RV{Y}|\RV{I}}(B|\rho) &= (\kernel{F}_{\rho^{-1}}\otimes \text{Id}_X )\prob{P}_\alpha^{\RV{Y}|\RV{I}}(B|\rho)\\
%     &= \prob{P}_\alpha^{\RV{Y}|\RV{I}}(B|\text{id}_{\mathbb{N}})
% \end{align}
% Therefore
% \begin{align}
%     \prob{P}_\alpha^{\RV{Y}|\RV{I}} &\overset{\prob{P}_C}{\cong} \text{erase}_{I}\otimes \prob{P}_\alpha^{\RV{Y}}
% \end{align}
% \end{proof}

% Theorem \ref{th:cc_ind_treat} presents a set of sufficient conditions for $\prob{P}_C^{\RV{Y}_i|\RV{H}\RV{D}_i}$ to be conditionally independent and identical response functions with respect to the standard directing random conditional $\RV{H}$:
% \begin{enumerate}
%     \item There exist variables $\RV{I}$ representing ``unique identifiers'' which satisfy the assumption that $\prob{P}_C^{\RV{Y}_i|\RV{J}\RV{D}_i\RV{I}_i}$ are a sequence of independent and identical response functions for some $\RV{J}$
%     \item The identifiers $\RV{I}$ can be swapped without altering the model of the consequences $\RV{Y}$
%     \item The inputs $\RV{D}$ and the choice $\text{id}_C$ are substitutable with respect to $\RV{Y}$ and $\RV{I}$: $\RV{YI}\CI^e_{\prob{P}_C} \text{id}_C|\RV{D}$ and $\RV{YI}\CI^e_{\prob{P}_C} \RV{D}|\text{id}_C$
% \end{enumerate}

% \begin{theorem}\label{th:cc_ind_treat}
% Given a sequential input-output model $(\prob{P}_C,(\RV{D},\RV{I}),\RV{Y})$, on $(\Omega,\sigalg{F})$ with $Y$ standard measurable and $C$ and $D$ countable, $\RV{D}$ infinitely supported, $\RV{I}$ an infinite sequence of unique identifiers, if there is some $\RV{J}$ such that
% \begin{align}
%     \prob{P}_\alpha^{\RV{Y}|\RV{I}} &= (\text{swap}_{\rho(I)})\prob{P}_\alpha^{\RV{Y}|\RV{I}}&\forall \text{ finite permutations }\rho\\
%     &\RV{YIJ}\CI^e_{\prob{P}_C} \RV{D}|\text{id}_C\\
%     &\RV{YIJ}\CI^e_{\prob{P}_C} \text{id}_C|\RV{D}\\
%     &\forall i,j\in \mathbb{N}: \sum_{\alpha\in C} \prob{P}_\alpha^{\RV{I}_i}(j)>0
% \end{align}
% and for each $\alpha$ $\prob{P}_\alpha^{\RV{Y}|\RV{JID}}$ is IO contractible over $\RV{J}$, then we have conditionally independent and identical responses $\prob{P}_\alpha^{\RV{Y}_i|\RV{D}_i\RV{H}}$ for all $i$, where $\RV{H}$ is the directing random conditional with respect to $(\RV{D},\RV{Y})$.
% \end{theorem}

% \begin{proof}
% Apply lemma \ref{lem:exch_to_ind} to get $\RV{Y}\CI^e_{\prob{P}_C} \RV{I} | \text{id}_C$, then apply Theorem \ref{th:ind} for $\prob{P}_C^{\RV{Y}|\RV{J}\RV{D}}$ IO contractible. The result follows from Theorem \ref{th:ciid_rep_kernel}.
% \end{proof}

% Theorem \ref{th:cc_ind_treat} can be extended to the case where decisions $\RV{D}$ are a one-to-one deterministic function of the choice, or a random mixtures of one-to-one deterministic functions of the choice. This extension is applicable a randomised controlled trial, where the treatments are deterministically controlled and randomly assigned.

% \begin{theorem}\label{cor:extend_to_randomised}
% Consider a sequential input-output model $(\prob{P}_{C'},\RV{D},\RV{Y})$ where $\prob{P}_{C'}^{\RV{Y}|\RV{WD}}$ is IO contractible over $\RV{W}$, and construct a second model $(\prob{P}_{C},\RV{D},\RV{Y})$ where $\prob{P}_C$ is the union of $\prob{P}_{C'}$ and its convex hull. Then $\prob{P}_{C}^{\RV{Y}|\RV{WD}}$ is also IO contractible.
% \end{theorem}

% \begin{proof}
% For all $\alpha\in C$, there is some probability measure $\mu:C'\to [0,1]$ such that
% \begin{align}
%     \prob{P}_\alpha^{\RV{Y}|\RV{WD}} &= \sum_{\beta\in C'} \mu(\beta) \prob{P}_\beta^{\RV{Y}|\RV{WD}}\\
%     &= \prob{P}_{C'}^{\RV{Y}|\RV{WD}}
% \end{align}
% thus
% \begin{align}
%     \prob{P}_C^{\RV{Y}|\RV{WD}} = \prob{P}_{C'}^{\RV{Y}|\RV{WD}}
% \end{align}
% and in particular, $\prob{P}_C^{\RV{Y}|\RV{WD}}$ is IO contractible.
% \end{proof}

% Theorem \ref{cor:extend_to_randomised} can be used to argue that, given a sequence of experiments IO contractible under deterministic choices, adding random mixtures of these choices also yields a IO contractible sequence. \citet{kasy_why_2016} argues that as long as the experimenter controls the treatment assignment, causal effects are identified (i.e. the randomisation step is not strictly necessary). Example \ref{ex:randomised_experiment} shows that this argument might be supported, but Example \ref{ex:bad_randomised_experiment} shows that there are subtle ways that might lead to this argument failing.

% We assume an infinite sequence, which is clearly unreasonable. Extending the representation theorems to the case of finite sequences, using for example the result of \citet{diaconis_finite_1980} with establishes that finite exchangeable distributions are approximately mixtures of independent and identically distributed sequences, would allow some implausible assumptions in the following example to be removed.

% Theorem \ref{th:cc_ind_treat} is used in the following example to argue that, under certain conditions, a controlled experiment supports a IO contractible model.

\begin{example}\label{ex:randomised_experiment}
A sequential experiment is modeled by a probability set $\prob{P}_C$ with binary treatments $\RV{D}:=(\RV{D}_i)_{i\in \mathbb{N}}$ and binary outcomes $\RV{Y}:=(\RV{Y}_i)_{i\in\mathbb{N}}$. The set of choices $C$ is the set of all probability distributions$\Delta(D^\mathbb{N})$.

The treatments are decided as follows: the decision maker consults the model $\prob{P}_C$, and, according to $\prob{P}_C$ and some previously agreed upon decision rule, comes up with a (possibly stochastic) sequence  of treatment distributions $\alpha:=(\mu_i)_{i\in \mathbb{N}}$ with each $\mu_i$ in $\Delta(\{0,1\})$. If $\mu_i$ is deterministic -- that is, it puts probability 1 on some treatment $d_i$, the decision maker will assign patient $i$ the treatment $d_i$. Otherwise, if $\mu_i$ is nondeterministic, the decision maker will consult a random number generator that yields treatment assignments according to $\mu_i$, and treatment will then be assigned deterministically according to the result.

If the inputs were always an invertible function of the choice, we would have $\RV{Y}\CI^e_{\prob{P}_C} \text{id}_C|\RV{D}$. We assume that the randomisation procedure does not change this fact.

How might the decision maker justify an assumption of IO contractibility of $\prob{P}_C^{\RV{Y}|\RV{HD}}$ in this context? A number of options are:
\begin{itemize}
    \item They might assess that given any two choices $\alpha$ and $\alpha'$ that yield treatment distributions that are equivalent up to a permutation of one another, the corresponding outcomes will also be equivalent up to the same permutation, and conclude exchange commutativity from this (see Section \ref{sec:exch_peop})
    \item They might assess that the problem of predicting an output from its input and some infinitely supported input-output sequence is essentially the same as the problem of predicting any other output from its corresponding input and any other infinitely supported input-output sequence (see Section \ref{sec:imp_iocont})
    \item They might speculate that each patient at each point in time contains, in some manner obscure to practically possible measurements, the outcome they would experience under any input they are given, something that is not influenced in any way by the decision maker's choice. They might further speculate that, owing to their ignorance about relevant differences between patients, their choices should be modeled as probabilistically independent of these ``potential outcomes'' \emph{and} these potential outcomes should be modeled exchangeably (Section \ref{sec:exch_po})
    \item They might assess that their choices are well-modeled by interventions in an uncertain unrolled causal Bayesian network (Definition \ref{def:unc_unr_cbn}, see also \citet[Ch. ~4]{pearl_book_2018})
\end{itemize}
\end{example}

We are also interested in the question of how the decision maker might conclude that the assumption of IO contractibility of $\prob{P}_C^{\RV{Y}|\RV{HD}}$ is false. To this end, we modify the example to consider an alternative experiment where IO contractibility probably should not be assumed. The construction is deliberately chosen to be somewhat awkward for the decision modelling approach -- we suppose that $\alpha$ is ``mostly chosen'' by someone other than the decision maker. In particular, the decision maker is able to choose $\mu_n$ only for some $n\in \mathbb{N}$, and someone else subsequently determines the rest of the ``choice''. How might the justifications in the above example change?

\begin{example}[I choose vs you choose]\label{ex:bad_randomised_experiment}
Consider the previous experiment, except the sequence $\alpha$ of (possibly stochastic) treatment assignments is provided by the decision maker's assistant with the exception of $\mu_n$. The decision maker considers it possible that the assistant has made this assignment with a view to promoting certain possible sequences of outputs $\RV{Y}$ over others.

Note that, for the deterministic subset $C'$ of $C$, $\RV{D}$ is still an invertible function of $\text{id}_C$. Calling $\text{id}_C$ a ``choice'' is now not so well justified.

\begin{itemize}
    \item The decision maker considers $\mu_n$ to be importantly different from the other features of the choice, and so $\alpha$ and $\alpha'$ that differ by a swap involving $\mu_n$ to lead to outcomes that differ by more than the corresponding swap
    \item The decision maker consideres the index $n$ to be importantly different from other indices, and so assesses that the problem of predicting $\RV{Y}_n$ given $\RV{D}_n$ and an arbitrary infinitely supported input-output sequence differs in important respects from the problem of predicting $\RV{Y}_m$ given $\RV{D}_m$ ($m\neq n$) given a similar infinitely supported sequence
    \item The decision maker speculates that their assistant might have some knowledge of the potential outcomes of the involved individuals, and so cannot conclude that the inputs are independent of these potential outcomes
    \item The decision maker assesses that their assistant might make the inputs depend on some unobserved confounder, and so any index $m$ under the assistant's control is not modelled by an intervention on $\RV{D}_m$
\end{itemize}
\end{example}

Perhaps it's too much to ask for any justification to be completely satisfactory. The two options that invoke symmetries feel like they want some unambiguously symmetric thing to justify their less transparent claims of symmetry. The invocation of potential outcomes ``written within'' each patient seems like it requires these potential outcomes to somehow be physically real, and it's plausible that they are not physically real. The approach based on interventions seems to beg the question of why the decision maker's actions are interventions but the assistant's are not. All four approaches seem to offer some insight into the question, and to us at least none are obvious implications of any of the other ones.

In this example, the choice $\text{id}_C$ ``depends on something unknown'' to the decision maker. Back in Section \ref{sec:actions} we opted not to consider models where the choice is a random variable because this introduced difficulties for modelling decision problems and didn't bring obvious benefits. However, this decision relied on the fact that $\text{id}_C$ was in fact the result of a decision maker's deliberation involving the model $\prob{P}_C$. A second purpose of Example \ref{ex:bad_randomised_experiment} is to make the point that calling some arbitrary variable a ``choice'' (or ``$\text{id}_C$'') and choosing not model its distribution does not make its distribution irrelevant.

\citet{kasy_why_2016} has argued that ``randomised controlled trials are not needed for causal identifiability, only controlled trials'', and suggested that experiments should sometimes be designed with deterministic assignments of patients to treatment and control groups, optimised according to the experiment designer's criteria. Following this, \citet{banerjee_theory_2020} suggested that deterministic rules might falter when an experimenter can't pick a function to balance covariates in a way that satisfies everyone in a panel of reviewers. As a final comment on these examples, we think the issue of randomised vs deterministic assignments in controlled experiments is more subtle than these authors claim. As these examples argue, there is a difference between a deterministic assignment selected by ``me'' according to my model and a deterministic assignment selected by ``you'' (or even one selected by ``me'' but with some opaque selection procedure). It may be possible to deal with this issue by specifying the model and assignment principle used to come up with a deterministic assignment, especially if it could be made clear that the procedure used cannot be easily modified post-hoc to alter conclusions.

% We call this example the ``I choose vs you choose'' problem. Suppose we have a decision maker (``DM'') and an administrator (``admin'') cooperating to collect data to support DM to make a choice.

% First, consider the ``I choose'' condition. Here, DM's choice $\alpha\in \{0,1\}^2$ deterministically sets the value of binary inputs $\RV{D}_1,\RV{D}_2$, and the decision maker is interested in evaluating the corresponding binary outputs $\RV{Y}_1,\RV{Y}_2$. The decision maker assesses that their knowledge of the real-world mechanisms that gives rise to each output $\RV{Y}_i$ in the context on an input $\RV{D}_i$ render these mechanisms indistinguishable. 's point of view, the input-output relations for each step are indistinguishable. In particular, they assess that the marginal probabilities of the outputs are the same given a corresponding input, and that the evidence that the first experiment brings to bear on the second is equivalent to the evidence that the second brings to bear on the first. Thus, they assess that exchange commutativity is appropriate; for all $\alpha$:
% \begin{align}
%     \prob{P}_\alpha^{\RV{Y}_1\RV{Y}_2|\RV{D}_1\RV{D}_2} &= \prob{P}_\alpha^{\RV{Y}_2\RV{Y}_1|\RV{D}_2\RV{D}_1}
% \end{align}


% but this example suggests another reason one might want to avoid deterministic treatment assignments. If the choices $\alpha$ are a deterministic sequence of assignments for each index $i$, this means that there is an enormous set of possible choices, and many degrees of freedom if the choices ``aren't actually chosen'' in the sense of the example above. In contrast, if the set of choices is a single parameter in $[0,1]$ which is then used to assign all treatments according to a random procedure depending only on this parameter, there are many fewer degrees of freedom to exploit if the choice ``isn't actually chosen''.

% A particular concern arises when the choice variable $\text{id}_C$ is not associated with the output of a decision procedure involving the model $\prob{P}_C$. In this situation, the value of $\text{id}_C$ can affect the model in potentially unexpected ways. ``Potentially unexpected'' is a vague notion, and we can't say whether $\text{id}_C$ being completely under the decision maker's control avoids ``unexpected'' dependence on $\text{id}_C$, but it seems to be less problematic.

% We set this up in terms of an ``analyst'' and an ``administrator'' who have responsibility for different parts of the procedure. They don't strictly need to be different people, but it helps make the issue clearer. The analyst's job is to construct a model $\prob{P}_C$, evaluate different options $\alpha\in C$ and offer advice regarding the choice. The administrator's job is to choose some $\alpha\in C$ satisfying the analyst's requirements and to carry out any procedure arising from this.

% This separation of concerns gives the administrator a degree of freedom in their choice, and they can potentially use this to choose $\alpha$ with access to information that the analyst lacks.

% In particular, suppose an experiment is modeled by a sequential input-output model $(\prob{P}_C,(\RV{D},\RV{U}),\RV{Y})$ and the set of choices $C=[0,1]^{\mathbb{N}}$ is a length $\mathbb{N}$ sequence of probability distributions in $\Delta(\{0,1\})$. The analyst, based on their knowledge of the experiment, constructs $\prob{P}_C$ such that $\prob{P}_C^{\RV{Y}_i|\RV{U}_i\RV{D}_i}(1|\cdot,\cdot)$ is given by:
% \begin{center}
% \begin{tabular}{ c | c | c }
%   & $\RV{D}_i=0$ & $\RV{D}_i=1$ \\\hline 
%  $\RV{U}_i=0$ & 0 & 0 \\ \hline 
%  $\RV{U}_i=1$ & 1 & 1   
% \end{tabular}
% \end{center}
% and the triples $(\RV{D}_i,\RV{U}_i,\RV{Y}_i)$ are mutually independent given $\text{id}_C$. This makes $\prob{P}_C^{\RV{Y}|\RV{UD}}$ IO contractible over $*$. Suppose also 
% \begin{align}
%     \prob{P}_\alpha^{\RV{D}_i}(1) &= \alpha_i
% \end{align}
% where $\alpha=(\alpha_i)_{i\in\mathbb{N}}$. From the analyst's point of view, both before and after making their recommendations the $\RV{U}_i$ are also IID. This will be expressed with a probability distribution $\prob{Q}$ representing the analyst's prior knowledge:
% \begin{align}
%     \prob{Q}^{\RV{U}_i}(1) &= 0.5
% \end{align}
% one might be tempted to reason that, if $\prob{Q}$ is the analyst's state of knowledge after making any reccomendation, then we should take $\prob{P}_C^{\RV{U}}=\prob{Q}^{\RV{U}}$. Call the resulting model $\prob{P}_C'$. Together with the other assumptions above, this would imply
% \begin{align}
%     \prob{P}_C^{\prime \RV{Y}_i|\RV{D}_i}(1|d) &= 0.5 & \forall d\in \{0,1\}
% \end{align}
% Thus $\prob{P}_C^{\RV{Y}|\RV{D}}$ is also IO contractible.

% However, the analyst's recommendation \emph{does not} fix the value of $\text{id}_C$. Suppose analyst actually recommends any $\alpha$ such that $\lim_{n\to\infty} \sum_i^n \frac{\alpha_i}{n} = 0.5$ (acknowledging that, in this contrived example, there's no obvious reason to do so). Suppose that the administrator operates by the following rule: \emph{first} they observe the value of $\RV{U}_i$, then they choose $\alpha_i$ equal to whatever they saw with an $\epsilon$ sized step towards $0.5$. That is, if they see $\RV{U}_i\yields 1$, they choose $\alpha_i=1-\epsilon$, where $\epsilon < 0.5$.

% Then the analyst should instead adopt the model
% \begin{align}
%     \prob{P}_\alpha^{\RV{U}_i}(1) &= \mathds{1}_{\alpha_i>0.5}
% \end{align}
% Take $\alpha$ such that $\alpha_i=1-\epsilon$ and $\alpha_j=\epsilon$. Then
% \begin{align}
%     \prob{P}_{\alpha}^{\RV{Y}_i|\RV{D}_i}(1|1) &= 1\\
%     &\neq \prob{P}_\alpha^{\RV{Y}_j|\RV{D}_j}(1|1)\\
%     &=0
% \end{align}
% everything has been assumed IID, so $\prob{P}_C^{\RV{Y}|\RV{HD}}$ is not IO contractible.

% The original justification for having a set of choices $C$ is that $C$ is the set of things that, after deliberation aided by the model $\prob{P}_C$, the decision maker might select. The present example does not conform to this understanding of the meaning of the set $C$, and it suggests that one should be cautious when modelling ``decision problems'' with ``choices'' that are not actually the things that are being chosen.

% This point is related to the question of why experimenters randomise. \citet{kasy_why_2016} argues that ``randomised controlled trials are not needed for causal identifiability, only controlled trials'', and suggests that experiments should sometimes be designed with deterministic assignments of patients to treatment and control groups, optimised according to the experiment designer's criteria. Following this, \citet{banerjee_theory_2020} suggested that deterministic rules might falter when one can't pick a function to balance covariates in a way that satisfies everyone in a panel of reviewers. 

% The condition $\RV{YIJ}\CI^e_{\prob{P}_C} \text{id}_C|\RV{D}$ without also having $\RV{YIJ}\CI^e_{\prob{P}_C} \RV{D}|\text{id}_C$ does \emph{not} imply the conclusion of Theorem \ref{th:cc_ind_treat}. Informally, if $\RV{D}$ gives some ``extra information'' over and above $\text{id}_C$, then any symmetry that holds before we observe $\RV{D}$ might not hold after $\RV{D}$ has been observed. We have argued somewhat informally that the choice $\text{id}_C$ should be completely under the decision maker's control -- for Theorem \ref{th:cc_ind_treat}, this perfect control has to extend to the sequence of inputs $\RV{D}$, as we show with Example \ref{ex:not_enough_control}.

% Example \ref{ex:not_enough_control} requires the hypotheses that any given identifier $k\in\mathbb{N}$ could be associated with one of two input-output maps $D\kto Y$. Thus the space of hypotheses is a sequence of binary values $J=\{0,1\}^{\mathbb{N}}$. Equipped with the product topology, $J$ is a countable product of separable, completely metrizable spaces and is therefore also separable and completely metrizable \citep[Thm. 16.4,Thm. 24.11]{willard_general_1970}. Thus $(J,\mathcal{B}(J))$ is a standard measurable space and, because it is uncountable, it is isomorphic to $([0,1],\mathcal{B}([0,1]))$.

% \begin{example}\label{ex:not_enough_control}
% Take $Y=C=D=\{0,1\}$ and take $(J,\sigalg{J})$ to be $\{0,1\}^{\mathbb{N}}$ equipped with the product topology. For any $i\neq 1$, $\RV{Y}_i\RV{I}_i\RV{D}_i\CI^e_{\prob{P}_C} \text{id}_C$, while $\prob{P}_\alpha^{\RV{D}_1}=\delta_\alpha$ and $\RV{I}_i\CI^e_{\prob{P}_C} \text{id}_C$.

% $\RV{YI}\CI^e_{\prob{P}_C} \text{id}_C|\RV{D}$ follows from the fact that $\text{id}_C$ can be written as a function of $\RV{D}$. However, except for $i=1$, $\RV{D}_i$ is independent of $\text{id}_C$.

% For all $i,\in \mathbb{N}$, $y,d\in \{0,1\}$, $j\in J$ set
% \begin{align}
%     \prob{P}_C^{\RV{Y}_i|\RV{J}\RV{I}_i\RV{D}_i}(y|j,k,d) &= \delta_1(p(k,j))\delta_d(y) + \delta_0(p(k,j))\delta_{1-d}(y)
% \end{align}
% where $p(k,j)$ projects the $k$-th component of $j$. That is, if $j$ maps $k$ to 1, $\RV{Y}$ goes with $\RV{D}$ while if $h$ maps $k$ to $0$, $\RV{Y}$ goes opposite $\RV{D}$. Suppose also 
% \begin{align}
%     \RV{Y}_i\CI_{\prob{P}_C}^e (\RV{X}_{<i},\RV{Y}_{<i},\RV{I}_{<i},\text{id}_C)|(\RV{X}_i,\RV{Y}_i,\RV{J})
% \end{align}
% Then $\prob{P}_C^{\RV{Y}|\RV{DI}}$ is IO contractible. Set $\prob{P}_{C}^{\RV{J}}$ to be the uniform measure on $(J,\sigalg{J})$ and for $i>1$
% \begin{align}
%     \prob{P}_C^{\RV{D}_i|\RV{I}_i\RV{J}}(d|k,j) &= \delta_{p(k,j)}(d)
% \end{align}
% that is, if $j$ maps $k$ to 1, $\RV{D}$ is 1 while if $j$ maps $k$ to $0$, $\RV{D}$ is 0. This also implies
% \begin{align}
%     \prob{P}_C^{\RV{I}_i|\RV{D}_i\RV{J}}(p(\cdot,j)^{-1}(d)|d,j) &= 1\label{eq:all_eq_d}
% \end{align}

% Then, for $i>1$
% \begin{align}
%     \prob{P}_\alpha^{\RV{Y}_i|\RV{J}\RV{D}_i}(y|j,d) &= \sum_{k\in \mathbb{N}} \delta_1(p(k,j))\delta_d(y)\prob{P}_C^{\RV{I}_i|\RV{D}_i\RV{J}}(k|d,j) + \delta_0(p(k,j))\delta_{1-d}(y)\prob{P}_C^{\RV{I}_i|\RV{D}_i\RV{J}}(k|d,j)\\
%     &= \sum_{k\in \mathbb{N}} \delta_1(d)\delta_d(y)\prob{P}_C^{\RV{I}_i|\RV{D}_i\RV{J}}(k|d,j) + \delta_0(d)\delta_{1-d}(y)\prob{P}_C^{\RV{I}_i|\RV{D}_i\RV{J}}(k|d,j)&\text{by Eq \eqref{eq:all_eq_d}}\\
%     &= \delta_1(y)\\
%     \implies \prob{P}_\alpha^{\RV{Y}_i|\RV{D}_i}(y|d) &= \delta_1(y)
% \end{align}

% For $q\in I$, set
% \begin{align}
%     \prob{P}_C^{\RV{I}|\RV{J}}(q|j)&= \begin{cases}
%         0.5 & q=(1,2,3,4,...) \text{ or } (1,3,2,4,...)\\
%         0&\text{otherwise}
%     \end{cases}
% \end{align}
% and set
% \begin{align}
%     \prob{P}_C^{\RV{J}|\RV{D}}(j) &= \begin{cases}
%         0.5 & j=(0,1,0,1,1,...)\text{ or }j=(0,0,1,1,1,...)\\
%         0 &\text{otherwise}
%     \end{cases}
% \end{align}
% Let $\overline{J}$ be the support of $\prob{P}_C^{\RV{J}|\RV{D}}(j)$.

% Then for $i=1$
% \begin{align}
%     \prob{P}_\alpha^{\RV{Y}_1|\RV{J}\RV{D}_1}(y|j,d) &= \sum_{h\in J} \sum_{j\in \mathbb{N}} \prob{P}_\alpha^{\RV{I}_1|\RV{D}_1\RV{J}}(j|d,j)\prob{P}_C^{\RV{J}|\RV{D}_1}(j|d)\left(\delta_1(p(j,h))\delta_d(y) + \delta_0(p(j,j))\delta_{1-d}(y)\right)\\
%     &= \sum_{j\in \overline{J}} 0.5( \delta_1(p(1,j))\delta_d(y) + \delta_0(p(1,h))\delta_{1-d}(y))\\
%     &= \delta_{1-d}(y))\\
%     &\neq  \prob{P}_\alpha^{\RV{Y}_i|\RV{J}\RV{D}_i}(y|j,d) & i\neq 1
% \end{align}
% Thus $\prob{P}_C^{\RV{Y}|\RV{J}\RV{D}}$ is not IO contractible by Theorem \ref{th:equal_of_condits}. 

% However, given any finite permutation $\rho:\mathbb{N}\to\mathbb{N}$
% \begin{align}
%     \prob{P}_\alpha^{\RV{Y}|\RV{I}}(y|q) &= \sum_{j\in \overline{J}}\sum_{d\in\{0,1\}^{\mathbb{N}}} \prod_{i\in \mathbb{N}} \prob{P}_C^{\RV{Y}_i|\RV{I}_i\RV{D}_i\RV{H}}(y_i|q_i,d_i,j) \prob{P}_\alpha^{\RV{D}_i|\RV{I}_i\RV{J}}(d_i|q_i,j)\prob{P}_C^{\RV{J}}(j)\\
%     &= \delta_{1-\alpha}(y_1)\delta_{(1)_{i\in\mathbb{N}}}(y_{>1})\\
%     &= \prob{P}_\alpha^{\RV{Y}|\RV{I}}(y|\rho^{-1}(q))\\
%     &= \kernel{F}_{\rho}\prob{P}_\alpha^{\RV{Y}|\RV{I}}(y|q)
% \end{align}
% \end{example}

% Theorem \ref{th:cc_ind_treat} requires both that the inputs fully identify which option was chosen -- there is no ``side channel'' by which different options may influence the outputs -- and that the chosen option fully determines the sequence of inputs. This is analogous to the result that randomised experiments partial compliance do not allow the same causal effects to be determined as randomised experiments with full compliance \citep[Sec. 8.1.2]{pearl_causality:_2009}.


\section{Conclusion}

We've shown how models constructed according to the causal Bayesian network and potential outcomes approaches can be translated to decision models. Both translations feature CIIR sequences, though in general these sequences involve unobserved inputs. In order to make this translation, we strengthened the assumptions underpinning causal Bayesian networks to rule out some possibilities we deemed undesirable that could arise when certain inputs have no support in the observational distribution. We also offered an interpretation for how the effects of choices could be represented in a potential outcomes model. We don't claim that either of these translations is the only possible way to translate models from either family to decision models, but we believe that the translations we offer are reasonable.

We only considered the simplest kind of interventions in casual Bayesian networks -- perfect or ``hard'' interventions. Decision models may offer a more precise formalisation of interventions whose effects are in some manner unknown, owing to their explicit representation of uncertainty over they hypothesis $\RV{H}$. Recalling the suggestion in Section \ref{sec:whats_the_point} that counterfactuals may also be modeled by considering them to be the consequences of certain (physically implausible) actions, it may also be possible to extend the translation we have presented to encompass ``counterfactual interventions''.

We showed in Theorem \ref{th:det_obs_to_cons} that the assumption of \emph{precedent} can, under a side condition of \emph{generic relationships} between certain conditionals, yield the conclusion that some sequence of inputs and corresponding outputs are related by conditionally independent and identical responses. The side condition we consider is similar to assumptions considered to be implied by causal direction, though the precise correspondence between notions of causal direction and the kind of generic relationship between conditionals we require is an open question.

Recalling the discussion in Section \ref{sec:nci_nco}, the conclusion of CIIR sequences from Theorem \ref{th:det_obs_to_cons} is not enough \emph{on its own} to be useful to a decision maker. The decision maker must \emph{also} bring some prior knowledge such as how their options affect the inputs in the sequence in question. As we point out Section \ref{sec:nci_nco}, this kind of prior knowledge itself can suggest that some ``input'' variables are susceptible to influence by individuals who are ignorant about the conditional distribution of ``outputs'' given these inputs. We raise as a further open question when, if ever, reasoning of this type might support an assumption of the right kind of ``generic relationship'' between conditionals required by Theorem \ref{th:det_obs_to_cons}.

% We also showed how a conclusion of causal identifiability can follow from the assumptions that an experiment with identified ``experimental units'' is (in a nontrivial manner) reorderable, that the distribution over outputs it is invariant to reorderings of the experimental units and that the inputs are ``completely under the decision maker's control''. The shuffling of experimental units that we consider have previously suggested to support an analogous conclusion, and we offer a precise mathematical account of what these informal assumptions of experimental symmetries must imply for the conclusion of identifiability to follow. Notably, in Theorem \ref{th:cc_ind_treat}, full compliance arises as a formal requirement rather than an additional informal constraint on the theory of randomised experiments.

% The assumptions of precedent and individual-level CIIR sequences employ two different methods to get around the problem discussed in Chapter \ref{ch:evaluating_decisions} that the assumption of CIIR sequences is often unreasonable. Precedent considers CIIR sequences with unobserved inputs, which makes the experimental symmetries identified in the previous chapter irrelevant on the grounds that there is no experiment that generates the required sequence of observations and therefore no altered experiment that it ought to be equivalent to. Individual-level CIIR sequences force every input in the sequence of inputs to have a unique value, which means that the ``hypothesis'' $\RV{H}$ cannot be determined as a function of the observed data, also ruling out the objectionable symmetries. The final open question we pose for this chapter is: are there any fundamental differences between these two approaches, or are they essentially similar?