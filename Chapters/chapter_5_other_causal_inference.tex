%!TEX root = ../main.tex

\chapter{Causal modelling with decision models}\label{ch:other_causal_frameworks}

In previous chapters we've proposed a general type for decision models and examined a particular assumption -- the assumption of CIIR sequences -- that allows one to predict future consequences of actions based on previously observed data. We noted that this assumption is often unreasonable. In this chapter, we consider the problem of constructing more broadly applicable decision models that license some kind of causal inference. 

Our approach is first to examine what kinds of decision models are induced by causal Bayesian networks and potential outcomes models, two widely used frameworks for causal modelling. We then work backwards to propose two assumptions applicable to decision models based on regularities imposed by these frameworks. These assumptions are \emph{precedent} and \emph{individual level CIIR sequences} respectively.

When we try to work out what kind of decision models are expressed by causal Bayesian networks and potential outcomes, there are small gaps that need to be filled in with additional assumptions. Causal Bayesian networks are ``rolled up'' and an assumption must be made about how exactly to ``unroll'' it to a sequential model. Some of the additional assumptions that we regard to be required to ensure that the resulting unrolled model is faithful to the original intent of the rolled-up model are nontrivial. Potential outcomes models, on the other hand, do not offer a notion of ``options'', and a judgement must be made about how a potential outcomes model induces a collection of options and their associated consequences. These gaps are minor, but they do reflect the different commitments of the different approaches. These differences are summarised in the table below:
\begin{center}
\begin{tabular}{ |c|p{4cm}|p{4cm}| } 
 \hline
  & Must & Optionally  \\
 \hline
 Potential outcomes & Represent potential outcomes of every ``output'' variable & Represent consequences of choosing different options \\ 
 Causal Bayesian networks & Represent consequences of choosing different options for ``generic variables'', options are interventions & Specify exactly which variables are affected by the choice \\
 Decision models & Represent consequences of choosing different options & Options are interventions, represent potential outcomes of output variables\\
 \hline
\end{tabular}
\end{center}
Section \ref{sec:how_to_cbn} sets out the kinds of decision models induced by causal Bayesian networks, and Section \ref{sec:potential_outcomes} sets out the decision models induced by potential outcomes.

The assumption of \emph{precedent}, explored in  is motivated by the general idea that in some cases a decision maker might guess that every option they have available has already been tried by someone, somewhere, though they do not know exactly when. On its own, it doesn't allow a decision maker to conclude very much (although it does allow them to surmise that, given enough data, the consequences of their choice must have occurred at least once). However, in conjunction with the assumption of \emph{generic relationships between conditionals}, it allows a decision maker to conclude from a conditional independence that a certain sequence of variable pairs features conditionally independent and identical responses. This result is similar in spirit to the principle underlying causal inference by invariant prediction \citep{peters_causal_2016}, which identifies causal parents on the basis of groups of variables that render a target variable conditionally independent of an ``environment'' variable. However, we offer substantially different assumptions -- instead of an assumption of generic relationships between conditionals, invariant prediction places restrictions on the kinds of ``interventions'' that can happen in each environment. The assumption of generic relationships between conditionals is itself similar to assumptions that have been used to justify inferences of causal direction \citep{meek_strong_1995,lemeire_replacing_2013}. Our result suggests that the assumption of precedent might facilitate understanding of the relationship between this notion of causal direction and invariance based causal inference without appealing to any notion of intervention. This is explored further in Section \ref{sec:precedent}

Individual-level CIIR sequences are a weaker version of the assumption of CIIR sequences explored in Section \ref{sec:ilevel_ccontract}. Here, we suppose each element of the sequence has a unique identifier. Importantly, it must be possible for any element of the sequence to receive any identifier, which rules out (for example) identifying the first element with ``1'' and the second element with ``2'' and so forth. We then make the assumption that we can identify a CIIR sequence of (input, identifier)-output pairs -- where, we stress, \emph{the identifier is one of the ``inputs''}. This strongly rules out the assumption that the inputs have infinite support -- in fact, by assumption, each input appears only once. Hypotheses in a model of this type associate each possible value of the identifier with a fixed stochastic input-output function. This is very similar to the setup of a potential outcomes model, except the ``potential outcomes'' are associated with particular values of the identifier variable. This change allows us to prove in Theorem \ref{th:cc_ind_treat} that an experiment in which identifiers are, in a sense, exchangeable and inputs (other than the identifier) are, in a sense, completely under the decision maker's control, then we also conclude that the input-output sequence (excluding the identifiers) also features CIIRs. This conclusion is very similar to the condition of \emph{ignorability} in the potential outcomes framework, and Theorem \ref{th:cc_ind_treat} in fact formalises a number of previously informal arguments for accepting ignorability in controlled experiments. We also note, in this section, that the assumption of individual-level CIIR sequences and how it works with other assumptions is somewhat opaque to us.

\section{Causal Bayesian networks}\label{sec:how_to_cbn}

Causal Bayesian networks are a family of structural interventional models. In the form presented here, they provide a ``hard intervention'' operation, which offers a rule for transforming a joint probability distribution $\prob{P}^{\RV{V}}$ over a sequence of variables $\RV{V}$ according to a directed acyclic graph $\graph{G}$ with nodes $\node{V}$ corresponding 1-to-1 to the variables in $\RV{V}$. There are many variations of causal Bayesian networks with more general classes of intervention \citep{yang_characterizing_2018}.

Structural Causal Models (SCMs) are an alternative class of structural interventional models. Such models distinguish a set of noises $\RV{U}$ from observed variables $\RV{V}$, and provide a collection of functions relating the observed variables to the noises and other observed variables. The noises are allowed to be stochastic. Functional relationships can be interpreted as deterministic conditional probabilities, and it is in principle possible to formulate SCMs using our approach. However SCMs are often extended in ways that causal Bayesian networks are not, for example with counterfactual operations \citep{barenboim_foundations_2020}, or to include cyclic causal relationships \citep{bongers_theoretical_2016,forre_causal_2020}, and because addressing these extensions in detail is beyond the scope of this work, we focus on causal Bayesian networks.

In order to represent causal Bayesian networks as decision models, we have to make two generalisations. First, the variables that appear in a causal Bayesian network are ``generic variables''. This is different to the ``observed random variables'' discussed in Section \ref{sec:rvs_mps} which are associated with measurement procedures. Generic variables are instead associated with a \emph{type} of measurement procedure. We can have a measurement procedure to measure \emph{your} height, and a type of measurement procedure that measures people's height. A sequence of measurements of different people's heights could be associated with a single type of measurement procedure. To make predictions about observed random variables, we need to generalise causal Bayesian networks to sequential models -- this is the first generalisation.

Causal Bayesian networks are specified by a directed acyclic graph $\graph{G}$ and a probability distribution $\prob{P}$. However, implicit in their use is the fact that the probability distribution $\prob{P}$ and maybe even the graph $\graph{G}$ (or parts of it) should be learned from the given data. A decision model makes the dependence of the model on the data explicit via a hypothesis variable $\RV{H}$. A causal Bayesian network might be thought of as a representation of a hypothesis, rather than a decision model in its own right. Constructing decision models with hypotheses corresponding to causal Bayesian networks is the second generalisation we make.

\subsection{Definition of a Causal Bayesian Network}\label{sec:def_cbn}

We follow the definition of a Causal Bayesian Network on \citet[page ~23-24]{pearl_causality:_2009}. There are a couple of technical differences: we require that interventional models are a measurable map from interventions to probability distributions, and we assume that there is a common sample space for every interventional distribution. There are also some non-technical differences: the notation is adapted for compatibility with the rest of the work in this thesis, and we separate the definition into two parts for clarity (Definitions \ref{def:interventional} and \ref{def:CBN}). These changes don't make a meaningful difference to the content of the theory.

An interventional model is a \emph{Causal Bayesian Network} with respect to a directed acyclic graph if it satisfies a number of compatibility requirements. The following definitions are standard, and reproduced here for convenience. The definitions here are terse, readers should refer to \citet[chap. ~1]{pearl_causality:_2009} for a more intuitive explanation.

\begin{definition}[Directed graph]\label{def:dir_graph}
A directed graph $\graph{G}=(\node{V},\node{E})$ is a set of nodes $\node{V}$ and edges, which are ordered pairs of nodes $\node{E}\subset \node{V}\times\node{V}$. Nodes are written using the font $\node{V}$.
\end{definition}

The parents of a target node are all nodes with an edge ending at the target node.

\begin{definition}[Parents]
Given a directed graph $\graph{G}=(\node{V},\node{E})$ and $\node{V}_i\subset \node{V}$, the parents of $\node{V}_i$ are $\PA{\graph{G}}{\node{V}_i}:=\{\node{V}_j\in \node{V}|(\node{V}_j,\node{V}_i)\in\node{E}\}$.
\end{definition}

We offer a recursive definition of \emph{descendants}.

\begin{definition}[Non-descendants]
Given a directed graph $\graph{G}=(\node{V},\node{E})$, a descendant of $\node{V}_i$ is a node $\node{V}_j$ such that $\node{V}_i$ is a parent of $\RV{V}_j$, or there is some parent $\node{V}_k$ of $\node{V}_j$ that is a descendant of $\node{V}_i$. Any node that is a not a descendant of $\node{V}_i$ is a non-descendant of $\node{V}_i$, and the set of non-descendants of $\node{V}_i$ is denoted $\mathrm{ND}({\node{V}_i})$
\end{definition}

A path is a sequence of edges such that the $i$th edge and the $i+1$th edge share exactly one node.

\begin{definition}[Path]
Given a directed graph $\graph{G}=(\node{V},\node{E})$, a path is a sequence of edges $(E_i)_{i\in A}$ (where $A$ is either $[n]$ or $\mathbb{N}$) such that for any $i$, $E_i$ and $E_{i+1}$ share exactly one node.
\end{definition}

A directed path is a sequence of edges such that the end of the $i$th edge is the beginning of the $i+1$th edge.

\begin{definition}[Directed path]
Given a directed graph $\graph{G}=(\node{V},\node{E})$, a directed path is a sequence of edges $(E_i)_{i\in A}$ (where $A$ is either $[n]$ or $\mathbb{N}$) such that for any $i$, $E_i=(\node{V}_k,\node{V}_l)$ implies $E_{i+1}=(\node{V}_l,\node{V}_m)$ for some $\node{V}_m\in \node{V}$.
\end{definition}

In an acyclic graph, directed paths never reach to the same node more than once.

\begin{definition}[Directed acyclic graph]
A directed graph $\graph{G}=(\node{V},\node{E})$ is acyclic if, for every path, each node appears at most once. Directed acyclic graph is abbreviated to ``DAG''.
\end{definition}

d-separation is a key property of directed acyclic graphs for defining causal Bayesian networks. It is defined with respect to undirected paths.

\begin{definition}[Blocked path]
Given a DAG $\graph{G}=(\node{V},\node{E})$, a path $p$ is blocked by $\node{V}_A\subset\node{V}$ iff
\begin{enumerate}
    \item $(\node{V}_i,\node{V}_j)\in p$ and $(\node{V}_j,\node{V}_k)\in p$ for all $\node{V}_j\in \node{V}_A$
    \item $(\node{V}_j,\node{V}_i)\in p$ and $(\node{V}_j,\node{V}_k)\in p$ for all $\node{V}_j\in \node{V}_A$
    \item $(\node{V}_i,\node{V}_j)\in p$ and $(\node{V}_k,\node{V}_j)\in p$ for all $\node{V}_j\cup \DE(\node{V}_j)\cap \node{V}_A=\emptyset$
\end{enumerate}
\end{definition}

\begin{definition}[d-separation]
Given a DAG $\graph{G}=(\node{V},\node{E})$, $\node{V}_A$ is $d$-separated from $\node{V}_B$ by $\node{V}_C$ (all subsets of \node{V}) if $\node{V}_C$ blocks every path starting at $\node{V}_A$ and ending at $\node{V}_B$. This is written $\node{V}_A\perp_{\mathcal{G}} \node{V}_B | \node{V}_C$.
\end{definition}

\begin{definition}[Variable-node association]
Given a graph $\graph{G}=(\node{V},\node{E})$ and a sequence of variables $\RV{V}_A:=(\RV{V}_i)_{i\in A}$, if $|A|=|\node{V}|$ we can associate a variable with each node of the graph with an invertible map $m:\{\RV{V}_i|i\in A\}\to \node{V}$ that sends $\RV{V}_i\mapsto \node{V})_i$. By convention, we give associated variables and nodes corresponding indices, and graphical operations are defined on variables through $m$, i.e. $\mathrm{Pa}(\RV{V}_i):=m(\mathrm{Pa}(m^{-1}(\RV{V}_i)))$.
\end{definition}

\begin{definition}[Compatibility]\label{def:compat}
Given a measurable space $(\Omega,\sigalg{F})$, a Markov kernel $\prob{P}_\cdot:C\kto \Omega$ and a sequence of variables $(\RV{V}_i)_{i\in A}$ with $\RV{V}_i:\Omega\to V_i$ and a DAG $\mathcal{G}$ with nodes $\{\node{V}_i\}_{i\in A}$ and the variable-node association $m$, $\prob{P}_\cdot$ is compatible with $\mathcal{G}$ relative to $m$ if for all $I,J,K\subset A$, $\node{V}_I\perp_{\mathcal{G}} \node{V}_J | \node{V}_K$ implies $\RV{V}_I\CI^e_{\prob{P}_\cdot} \RV{V}_J | (\RV{V}_K,\text{id}_C)$.
\end{definition}

The following definition is reproduced from \citet{pearl_causality:_2009} with the differences previously mentioned: notation has been matched to ours, the interventional model is assumed to be measurable and the interventional distributions are assumed to be defined on a common sample space.

\begin{definition}[Interventional model]\label{def:interventional}
An interventional model is a tuple $(\prob{P}_C,\Omega,(\RV{V}_A)$ where $(\Omega,\sigalg{F})$ is a measurable space,  $\RV{V}:=(\RV{V}_i)_{i\in A}$, $A\subset\mathbb{N}$ a sequence of variables with $\RV{V}_i:\Omega\to V_i$, and where the option set $C$ given by
\begin{align}
    C:=\{\mathrm{do}_{\emptyset}\}\cup \{(\mathrm{do}_B,v_B)|B\subset A,v_B\in \mathrm{Range}(V_B)\}
\end{align}
That is, we take every subsequence $\RV{V}_B$ of $\RV{V}_A$ and for each subsequence add to $C$ every element of the range of $\RV{V}_B$, each labeled with the symbol $\mathrm{do}_B$.
\end{definition}

\begin{definition}[Causal Bayesian network]\label{def:CBN}
Given an interventional model $(\prob{P}_C,\Omega,\RV{V}_A)$ and a directed acyclic graph $\graph{G}$ with nodes $\node{V}$, $(\prob{P}_C,\Omega,\RV{V}_A,\graph{G})$  is a \emph{causal Bayesian network} with respect the node-variable association $m$ if:
\begin{enumerate}
    \item $\prob{P}_\cdot$ is compatible with $\graph{G}$ with respect to $m$
    \item $B\neq \emptyset \implies \prob{P}_{(\mathrm{do}_{B},v_B)}^{\RV{V}_B} = \delta_{v_B}$
    \item $\prob{P}_{(\mathrm{do}_{B},v_B)}^{\RV{V}_i|\mathrm{Pa}(\RV{V}_i)}\overset{\prob{P}_{(\mathrm{do}_{B},v_B)}}{\cong}\prob{P}_{\mathrm{do}_{\emptyset}}^{\RV{V}_i|\mathrm{Pa}(\RV{V}_i)}$ for all $i\not\in B$
\end{enumerate}
\end{definition}

This definition of a causal Bayesian network is agnostic about how one actually goes about constructing a model of this type. As a practical matter, if in the course of trying to to construct a causal Bayesian network one selects a set of variables $\RV{V}_A$ because they are convenient or for some other reason that's not downstream of the chosen modelling, one has to be careful make sure the chosen variables $\RV{V}_A$ are ``interventionally compatible''. In particular, we require $v_{\mathrm{ND}(\RV{V}_i)}\mapsto \delta_{v_i}$ be a $\RV{V}_i|\mathrm{ND}(\RV{V}_i)$-valid conditional for all $i$ (Definition \ref{def:valid_conditional_prob}, or else the probability set for some interventions on $\RV{V}_i$ may be empty. For a contrived example, the sequence $(\RV{V}_i, 2*\RV{V}_i)$ is not interventionally compatible, as at least one variable must be a non-descendant of the other, but it is not possible to set the value of one independently of the other. See also the discussion of body mass index in Example \ref{ex:invalidity} for a real world example arising from a failure to perform this check.

For continuously valued variables, the fact that this definition offers the ability to pick a version of the conditional probability for each intervention is problematic. Suppose $\node{V}_i$ is a parent of $\node{V}_j$, and the associated variable $\RV{V}_i$ is continuously valued and $\prob{P}_{\mathrm{do}_{\emptyset}}^{\RV{V}_i}(\{v_i\})=0$ for all singletons $v_i\in V_i$. Then for every intervention $\mathrm{do}_{\{i\}}(v_i)$, we can choose a version of $\prob{P}_{\mathrm{do}_{\emptyset}}^{\RV{V}_j|\RV{V}_i}$ that takes an arbitrary value at the point $v_i$ (because this point has measure 0), so property (3) is satisfied trivially. Some additional consistency condition seems to be required for this case, but we do not explore what it is here.

The freedom to choose versions of the conditional distributions where the ``passive'' distribution has no support might actually be a feature that distinguishes causal Bayesian networks from SCMs, but we don't investigate this point in detail.

\subsection{Unrolled causal Bayesian networks}\label{sec:unrolling}

Given a probability space $(\prob{P},\Omega,\sigalg{F})$ and an independent and identically distributed (IID) sequence $\RV{X}:=(\RV{X}_i)_{i\in [n]}$, it is common to ``roll up'' the joint distribution $\prob{P}^{\RV{X}}\in\Delta(X^n)$ to a single representative distribution $\prob{P}^{\RV{X}_0}\in \Delta(X)$ and say something like ``the $\RV{X}_i$ are IID according to $\prob{P}^{\RV{X}_0}$''. Because of the IID assumption, the full joint distribution $\prob{P}^{\RV{X}}\in\Delta(X^n)$ can be unambiguously reconstructed from a statement like this.

A causal Bayesian network is similarly a rolled-up representation of a model of some sequence of variables. Unlike an IID sequence, it isn't completely unambiguous how to unroll it, though the ambiguity doesn't seem to be especially problematic for ordinary causal Bayesian networks. In Definition \ref{def:unr_CBN}, we propose a canonical form of an unrolled causal Bayesian network. 

\begin{definition}[Unrolled causal Bayesian network]\label{def:unr_CBN}
Given an interventional model $(\prob{P}_\cdot,C,\Omega,\RV{V}_{A\times \mathbb{N}})$ and a directed acyclic graph $\graph{G}$ with nodes $\node{V}_A$, $(\prob{P}_\cdot,C,\Omega,\RV{V}_{A\times \mathbb{N}})$  is an \emph{unrolled causal Bayesian network} with respect to the node-variable association maps $m_j:\RV{V}_{ij}\mapsto \node{V}_i$ if, for all $j,k\in \mathbb{N}$:
\begin{itemize}
    \item [1*] $\prob{P}_\cdot^{\RV{V}_{Aj}}$ is compatible with $\graph{G}$ with respect to $m_j$ for all $j\in \mathbb{N}$
    \item [2*] $\pi_j(\alpha) = (\mathrm{do}_{B_j},v_{B_j})$ and $B_j\neq \emptyset$ implies $\prob{P}_{(\mathrm{do}_{B_j},v_{B_j})}^{\RV{V}_{Bj}} = \delta_{v_{B_j}}$
    \item [3*] If $\pi_j(\alpha) = (\mathrm{do}_{B_j},v_{B_j})$, $\pi_k(\alpha) = (\mathrm{do}_{B_k},v_{B_k})$ and $i\not \in B_j\cup B_k$ then $\prob{P}_{\alpha}^{\RV{V}_{ij}|\mathrm{Pa}(\RV{V}_{ij})}\overset{\prob{P}_{\alpha}}{\cong}\prob{P}_{\mathrm{do}_{\emptyset}}{\RV{V}_{ik}|\mathrm{Pa}(\RV{V}_{ik})}$
    \item [4*] $\pi_j(\alpha)=\pi_j(\alpha')$ implies $\prob{P}_\alpha^{\RV{V}_{Aj}}= \prob{P}_{\alpha'}^{\RV{V_{Aj}}}$
    \item [5*] $\RV{V}_{Aj}\CI^e_{\prob{P}_{C}} \RV{V}_{A \mathbb{N}\setminus\{j\}} |\text{id}_C$
\end{itemize}
\end{definition}

\subsubsection[Unrolled CBNs]{Explaining the definition of unrolled causal Bayesian networks}

Suppose we have a causal Bayesian network $(\prob{P}_C,\Omega,\RV{V}_A)$ that we want to extend to a sequence of variables $\RV{V}:= (\RV{V}_{ij})_{i\in A,j\in \mathbb{N}}$. We need to propose a set of interventions for the unrolled model, and ensure that we have appropriate analogues of all of the causal Bayesian network compatibility conditions that hold for each element of the sequence. There is a little ambiguity in the choice of an extended set of interventions (though it may not be problematic in practice). Unrolled a causal Bayesian networks have not been defined in the literature, and there are other formal ambiguities -- for example, we could omit condition 4*, but omitting this condition would allow for unrolled models that roll up to different causal Bayesian networks depending on the precise extended intervention chosen, which clashes with to our understanding of what a causal Bayesian network is ``supposed'' to model.

First, we extend the set $C$ to be the set of sequences of interventions 
\begin{align}
\{(\mathrm{do}_{B_j j}(v_B))_{j\in \mathbb{N}}|\forall j: B_j \subset A,v_{B_j}\in \mathrm{Range}(\RV{V}_{B_j})\}
\end{align}
i.e. $C$ now consists of all sequences of separate interventions to each subsequence of variables $\RV{V}_{Aj}:=(\RV{V}_{ij})_{i\in A}$, understood to refer to variables arising from a particular iteration of the decision procedure. 

This specification of interventions in an unrolled causal Bayesian network differs slightly from the method explored in \citet{lattimore_causal_2019} which unrolls a causal Bayesian network to a length 2 sequence and forces the intervention on the first element of the sequence to be the passive intervention. This difference seems like it would usually be unproblematic insofar as the specification of which elements of the sequence might be influenced by the choices of the decision maker may often be pretty obvious.

Given a graph $\graph{G}=(\node{V},\node{E})$, we now have a collection of variable-node association maps $m_j:\{\RV{V}_{ij}|i\in A\}\to \node{V}$ such that $m_j(\RV{V}_{ij})=\node{V}_i$.

We now need to specify how variables in an unrolled causal Bayesian network are distributed, given some sequence of interventions. By analogy with the original case of IID variables, we conclude that the $\RV{V}_{Aj}:=(\RV{V}_{ij})_{i\in A}$ are mutually independent given any particular sequence of interventions. Furthermore, Definition \ref{def:CBN} constrains the distribution of each variable given a particular sequence of interventions from $C$. For a sequence of interventions $\alpha\in C$, let $\pi_j(\alpha)$ be the $j$th intervention in the sequence. One might posit the following analogue of condition (3): 
\begin{itemize}
    \item [3'] $\pi_j(\alpha)=(\mathrm{do}_{B_j},v_{B_j})$ implies $\prob{P}_{\alpha}^{\RV{V}_{ij}|\mathrm{Pa}(\RV{V}_{ij})}\overset{\prob{P}_{\alpha}}{\cong}\prob{P}_{\mathrm{do}_{\emptyset}}^{\RV{V}_{i1}|\mathrm{Pa}(\RV{V}_{i1})}$ for all $i\not\in B$
\end{itemize}
Where $\mathrm{do}_{\emptyset}^n$ is a sequence of $n$ $\mathrm{do}_{\emptyset}$ interventions. This is a combination of an assumption that variables in the sequence are conditionally independent and identically distributed given appropriate interventions and condition (3) from Definition \ref{def:CBN}. However, it's not quite satisfactory. Take $B_j:=\mathrm{Pa}(\RV{V}_{ij})$, and suppose $\prob{P}_{\mathrm{do}_{\emptyset}^n}^{\RV{B}_{1}}(\{x\})=0$. Then (3') would be satisfied by a model for which
\begin{align}
    \prob{P}_{(\mathrm{do}_{B_1},x,\mathrm{do}_{B_2},x)}^{\RV{V}_{i1}|\RV{V}_{B_1}}(U|x) &= \delta_0(U)\\
    \prob{P}_{(\mathrm{do}_{B_1},x,\mathrm{do}_{B_2},x)}^{\RV{V}_{i2}|\RV{V}_{B_2}}(U|x) &= \delta_1(U)
\end{align}
that is, if the empty intervention is unsupported over some element of the range of a variable, then (3') allows models that assign different consequences to repetitions of the same intervention on this variable, if those interventions force the variable into the region that originally had no support.

We propose instead a restricted assumption of identical response functions: for any pair $\RV{V}_{ij}$ and $\RV{V}_{ik}$, unless $i$ is intervened on by $\pi_j(\alpha)$ and not intervened on by $\pi_{k}(\alpha)$, then then the conditional probability of $\RV{V}_{ij}$ given its parents is almost surely equal (with respect to $\prob{P}_\alpha$) to the conditional probability of $\RV{V}_{ik}$ given its parents. This is condition 3*.

In order to be able to ``roll up'' a sequence of interventions, we also require that the response to the $j$th intervention does not depend on any of the interventions other than the $j$th. If this were not the case, then even if the restricted assumption of identical response functions were satisfied, different sequences of interventions might ``roll up'' to different interventional models. In particular, consider $\prob{P}_{\mathrm{do}_{\emptyset}^n}^{\RV{B}_{1}}(\{x\})=0$ again, $\RV{B}_j$ as before. Then we might have, consistently with 1*-3*,
\begin{align}
    \prob{P}_{(\mathrm{do}_{B_1},x,\mathrm{do}_{B_2},y)}^{\RV{V}_{i1}|\RV{V}_{B_1}}(U|x) &= \delta_0(U)\\
    \prob{P}_{(\mathrm{do}_{B_1},x,\mathrm{do}_{B_2},y')}^{\RV{V}_{i1}|\RV{V}_{B_1}}(U|x) &= \delta_1(U)
\end{align}
There is some freedom in choosing the conditional distribution of $\RV{V}_i$ given its parents because it has no support under the passive intervention, and without 4* this freedom allows us to make different choices when we intervene in different ways on unrelated elements of the sequence.

Condition 5* is the requirement that observations are mutually independent.

\subsection{Causal Bayesian networks with uncertain joint distributions}

Condition 3* of Definition \ref{def:unr_CBN} establishes that in a causal Bayesian network we can identify some input and output sequences with identical responses, with exactly which ones we can identify depending on the interventions chosen. In Chapter \ref{ch:evaluating_decisions}, we considered response functions that were identical \emph{conditional on some hypothesis} $\RV{H}$. The joint distribution $\prob{P}$ that appears in the previous definition of a causal Bayesian network is implicitly understood to be unknown. We can make this explicit by adding a hypothesis $\RV{T}$ and requiring that the causal Bayesian network properties hold for each value of $\RV{T}$. We could have $\RV{T}$ index both joint distributions and directed graphs, but here we will consider the case where the graph is known in advance and only the joint distribution is not.

\begin{definition}[Uncertain unrolled causal Bayesian network]\label{def:unc_unr_cbn}
Given an interventional model $(\prob{P}_\cdot,C,\Omega,(\RV{V}_{ij})_{i\in A,j\in \mathbb{N}})$ and a directed acyclic graph $\graph{G}$, $(\prob{P}_\cdot,C,\Omega,(\RV{V}_{ij})_{i\in A,j\in \mathbb{N}},\RV{T},\graph{G})$ is an \emph{uncertain unrolled causal Bayesian network} with respect to some hypothesis $\RV{T}:\Omega\to T$ if for each $h\in H$, defining $\prob{P}_{\cdot,t}:= \alpha\mapsto \prob{P}_\alpha^{\mathrm{id}_{\Omega}|\RV{T}}(\cdot|t)$, $(\prob{P}_{\cdot,t},C,\Omega,(\RV{V}_i)_{i\in A},\graph{G})$ is an unrolled causal Bayesian network.
\end{definition}

Recalling the discussion in Section \ref{sec:probability_set_models}, Definition \ref{def:unc_unr_cbn} associates each intervention with a unique probability distribution. One could suggest that uncertain unrolled causal Bayesian networks should therefore be called ``Bayesian causal Bayesian networks''. We could also consider models with a non-stochastic hypothesis $\RV{H}$ which we might call ``non-Bayesian causal Bayesian networks''.

An uncertain unrolled causal Bayesian network \emph{almost} features a number of CIIR input-output sequences. Due to 3*, such a model features conditionally independent and identical response functions with inputs $\mathrm{Pa}(\RV{V}_i)$ and outputs $\RV{V}_i$ wherever $\alpha$ consists of a sequence of interventions none of which target $\RV{V}_{ij}$ for any $j$. This leads us to the key result of this section: considering a subset of the interventions in $C$, an uncertain unrolled causal Bayesian network is IO contractible (with respect to some parameter $\RV{H}$) by application of Theorem \ref{th:ciid_rep_kernel}.

\begin{theorem}[IO contractibility of CBNs]\label{th:causal_contractibility_cbn}
Given an uncertain unrolled causal Bayesian network $(\prob{P}_\cdot,C,\Omega,(\RV{V}_{ij})_{i\in A,j\in \mathbb{N}},\RV{H},\graph{G})$, take $C'\subset C$ to be sequences of interventions that, for some $i\in A$, do not target a particular $\RV{V}_{ij}$ for any $j\in \mathbb{N}$ and ensure every sequence $\RV{V}_{j\mathbb{N}}$ has infinite support. Then $\RV{V}_{i\mathbb{N}}\CI^e_{\prob{P}_{C'}}\text{id}_C|(\RV{H}_i,\mathrm{Pa}(\RV{V}_{i \mathbb{N}}))$ and $\prob{P}_C^{\RV{V}_{i\mathbb{N}}|\RV{H}_i\mathrm{Pa}(\RV{V}_{i \mathbb{N}})}$ is IO contractible over $\RV{H}_i$ where $\RV{H}_i$ is the directing random conditional with respect to $(\prob{P}_C,\mathrm{Pa}(\RV{V}_{i \mathbb{N}}),\RV{V}_{i\mathbb{N}})$. 
\end{theorem}

\begin{proof}
Appendix \ref{sec:cbn_proofs}.
\end{proof}

\subsection[Probabilistic Graphical Models]{Probabilistic Graphical Models}

\citet{lattimore_replacing_2019,lattimore_causal_2019} have demonstrated how to ``unroll'' causal Bayesian networks into what they call ``Probabilistic Graphical Models''. Their construction is very similar to ours, as we will show here, and for readers interested in how identifiability results from regular causal Bayesian networks translate to our scheme, Lattimore and Rohde's work provides several examples.

Precisely, a probabilistic graphical model is a map $\prob{P}_\cdot$ from the set of single-node interventions $C$ to probability distributions $\prob{P}_\alpha$ defined on $(\Omega,\sigalg{F})$. A probabilistic graphical model is typically associated with a causal Bayesian network $(\prob{Q}_\cdot,C,\Omega',(\RV{V}_i)_{i\in A},\graph{G})$ where, for each $\RV{V}_i:\Omega\to V_i$ in the original causal Bayesian network, two variables $\RV{V}_i$ and $\RV{V}_i^*$ are defined on $(\Omega,\sigalg{F})$. 

The probabilistic graphical model also adds a ``parameter'' $\RV{W}_i$ for each variable pair $(\RV{V}_i,\RV{V}_i^*)$ such that, taking $C'$ to be interventions not targeting $\RV{V}_i^*$, for any $\alpha\in C'$, $\prob{P}_\alpha^{\RV{V}_i|\RV{W}_i\mathrm{Pa}(\RV{V}_i)}=\prob{P}_\alpha^{\RV{V}_i^*|\RV{W}_i\mathrm{Pa}(\RV{V}^*_i)}$ and $\RV{V}_i\CI^e_{\prob{P}_{C'}} (\RV{V}^*_A,\text{id}_C)|(\RV{W}_i)$ (where parents are assessed relative to the graph $\graph{G}$). $\RV{W}_i$ serves precisely the same role as $\RV{H}_i$ in Theorem \ref{th:causal_contractibility_cbn}, except it is not defined in terms of the directing random conditional.

A depiction of probabilistic graphical models and uncertain unrolled causal Bayesian networks using string diagrams gives some intuition regarding the structure of these different types of models, as well as some of the ``off-page'' assumptions of ordinary causal Bayesian networks.

Here is the original graph $\graph{G}$ associated with $(\prob{Q}_\cdot,C,\Omega',(\RV{V}_i)_{i\in A},\graph{G})$:

\begin{align}
    \tikzfig{cbn_example_cgm}
\end{align}

Here is the probabilistic graphical model associated with the intervention $(\mathrm{do}_2,v_2)$

\begin{align}
    &\phantom{=} \prob{P}_\alpha^{\RV{V}\RV{V}^*}\\
    &=\tikzfig{cgm}
\end{align}

and here is the uncertain unrolled CBN associated with the restricted set of interventions $C'$ that consists of, for each element of the sequence, either the empty intervention or some intervention targeting $\RV{V}_2$

\begin{align}
    &\phantom{=}\prob{P}_\alpha^{\RV{V}_{[1,3][n]}}\\
     &=\tikzfig{cbn_unrolled_example}
\end{align}

where 

\begin{align}
    \prob{P}_\alpha^{\RV{V}_{2i}|\RV{V}_{1i}\RV{H}} &= \begin{cases}
        \delta_{v} & \pi_i(\alpha)=(\mathrm{do}_2,v)\\
        \prob{P}_{mathrm{do}_{\emptyset}}^{\RV{V}_{2i}|\RV{V}_{1i}\RV{H}} & \text{otherwise}
    \end{cases}
\end{align}

\subsection{Unobserved confounders and precedent}\label{sec:precedent}

As we have pointed out, the assumption of CIIR sequences is often unreasonable. Causal Bayesian networks ``almost'' make this assumption with respect to inputs $\text{Pa}(\RV{V}_{i\mathbb{N}})$ and outputs $\RV{V}_{i\mathbb{N}}$. One of the ways that this appraoch gets around the fact that the assumption is unreasonable is to assume that some elements of $\text{Pa}(\RV{V}_{i\mathbb{N}})$ are unobserved. In this case, the ``interchangeable conditioning sequences'' are never actually observed, and so the question of whether or not they are interchangeable is moot.

We are limiting our attention to data-independent models (recall Definition \ref{def:weak_di}), which means that unobservability of some variable does not have any implications within the model -- only that it isn't attached to a measurement procedure. If we were considering a data-dependent variation of causal Bayesian networks, the fact that $\RV{V}_{1\mathbb{N}}$ is unobserved may have implications within the model -- for example, that input $\RV{D}_i$ may not depend on $\RV{V}_{1j}$ for any $j$.

Unobserved variables in the set of parents of a particular variable of interest may be called \emph{unobserved confounders}\footnote{To be a confounder, we also require an observed parent to be a descendant of the unobserved parent, but this detail isn't important for this discussion}. Suppose we have an uncertain unrolled causal Bayesian network $(\prob{P}_\cdot,C,\Omega,(\RV{V}_{ij})_{i\in [3],j\in \mathbb{N}},\RV{H},\graph{G})$ where the graph $\graph{G}$ is as follows:
\begin{align}
    \tikzfig{cbn_example_cgm}
\end{align}
and we consider the subset $C'\subset C$ of interventions that are either empty or target $\RV{V}_2$ only. We note that Theorem \ref{th:causal_contractibility_cbn} implies that $\prob{P}_C^{\RV{V}_{3\mathbb{N}}|\RV{H}_3\RV{V}_{1\mathbb{N}}\RV{V}_{2[n]}}$ is IO contractible, but not $\prob{P}_C^{\RV{V}_{3\mathbb{N}}|\RV{H}_3\RV{V}_{2\mathbb{N}}}$. We declare that $\RV{V}_{1[n]}$ is not observed -- that is, it is not associated with a measurement procedure.

We observe in Theorem \ref{th:condit_exchange} that the IO contractibility of $\prob{P}_C^{\RV{V}_{3\mathbb{N}}|\RV{H}\RV{V}_{1\mathbb{N}}\RV{V}_{2\mathbb{N}}}$ implies that $\prob{P}_C^{\RV{V}_{3\mathbb{N}}|\RV{H}\RV{V}_{2\mathbb{N}}}$ is unchanged by any swaps that leave the value of the full vector $\RV{V}_{1\mathbb{N}}$ unchanged. That is, the assumption of an unobserved confounder in this case implies a kind of ``partial IO contractibility''.

\begin{theorem}\label{th:condit_exchange}
Given $(\prob{P}_\cdot,C,\Omega,(\RV{V}_{ij})_{i\in [3],j\in \mathbb{N}},\RV{H},\graph{G})$ with $\prob{P}_C^{\RV{V}_{3\mathbb{N}}|\RV{H}_3\RV{V}_{1\mathbb{N}}\RV{V}_{2\mathbb{N}}}$ IO contractible over $\RV{H}_3$, $V_i$ discrete for all $i\in [3]$ and $(\RV{V}_{1\mathbb{N}},\RV{V}_{2\mathbb{N}})$ infinitely supported over $\RV{H}_3$, let $\RV{Q}:\Omega\to \mathbb{N}^\mathbb{N}$ be a random finite permutation of $\mathbb{N}$ such that $\RV{V}_{1\mathbb{N}}\overset{\prob{P}_C}{\cong}\RV{V}_{1\RV{Q}(\mathbb{N})}$. Then
\begin{align}
    \prob{P}_C^{\RV{V}_{3\mathbb{N}}|\RV{H}_3\RV{V}_{2\mathbb{N}}} &\overset{\prob{P}_C}{\cong} \prob{P}_C^{\RV{V}_{3\RV{Q}(\mathbb{N})}|\RV{H}_3\RV{V}_{2\RV{Q}(\mathbb{N})}}
\end{align}
\end{theorem}

\begin{proof}
By IO contractibility of $\prob{P}_C^{\RV{V}_{3\mathbb{N}}|\RV{H}\RV{V}_{1\mathbb{N}}\RV{V}_{2\mathbb{N}}}$ over $\RV{H}_3$
\begin{align}
    \prob{P}_C^{\RV{V}_{3\mathbb{N}}|\RV{H}_3\RV{V}_{1\mathbb{N}}\RV{V}_{2\mathbb{N}}} &= \prob{P}_C^{\RV{V}_{3\RV{Q}(\mathbb{N})}|\RV{H}_3\RV{V}_{1\RV{Q}(\mathbb{N})}\RV{V}_{2\RV{Q}(\mathbb{N})}}\\
    &= \prob{P}_C^{\RV{V}_{3\RV{Q}(\mathbb{N})}|\RV{H}_3\RV{V}_{1\mathbb{N}}\RV{V}_{2\RV{Q}(\mathbb{N})}}
\end{align}
Thus
\begin{align}
    &\prob{P}_C^{\RV{V}_{3\mathbb{N}}|\RV{H}\RV{V}_{2\mathbb{N}}}\\
     &= \tikzfig{contractible_over_hidden}\\
     &= \tikzfig{contractible_over_hidden_2}\\
     &= \tikzfig{contractible_over_hidden_3}\\
     &= \prob{P}_C^{\RV{V}_{3\RV{Q}(\mathbb{N})}|\RV{H}\RV{V}_{2\RV{Q}(\mathbb{N})}}
\end{align}
\end{proof}

Theorem \ref{th:condit_exchange} says that IO contractibility of $\prob{P}_C^{\RV{V}_{3\mathbb{N}}|\RV{H}\RV{V}_{1\mathbb{N}}\RV{V}_{2\mathbb{N}}}$ implies the ``exchange commutativity'' of $\prob{P}_C^{\RV{V}_{3\mathbb{N}}|\RV{H}\RV{V}_{2\mathbb{N}}}$ with respect to the random permutation $\RV{Q}$ of the indices of $\mathbb{N}$ -- in fact, for all such random permutations that preserve $\RV{V}_{1\mathbb{N}}$. Note that $\RV{Q}$ generally cannot be deduced from observations, because $\RV{V}_{1\mathbb{N}}$ cannot be deduced from observations. That is, $\RV{V}_{1\mathbb{N}}$ determines some subset of indices that are interchangeable, even though not \emph{all} indices are interchangeable. 

Suppose $\RV{V}_{[3]n}$ is the result of some intervention targeting $\RV{V}_{2n}$, and $\RV{V}_{[3]\{n\}^\complement}$ are all the result of passive observation. We can interpret Theorem \ref{th:condit_exchange} as expressing the idea that there is some (unknown) random subset of $\{n\}^\complement$ that is ``just like'' the intervened sample $\RV{V}_{[3]n}$. We could say that this random subset is a \emph{precedent} for the intervention.

Thus, using causal Bayesian networks we can express the idea that some intervention has precedent in the observed data, and indeed such an assumption appears to be implicit in the notion of unobserved parents in a causal Bayesian network. This raises the question: do we \emph{need} causal Bayesian networks to express this notion? The sentiment of this assumption long predates formal theories of causation. In fact, we can find it in the Bible:
\begin{quote}
Everything that happens has happened before; nothing is new, nothing under the sun. Ecclesiastes 1:9, Contemporary English Version
\end{quote}

We offer a simple definition of precedent. The idea is that we're given some sequence $\RV{Z}:=(\RV{Z}_i)_{i\in\mathbb{N}}$ and our choice influences $\RV{Z}_n$ and, no matter what we do, there is some subsequence $\RV{Z}_A$ of $\RV{Z}_{\{n\}^\complement}$ such that $\RV{Z}_{A\cup\{n\}}$ is exchangeable ($A$ may, in general, depend on the option chosen). We propose the following properties to precisely characterise precedent:

\begin{definition}[Precedent]\label{def:preemption}
A decision model $(\prob{P}_\cdot,(\Omega,\sigalg{F}),(C,\sigalg{C}))$ along with a sequence of variables $\RV{Z}:=(\RV{Z}_i)_{i\in\mathbb{N}}$ is $n$-\emph{precedented} if
\begin{enumerate}
    \item The sample space can be extended with a discrete variable $\RV{T}:=(\RV{T}_i)_{i\in\mathbb{N}}$ such that for every $\alpha\in C$, each $\RV{T}_i$ indexes an exchangeable subsequence to which the corresponding $\RV{Z}_i$ belongs; i.e. $\prob{P}_\alpha^{\RV{Z}|\RV{T}}$ is exchange commutative for all $\alpha\in C$ (over $*$)
    \item Neither $\RV{Z}_{\{n\}^\complement}$ nor $\RV{T}_{\{n\}^\complement}$ depend on the choice $\alpha$; $(\RV{Z}_{\{n\}^\complement},\RV{T}_{\{n\}^\complement} )\CI^e_{\prob{P}_C} \text{id}_C$
    \item $(\RV{Z}_i,\RV{T}_i)_{i\in\{n\}^\complement}$ is exchangeable
    \item $\RV{T}$ is infinitely supported over to the $(\prob{P}_C,\RV{T},\RV{Z})$ directing conditional $\RV{H}$
    \item $\prob{P}_\alpha^{\RV{T}_n}(t)>0$ for $t\in T$
\end{enumerate}
\end{definition}
Property (1) guarantees that for every available choice, $\RV{Z}$ can be partitioned into exchangeable subsequences, and we identify which exchangeable subsequence $\RV{Z}_i$ belongs to with $\RV{T}_i$. Property (2) identifies $(\RV{Z}_{\{n\}^\complement},\RV{T}_{\{n\}^\complement})$ as passive observations that the decision maker can't affect. Property (3) assumes for convenience that these observations are exchangeable, a common assumption to make of observations. Properties (4) and (5) are convenient and (as far as we can tell) do no harm.

From this, it follows that, for $\RV{H}$ the directing random conditional of $(\prob{P}_C,\RV{T},\RV{Z})$, $\prob{P}_C^{\RV{Z}|\RV{HT}}$ is IO contractible.

\begin{theorem}[IO contractibility of precedented decision models]
Given $(\prob{P}_\cdot,(\Omega,\sigalg{F}),(C,\sigalg{C}))$, if $\RV{Z}:=(\RV{Z}_i)_{i\in\mathbb{N}}$ is $n$-precedented with indexing variable $\RV{T}$, then taking $\RV{H}$ to be the directing random conditional of $(\prob{P}_C,\RV{T},\RV{Z})$, $\RV{Z}\CI^e_{\prob{P}_C} \text{id}_C | (\RV{H},\RV{T})$ and $\prob{P}_C^{\RV{Z}|\RV{HT}}$ is IO contractible.
\end{theorem}

\begin{proof}
By assumption, $\prob{P}_\alpha^{\RV{Z}|\RV{T}}$ is exchange commutative. In addition, also by assumption, $\prob{P}_\alpha^{\RV{T}_{\{n\}^\complement}}$ is exchangeable. But then $\prob{P}_\alpha^{\RV{T}}$ is exchangeably dominated, and the result follows from Theorem \ref{lem:exch_prod_ciid}.
\end{proof}

Thus precedent implies ``garden variety'' IO contractibility of $\prob{P}_C^{\RV{Z}|\RV{HT}}$. Apart from the stronger assumptions about observations, the only difference to the discussion of IO contractibility in Chapter \ref{ch:evaluating_decisions} is in our interpretation; we don't assume that $\RV{T}$ is observed, and we don't assume that we know anything in advance about how $\RV{T}$ responds to $\alpha$.

The assumption of precedent as given in Definition \ref{def:preemption} can, under some side conditions, yield nontrivial conclusions, as we show in Theorem \ref{th:det_obs_to_cons}. Before we state this theorem, it is worth going through an example first.

\begin{example}\label{ex:doctor_precedent}
Suppose we have a collection of doctors who each see a series of patients, offer a treatment $\RV{X}_i$ and report their results $\RV{Y}_i$. Each doctor may decide whether or not to prescribe based on any number of unobserved factors, and may offer additional unrecorded treatments, vary in their bedside manner and so forth, and these decisions could be stochastic or deterministic. The decision maker is \emph{also} a doctor, and is reviewing the data contained in the sequences $(\RV{Z}_i,\RV{X}_i,\RV{Y}_i)_{i\in [n]}$, where $\RV{Z}_i$ identifies the doctor involved in the $i$th treatment interaction. The decision maker supposes that whatever overall treatment plan the decision maker will adopted (which could and probably does also involve features not listed in this set of variables), the same thing has probably been done at least sometimes by some of these other doctors -- that is, their treatment protocol is precedented in the data. Because the other doctors have some variation in their treatment behaviour, it stands to reason that different doctors making the same prescription decisions should see different results \emph{if the different unobserved treatment plans actually lead to different results}. Conversely, if there is \emph{no} variation in results different doctors obtain, then whether or not treatment occurred is the \emph{only} important feature of any treatment plan.

This story might fail if the doctors all knew exactly the long-run probabilistic outcomes of different treatment plans and coordinated with one another to mask any variation they induced. For example, doctor 1 picks a medium effectiveness unobserved plan 100\% of the time, while doctor 2 picks a highly effective unobserved plan 50\% of the time and a low effectiveness unobserved plan 50\% of the time, leading to the same distribution over outcomes. Our decision maker also has to assume that there is some ``randomness'' in each doctor's choice of unobserved treatment plan so that they cannot precisely coordinate in this manner. In particular, one doctor's hidden treatment plan cannot be a deterministic function of any other doctor's hidden treatment plan and the ``true'' effectiveness of the treatment plans.
\end{example}


\begin{notation}[Matrix notation]
Given a sequential input-output model $(\prob{P}_C,\RV{D},\RV{Y})$ with $D,Y$ discrete, the directing random conditional $\RV{H}$ takes values in the set of Markov kernels $D\kto Y$, which can be identified with a subset of matrices in $\mathbb{R}^{|D|\times |Y|}$. We can therefore refer to elements of $\RV{H}$ as matrix elements $(\RV{H}_d^y)_{d\in D,y\in Y}$ with $\sum_{y\in Y} \RV{H}_d^y \overset{\prob{P}_C}{\cong} 1$ for all $d$, and $\RV{H}_d^y\overset{\prob{P}_\alpha}{\cong}\prob{P}_\alpha^{\RV{Y}|\RV{H}\RV{D}}(y|h,d)$.
\end{notation}

\begin{theorem}\label{th:det_obs_to_cons}
Suppose we have a probability set $\prob{P}_C$ on $(\Omega,\sigalg{F})$ with variables $\RV{Y}:=(\RV{Y}_i)_{i\in \mathbb{N}}$, $\RV{X}:=(\RV{X}_i)_{i\in \mathbb{N}}$ and $\RV{Z}_i:= (\RV{Z}_i)_{i\in \mathbb{N}}$, with $X,Y,Z$ discrete and $n$-precedented with indexing variable $\RV{T}$. Let $\RV{G}$ be the directing random conditional of $(\RV{P}_C,\RV{T},\RV{Z})$ and $\RV{H}$ the directing random conditional of $(\prob{P}_C,(\RV{T},\RV{Z}),(\RV{X},\RV{Y}))$. Suppose there is some $N\in\sigalg{H}$ such that $\prob{P}_\alpha^{\RV{H}}(N)>0$ for all $\alpha$, and define the probability conditioned on $\RV{H}\in N$:
\begin{align}
prob{P}_{\alpha,\RV{H}\in N} (A):= \frac{\prob{P}_\alpha^{\text{id}_\Omega \RV{H}}(A\times N)}{\prob{P}_\alpha^{\RV{H}}(N)}
\end{align}

Suppose for all $i$, $\RV{Y}_i\CI^e_{\prob{P}_{C,\RV{H}\in N}} \RV{Z}_i|(\RV{H},\RV{X}_i)$, and for all $t\in T$ and $z,z'\in Z$,
\begin{align}
    \prob{P}_{\alpha,\RV{H}\in N}^{\RV{G}^d_{z}|\RV{H}\RV{G}^d_{z'}}(\cdot|h,g^d_{z'}) \ll U_{[0,1]}\label{eq:lebesgue_dom}
\end{align}
where $U_{[0,1]}$ is the uniform probability measure on $([0,1],\mathcal{B}([0,1]))$.

Then $\prob{P}_{\alpha,\RV{H}\in N}^{\RV{Y}|\RV{X}\RV{H}}$ is IO contractible over $\RV{H}$.
\end{theorem}

\begin{proof}
We show that the assumption of conditional independence imposes a polynomial constraint on $\RV{G}^d_z$ which is nontrivial unless $\RV{Y}_i\CI^e_{\prob{P}_{C,\RV{H}\in\mathbb{N}}} (\RV{Z}_i,\RV{T}_i,\text{id}_C)|(\RV{H},\RV{X}_i)$, and hence the solution set $S$ for this constraint is empty when this conditional independence does not hold.

Full proof in Appendix \ref{sec:proof_precedent}.
\end{proof}

Theorem \ref{th:det_obs_to_cons} provides some support for the kind of inference explained in Example \ref{ex:doctor_precedent}. We offer the assumption of precedent as a formalisation of the idea that ``whatever the decision maker does, it's been done before''. If the decision maker can conclude that the hypothesis $\RV{H}$ lies in a set $N$ that implies the outcome is independent of the doctor identifiers $\RV{Z}_i$ given the treatment decisions $\RV{X}_i$, and further believes that within the set $N$, there is nevertheless zero probability that doctors coordinate precisely enough to mask any variation due to unobserved differences in treatment protocol, then the decision maker can conclude that the observed treatment-outcome map is identical to the treatment-outcome map they should use to assess the decision of whether or not to treat.

A substantial shortcoming of Theorem \ref{th:det_obs_to_cons} is the fact that, if the distribution of $\RV{H}$ is dominated by the uniform measure on $[0,1]^{|T||Z||X||Y||}$, then $\RV{H}\in N$ will also be a set of measure 0, as the conditional independence condition required by $N$ is itself a nontrivial polynomial restriction on values of $\RV{H}$. Thus assuming some continuous distribution of $\RV{H}$ does not satisfy the conditions required by the theorem.

This is reminiscent of problems encountered by other conditional independence based approaches to causal inference. In particular, we draw conclusions from conditional independence -- a measure 0 event under a continuous distribution of $\RV{H}$ -- but also from the assumption that \emph{other} measure 0 events under continuous distributions can be neglected. One way that this has been addressed is to treat approximate conditional independence as if it was conditional independence \citep{zhang_strong_2003}.

The appropriate strengthening of the assumptions in Example \ref{ex:doctor_precedent} seems to to require not only that the other doctor's actions aren't perfectly coordinated, but that we can further place a limit on how precisely they might be approximately coordinated. In this case, insufficient variation in overall outcomes between doctors might still license the conclusion that the unobserved treatment variations \emph{probably} don't have \emph{much} influence. Strengthening the required conditions like this has substantial practical implications; our decision maker is interested in $\RV{Y}$ presumably because they are interested in influencing $\RV{Y}$. However, for the same reason, all of the other doctors may well be interested in influencing $\RV{Y}$ in exactly the same way. Thus, while it's unlikely they can all coordinate \emph{perfectly}, they may well achieve a good approximation of coordination, which, unless the decision maker has a very large amount of data, might be enough to prevent them from drawing conclusions from an apparent lack of variation in outcomes between doctors.

Technically, we could consider an $\epsilon$-enlargement of the set of solutions $S$ constructed in the proof of Theorem \ref{th:det_obs_to_cons}, and place an upper limit on the probability of parameters falling into this set. It may then be possible to conclude a similar result from a suitably precise approximation of the assumed conditional independence, but demonstrating this remains an open question.

Assumptions similar to Eq. \eqref{eq:lebesgue_dom} appear in two other capacities in the causal inference literature. First, in justifications for \emph{causal faithfulness} it is assumed that, if we have variables ordered according to their structural causal relationships then an assumption of this type holds \citet{meek_strong_1995}. Secondly, according to the principle of \emph{independence of conditionals}, researchers have proposed a number of measures of ``genericity'' that are maximised when conditionals are computed in a manner compatible with their structural causal relationships \citep{lemeire_replacing_2013}. The common thread here is that an assumption like Eq. \eqref{eq:lebesgue_dom} is considered to be implied by the direction of causal relationships in a structural model. Our result is somewhat different -- we start with the assumption of a generic relationship between conditionals, rather than a causal direction, and draw a conclusion of identical responses. These approaches might be combined -- under the right conditions, assuming or inferring a causal direction might be sufficient to apply Theorem \ref{th:det_obs_to_cons}, though exactly when such reasoning is valid is an open question.

% Theorem \ref{th:preempt_diverse} is a more plausibly useful theorem than \ref{th:det_obs_to_cons}. It shows that, if we believe that $\prob{P}_\alpha^{\RV{Y}_0}$ is preempted by $\prob{P}_\alpha^{\RV{Y}_{-\mathbb{N}}}$, and after conditioning on some $\RV{V}$ the set of distributions of preempting actions is suitably diverse, then we can conclude conditional independence in the consequences of the decision maker's actions from conditional independence in the given observations.

% \begin{theorem}\label{th:preempt_diverse}
% Suppose, given a probability set $\prob{P}_C$ on $(\Omega,\sigalg{F})$ with variables $\RV{Y}:=(\RV{Y}_i)_{-i\in \{0\}\cup\mathbb{N}}$, $\RV{D}:=(\RV{D}_i)_{-i\in \{0\}\cup\mathbb{N}}$ and for each $i$, $(\RV{X}_i,\RV{V}_i,\RV{Z}_i) = f\circ \RV{Y}_i$ for some $f$ with both $D$ and $X$ denumerable, $\prob{P}_\alpha^{\RV{Y}_0}$ is preempted by $\prob{P}_\alpha^{\RV{Y}_{-\mathbb{N}}}$ with respect to $\RV{D}$ and and some $\RV{W}$. Let $\RV{H}$ be the directing random measure of the input-output model $(\prob{P}_C,\RV{D},\RV{Y})$ (Definition \ref{def:dir_rand_meas}). If for each $h\in H$ and each $x\in X$, $i<0$ there is some set of real-valued weights $\{a_i|i\in V\}$ such that
% \begin{align}
%     \prob{P}_\alpha^{\RV{Z}_i|\RV{H}\RV{D}_i\RV{X}_i}(\cdot|h,d,\cdot) &= \sum_{j\in V} a_j \prob{P}_\alpha^{\RV{Z}_i|\RV{H}\RV{V}_i\RV{X}_i}(\cdot|h,j,\cdot)
% \end{align}
% and if for all $i$, $\RV{Z}_i\CI^e_{\prob{P}_C} \RV{V}_i|(\RV{X}_i,\RV{H},\text{id}_C)$ then $\prob{P}_\alpha^{\RV{Z}|\RV{HX}}$ is IO contractible over $\RV{H}$.
% \end{theorem}

% \begin{proof}
% By assumption
% \begin{align}
%     \prob{P}_\alpha^{\RV{Z}_0|\RV{HD}_0\RV{X}_0}(A|h,d,x) &= \prob{P}_\alpha^{\RV{Z}_{-1}|\RV{HD}_{-1}\RV{X}_{-1}}(A|h,d,x)
% \end{align}
% and also by assumption, for all $A,h,d,x$:
% \begin{align}
%     \prob{P}_\alpha^{\RV{Z}_{-1}|\RV{H}\RV{D}_i\RV{X}_{-1}}(A|h,d,x) &= \sum_{j\in V} a_j \prob{P}_\alpha^{\RV{Z}_{-1}|\RV{H}\RV{V}_{-1}\RV{X}_{-1}}(A|h,j,x)\\
%         &= \sum_{j\in V} a_j \prob{P}_\alpha^{\RV{X}_{-1}|\RV{H}\RV{X}_{-1}}(A|h,x)&\text{by }\RV{Z}_{-1}\CI^e_{\prob{P}_C} \RV{V}_{-1}|(\RV{X}_{-1},\RV{H},\text{id}_C)\\
%     &= \prob{P}_\alpha^{\RV{Z}_{-1}|\RV{H}\RV{X}_{-1}}(A|h,x)\sum_{j\in V} a_j\\
%     &= \prob{P}_\alpha^{\RV{Z}_{-1}|\RV{H}\RV{X}_{-1}}(A|h,x)&\text{ by taking }A=Z
% \end{align}
% thus
% \begin{align}
%     \prob{P}_\alpha^{\RV{Z}_0|\RV{HD}_0\RV{X}_0}(A|h,d,x) &= \prob{P}_\alpha^{\RV{Z}_{-1}|\RV{H}\RV{D}_{-1}\RV{X}_{-1}}(A|h,d,x)\\
%     &= \prob{P}_\alpha^{\RV{Z}_{-1}|\RV{H}\RV{X}_{-1}}(A|h,x)\\
%     &= \prob{P}_\alpha^{\RV{Z}_0|\RV{H}\RV{X}_0}(A|h,x)\label{eq:indep_of_d}
% \end{align}
% Furthermore, by assumption $\RV{Z}_0\CI^e_{\prob{P}_C} (\RV{Z}_{\{0\}^C},\RV{D}_{\{0\}^C}|(\RV{H},\RV{D}_0,\RV{X}_0,\text{id}_C)$ and $\RV{Z}_0\CI^e_{\prob{P}_C} \RV{D}_{0}|(\RV{H},\RV{D}_0,\text{id}_C)$ by Equation \eqref{eq:indep_of_d}, thus $\prob{P}_\alpha^{\RV{Z}|\RV{HX}}$ is IO contractible over $\RV{H}$.
% \end{proof}

% Note that this is a result similar to \citet{peters_causal_2016}, where our variable $\RV{V}$ is understood to be analogous to the environment $e$ in that work.

% There is a substantial literature that aims to draw causal conclusions from observational data by first applying a graph learning algorithm to a sequence of observational data, and then using the graph obtained as a DAG for a causal Bayesian network. Earlier examples treat the graph learning problem as a discrete optimization problem and include the PC algorithm and the Causal Inference Algorithm \citet[Ch. 5\& 6]{spirtes_causation_1993} and Greedy Equivalent Search \citet{chickering_optimal_2003,chickering_learning_2002}. More recent examples pose graph learning as a continuous optimization problem \citet{zheng_dags_2018,ng_graph_2019}. Underpinning all of these approaches are a number of key assumptions, which include the assumption of \emph{faithfulness} -- that missing edges in the learned graph correspond to missing edges in the appropriate causal DAG -- and often also the assumption of \emph{causal sufficiency}, which is the assumption that there are ``no relevant unobserved variables''. Together, these assumptions imply that certain conditional independences in the observational data sequence imply the same conditional independences in the data produced under intervention. One open question we raise is: can this implication also be understood as a special case of the interventional data being preempted by the observational data?

\section{Potential Outcomes models}\label{sec:potential_outcomes}

Potential outcomes is another popular framework for modelling causal problems. There are two key differences between the potential outcomes approach and the causal Bayesian network approach. Potential outcomes models are sequential and they feature no notion of ``intervention''. A third difference relates to the possibility of expressing ``counterfactual'' statements, although this difference seems to be contingent on the particular manner we use to unroll a causal Bayesian network - see Section \ref{sec:counterfactual_statements}, and recall from Section \ref{sec:unrolling} that we had to make some choices in our construction of unrolled causal Bayesian networks, Definition \ref{def:unr_CBN}.

To formulate a decision making model from a potential outcomes model, we do have to make a judgement about what the ``options'' are (CBNs provide the notion of ``intervention'' for this role), but we do not need to make any judgements about how to unroll a potential outcomes model. For the following, we rely on \citet{rubin_causal_2005} for the definition of a potential outcomes model.

Our definition of potential outcomes is formally similar to the tabulated conditional distribution (Definition \ref{def:tab_cd}), but not quite identical: in particular, for any $d\in D$, a potential outcomes model holds that $\prob{P}_\alpha^{\RV{Y}_i|\RV{Y}^D_i\RV{D}_i}(y^d|y^D,d)=1$, but this is generally false for the tabulated conditionals in Definition \ref{def:tab_cd}.

\begin{definition}[Potential outcomes]\label{def:potential_outcomes}
Given $(\prob{P}_C,\Omega,\sigalg{F})$ and, for some $i$, variables $\RV{D}_i:\Omega\to D$ ($D$ denumerable), $\RV{Y}_i:\Omega\to Y$ and $\RV{Y}^D_i:\Omega\to Y^D$, $\RV{Y}^D_i$ is a vector of \emph{potential outcomes} with respect to $\RV{D}_i$ for all $\alpha$
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_i|\RV{Y}^D_i\RV{D}_i} &= \tikzfig{lookup_representation_single}
\end{align}
Where $\prob{F}_{\text{lus}}$ is the Markov kernel associated with the single-shot lookup map
\begin{align}
    \text{lus}:Y^D\times D &\to Y\\
    (d,(y_{i})_{i\in D})&\mapsto y_{d}
\end{align}
\end{definition}

Note that $|D|$ copies of $\RV{Y}_i$ $(\RV{Y}_i,\RV{Y}_i,\RV{Y}_i,...\RV{Y}_i)$ always satisfies Definition \ref{def:potential_outcomes}. This definition is not the sole constraint on potential outcomes, but the additional constraints come from what we want them to model, and are therefore not able to be formally stated.

A ``potential outcomes model'' is simply some probabilistic model with potential outcomes. Traditionally, potential outcomes models did not feature any set of options, and so are modeled by a single probability distribution. That is, a ``traditional'' potential outcomes model is a probability space $(\prob{P},\Omega,\sigalg{F})$ rather than a probability function, but our definition also allows for decision models with potential outcomes.

\begin{definition}[Potential outcomes model]
$(\prob{P}_C,\Omega,\sigalg{F})$ is a potential outcomes model with respect to $\RV{Y}^D:=(\RV{Y}^D_i)_{i\in A}$, $\RV{Y}:=(\RV{Y}_i)_{i\in A}$ and $(\RV{D}_i)_{i\in A}$ if $\RV{Y}^D_i$ is a vector of potential outcomes with respect to $\RV{D}_i$ and $\RV{Y}_i$ for all $i\in A$.
\end{definition}

\begin{theorem}
A potential outcomes model $(\prob{P}_C,\Omega,\sigalg{F})$ with respect to $\RV{D}_i:\Omega\to D$, $\RV{Y}_i:\Omega\to Y$ and $\RV{Y}^D_i:\Omega\to Y^D$, $\RV{Y}^D_i$ has $\RV{Y}\CI_{\prob{P}_C}^e\text{id}_C|(\RV{D},\RV{Y}^D)$ and $\prob{P}_C^{\RV{Y}|\RV{Y}^D\RV{D}}$ is IO contractible (with respect to $*$).
\end{theorem}

\begin{proof}
IO contractibility of $\prob{P}_C^{\RV{Y}|\RV{Y}^D\RV{D}}$ follows from the fact that $\RV{Y}_i$ is deterministic given $\RV{Y}^D_i$ and $\RV{D}_i$, and thus $\RV{Y}_i\CI^e_{\prob{P}_C} (\RV{D}_{\{i\}^{\complement}},\RV{Y}_{\{i\}^{\complement}},\text{id}_C)|(\RV{Y}^D_{i},\RV{D}_i)$. Furthermore, for all $i,j$
\begin{align}
    \prob{P}_C^{\RV{Y}_i|\RV{Y}^D_i\RV{D}_i} &= \prob{P}_C^{\RV{Y}_j|\RV{Y}^D_j\RV{D}_j}
\end{align}
hence the $\prob{P}_C^{\RV{Y}_i|\RV{Y}^D_i\RV{D}_i}$ are independent and identical response functions conditional on $*$.

From Definition \ref{def:potential_outcomes}, $\prob{P}_\alpha^{\RV{Y}_i|\RV{Y}^D_i\RV{D}_i}$ is the same for all $\alpha\in C$, and by the argument above,
\begin{align}
    \prob{P}_C^{\RV{Y}_i|\RV{Y}^D_i\RV{D}_i\RV{Y}^D_{\{i\}^{\complement}}\RV{D}_{\{i\}^{\complement}}} &= \prob{P}_C^{\RV{Y}_i|\RV{Y}^D_i\RV{D}_i}\otimes \mathrm{Del}_{Y^{|D\times A\setminus\{i\}|}\times D^{|A|}}
\end{align}
hence
\begin{align}
    \prob{P}_C^{\RV{Y}|\RV{Y}^D\RV{D}} &= \bigotimes_{i\in A} \prob{P}_C^{\RV{Y}_1|\RV{Y}^D_1\RV{D}_1}
\end{align}
hence $\RV{Y}\CI_{\prob{P}_C}^e\text{id}_C|(\RV{D},\RV{Y}^D)$.
\end{proof}

A key theorem of potential outcomes is that, if $\RV{D}$ is ``strongly ignorable'' with respect to $\RV{Y}^D$, then the average treatment effect is identified. ``Strong ignorability'' here means that the probability $\prob{P}_\alpha^{\RV{D}_i}(d)>0$ for each $d$ and for each choice $\alpha$ the inputs $\RV{D}$ are independent of the potential outcomes $\RV{Y}^D$ given the covariates and the choice. We reproduce this theorem in terms of IO contractibility. Note that Theorem \ref{th:potential_outcomes_identifiability} applies to potential outcomes models with sets of choices, rathter than simply to single probability distributions.

\begin{theorem}[Potential outcomes identifiability]\label{th:potential_outcomes_identifiability}
If $(\prob{P}_C,\Omega,\sigalg{F})$ is a potential outcomes model with respect to $\RV{Y}^D:=(\RV{Y}^D_i)_{i\in \mathbb{N}}$, $\RV{Y}:=(\RV{Y}_i)_{i\in \mathbb{N}}$ and $(\RV{D}_i)_{i\in \mathbb{N}}$, each value of $D$ occurs infinitely often with probability 1, there is some $\RV{X}:= (\RV{X}_i)_{i\in \mathbb{N}}$ such that $\prob{P}_\alpha^{\RV{Y}^D\RV{X}}$ is exchangeable for all $\alpha$ and $\RV{D} \CI^e_{\prob{P}_C} \RV{Y}^D | (\RV{X},\RV{Y},\text{id}_C)$ and for each $\alpha$ $\prob{P}_\alpha^{\RV{D}}$ is absolutely continuous with respect to some exchangeable distribution in $\Delta(D^{\mathbb{N}})$, then there is some $\RV{W}$ such that for all $\alpha$ $\prob{P}_\alpha^{\RV{Y}|\RV{WXD}}$ is IO contractible over $\RV{W}$.
\end{theorem}

\begin{proof}
By exchangeability of $\prob{P}_\alpha^{\RV{Y}^D\RV{X}}$, $\prob{P}_\alpha^{\RV{Y}^D|\RV{X}}$ commutes with exchange. Because $\RV{Y}$ is deterministic given $\RV{D}$ and $\RV{Y}^D$, $\RV{Y}\CI^e_{\prob{P}_C} (\RV{X},\text{id}_C)|(\RV{Y}^D,\RV{D})$ Thus, for some finite permutation $\rho$, by IO contractibility of $\prob{P}_C^{\RV{Y}|\RV{Y}^D\RV{D}}$
\begin{align}
    \prob{P}_\alpha^{\RV{Y}|\RV{XD}} &= \tikzfig{potential_outcomes_ccontract1}\\
    &= \tikzfig{potential_outcomes_ccontract2}\\
    &= \tikzfig{potential_outcomes_ccontract3}
\end{align}
IO contractibility of $\prob{P}_\alpha^{\RV{Y}|\RV{WXD}}$ over some $\RV{W}$ follows from Theorem \ref{lem:exch_prod_ciid}.
\end{proof}

% \subsection{Counterfactual statements}\label{sec:counterfactual_statements}

% \section{Weaker assumptions than IO contractibility}\label{sec:weaker_assumptions}

% The results so far apply to purely observational models or to models where every ``input'' in the sequence is fixed at the point of choosing $\alpha$ (or a fixed random function is chosen at this point). Most of the interest in causal inference is how to use observational data -- which is plentiful -- to deduce consequences of choices. Suppose in the following that superscript ``$o$'' refers to observational variables (obtained by some measurement procedure not responsive to choices) and ``$v$'' refers to interventional variables (obtained by some measurement procedure responsive to choices). That is $\RV{Y}^o:=(\RV{Y}_i^o)_{i\in \mathbb{N}}$ is a sequence of observational variables, $\RV{Y}^v$ a sequence of interventional variables and $\RV{Y}^{o,v}:=(\RV{Y}^o_i,\RV{Y}^v_i)_{i\in\mathbb{N}}$ is a mixed sequence of both observational and interventional variables. $\RV{Y}_i^o$ and $\RV{Y}_i^v$ are assumed to take values in the same set $Y$.

% One approach to bridging the gap between observations and interventions is to assume ``causal sufficiency'', which is tantamount (in the data-independent case) to assuming IO contractibility of $\prob{P}_C^{\RV{Y}^{o,v}|\RV{X}^{o,v}\RV{D}^{o,v}}$ with $\RV{D}^v$ responsive to choices and $\RV{X}^v$ unresponsive (see Example \ref{pgph:backdoor}). As discussed, this is rarely a reasonable assumption -- it implies interchangeability between observational and interventional samples.

% A weaker assumption that is often adopted is to consider models satisfying IO contractibility with respect to $\prob{P}_C^{\RV{Y}^{o,v}|\RV{U}^{o,v}\RV{D}^{o,v}}$, where $\RV{U}^{o,v}$ is unobserved. That is, while $\RV{U}^{o,v}$ appears in the model, it is not associated with any measurement procedure. This model still asserts that $(\RV{U}^o_i,\RV{X}^o_i,\RV{Y}^o_i)$ triples are interchangeable with $(\RV{U}^v_i,\RV{X}^v_i,\RV{Y}^v_i)$ triples, but neither of these are measurement outcomes. On the other hand, $(\RV{D}_i^o,\RV{Y}_i^o)$ pairs are not generally interchangeable with $(\RV{D}_i^v,\RV{Y}_i^v)$.

% Consider models that satisfy IO contractibility with respect to $\prob{P}_C^{\RV{Y}^{o,v}|\RV{W}^{o,v}}$, where no comment is made about whether $\RV{W}^{o,v}$ is observed, unobserved or some function of observed and unobserved variables. This is a generalisation of the class of models discussed in the previous paragraph.  In isolation, this assumption is not especially interesting -- for example, the support of $\RV{W}^{o}_i$ and $\RV{W}^v_i$ might be disjoint. Suppose also, then, that $W$ is finite and $\RV{W}^o_i$ has full support. This assumption amounts to the assumption that, no matter what choice is made, ``nothing truly new can be done'' (which we call ``Ecclesiastes' assumption''\footnote{Ecclesiastes 1:9 reads ``Everything that happens has happened before; nothing is new, nothing under the sun.''\citep{noauthor_holy_1995}}). More precisely, for any choice $\alpha\in C$ and any consequence $\RV{Y}_i^v$, there is a random subsequence $\RV{Q}$ of indices $(1,2,3,....)$ such that the distribution $\prob{P}_\alpha^{\RV{Y}^{o,v}}$ is unchanged by permutations that only swap elements in the sequence $(RV{Y}^o_\RV{Q},\RV{Y}^v_i)$.

% \begin{theorem}\label{th:condit_exchange}
% Given just-do model $\prob{P}_C$ with $\prob{P}_C^{\RV{Y}^{o,v}|\RV{W}^{o,v}}$ IO contractible, $W$ finite and $\prob{P}_C^{\RV{W}^o|\RV{H}}(w|h)>0$ for all $w,h$, define $q:W^{\mathbb{N}}\times W\to (\{*\}\cup \mathscr{P}(\mathbb{N})$ by 
% \begin{align}
%     q:((w^o_j)_{\mathbb{N}},w^v_i)&\mapsto \{j|w^o_j=w^v_i\}
% \end{align}
% and take $\RV{Q}:=q\circ(\RV{W}^o,\RV{W}^v_i)$ for arbitrary $i\in \mathbb{N}$. For an index set $U\in\mathbb{N}$ take $\text{Swap}_{\cdot}:Y^{\mathbb{N}}\times Y^{\mathbb{N}}\to Y^{\mathbb{N}}\times Y^{\mathbb{N}}$ to be an arbitrary finite swap that acts as the identity on all indices $(j,x)\not\in \RV{Q}\times \{o\}\cup\{(i,v)\}$. Then $\prob{P}^{\RV{Y}^{o}\RV{Y}^v_i}\text{Swap}_{\RV{Q}} = \prob{P}^{\RV{Y}^{o}\RV{Y}^v_i}$.
% \end{theorem}

% \begin{proof}
% Note that for $B_j\in \sigalg{W}$, where $\rho_q:\mathbb{N}\times\{i,v\}\to \mathbb{N}\times\{i,v\}$ is the permutation function associated with $\text{Swap}_{q}$
% \begin{align}
%     \prob{P}_\alpha^{\RV{W}^o\RV{W}^v_i}\text{Swap}_{\RV{Q}} (\bigtimes_{j\in\mathbb{N}} B_j) &= \int_{W^{\mathbb{N}}}\int_{\mathscr{P}(\mathbb{N})} \prod_{k\not\in q\times\{o\}\cup\{(i,v)\}} \delta_{w_k}(B_k) \prod_{l\in q\times\{o\}\cup\{(i,v)\}} \delta_{\rho_q(w_l)} (B_l) \prob{P}_\alpha^{\RV{Q}|\RV{W}^o\RV{W}^v_i}(\mathrm{d}q|w)\prob{P}_\alpha^{}(\mathrm{d}w)\\
%     &= \int_{W^{\mathbb{N}}}\int_{\mathscr{P}(\mathbb{N})} \prod_{k\not\in q\times\{o\}\cup\{(i,v)\}} \delta_{w_k}(B_k) \prod_{l\in q\times\{o\}\cup\{(i,v)\}} \delta_{w_l} (B_l) \prob{P}_\alpha^{\RV{Q}|\RV{W}^o\RV{W}^v_i}(\mathrm{d}q|w)\prob{P}_\alpha^{}(\mathrm{d}w)\label{eq:all_the_same}\\
%     &= \prob{P}_\alpha^{\RV{W}^o\RV{W}^v_i}
% \end{align}
% where Eq. \eqref{eq:all_the_same} follows from the fact that for every $k,l\in q\times\{o\}\cup\{(i,v)\}$, $w_k=w_l$.

% Thus for $A\in \sigalg{Y}^{\mathbb{N}}$
% \begin{align}
%     \prob{P}_\alpha^{\RV{Y}^{o}\RV{Y}^v_i}\text{Swap}_{\RV{Q}}(A) &= [\prob{P}_\alpha^{\RV{W}^o\RV{W}^v_i} \prob{P}_\alpha^{\RV{Y}^{o}\RV{Y}^v_i|\RV{Q}\RV{W}^o\RV{W}^v_i}\text{Swap}_{\RV{Q}}](A)\\
%     &= [\prob{P}_\alpha^{\RV{W}^o\RV{W}^v_i} \text{Swap}_{\RV{Q}^{-1}} \prob{P}_\alpha^{\RV{Y}^{o}\RV{Y}^v_i|\RV{W}^o\RV{W}^v_i}\text{Swap}_{\RV{Q}}](A)\\
%     &= \prob{P}_\alpha^{\RV{Y}^{o}\RV{Y}^v_i}\label{eq:by_cc1}
% \end{align}
% Where Eq. \eqref{eq:by_cc1} follows from IO contractibility of $\prob{P}_\alpha^{\RV{Y}^{o}\RV{Y}^v_i|\RV{W}^o\RV{W}^v_i}$.
% \end{proof}

% It also follows from Ecclesiastes' assumption and finite $W$ that if some $\RV{X}_i^o$, $\RV{Z}_i^o$ are \emph{deterministically} related given $\RV{W}$, then $\prob{P}_C^{\RV{Z}|\RV{X}}$ is IO contractible.

% \begin{theorem}\label{th:det_obs_to_cons}
% Given just-do model $\prob{P}_C$ with $\prob{P}_C^{\RV{X}^{o,v}\RV{Z}^{o,v}|\RV{W}^{o,v}}$ IO contractible, $W$ finite and $\prob{P}_C^{\RV{W}^o_i|\RV{H}}(w|h)>0$ for all $w,h$, if $\prob{P}_C^{\RV{Z}^o_0|\RV{X}^o_0\RV{H}}$ is deterministic then $\prob{P}_C^{\RV{Z}^{o,v}|\RV{X}^{o,v}}$ is IO contractible.
% \end{theorem}

% \begin{proof}
% Because $\prob{P}_\alpha^{\RV{W}_0^o}\prob{P}_C^{\RV{Z}^o_0|\RV{X}^o_0\RV{W}^o_0\RV{H}}$ is deterministic, so is $\prob{P}_C^{\RV{Z}_0|\RV{X}_0\RV{W}_0\RV{H}}$.

% Fix $h\in H$.  Suppose there is some $w,w'\in W$ such that
% \begin{align}
%     \prob{P}_C^{\RV{Z}_0|\RV{X}_0\RV{W}_0\RV{H}}(A|x,w,h) &\neq \prob{P}_C^{\RV{Z}_0|\RV{X}_0\RV{W}_0\RV{H}}(A|x,w',h)
% \end{align}
% then, by determinism, we can assume without loss of generality
% \begin{align}
%     \prob{P}_C^{\RV{Z}_0|\RV{X}_0\RV{W}_0\RV{H}}(A|x,w,h) = 1\\
%     \prob{P}_C^{\RV{Z}_0|\RV{X}_0\RV{W}_0\RV{H}}(A|x,w',h) = 0
% \end{align}
% but $W$ is finite and $\prob{P}_C^{\RV{W}^o_i|\RV{H}}(w|h)>0$ for all $w$, so there is some $a>0$ such that $\prob{P}_C^{\RV{W}^o_i|\RV{H}}(w|h)\geq a$ for all $w$, and so
% \begin{align}
%     a \leq \sum_{w\in W} \prob{P}_C^{\RV{W}^o_0|\RV{X}^o_0,\RV{H}}(w|x,h)\prob{P}_C^{\RV{Z}_0|\RV{X}_0\RV{W}_0\RV{H}}(A|x,w,h)\leq 1-a
% \end{align}
% contradicting determinism of $\prob{P}_C^{\RV{Z}^o_0|\RV{X}^o_0\RV{H}}$.

% Thus for all $w,w'$
% \begin{align}
%     \prob{P}_C^{\RV{Z}_0|\RV{X}_0\RV{W}_0\RV{H}}(A|x,w,h) &= \prob{P}_C^{\RV{Z}_0|\RV{X}_0\RV{W}_0\RV{H}}(A|x,w',h)
% \end{align}
% i.e. $\RV{Z}_0\CI^e_{\prob{P}_C} (\RV{W}_0,\text{id}_C)|(\RV{X}_0,\RV{H})$. But then there is some $\prob{P}_C^{\RV{Z}_0|\RV{X}_0\RV{H}}$ such that
% \begin{align}
%     \prob{P}_C^{\RV{Z}_0|\RV{X}_0\RV{W}_0\RV{H}} &= \prob{P}_C^{\RV{Z}_0|\RV{X}_0\RV{H}}\otimes \text{erase}_W\\
%     \implies \prob{P}_\alpha^{\RV{Z}^v_i|\RV{X}^v_i\RV{H}} &= \prob{P}_C^{\RV{Z}_0|\RV{X}_0\RV{H}}
% \end{align}
% \end{proof}

% Theorem \ref{th:det_obs_to_cons} doesn't hold in the case of approximate determinism, however. Intuitively, approximate determinism can hold if there is some value of $\RV{W}$ for which $\RV{Z}$ is not conditionally independent given $\RV{H}$ and $\RV{X}$, but it only ocurrs very rarely in observations. On the other hand, values of $\RV{W}$ rare in observations might, under some choices, become common. 

% \begin{example}
% Say $\prob{P}_C^{\RV{Z}^o_i|\RV{X}^o_i\RV{H}}$ is \emph{approximately deterministic} if $\prob{P}_C^{\RV{Z}^o_i|\RV{X}^o_i\RV{H}}(A|x,h)\in [0,\epsilon]\cup[1-\epsilon,1]$ for all $A\in\sigalg{Z}$, $x,h\in X\times H$.

% Take $Z=X=W=H=\{0,1\}$. Set
% \begin{align}
%     \prob{P}_C^{\RV{Z}_0|\RV{X}_0\RV{W}_0\RV{H}}(1|1,1,1) = 1\\
%     \prob{P}_C^{\RV{Z}_0|\RV{X}_0\RV{W}_0\RV{H}}(1|1,0,1) = 0
% \end{align}
% and
% \begin{align}
%     \prob{P}_C^{\RV{W}^o_0|\RV{H}}(1|1)=1-\epsilon
% \end{align}
% then
% \begin{align}
%     \prob{P}_C^{\RV{Z}_0|\RV{X}_0\RV{H}}(1|1,1) = 1-\epsilon
% \end{align}
% however, suppose there is some $\alpha$ such that
% \begin{align}
%     \prob{P}_\alpha^{\RV{W}^v_i|\RV{H}}(1|1)=0
% \end{align}
% then
% \begin{align}
%     \prob{P}_\alpha^{\RV{Z}_0|\RV{X}_0\RV{H}}(1|1,1) = 0\\
%     &\neq \prob{P}_C^{\RV{Z}_0|\RV{X}_0\RV{H}}(1|1,1)
% \end{align}
% \end{example}


\section{Individual-level response functions}\label{sec:ilevel_ccontract}

Exchangeability of potential outcomes, a key assumption in Theorem \ref{th:potential_outcomes_identifiability}, is hard to explain in terms of symmetries of experiments. Given some experiment producing a sequence of pairs $(\RV{D}_i,\RV{Y}_i)_{i\in\mathbb{N}}$, say where $\RV{D}_i$s are treatment administrations and $\RV{Y}_i$s are health outcomes, there's no obvious generic way to design a related experiment whose model is the same as the original except with potential outcomes $\RV{Y}^D_i$ interchanged. This is in sharp contrast to the assumption of exchangeability of observed outcomes - say, instead of the potential outcomes being exchangeable, we hold that the pairs $(\RV{D}_i,\RV{Y}_i)$ are exchangeable in the original experiment. Then we commit ourselves to the proposition that an alternative experiment which proceeds exactly as the first except, before being ``committed to memory'', the experimental results are interchanged should be modeled exactly as the first.

One could propose that exchangeability of potential outcomes in our example experiment corresponds to an \emph{exchangeability of patients}; perhaps, if we believe the model should be unchanged after we swap the order in which patients are seen, then we should accept that the model has exchangeable potential outcomes. However, it certainly does not follow from a potential outcomes model that exchangeability of patients implies exchangeability potential outcomes. If each patient were assumed to have a fixed but unknown vector of potential outcomes that is unchanged by the swapping operation, then swapping patients does indeed correspond to swapping potential outcomes. On the other hand, if we instead suppose that each index is associated with a fixed vector of potential outcomes regardless of any exchanges of patients that might be made, we could swap patients without altering the model and yet the potential outcomes would not in general be exchangeable.

We formalise the idea of ``potential outcomes attached to individuals'' as \emph{individual-level response functions}. We offer a formal definition of the assumption of individual-level response functions, but like exchangeability of potential outcomes it is difficult to understand fundamentally what this assumption entails, or what it might be motivated by. Nevertheless, it does allow us to separate the assumption of ``exchangeable potential outcomes'' into the assumption of individual level response functions and the assumption of exchangeability of individuals. We also use this notion to prove Theorem \ref{th:cc_ind_treat}. At a high level, it plays a similar role to Theorem \ref{th:potential_outcomes_identifiability}: it seems to justify causal identifiability in certain kinds of controlled experiments. However, the content of the two theorems is very different. While Theorem \ref{th:potential_outcomes_identifiability} concerns independence of the inputs and potential outcomes along with exchangeability of the potential outcomes, Theorem \ref{th:cc_ind_treat} says (informally) if:
\begin{itemize}
    \item There are individual-level response functions
    \item Individuals can be swapped without meaningfully altering the experiment
    \item Inputs are deterministically controlled by the choice
    \item There is only one choice for each value of the inputs
\end{itemize}
then the model is also IO contractible with respect to the inputs and the outputs only (ignoring the individual identifiers). In our view, this comes closer to a set of assumptions that are directly appliccable to a controlled experiment than those in Theorem \ref{th:potential_outcomes_identifiability}, and reflects \citet{kasy_why_2016}'s dictum that, for the identifiability of causal effects, a ``controlled experiment'' is sufficient.

\subsection{References to individual-level IO contractibility}

The role of individuals has often been mentioned in literature on causal inference. For example, \citet{greenland_identifiability_1986} explain
\begin{quote}
    Equivalence of response type may be thought of in terms of exchangeability of individuals: if the exposure states of the two individuals had been exchanged, the same data distribution would have resulted.
\end{quote}
Here, the idea of ``exchangeable individuals'' plays a role in the author's reasoning about model construction, but ``individuals'' are not actually referenced by the resulting model, and ``exchanging individuals'' does not correspond to a model transformation.

\citet{dawid_decision-theoretic_2020} suggests (with some qualifications) that ``post-treatment exchangeability'' for a decision problem regarding taking aspirin to treat a headache may be acceptable if the data are from
\begin{quote}
    A group of individuals whom I can regard, in an intuitive sense, as similar to myself, with headaches similar to my own.
\end{quote}
As in the previous work, the similarity of individuals involved in an experiment is raised when justifying particular model constructions, but the individuals are not referenced by the model.

\citet[pg. 98]{pearl_causality:_2009} writes
\begin{quote}
    Although the term unit in the potential-outcome literature normally stands for the identity of a specific individual in a population, a unit may also be thought of as the set of attributes that characterize that individual, the experimental conditions under study, the time of day, and so on – all of which are represented as components of the vector $u$ in structural modeling.
\end{quote}
Once again, the idea of an individual (or a particular set of conditions) is raised in the context of explaining modelling choices. Unlike the previous authors, Pearl introduces a vector $u$ to stand for the ``unit''. However, he subsequently assumes that $u$ is a sequence of \emph{independent samples} from some distribution. This seems to contradict an important feature of ``individuals'' or ``units'': individuals are typically supposed to be unique, a property that will usually not be satisfied by independently sampling from some distribution (at least, as long as the distribution is discrete).

Finally, \citet{rubin_causal_2005} writes:
\begin{quote}
    Here there are $N$ units, which are physical objects at particular points in time (e.g., plots of land, individual people, one person at repeated points in time).
\end{quote}
Note that Rubin's explanation of \emph{units} guarantees that they are unique: they are particular things at particular times. These units are associated with input-output functions (the \emph{potential outcomes}), which are later assumed to be exchangeable:
\begin{quote}
    the indexing of the units is, by definition, a random permutation of $1,..., N$, and thus any distribution on the science must be row-exchangeable
\end{quote}

Our proposition is: can the intuition that unique individuals are an important for the motivation for causal models, be captured by considering models that feature ``unique identifier'' variables referencing these unique individuals?

\subsection{Unique identifiers}

A sequence of \emph{unique identifiers} is a vector of finite or infinite length such that no two coordinates are equal. We are interested in models that assign positive probability to any particular coordinate having any particular value. This is straightforward in the finite case. In the infinite case, note that a vector of $|\mathbb{N}|$ unique values with an arbitrary entry $k$ in the $j$th coordinate can be obtained by starting with $(i)_{i\in \mathbb{N}}$ and then transposing $j$ with $k$. More generally, we consider infinite length sequences of unique identifiers to be elements of the set of finite permutations $\mathbb{N}\to\mathbb{N}$.

\begin{definition}[Measurable space of unique identifiers]
The measurable space of unique identifiers $(I,\sigalg{I})$ is the set $I$ of finite permutations $\mathbb{N}\to \mathbb{N}$ with the discrete $\sigma$-algebra $\sigalg{I}$.
\end{definition}

The set $I$ is countable, as it is the countable union of finite subsets (i.e. the permutations that leave all but the first $n$ numbers unchanged for all $n$).

\begin{definition}[Unique identifier]
Given a sample space $(\Omega,\sigalg{F})$, a \emph{sequence of unique identifiers} $\sigalg{I}:\Omega\to I$ is a variable taking values in $I$.
\end{definition}

The values of each coordinate of sequence of unique identifiers is just called an identifier (for obvious reasons, we don't call it an identity).

\begin{definition}[Identifiers]
Given $\RV{I}$, define the $i$-th \emph{identifier} $\RV{I}_i:=\mathrm{ev}(i,\RV{I})$, where $\mathrm{ev}:\mathbb{N}\times I\to \mathbb{N}$ is the evaluation map $(i,f)\mapsto f(i)$.
\end{definition}

For \emph{any} sample space $(\Omega,\sigalg{F})$ we can define a trivial $\sigalg{I}$ that maps every $\omega\in\Omega$ to $(1,2,3,....)=:(\mathbb{N})$. In this case, the identifiers are all known by the modeller at the outset. Using this sequence of identifiers renders exchange commutativity trivial.

\begin{example}\label{eq:il_exchc}
Given a sequential input-output model $(\prob{P}_C, (\RV{D},\RV{I}),\RV{Y})$ where $\RV{I}$ is the identifier variable $\omega\mapsto (\mathbb{N})$, $\prob{P}_\alpha^{\RV{Y}|\RV{DI}}$ commutes with exchange.

This is because for any permutation $\rho:\mathbb{N}\to\mathbb{N}$ except the identity, $\prob{P}_\alpha^{\RV{Y}|\RV{DI}}$ and $\text{Swap}_{\rho}\prob{P}_\alpha^{\RV{Y}|\RV{DI}}$ will have no common support; the first will be supported on $\RV{I}\yields (\mathbb{N})$ only, and the second only on $\RV{I}\yields \rho(\mathbb{N})$.
\end{example}

We are particularly interested in models where exchange commutativity is not trivial, so we focus on the case where each identifier $\RV{I}_i$ has some non-zero probability of taking any value in $\mathbb{N}$. 

% \subsection{Individual-level IO contractibility and unobserved confounding}

% Our first result is that some models with individual-level IO contractibility can be seen as models with unobserved confounding. A model $\prob{P}_C$ with individual-level IO contractibility features a IO contractible $\prob{P}_C^{\RV{Y}|\RV{ID}}$ for a sequence of outputs $\RV{Y}$, inputs $\RV{D}$ and individual identifiers $\RV{I}$. A model with unobserved confounding features IO contractible $\prob{P}_C^{\RV{Y}|\RV{UD}}$ where $\RV{Y}$ and $\RV{D}$ are as before and $\RV{U}$ is an ``unobserved confounder''. They key difference between $\RV{I}$ and $\RV{U}$ is that the individual identifier for each observation is unique, while unobserved variables (typically) have $|U|<N$ where $N$ is the number of observations.

\subsection[Identification]{Identification with individual-level response functions}

The key result of this section is Theorem \ref{th:cc_ind_treat}, which establishes sufficient conditions that, in conjunction with the key assumption of \emph{individual-level response functions}, yield a conclusion of observable CIIR sequences. Individual-level response functions is the assumption that, given a sequential input-output model $(\prob{P}_C, (\RV{D},\RV{I}),\RV{Y})$, there is some $\RV{J}$ such that $\prob{P}_C^{\RV{Y}_i|\RV{D}_i\RV{I}_i\RV{J}}$ are mutually independent and identical responses for all $\alpha$ and $i$. $\RV{J}$, unlike the directing random conditional $\RV{H}$ (Definition \ref{def:dir_rand_cond}), is not necessarily a function of the inputs and outputs, as we do not have infinitely supported inputs.

This assumptions are somewhat opaque. We could consider $\RV{J}$ to be a variable assigning a stochastic response for each individual independent of the order of treatment or the treatment of any other individual. That is, $\RV{J}$ could be considered a specification of stochastic potential outcomes ``bound to the identifiers''. There may be other viable interpretations of $\RV{J}$, but we are not aware of them at this point. It may be generically possible to extend a model with individual identifiers to feature individual level response functions, but this is an open question.

Lemma \ref{lem:ind_to_cc} and Theorem \ref{th:ind} do \emph{not} require that $\RV{I}$ be a sequence of unique identifiers, they hold just as well if it is a sequence of non-unique labels; that is, if $\RV{I}_i\yields \RV{I}_j$ had positive measure for some $i\neq j$. The reason why we are interested mainly in the case where $\RV{I}$ is a sequence of unique identifiers is that the assumption $\RV{Y}\CI^e_{\prob{P}_C} \RV{I}|\text{id}_C$ is substantially more limiting in the case that $\RV{I}$ is a non-unique sequence of labels. In particular, it implies that the conditional probability of $(\RV{Y}_1,\RV{Y}_2)$ given $(\RV{I}_1=1,\RV{I}_2=1)$ is exactly the same as the conditional probability of $(\RV{Y}_1,\RV{Y}_2)$ given $(\RV{I}_i=1,\RV{I}_2=2)$; observations associated with equal labels are no more relevant that observations associated with different labels.

In the following, it is helpful to assume that each sub-experiment has a ``unique identifier'' $\RV{I}_i$, with the sequence of all sub-experiment labels given by $\RV{I}$. With this, if $\prob{P}_C^{\RV{Y}|\RV{DI}}$ is assumed IO contractible, then it's possible to talk about the individual response functions $\prob{P}_C^{\RV{Y}_i|\RV{J}\RV{I}_i\RV{D}_i}$. These plays a role very similar to the $i$th vector of potential outcomes $\RV{Y}^D_i$. Because $\RV{I}_i$ is unique (i.e. never equal to $\RV{I}_j$ for $j\neq i$), only one observation of any individual is ever given, just like only one element of a vector of potential outcomes is ever observed.

Theorem \ref{th:cc_ind_treat} can also be extended to the case where $\RV{D}$ is a function of the choice $\alpha$ and a ``random signal'' $\RV{R}$, as in Theorem \ref{cor:extend_to_randomised}.

\begin{lemma}\label{lem:ind_to_cc}
Given sequential input-output model $(\prob{P}_C,(\RV{D},\RV{I}),\RV{Y})$ with $\prob{P}_\alpha^{\RV{Y}|\RV{WDI}}$ IO contractible over $\RV{W}$, if $\RV{Y}\CI_{\prob{P}_C}^e (\RV{I},\text{id}_C)|(\RV{W},\RV{D})$ and for any $j\in I$, $\sum_{\alpha\in C} \prob{P}_\alpha^{\RV{I}_i}(j)>0$, then there is some $\RV{W}'$ such that $\prob{P}_\alpha^{\RV{Y}|\RV{W}'\RV{D}}$ is also IO contractible over $\RV{W}$.
\end{lemma}

\begin{proof}
Fix arbitrary $\nu\in \Delta(I^{\mathbb{N}})$ such that $\sum_{\alpha\in C} \prob{P}_\alpha^{\RV{I}_i} \gg \nu$. By assumption of IO contractibility of $\prob{P}_C^{\RV{Y}|\RV{WDI}}$ and Theorem \ref{th:ciid_rep_kernel}
\begin{align}
    \prob{P}_C^{\RV{Y}|\RV{WDI}} &\overset{\prob{P}_C}{\cong} \tikzfig{index_independence_1}\\
    &\overset{\prob{P}_C}{\cong} \tikzfig{index_independence_2}
\end{align}
Where $\Pi_{D,i}:D^{\mathbb{N}}\kto D$ projects the $i$th coordinate, and similarly for $\Pi_{Y,i}$.

In particular, for any $i\in \mathbb{N}$, $j\in I$, this holds for some $\nu$ such that $\nu(\Pi_{Y,i}^{-1} (j))=1$ and by extension for any finite $A\subset \mathbb{N}$ we can find $\nu$ such that $\nu(\Pi_{Y,i}^{-1} (j))=1$ for all $i\in A$, $j\in I$. Thus, for any $n\in \mathbb{N}$
\begin{align}
    \prob{P}_C^{\RV{Y}_{[n]}|\RV{W}\RV{D}_{[n]}\RV{I}_{[n]}} &\overset{\prob{P}_C}{\cong} \tikzfig{index_independence_3}\label{eq:follows_from_determinism}\\
    &\overset{\prob{P}_C}{\cong} \tikzfig{index_independence_4}\label{eq:follows_from_equality}
\end{align}

where Equation \eqref{eq:follows_from_determinism} follows from Theorem \ref{th:fong_det_kerns} and Equation \eqref{eq:follows_from_equality} follows from the fact that Equation \eqref{eq:follows_from_determinism} holds for arbitrary $j\in I$.

Thus by Lemma \ref{lem:infinitely_extended_kernels}
\begin{align}
    \prob{P}_C^{\RV{Y}|\RV{WD}} &= \tikzfig{index_independence_5}
\end{align}
Applying Theorem \ref{th:ciid_rep_kernel}, $\prob{P}_C^{\RV{Y}|\RV{WD}}$ is IO contractible over $\RV{W}$.
\end{proof}

\begin{theorem}\label{th:ind}
Given a sequential input-output model $(\prob{P}_C,(\RV{D},\RV{I}),\RV{Y})$ on $(\Omega,\sigalg{F})$ with $Y$ standard measurable and $C$ countable, if there is some $\RV{J}$ such that for each $\alpha$
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_i|\RV{J}\RV{I}_i\RV{D}_i} &= \prob{P}_\alpha^{\RV{Y}_i|\RV{J}\RV{I}_i\RV{D}_i} &\forall i,j\in \mathbb{N}\\
    \RV{Y}_i&\CI^e_{\prob{P}_C} (\RV{I}_{\{i\}^{\complement}},\RV{D}_{\{i\}^{\complement}})|(\RV{J},\RV{I}_i,\RV{D}_i)
\end{align}
and
\begin{align}
    &\RV{Y}\CI^e_{\prob{P}_C} \RV{I} | \text{id}_C\\
    &\RV{YIJ}\CI^e_{\prob{P}_C} \RV{D}|\text{id}_C\\
    &\RV{YIJ}\CI^e_{\prob{P}_C} \text{id}_C|\RV{D}\\
    &\forall i,j\in \mathbb{N}: \sum_{\alpha\in C} \prob{P}_\alpha^{\RV{I}_i}(j)>0
\end{align}
then $\prob{P}_C^{\RV{Y}|\RV{JD}}$ is IO contractible over $\RV{J}$.
\end{theorem}

\begin{proof}
For any $\alpha\in C$
\begin{align}
    \prob{P}_\alpha^{\RV{YJ}|\RV{I}} &= \tikzfig{kernel_fac_with_idents}\\
    &= \tikzfig{kernel_fac_with_idents_indepped}
\end{align}

Define $\kernel{Q}$ by $\alpha\mapsto \prob{P}_\alpha$ and $\kernel{Q}^{\cdot|\cdot\text{id}_C}$ by $\alpha\mapsto \prob{P}_\alpha^{*}$ and $\kernel{Q}^{\text{id}_C}$ is an arbitrary distribution in $\Delta(C)$ with full support. Note that the support of $\kernel{Q}^{\RV{IDYJ}}$ is the union of the support of $\prob{P}^{\RV{IDYJ}}_\alpha$ for all $\alpha$. Then
\begin{align}
    \kernel{Q}^{\RV{YJ}|\RV{IC}} &\overset{\prob{Q}}{\cong} \tikzfig{kernel_fac_with_idents_kernelised}
\end{align}

By assumption $\RV{YI}\CI^e_{\prob{P}_C} \RV{D}|\text{id}_C$, it is also the case that
\begin{align}
    \kernel{Q}^{\RV{Y}|\RV{ID}} &\overset{\prob{Q}}{\cong} \tikzfig{kernel_Q_fac_with_idents}\\
    &\overset{\prob{Q}}{\cong} \tikzfig{kernel_Q_fac_with_idents_indepped}\\
    &\overset{\prob{Q}}{\cong} \tikzfig{kernel_Q_fac_with_idents_subbed}
\end{align}
But
\begin{align}
    \kernel{Q}^{\RV{Y}|\RV{ID}}=\sum_{\alpha\in C} \prob{P}_\alpha^{\RV{Y}|\RV{ID}}\kernel{Q}^{\text{id}_C}(\alpha)\\
    &= \prob{P}_C^{\RV{Y}|\RV{ID}}\\
    \implies \tikzfig{kernel_Q_fac_with_idents_subbed} &= \prob{P}_C^{\RV{Y}|\RV{ID}}
\end{align}

Furthermore, by assumption $\RV{Y}\CI^e_{\prob{P}_C} \RV{I} | \text{id}_C$, so there is some $\kernel{K}:C\kto Y\times W$ such that
\begin{align}
    \kernel{Q}^{\RV{YJ}|\RV{IC}} &\overset{\prob{Q}}{\cong} \tikzfig{kernel_Q_indepped}\\
    \implies \prob{P}_C^{\RV{YJ}|\RV{ID}} &= \tikzfig{kernel_Q_fac_with_idents_swapped}\\
    &= \tikzfig{kernel_P_indep}
\end{align}
Then by Lemma \ref{lem:ind_to_cc}, $\prob{P}_C^{\RV{Y}|\RV{JD}}$ is IO contractible over $\RV{J}$.
\end{proof}

Lemma \ref{lem:exch_to_ind} is used to apply Theorem \ref{th:ind} to models where $\RV{I}$ is a sequence of unique identifiers. Only in this case, exchangeability of the unique identifiers implies the identifiers are independent of the outcomes $\RV{Y}$.

\begin{lemma}\label{lem:exch_to_ind}
Given any probability set $\prob{P}_C$ where $\RV{Y}\CI_{\prob{P}_C}^e \text{id}_C|(\RV{D},\RV{I})$ and $\RV{I}:\Omega\to I$ is an infinite sequence of unique identifiers, if for each finite permutation $\rho:\mathbb{N}\to \mathbb{N}$
\begin{align}
    \prob{P}_\alpha^{\RV{Y}|\RV{I}} &= (\text{Swap}_{\rho(I)}\otimes \text{Id}_X )\prob{P}_\alpha^{\RV{Y}|\RV{I}}
\end{align}
then $\RV{Y}\CI_{\prob{P}_C}^e \RV{I}|\text{id}_C$.
\end{lemma}

\begin{proof}
By definition of the set $I$ of finite permutations, for every $\rho\in I$, $B\in\sigalg{Y}^{\mathbb{N}}$, $d\in D^{\mathbb{N}}$ there is a finite permutation $\rho^{-1}\in I$ such that $\rho\circ\rho^{-1}=\text{id}_{\mathbb{N}}$. Then
\begin{align}
    \prob{P}_\alpha^{\RV{Y}|\RV{I}}(B|\rho) &= (\kernel{F}_{\rho^{-1}}\otimes \text{Id}_X )\prob{P}_\alpha^{\RV{Y}|\RV{I}}(B|\rho)\\
    &= \prob{P}_\alpha^{\RV{Y}|\RV{I}}(B|\text{id}_{\mathbb{N}})
\end{align}
Therefore
\begin{align}
    \prob{P}_\alpha^{\RV{Y}|\RV{I}} &\overset{\prob{P}_C}{\cong} \text{erase}_{I}\otimes \prob{P}_\alpha^{\RV{Y}}
\end{align}
\end{proof}

Theorem \ref{th:cc_ind_treat} presents a set of sufficient conditions for $\prob{P}_C^{\RV{Y}_i|\RV{H}\RV{D}_i}$ to be conditionally independent and identical response functions with respect to the standard directing random conditional $\RV{H}$:
\begin{enumerate}
    \item There exist variables $\RV{I}$ representing ``unique identifiers'' which satisfy the assumption that $\prob{P}_C^{\RV{Y}_i|\RV{J}\RV{D}_i\RV{I}_i}$ are a sequence of independent and identical response functions for some $\RV{J}$
    \item The identifiers $\RV{I}$ can be swapped without altering the model of the consequences $\RV{Y}$
    \item The inputs $\RV{D}$ and the choice $\text{id}_C$ are substitutable with respect to $\RV{Y}$ and $\RV{I}$: $\RV{YI}\CI^e_{\prob{P}_C} \text{id}_C|\RV{D}$ and $\RV{YI}\CI^e_{\prob{P}_C} \RV{D}|\text{id}_C$
\end{enumerate}

\begin{theorem}\label{th:cc_ind_treat}
Given a sequential input-output model $(\prob{P}_C,(\RV{D},\RV{I}),\RV{Y})$, on $(\Omega,\sigalg{F})$ with $Y$ standard measurable and $C$ and $D$ countable, $\RV{D}$ infinitely supported, $\RV{I}$ an infinite sequence of unique identifiers, if there is some $\RV{J}$ such that
\begin{align}
    \prob{P}_\alpha^{\RV{Y}|\RV{ID}} &= (\text{Swap}_{\rho(I)}\otimes \text{Id}_D )\prob{P}_\alpha^{\RV{Y}|\RV{ID}}&\forall \text{ finite permutations }\rho\\
    &\RV{YIJ}\CI^e_{\prob{P}_C} \RV{D}|\text{id}_C\\
    &\RV{YIJ}\CI^e_{\prob{P}_C} \text{id}_C|\RV{D}\\
    &\forall i,j\in \mathbb{N}: \sum_{\alpha\in C} \prob{P}_\alpha^{\RV{I}_i}(j)>0
\end{align}
and for each $\alpha$ $\prob{P}_\alpha^{\RV{Y}|\RV{JID}}$ is IO contractible over $\RV{J}$, then we have conditionally independent and identical responses $\prob{P}_\alpha^{\RV{Y}_i|\RV{D}_i\RV{H}}$ for all $i$, where $\RV{H}$ is the directing random conditional with respect to $(\RV{D},\RV{Y})$.
\end{theorem}

\begin{proof}
Apply lemma \ref{lem:exch_to_ind} to get $\RV{Y}\CI^e_{\prob{P}_C} \RV{I} | \text{id}_C$, then apply Theorem \ref{th:ind} for $\prob{P}_C^{\RV{Y}|\RV{J}\RV{D}}$ IO contractible. The result follows from Theorem \ref{th:ciid_rep_kernel}.
\end{proof}

Theorem \ref{th:cc_ind_treat} can be extended to the case where decisions $\RV{D}$ are a one-to-one deterministic function of the choice, or a random mixtures of one-to-one deterministic functions of the choice. This extension is applicable a randomised controlled trial, where the treatments are deterministically controlled and randomly assigned.

\begin{theorem}\label{cor:extend_to_randomised}
Consider a sequential input-output model $(\prob{P}_{C'},\RV{D},\RV{Y})$ where $\prob{P}_{C'}^{\RV{Y}|\RV{WD}}$ is IO contractible over $\RV{W}$, and construct a second model $(\prob{P}_{C},\RV{D},\RV{Y})$ where $\prob{P}_C$ is the union of $\prob{P}_{C'}$ and its convex hull. Then $\prob{P}_{C}^{\RV{Y}|\RV{WD}}$ is also IO contractible.
\end{theorem}

\begin{proof}
For all $\alpha\in C$, there is some probability measure $\mu:C'\to [0,1]$ such that
\begin{align}
    \prob{P}_\alpha^{\RV{Y}|\RV{WD}} &= \sum_{\beta\in C'} \mu(\beta) \prob{P}_\beta^{\RV{Y}|\RV{WD}}\\
    &= \prob{P}_{C'}^{\RV{Y}|\RV{WD}}
\end{align}
thus
\begin{align}
    \prob{P}_C^{\RV{Y}|\RV{WD}} = \prob{P}_{C'}^{\RV{Y}|\RV{WD}}
\end{align}
and in particular, $\prob{P}_C^{\RV{Y}|\RV{WD}}$ is IO contractible.
\end{proof}

Theorem \ref{cor:extend_to_randomised} can be used to argue that, given a sequence of experiments IO contractible under deterministic choices, adding random mixtures of these choices also yields a IO contractible sequence. \citet{kasy_why_2016} argues that as long as the experimenter controls the treatment assignment, causal effects are identified (i.e. the randomisation step is not strictly necessary). Example \ref{ex:randomised_experiment} shows that this argument might be supported, but Example \ref{ex:bad_randomised_experiment} shows that there are subtle ways that might lead to this argument failing.

We assume an infinite sequence, which is clearly unreasonable. Extending the representation theorems to the case of finite sequences, using for example the result of \citet{diaconis_finite_1980} with establishes that finite exchangeable distributions are approximately mixtures of independent and identically distributed sequences, would allow some implausible assumptions in the following example to be removed.

Theorem \ref{th:cc_ind_treat} is used in the following example to argue that, under certain conditions, a controlled experiment supports a IO contractible model.

\begin{example}\label{ex:randomised_experiment}
A sequential experiment is modeled by a probability set $\prob{P}_C$ with binary treatments $\RV{D}:=(\RV{D}_i)_{i\in \mathbb{N}}$ and binary outcomes $\RV{Y}:=(\RV{Y}_i)_{i\in\mathbb{N}}$. The set of choices $C$ is the set of all probability distributions$\Delta(D^N)$ for some $N\subset\mathbb{N}$ (this is to ensure $C$ is countable).

Each treatment $\RV{D}_i$ is given to a patient, and each patient provides a unique identifier $\RV{I}_i$ which for simplicity we assume is a number in $\mathbb{N}$ (instead of, say, a driver's license number and state of issue), and that (implausibly) there is a positive probability for $\RV{I}_i$ to take any value in $\mathbb{N}$ for any choice $\alpha$.

The treatments are decided as follows: the analyst consults the model $\prob{P}_C$, and, according to $\prob{P}_C$ and some previously agreed upon decision rule, comes up with a (possibly stochastic) sequence  of treatment distributions $\alpha:=(\mu_i)_{i\in N}$ with each $\mu_i$ in $\Delta(\{0,1\})$. If $\mu_i$ is deterministic -- that is, it puts probability 1 on some treatment $d_i$, the experiment administrator will assign patient $i$ the treatment $d_i$. Otherwise, if $\mu_i$ is nondeterministic, the administrator will consult a random number generator that yields treatment assignments according to $\mu_i$, and treatment will then be assigned deterministically according to the result. Letting $C'\subset C$ be the deterministic elements of $C$, this scheme is assumed by the analyst to support the assumptions $\RV{YIJ}\CI^e_{\prob{P}_{C'}} \RV{D}|\text{id}_C$ and $\RV{YIJ}\CI^e_{\prob{P}_{C'}} \text{id}_C|\RV{D}$ for any $\RV{J}$, and the randomisation procedure is deemed sufficient to ensure that for any mixed $\alpha\in C$ where $\alpha = \sum_{\beta\in C'} \mu(\beta) \beta$, $\prob{P}_\alpha = \sum_{\beta\in C'} \prob{P}_\beta$.

Furthermore, assume $\prob{P}_C^{\RV{Y}|\RV{DI}}$ is IO contractible. Then by Theorem \ref{th:ciid_rep_kernel_k}, there is some $\RV{J}$ such that, conditional on $\RV{J}$, $\prob{P}_C^{\RV{Y}_i|\RV{J}\RV{D}_i\RV{I}_i}$ are conditionally independent and identical response functions. The analyst constructing the model has no particular knowledge about any identifier, and so for any choice the associated model is assumed invariant to permutations of identifiers - that is $\RV{Y}\CI^e_{\prob{P}_C} \RV{I}|\text{id}_C$ (see Lemma \ref{th:ind}). The assumption that this holds given any choice can be tricky -- not only must the identifiers appear symmetric to the analyst constructing the model, but nothing breaking this symmetry may be learned from the choice $\alpha$ (see the Example \ref{sec:dm_control}). One reason supporting this assumption is that the decision maker selects $\alpha$ according to a rule known in advance, so they do not ``learn'' anything upon picking a particular $\alpha$.

Then, for the deterministic subset $C'\subset C$, application of Theorem \ref{th:cc_ind_treat} yields $\prob{P}_{C'}^{\RV{Y}|\RV{HD}}$ is IO contractible over $\RV{H}$, and by application of Corollary \ref{cor:extend_to_randomised}, so is $\prob{P}_{C}^{\RV{Y}|\RV{D}}$.
\end{example}

Permutability of identifiers can fail when the rule for selecting $\alpha$ is not known in advance. The following example is extreme in order to illustrate the issue clearly. The distinction between the analyst and the administrator is also intended to make the example easier to parse. The key point is that, when the rule for selecting $\alpha$ is not known in advance, symmetries that are apparent at the time of model construction do not necessarily hold for every choice $\alpha$, and this remains true if e.g. the selection of choices leads to less extreme confounding or the analyst and the administrator are actually the same person.

The following example involves the choice $\text{id}_C$ depending on some covariate $\RV{U}$. It is not straightforward to express the idea that ``$\text{id}_C$ depends on $\RV{U}$'' in decision model $\prob{P}_C$. Back in Section \ref{sec:actions} we opted not to try to represent models where the choice is a random variable because this introduces difficulties for modelling decision problems and doesn't bring obvious benefits. However, if $\text{id}_C$ actually represents something \emph{not} selected according to some deliberation involving the decision model $\prob{P}_C$ then the representational choices in this thesis are likely to be inappropriate. This is what the following example shows.

\begin{example}[I choose vs you choose]
Consider the following ``I choose'' experiment. A DM makes a choice $\alpha\in \{0,1\}^\mathbb{N}$ that deterministically sets the value of binary inputs $(\RV{D}_i)_{i\in\mathbb{N}}$. This decision maker is interested in evaluating the corresponding binary outputs $(\RV{D}_i)_{i\in\mathbb{N}}$, and regards the identifiers $(\RV{I}_i)_{i\in \mathbb{N}})$ permutable on account of no prior information that distinguishes any identifier from the other. The DM accepts the assumption of individual-level response functions. From Theorem \ref{th:cc_ind_treat}, IO contractibility of $\prob{P}_C^{\RV{Y}|\RV{HD}}$ follows.

Consider instead a ``you choose'' experiment. Here, a DM's assistant makes a choice $\alpha\in \{0,1\}^\mathbb{N}$ that deterministically sets the value of binary inputs $(\RV{D}_i)_{i\in\mathbb{N}}$. This decision maker is interested in evaluating the corresponding binary outputs $(\RV{D}_i)_{i\in\mathbb{N}}$. The decision maker assesses there are no distinguishing differences between identifiers \emph{before they're aware of their assistant's choice}. However, their assistant's choice itself endows the identifiers with distinguishing feature; namely, which selection was made for each identifier. While DM doesn't know if their assistant employs any strategy in their selection, they consider it plausible and hence permutability of identifiers.
\end{example}

\citet{kasy_why_2016} argues that ``randomised controlled trials are not needed for causal identifiability, only controlled trials'', and suggests that experiments should sometimes be designed with deterministic assignments of patients to treatment and control groups, optimised according to the experiment designer's criteria. Following this, \citet{banerjee_theory_2020} suggested that deterministic rules might falter when one can't pick a function to balance covariates in a way that satisfies everyone in a panel of reviewers. We note that the issue is somewhat subtle; while \emph{I} might be able to make a nonrandom selection of inputs for which \emph{I} am convinced of IO contractibility, it does not follow that a third party will necessarily share my judgement. On the other hand, if both myself and the third party agree that a random rule was uniformly applied to the selection of inputs for every individual, then the selection gives neither of us a reason to reject an initial judgment of permutability.

% We call this example the ``I choose vs you choose'' problem. Suppose we have a decision maker (``DM'') and an administrator (``admin'') cooperating to collect data to support DM to make a choice.

% First, consider the ``I choose'' condition. Here, DM's choice $\alpha\in \{0,1\}^2$ deterministically sets the value of binary inputs $\RV{D}_1,\RV{D}_2$, and the decision maker is interested in evaluating the corresponding binary outputs $\RV{Y}_1,\RV{Y}_2$. The decision maker assesses that their knowledge of the real-world mechanisms that gives rise to each output $\RV{Y}_i$ in the context on an input $\RV{D}_i$ render these mechanisms indistinguishable. 's point of view, the input-output relations for each step are indistinguishable. In particular, they assess that the marginal probabilities of the outputs are the same given a corresponding input, and that the evidence that the first experiment brings to bear on the second is equivalent to the evidence that the second brings to bear on the first. Thus, they assess that exchange commutativity is appropriate; for all $\alpha$:
% \begin{align}
%     \prob{P}_\alpha^{\RV{Y}_1\RV{Y}_2|\RV{D}_1\RV{D}_2} &= \prob{P}_\alpha^{\RV{Y}_2\RV{Y}_1|\RV{D}_2\RV{D}_1}
% \end{align}


% but this example suggests another reason one might want to avoid deterministic treatment assignments. If the choices $\alpha$ are a deterministic sequence of assignments for each index $i$, this means that there is an enormous set of possible choices, and many degrees of freedom if the choices ``aren't actually chosen'' in the sense of the example above. In contrast, if the set of choices is a single parameter in $[0,1]$ which is then used to assign all treatments according to a random procedure depending only on this parameter, there are many fewer degrees of freedom to exploit if the choice ``isn't actually chosen''.

% A particular concern arises when the choice variable $\text{id}_C$ is not associated with the output of a decision procedure involving the model $\prob{P}_C$. In this situation, the value of $\text{id}_C$ can affect the model in potentially unexpected ways. ``Potentially unexpected'' is a vague notion, and we can't say whether $\text{id}_C$ being completely under the decision maker's control avoids ``unexpected'' dependence on $\text{id}_C$, but it seems to be less problematic.

% We set this up in terms of an ``analyst'' and an ``administrator'' who have responsibility for different parts of the procedure. They don't strictly need to be different people, but it helps make the issue clearer. The analyst's job is to construct a model $\prob{P}_C$, evaluate different options $\alpha\in C$ and offer advice regarding the choice. The administrator's job is to choose some $\alpha\in C$ satisfying the analyst's requirements and to carry out any procedure arising from this.

% This separation of concerns gives the administrator a degree of freedom in their choice, and they can potentially use this to choose $\alpha$ with access to information that the analyst lacks.

% In particular, suppose an experiment is modeled by a sequential input-output model $(\prob{P}_C,(\RV{D},\RV{U}),\RV{Y})$ and the set of choices $C=[0,1]^{\mathbb{N}}$ is a length $\mathbb{N}$ sequence of probability distributions in $\Delta(\{0,1\})$. The analyst, based on their knowledge of the experiment, constructs $\prob{P}_C$ such that $\prob{P}_C^{\RV{Y}_i|\RV{U}_i\RV{D}_i}(1|\cdot,\cdot)$ is given by:
% \begin{center}
% \begin{tabular}{ c | c | c }
%   & $\RV{D}_i=0$ & $\RV{D}_i=1$ \\\hline 
%  $\RV{U}_i=0$ & 0 & 0 \\ \hline 
%  $\RV{U}_i=1$ & 1 & 1   
% \end{tabular}
% \end{center}
% and the triples $(\RV{D}_i,\RV{U}_i,\RV{Y}_i)$ are mutually independent given $\text{id}_C$. This makes $\prob{P}_C^{\RV{Y}|\RV{UD}}$ IO contractible over $*$. Suppose also 
% \begin{align}
%     \prob{P}_\alpha^{\RV{D}_i}(1) &= \alpha_i
% \end{align}
% where $\alpha=(\alpha_i)_{i\in\mathbb{N}}$. From the analyst's point of view, both before and after making their recommendations the $\RV{U}_i$ are also IID. This will be expressed with a probability distribution $\prob{Q}$ representing the analyst's prior knowledge:
% \begin{align}
%     \prob{Q}^{\RV{U}_i}(1) &= 0.5
% \end{align}
% one might be tempted to reason that, if $\prob{Q}$ is the analyst's state of knowledge after making any reccomendation, then we should take $\prob{P}_C^{\RV{U}}=\prob{Q}^{\RV{U}}$. Call the resulting model $\prob{P}_C'$. Together with the other assumptions above, this would imply
% \begin{align}
%     \prob{P}_C^{\prime \RV{Y}_i|\RV{D}_i}(1|d) &= 0.5 & \forall d\in \{0,1\}
% \end{align}
% Thus $\prob{P}_C^{\RV{Y}|\RV{D}}$ is also IO contractible.

% However, the analyst's recommendation \emph{does not} fix the value of $\text{id}_C$. Suppose analyst actually recommends any $\alpha$ such that $\lim_{n\to\infty} \sum_i^n \frac{\alpha_i}{n} = 0.5$ (acknowledging that, in this contrived example, there's no obvious reason to do so). Suppose that the administrator operates by the following rule: \emph{first} they observe the value of $\RV{U}_i$, then they choose $\alpha_i$ equal to whatever they saw with an $\epsilon$ sized step towards $0.5$. That is, if they see $\RV{U}_i\yields 1$, they choose $\alpha_i=1-\epsilon$, where $\epsilon < 0.5$.

% Then the analyst should instead adopt the model
% \begin{align}
%     \prob{P}_\alpha^{\RV{U}_i}(1) &= \mathds{1}_{\alpha_i>0.5}
% \end{align}
% Take $\alpha$ such that $\alpha_i=1-\epsilon$ and $\alpha_j=\epsilon$. Then
% \begin{align}
%     \prob{P}_{\alpha}^{\RV{Y}_i|\RV{D}_i}(1|1) &= 1\\
%     &\neq \prob{P}_\alpha^{\RV{Y}_j|\RV{D}_j}(1|1)\\
%     &=0
% \end{align}
% everything has been assumed IID, so $\prob{P}_C^{\RV{Y}|\RV{HD}}$ is not IO contractible.

% The original justification for having a set of choices $C$ is that $C$ is the set of things that, after deliberation aided by the model $\prob{P}_C$, the decision maker might select. The present example does not conform to this understanding of the meaning of the set $C$, and it suggests that one should be cautious when modelling ``decision problems'' with ``choices'' that are not actually the things that are being chosen.

% This point is related to the question of why experimenters randomise. \citet{kasy_why_2016} argues that ``randomised controlled trials are not needed for causal identifiability, only controlled trials'', and suggests that experiments should sometimes be designed with deterministic assignments of patients to treatment and control groups, optimised according to the experiment designer's criteria. Following this, \citet{banerjee_theory_2020} suggested that deterministic rules might falter when one can't pick a function to balance covariates in a way that satisfies everyone in a panel of reviewers. 

The condition $\RV{YIJ}\CI^e_{\prob{P}_C} \text{id}_C|\RV{D}$ without also having $\RV{YIJ}\CI^e_{\prob{P}_C} \RV{D}|\text{id}_C$ does \emph{not} imply the conclusion of Theorem \ref{th:cc_ind_treat}. Informally, if $\RV{D}$ gives some ``extra information'' over and above $\text{id}_C$, then any symmetry that holds before we observe $\RV{D}$ might not hold after $\RV{D}$ has been observed. We have argued somewhat informally that the choice $\text{id}_C$ should be completely under the decision maker's control -- for Theorem \ref{th:cc_ind_treat}, this perfect control has to extend to the sequence of inputs $\RV{D}$. 

Example \ref{ex:not_enough_control} requires the hypotheses that any given identifier $i\in\mathbb{N}$ could be associated with one of two input-output maps $D\kto Y$. Thus the space of hypotheses is a sequence of binary values $H=\{0,1\}^{\mathbb{N}}$. Equipped with the product topology, $H$ is a countable product of separable, completely metrizable spaces and is therefore also separable and completely metrizable \citep[Thm. 16.4,Thm. 24.11]{willard_general_1970}. Thus $(H,\mathcal{B}(H))$ is a standard measurable space and, because it is uncountable, it is isomorphic to $([0,1],\mathcal{B}([0,1]))$.

\begin{example}\label{ex:not_enough_control}
Take $Y=C=D=\{0,1\}$ and take $(H,\sigalg{H})$ to be $\{0,1\}^{\mathbb{N}}$ equipped with the product topology. For any $i\neq 1$, $\RV{Y}_i\RV{I}_i\RV{D}_i\CI^e_{\prob{P}_C} \text{id}_C$, while $\prob{P}_\alpha^{\RV{D}_1}=\delta_\alpha$ and $\RV{I}_i\CI^e_{\prob{P}_C} \text{id}_C$.

$\RV{YI}\CI^e_{\prob{P}_C} \text{id}_C|\RV{D}$ follows from the fact that $\text{id}_C$ can be (almost surely) written as a function of $\RV{D}$.

For all $i,\in \mathbb{N}$, $y,d\in \{0,1\}$, $h\in H$ set
\begin{align}
    \prob{P}_C^{\RV{Y}_i|\RV{H}\RV{I}_i\RV{D}_i}(y|h,j,d) &= \delta_1(p(j,h))\delta_d(y) + \delta_0(p(j,h))\delta_{1-d}(y)
\end{align}
where $p(j,h)$ projects the $j$-th component of $h$. That is, if $h$ maps $j$ to 1, $\RV{Y}$ goes with $\RV{D}$ while if $h$ maps $j$ to $0$, $\RV{Y}$ goes opposite $\RV{D}$. Suppose also 
\begin{align}
    \RV{Y}_i\CI_{\prob{P}_C}^e (\RV{X}_{<i},\RV{Y}_{<i},\RV{I}_{<i},\text{id}_C)|(\RV{X}_i,\RV{Y}_i,\RV{H})
\end{align}
Then $\prob{P}_C^{\RV{Y}|\RV{DI}}$ is IO contractible. Set $\prob{P}_{C}^{\RV{H}}$ to be the uniform measure on $(H,\sigalg{H})$ and for $i>1$
\begin{align}
    \prob{P}_C^{\RV{D}_i|\RV{I}_i\RV{H}}(d|j,h) &= \delta_{p(j,h)}(d)
\end{align}
that is, if $h$ maps $j$ to 1, $\RV{D}$ is 1 while if $h$ maps $j$ to $0$, $\RV{D}$ is 0. This also implies
\begin{align}
    \prob{P}_C^{\RV{I}_i|\RV{D}_i\RV{H}}(p(\cdot,h)^{-1}(d)|d,h) &= 1\label{eq:all_eq_d}
\end{align}

Then, for $i>1$
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_i|\RV{H}\RV{D}_i}(y|h,d) &= \sum_{j\in \mathbb{N}} \delta_1(p(j,h))\delta_d(y)\prob{P}_C^{\RV{I}_i|\RV{D}_i\RV{H}}(j|d,h) + \delta_0(p(j,h))\delta_{1-d}(y)\prob{P}_C^{\RV{I}_i|\RV{D}_i\RV{H}}(j|d,h)\\
    &= \sum_{j\in \mathbb{N}} \delta_1(d)\delta_d(y)\prob{P}_C^{\RV{I}_i|\RV{D}_i\RV{H}}(j|d,h) + \delta_0(d)\delta_{1-d}(y)\prob{P}_C^{\RV{I}_i|\RV{D}_i\RV{H}}(j|d,h)&\text{by Eq \eqref{eq:all_eq_d}}\\
    &= \delta_1(y)\\
    \implies \prob{P}_\alpha^{\RV{Y}_i|\RV{D}_i}(y|d) &= \delta_1(y)
\end{align}

For $q\in I$, set
\begin{align}
    \prob{P}_C^{\RV{I}|\RV{H}}(q|h)&= \begin{cases}
        0.5 & q=(1,2,3,4,...) \text{ or } (1,3,2,4,...)\\
        0&\text{otherwise}
    \end{cases}
\end{align}
and set
\begin{align}
    \prob{P}_C^{\RV{H}|\RV{D}}(h) &= \begin{cases}
        0.5 & h=(0,1,0,1,1,...)\text{ or }h=(0,0,1,1,1,...)\\
        0 &\text{otherwise}
    \end{cases}
\end{align}
Let $\overline{H}$ be the support of $\prob{P}_C^{\RV{H}|\RV{D}}(h)$.

Then for $i=1$
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_1|\RV{D}_1}(y|h,d) &= \sum_{h\in H} \sum_{j\in \mathbb{N}} \prob{P}_\alpha^{\RV{I}_1|\RV{D}_1\RV{H}}(j|d,h)\prob{P}_C^{\RV{H}|\RV{D}_1}(h|d)\left(\delta_1(p(j,h))\delta_d(y) + \delta_0(p(j,h))\delta_{1-d}(y)\right)\\
    &= \sum_{h\in \overline{H}} 0.5( \delta_1(p(1,h))\delta_d(y) + \delta_0(p(1,h))\delta_{1-d}(y))\\
    &= \delta_{1-d}(y))\\
    &\neq  \prob{P}_\alpha^{\RV{Y}_i|\RV{D}_i}(y|h,d) & i\neq 1
\end{align}
Thus $\prob{P}_C^{\RV{Y}|\RV{D}}$ is not IO contractible by Theorem \ref{th:equal_of_condits}. 

However, given any finite permutation $\rho:\mathbb{N}\to\mathbb{N}$
\begin{align}
    \prob{P}_\alpha^{\RV{Y}|\RV{I}}(y|q) &= \sum_{h\in \overline{H}}\sum_{d\in\{0,1\}^{\mathbb{N}}} \prod_{i\in \mathbb{N}} \prob{P}_C^{\RV{Y}_i|\RV{I}_i\RV{D}_i\RV{H}}(y_i|q_i,d_i,h) \prob{P}_\alpha^{\RV{D}_i|\RV{I}_i\RV{H}}(d_i|q_i,h)\prob{P}_C^{\RV{H}}(h)\\
    &= \delta_{1-\alpha}(y_1)\delta_{(1)_{i\in\mathbb{N}}}(y_{>1})\\
    &= \prob{P}_\alpha^{\RV{Y}|\RV{I}}(y|\rho^{-1}(q))\\
    &= \kernel{F}_{\rho}\prob{P}_\alpha^{\RV{Y}|\RV{I}}(y|q)
\end{align}
\end{example}

\subsection{Other examples}



% \paragraph{Example 6: the provenance of the choice variable}\label{sec:dm_control}

% \todo[inline]{Use individual-level ccont}

% The point of this example is to clarify the idea of a ``choice'' variable. If we say that some value is the outcome of a choice, a straightforward interpretation of this term suggests that this value was chosen by someone, somewhere. However, for the purposes of decision making models, there are important differences between:
% \begin{itemize}
%     \item Values chosen by someone, somewhere
%     \item Values chosen by the decision make, using the decision making model
% \end{itemize}

% We call this example the ``I choose vs you choose'' problem. Suppose we have a decision maker (``DM'') and an administrator (``admin'') cooperating to collect data to support DM to make a choice.

% First, consider the ``I choose'' condition. Here, DM's choice $\alpha\in \{0,1\}^2$ deterministically sets the value of binary inputs $\RV{D}_1,\RV{D}_2$, and the decision maker is interested in evaluating the corresponding binary outputs $\RV{Y}_1,\RV{Y}_2$. The decision maker assesses that their knowledge of the real-world mechanisms that gives rise to each output $\RV{Y}_i$ in the context on an input $\RV{D}_i$ render these mechanisms indistinguishable. 's point of view, the input-output relations for each step are indistinguishable. In particular, they assess that the marginal probabilities of the outputs are the same given a corresponding input, and that the evidence that the first experiment brings to bear on the second is equivalent to the evidence that the second brings to bear on the first. Thus, they assess that exchange commutativity is appropriate; for all $\alpha$:
% \begin{align}
%     \prob{P}_\alpha^{\RV{Y}_1\RV{Y}_2|\RV{D}_1\RV{D}_2} &= \prob{P}_\alpha^{\RV{Y}_2\RV{Y}_1|\RV{D}_2\RV{D}_1}
% \end{align}


% but this example suggests another reason one might want to avoid deterministic treatment assignments. If the choices $\alpha$ are a deterministic sequence of assignments for each index $i$, this means that there is an enormous set of possible choices, and many degrees of freedom if the choices ``aren't actually chosen'' in the sense of the example above. In contrast, if the set of choices is a single parameter in $[0,1]$ which is then used to assign all treatments according to a random procedure depending only on this parameter, there are many fewer degrees of freedom to exploit if the choice ``isn't actually chosen''.

% A particular concern arises when the choice variable $\text{id}_C$ is not associated with the output of a decision procedure involving the model $\prob{P}_C$. In this situation, the value of $\text{id}_C$ can affect the model in potentially unexpected ways. ``Potentially unexpected'' is a vague notion, and we can't say whether $\text{id}_C$ being completely under the decision maker's control avoids ``unexpected'' dependence on $\text{id}_C$, but it seems to be less problematic.

% We set this up in terms of an ``analyst'' and an ``administrator'' who have responsibility for different parts of the procedure. They don't strictly need to be different people, but it helps make the issue clearer. The analyst's job is to construct a model $\prob{P}_C$, evaluate different options $\alpha\in C$ and offer advice regarding the choice. The administrator's job is to choose some $\alpha\in C$ satisfying the analyst's requirements and to carry out any procedure arising from this.

% This separation of concerns gives the administrator a degree of freedom in their choice, and they can potentially use this to choose $\alpha$ with access to information that the analyst lacks.

% In particular, suppose an experiment is modeled by a sequential input-output model $(\prob{P}_C,(\RV{D},\RV{U}),\RV{Y})$ and the set of choices $C=[0,1]^{\mathbb{N}}$ is a length $\mathbb{N}$ sequence of probability distributions in $\Delta(\{0,1\})$. The analyst, based on their knowledge of the experiment, constructs $\prob{P}_C$ such that $\prob{P}_C^{\RV{Y}_i|\RV{U}_i\RV{D}_i}(1|\cdot,\cdot)$ is given by:
% \begin{center}
% \begin{tabular}{ c | c | c }
%   & $\RV{D}_i=0$ & $\RV{D}_i=1$ \\\hline 
%  $\RV{U}_i=0$ & 0 & 0 \\ \hline 
%  $\RV{U}_i=1$ & 1 & 1   
% \end{tabular}
% \end{center}
% and the triples $(\RV{D}_i,\RV{U}_i,\RV{Y}_i)$ are mutually independent given $\text{id}_C$. This makes $\prob{P}_C^{\RV{Y}|\RV{UD}}$ IO contractible over $*$. Suppose also 
% \begin{align}
%     \prob{P}_\alpha^{\RV{D}_i}(1) &= \alpha_i
% \end{align}
% where $\alpha=(\alpha_i)_{i\in\mathbb{N}}$. From the analyst's point of view, both before and after making their recommendations the $\RV{U}_i$ are also IID. This will be expressed with a probability distribution $\prob{Q}$ representing the analyst's prior knowledge:
% \begin{align}
%     \prob{Q}^{\RV{U}_i}(1) &= 0.5
% \end{align}
% one might be tempted to reason that, if $\prob{Q}$ is the analyst's state of knowledge after making any reccomendation, then we should take $\prob{P}_C^{\RV{U}}=\prob{Q}^{\RV{U}}$. Call the resulting model $\prob{P}_C'$. Together with the other assumptions above, this would imply
% \begin{align}
%     \prob{P}_C^{\prime \RV{Y}_i|\RV{D}_i}(1|d) &= 0.5 & \forall d\in \{0,1\}
% \end{align}
% Thus $\prob{P}_C^{\RV{Y}|\RV{D}}$ is also IO contractible.

% However, the analyst's recommendation \emph{does not} fix the value of $\text{id}_C$. Suppose analyst actually recommends any $\alpha$ such that $\lim_{n\to\infty} \sum_i^n \frac{\alpha_i}{n} = 0.5$ (acknowledging that, in this contrived example, there's no obvious reason to do so). Suppose that the administrator operates by the following rule: \emph{first} they observe the value of $\RV{U}_i$, then they choose $\alpha_i$ equal to whatever they saw with an $\epsilon$ sized step towards $0.5$. That is, if they see $\RV{U}_i\yields 1$, they choose $\alpha_i=1-\epsilon$, where $\epsilon < 0.5$.

% Then the analyst should instead adopt the model
% \begin{align}
%     \prob{P}_\alpha^{\RV{U}_i}(1) &= \mathds{1}_{\alpha_i>0.5}
% \end{align}
% Take $\alpha$ such that $\alpha_i=1-\epsilon$ and $\alpha_j=\epsilon$. Then
% \begin{align}
%     \prob{P}_{\alpha}^{\RV{Y}_i|\RV{D}_i}(1|1) &= 1\\
%     &\neq \prob{P}_\alpha^{\RV{Y}_j|\RV{D}_j}(1|1)\\
%     &=0
% \end{align}
% everything has been assumed IID, so $\prob{P}_C^{\RV{Y}|\RV{HD}}$ is not IO contractible.

% The original justification for having a set of choices $C$ is that $C$ is the set of things that, after deliberation aided by the model $\prob{P}_C$, the decision maker might select. The present example does not conform to this understanding of the meaning of the set $C$, and it suggests that one should be cautious when modelling ``decision problems'' with ``choices'' that are not actually the things that are being chosen.

% This point is related to the question of why experimenters randomise. \citet{kasy_why_2016} argues that ``randomised controlled trials are not needed for causal identifiability, only controlled trials'', and suggests that experiments should sometimes be designed with deterministic assignments of patients to treatment and control groups, optimised according to the experiment designer's criteria. Following this, \citet{banerjee_theory_2020} suggested that deterministic rules might falter when one can't pick a function to balance covariates in a way that satisfies everyone in a panel of reviewers. 

% Without solving the problem, we observe that the terms ``control'' and ``choice'' here subsume both different kinds of choice indicated above, each of which has different implications for the construction of decision making models. We offer a speculative alternative explanation for randomisation: perhaps that the same model may be appropriate for both notions of ``choice'' under randomised choices, but not under nonrandomised choices.


% \citet{savje_randomization_2021} argues that random assignment (under his definition) does not imply unconfoundedness

\section{Conclusion}

