%!TEX root = main.tex

\chapter[Repeatable decision problems]{Models of repeatable decision problems}\label{ch:evaluating_decisions}

Chapter \ref{ch:tech_prereq} introduced probability sets as generic tools for causal modelling, while Chapter \ref{ch:2p_statmodels} examined how probability set models can be used in decision problems and Section \ref{sec:cons_to_sdp} in particular introduced \emph{see-do} models, which featured variables representing observations and consequences, choices and hypotheses. So far, no attention has been paid to how exactly anybody might use a see-do model to help them ``learn from observations''. Observations and consequences in see-do models could be just about anything -- they need not take values in the same set, nor be related to one another in any particular way. While there might be interesting problems involving models where observations and consequences are different kinds of things, causal inference in practice is usually concerned with problems where a sequence of observations is available and the consequences of interest are of the ``same type'' as observations. Such problems involve \emph{repeatable} measurement procedures - the procedure can be broken down into a sequence sub-procedures that all have something in common with one another. This chapter aims to better understand the ``something in common'' that these sub-procedures share.

Repeatable procedures in classical statistics are associated with the assumption of \emph{exchangeability of observations}. Exchangeable models express the assumption that the results of sub-procedures can be swapped without changing the problem in any important way -- this is the ``something in common'' that exchangeable sub-procedures have (note that in a non-Bayesian setting, this assumption is called permutability and is weaker than exchangeability, see \citet[pg. 463]{walley_statistical_1991}). Decision problem models usually aren't compatible with this assumption; often, the decision maker's choices will affect some sub-procedures in different ways to others. For example, the decision maker might choose to record $10$ values without interaction, then tweak the system of interest before recording their $11$th value -- in this case, swapping the results of the $11$th sub-procedure with the $1$st \emph{does} change the problem in an important way, because it changes the rank order of the ``tweaked'' sub-procedure. The need to account for different possible ``inputs'' to each sub-procedure makes the question of commonality between these sub-procedures substantially more complicated than the assumption of exchangeability.

This chapter introduces \emph{conditionally independent and identical response functions} as the ``something in common'' that a sequence of subprocedures might have. The key theorem in this chapter, Theorem \ref{th:ciid_rep_kernel}, relates the assumption of conditionally independent and identical response functions to a kind of symmetry which we call \emph{causal contractibility}. A model with data-independent actions features conditionally independent and identical response functions if and only if it is causally contractible. Theorem \ref{th:response_is_cc_hdep} introduces a more general notion of causal contractibility and relaxes the data-independent assumption, but comes with some different side conditions.

\paragraph{Chapter outline}

Section \ref{sec:prev_work} surveys previous work, particularly related to symmetries of causal models. Section \ref{sec:response_functions} defines and explains the idea of conditionally independent and identical response functions. Section \ref{sec:ccontracibility} defines causal contractibility, as well as setting out key definitions, lemmas and the proof of Theorem \ref{th:ciid_rep_kernel}. Section \ref{sec:examples} presents a collection of examples that illustrate various features of models that are (or are not) causally contractible. Section \ref{sec:data_dependent} extends the work from Section \ref{sec:ccontracibility} to models where inputs can be data-dependent. The extension is dense and retreads a lot of ground already covered in a slightly different way, but Section \ref{sec:def_combs} introduces the notion of a comb, which is an extension of a conditional probability, that has applications in areas of causal inference beyond what is covered in this chapter, and this subsection stands on its own. Finally, some concluding remarks are in Section \ref{sec:discussion}.

\section{Previous work}\label{sec:prev_work}

This chapter draws on three different lines of work. The first is the study of representations of symmetric of probability models. The equivalence between infinite exchangeable probability models and mixtures of independent and identically distributed models was shown by \cite{de_finetti_foresight_1992}. This result has been extended in many ways, including to finite sequences \citet{kerns_definettis_2006,diaconis_finite_1980} and for partially exchangeable arrays \citet{aldous_representations_1981}. A comprehensive overview of results is presented in \citet{kallenberg_probabilistic_2005}. Particularly similar to our result is the notion of ``partial exchangeability'' from \citet{diaconis_recent_1988}.

The second line of work is the study of exchangeability-like assumptions in causal models. \citet{lindley_role_1981} discuss models consisting of a sequence of exchangeable observations along with ``one more observation'', a structure that is similar to the models with observations and consequences discussed in section \ref{pgph:two_kinds}. Lindley discusses the application of this model to questions of causation, but does not explore this deeply due to the perceived difficulty of finding a satisfactory definition of causation. \citet{rubin_causal_2005}'s overview of causal inference with potential outcomes along with the text \citet{imbens_causal_2015} make use of the assumption of exchangeable potential outcomes to prove several identification results. \citet{saarela_role_2020}, using structural causal models, proposes \emph{conditional exchangeability}, which refers to the invariance of a joint distribution over outcomes under ``surgical switches'' of the values of causal variables of interest.

Exchangeability in the setting of causal models is often discussed in terms of the exchangeability of \emph{people} (or more generically, \emph{experimental units}). \citet{hernan_beyond_2012,greenland_identifiability_1986,banerjee_chapter_2017,dawid_decision-theoretic_2020} all discuss assumptions along these lines.

A stronger assumption than commutativity of exchange is \emph{causal contractibility} (Definition \ref{def:caus_cont}), which adds the assumption of \emph{locality}. This additional assumption appears to have similarities to the stable unit treatment distribution assumption (SUTDA) in \citet{dawid_decision-theoretic_2020}, and the stable unit treatment value assumption (SUTVA) in \citep{rubin_causal_2005}:
\begin{blockquote}
(SUTVA) comprises two sub-assumptions. First, it assumes that \emph{there is no interference between units (Cox 1958)}; that is, neither $Y_i(1)$ nor $Y_i(0)$ is affected by what action any other unit received. Second, it assumes that \emph{there are no hidden versions of treatments}; no matter how unit $i$ received treatment $1$, the outcome that would be observed would be $Y_i(1)$ and similarly for treatment $0$.
\end{blockquote}

Finally, the idea of \emph{combs} in probabilistic models was first proposed by \citet{chiribella_quantum_2008} and an application to causal models was developed by \citet{jacobs_causal_2019}.

There are two differences between the work in this chapter and the examples of ``causal exchangeability'' discussed here. First, in the definition of causal contractibility (Definition \ref{def:ccontract}), we say a model $\prob{P}_C$ is causally contractible with respect to a sequence of inputs $\RV{D}$ and a sequence of outputs $\RV{Y}$ \emph{over some other variable} $\RV{W}$. None of the examples discussed have a clear analogue of $\RV{W}$ (see \ref{pgph:passive_strategic} for an example addressing the role of $\RV{W}$).  Secondly, causal contractibility is a mathematical symmetry rather than a symmetry of measurement procedures (for example, it is not a symmetry with respect to physically swapping patients in an experiment), and it doesn't presuppose ``higher level'' response conditionals in the form of potential outcomes or a structural causal model.

\section[Response functions]{Conditionally independent and identical response functions}\label{sec:response_functions}

Suppose a decision maker is implementing a decision procedure where they'll make a choice and subsequently receive a sequence of paired values $(\proc{D}_i,\proc{Y}_i)$, with their objective depending on the output values yielded by $\proc{Y}_i$s only. Usually the $\proc{D}_i$s, which we call ``inputs'', are under the decision maker's control to some extent, but this might not always be the case. For example, perhaps the first $m$ pairs come from data collected by someone else, where the decision maker has no control over inputs, and the next $n$ depend on their own actions where they have complete control over the inputs.

Suppose the decision maker uses a probability set $\prob{P}_C$ to model such a procedure, and variables $(\RV{D}_i,\RV{Y}_i)$ are associated with the inputs and outputs. There are two different relationships between $\RV{D}_i$ and $\RV{Y}_i$ that might be of interest to the decision maker:
\begin{itemize}
    \item For some choice $\alpha$, $j>m$ and some fixed value of $\RV{D}_j$, what are the \emph{likely consequences} with regard to $\RV{Y}_j$?
    \item For some choice $\alpha$, all $i\leq m$ with some fixed value of $\RV{D}_i$, what is the \emph{relative frequency} of different values of $\RV{Y}_i$?
\end{itemize}
The first is what the decision maker wants to know in order to make a good decision, and the second is something they can learn from the data before taking any actions. In particular, if the decision maker has a good reason to thing that the two relationships should be (approximately) the same \emph{and} independent of the decision maker's overall choice $\alpha$, then they may reduce the overall problem of choosing $\alpha$ to the problem of set (or influence) the inputs under their control $\RV{D}_j$ for $j>m$ to values that have been associated to with favourable consequences according to the preceding data.

More precisely, we are interested in models $\prob{P}_C$ where the probabilistic relationship between each $\RV{D}_i$ and the corresponding $\RV{Y}_i$ is unknown but identical for all indices $i$. To model this, we introduce a hypothesis $\RV{H}$ that represents this unknown relationship, and assert that the distribution of $\RV{Y}_i$ given $(\RV{D}_i,\RV{H})$ is identical for all $i$, independent of all data prior to $i$.

\begin{definition}[Conditionally independent and identical response functions]\label{def:cii_rf}
A probability set $\prob{P}_C$ on $(\Omega,\sigalg{F})$ with variables $\RV{Y}:=(\RV{Y}_i)_{i\in \mathbb{N}}$ and $\RV{D}:=(\RV{D}_i)_{i\in\mathbb{N}}$ has \emph{independent and identical response functions conditional on} $\RV{H}$ if for all $i$, $\RV{Y}_i\CI^e_{\prob{P}_C} (\RV{D}_{[1,i)},\RV{Y}_{[1,i)})|(\RV{H},\RV{C})$ and $\prob{P}_\alpha^{\RV{Y}_i|\RV{D}_i\RV{H}}=\prob{P}_\alpha^{\RV{Y}_j|\RV{D}_j\RV{H}}$ for all $i,j$.
\end{definition}

We only require outputs $\RV{Y}_i$ to be independent of \emph{previous} inputs and outputs, conditional on $\RV{H}$ and $\RV{D}_i$. If $\RV{D}_i$ is selected based on previous data, then in general there may be relationships between $\RV{D}_j$ and $\RV{Y}_i$ for $j>i$ even after conditioning on $\RV{D}_i$ and $\RV{H}$ (e.g. $\RV{D}_j$ is chosen deterministically equal to $\RV{Y}_i$). However, this dependence is not helpful for the problem of selecting $\RV{D}_i$ based on previous data. For much of this chapter, we will focus on the simpler case where inputs are \emph{weakly data-independent}, which means that conditional on $\RV{H}$, the $\RV{Y}_i$ are also independent of future inputs. This assumption is relaxed in Section \ref{sec:data_dependent}.

We show that for weakly data-independent models with conditionally independent and identical response functions, there is some variable $\RV{W}$ such that the conditional probability $\prob{P}_C^{\RV{Y}|\RV{WD}}$ is causally contractible. On the other hand, for data-dependent models, we instead require the \emph{comb} (Section \ref{sec:def_combs}) $\prob{P}_C^{\RV{Y}\combbreak \RV{D}|\RV{W}}$ causally contractible for some $\RV{W}$.


\section[Causal contractibility]{Causal contractibility}\label{sec:ccontracibility}

In this section we define causal contractibility and prove that (under some side conditions) probability sets feature conditionally independent and identical response functions if and only if they are causally contractible.

The assumptions of exchange commutativity and \emph{locality} together make causal contractibility. Exchange commutativity expresses a sense in which a Markov kernel $\kernel{K}:X^{\mathbb{N}}\kto Y^{\mathbb{N}}$ can be symmetric: if it yields the same result from shuffling inputs as it does from shuffling outputs. Locality is the assumption that subsets of outputs are probabilistically independent of the non-corresponding inputs, conditioned on the corresponding inputs.

Graphical notation can offer an intuitive picture of these two assumptions. In the simplified case of a sequence of length 2 (that is, $\kernel{K}:X^2\kto Y^2$), exchange commutativity for two inputs and outputs is given by the following equality:
\begin{align}
    \tikzfig{commutativity_of_exchange}
\end{align}
swapping the inputs is equivalent to applying the same swap to the outputs. Locality is given by the following pair of equalities:
\begin{align}
    \tikzfig{cons_locality_1}\\
    \tikzfig{cons_locality_2}
\end{align}
and expresses the idea that the outputs are independent of the non-corresponding input, conditional on the corresponding input.

The definitions follow.

Call a model $\prob{P}_C$ with sequential outputs $\RV{Y}$ and a corresponding sequence of inputs $\RV{D}$ a ``sequential input-output model''.

\begin{definition}[Sequential input-output model]
A \emph{sequential input-output model} is a triple $(\prob{P}_C,\RV{D},\RV{Y})$ where $\prob{P}_C$ is a probability set on $(\Omega,\sigalg{F})$, $\RV{D}$ is a sequence of ``inputs'' $\RV{D}:=(\RV{D}_i)_{i\in\mathbb{N}}$ and $\RV{Y}$ is a corresponding sequence of ``outputs'' $\RV{Y}=(\RV{Y}_i)_{i\in\mathbb{N}}$ where $\RV{D}_i:\Omega\to D$ and $\RV{Y}_i:\Omega\to Y$.
\end{definition}

In the following three definitions we use the notation ``$\prob{P}_\cdot^{\RV{Y}|\RV{WD}}$ is causally contractible over $\RV{W}$''. This notation is intended to convey that causal contractibility is a property that holds for all $\prob{P}_\alpha^{\RV{Y}|\RV{WD}}$, even if the uniform conditional $\prob{P}_C^{\RV{Y}|\RV{WD}}$ doesn't exist.

\begin{definition}[Locality]\label{def:caus_cont}
Given a sequential input-output model $(\prob{P}_C,\RV{D},\RV{Y})$ along with some $\RV{W}:\Omega\to W$, we say $\prob{P}_\cdot^{\RV{Y}|\RV{WD}}$ is \emph{local} over $\RV{W}$ if for all $\alpha\in C$, $n\in \mathbb{N}$
\begin{align}
    \tikzfig{local_lhs} &= \tikzfig{local_rhs}\\
    &\iff\\
    \prob{P}_\alpha^{\RV{Y}|\RV{WD}}(\bigtimes_{i\in [n]} A_i\times Y^{\mathbb{N}}|w, x_{[n]},x_{[n]^C}) &= \prob{P}_C^{\RV{Y}_{[n]}|\RV{WD}_{[n]}}(\bigtimes_{i\in [n]} A_i|w,x_{[n]}) \\&  \forall A_i\in \sigalg{Y},(x_{[n]},x_{[n]^C})\in\mathbb{N},w\in W
\end{align}
That is, $\RV{Y}_i\CI^e_{\prob{P}_C} \RV{X}_{(i,\infty)}|(\RV{W},\RV{X}_i,\RV{C})$.
\end{definition}

\begin{definition}[Exchange commutativity]\label{def:caus_exch}
Given a sequential input-output model $(\prob{P}_C,\RV{D},\RV{Y})$ along with some $\RV{W}:\Omega\to W$, we say $\prob{P}_\cdot^{\RV{Y}|\RV{WD}}$ \emph{commutes with exchange} over $\RV{W}$ if for all finite permutations $\rho:\mathbb{N}\to\mathbb{N}$
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_\rho|\RV{WD}_\rho} &=  \prob{P}_\alpha^{\RV{Y}|\RV{WD}}
\end{align}
\end{definition}

Causal contractibility is the conjunction of both assumptions.
\begin{definition}[Causal contractibility]\label{def:ccontract}
Given a sequential input-output model $(\prob{P}_C,\RV{D},\RV{Y})$ along with some $\RV{W}:\Omega\to W$, $\prob{P}_\cdot^{\RV{Y}|\RV{WD}}$ is \emph{causally contractible} over $\RV{W}$ if it is local and commutes with exchange.
\end{definition}

\subsection[Causally contractible sequences]{Properties of causally contractible input-output models}

An input-output model $(\prob{P}_C,\RV{D},\RV{Y})$ with $\prob{P}_\cdot^{\RV{Y}|\RV{WD}}$ causally contractible over $\RV{W}$ can be ``contracted'' to some subsequence of $\RV{D}$ and $\RV{Y}$, and for any two subsequences $A,B\subset\mathbb{N}$, provided the subsequences are of equal length, the distribution of $\RV{Y}_A$ given $\RV{W}$ and $\RV{X}_A$ will be the same as the distribution of $\RV{Y}_B$ given $\RV{W}$ and $\RV{X}_B$ (Theorem \ref{th:equal_of_condits}). This feature is the motivation for the name \emph{causal contractibility}. 

Theorem \ref{th:no_implication} shows that exchange commutativity and locality are independent assumptions.

Before these theorems are proved, the following definition and Lemma will prove helpful.

All swaps can be written as a product of transpositions, so proving that a property holds for all finite transpositions is enough to show it holds for all finite swaps. It's useful to define a notation for transpositions.
\begin{definition}[Finite transposition]
Given two equally sized sequences $A=(a_i)_{i\in [n]}$, $B=(b_i)_{i\in [n]}$, ${A\leftrightarrow B}:\mathbb{N}\to \mathbb{N}$ is the permutation that sends the $i$th element of $A$ to the $i$th element of $B$ and vise versa. Note that $A\leftrightarrow B$ is its own inverse.
\end{definition}

Lemma \ref{lem:infinitely_extended_kernels} is used to extend finite sequences to infinite ones, and is used in a number of upcoming theorems.

\begin{lemma}[Infinitely extended kernels]\label{lem:infinitely_extended_kernels}
Given a collection of Markov kernels $\kernel{K}_i:W\times X^{\mathbb{N}}\kto Y^i$ for all $i\in \mathbb{N}$, if we have for every $j>i$
\begin{align}
    \kernel{K}_j(\text{id}_{Y^i}\otimes \text{del}_{Y^{j-i}}) &= \kernel{K}_i\otimes \text{del}_{X^{j-i}}\label{eq:marginalise_comb}
\end{align} 
then there is a unique Markov kernel $\kernel{K}:X^{\mathbb{N}}\kto Y^{\mathbb{N}}$ such that for all $i,j\in \mathbb{N}$,$j>i$
\begin{align}
    \kernel{K}(\text{id}_{Y^i}\otimes \text{del}_{Y^{\mathbb{N}}})&= \kernel{K}_i\otimes \text{del}_{X^{j-i}}
\end{align}
\end{lemma}

\begin{proof}
Take any $x\in X^{\mathbb{N}}$ and let $x_{|m}\in X^n$ be the first $n$ elements of $x$. By Equation \ref{eq:marginalise_comb}, for any $A_i\in \sigalg{Y}$, $i\in [m]$
\begin{align}
    \kernel{K}_n(\bigtimes_{i\in [m]}A_i\times Y^{n-m}|x_{|n}) &= \kernel{K}_m(\bigtimes_{i\in [m]}A_i|x_{|m})
\end{align}

Furthermore, by the definition of the $\mathrm{swap}$ map for any permutation $\rho:[n]\to[n]$
\begin{align}
    \kernel{K}_n\mathrm{swap}_{\rho}(\bigtimes_{i\in [m]}A_{\rho(i)}\times Y^{n-m}|x_{|n}) &= \kernel{K}_n(\bigtimes_{i\in [m]}A_{i}\times Y^{n-m}|x_{|n})
\end{align}
thus by the Kolmogorov Extension Theorem \citep{cinlar_probability_2011}, for each $x\in X^{\mathbb{N}}$ there is a unique probability measure $\prob{Q}_x\in \Delta(Y^{\mathbb{N}}$ satisfying
\begin{align}
    \prob{Q}_d(\bigtimes_{i\in [n]}A_i\times Y^{\mathbb{N}}) &= \kernel{K}_n(\bigtimes_{i\in [n]}A_{\rho(i)}|d_{|n})\label{eq:q_is_Markov}
\end{align}

Furthermore, for each $\{A_i\in\sigalg{Y}|i\in \mathbb{N}\}$, $n\in \mathbb{N}$ note that for $p>n$
\begin{align}
\prob{Q}_d(\bigtimes_{i\in[n]} A_i \times Y^{\mathbb{N}})&\geq \prob{Q}_d(\bigtimes_{i\in [p]} A_i\times Y^{\mathbb{N}})\\
&\geq \prob{Q}_d(\bigtimes_{i\in \mathbb{N}} A_i)
\end{align}
so by the Monotone convergence theorem, the sequence $\prob{Q}_d(\bigtimes_{i\in[n]} A_i)$ converges as $n\to \infty$ to $\prob{Q}_d(\bigtimes_{i\in\mathbb{N}} A_i)$. $d\mapsto \prob{Q}_d^{\RV{Z}_n}(\bigtimes_{i\in[n]} A_i)$ is measurable for all $n$, $\{A_i\in\sigalg{Y}|i\in \mathbb{N}\}$ by Equation \ref{eq:q_is_Markov}, and so $d\mapsto Q_d$ is also measurable.

Thus $d\mapsto Q_d$ is the desired $\prob{P}_C^{\RV{Y}_{\mathbb{N}}\combbreak \RV{D}_{\mathbb{N}}}:D^\mathbb{N}\kto Y^\mathbb{N}$.
\end{proof}

\begin{corollary}\label{cor:equal_subconditionals}
Given $(\prob{P}_C,\Omega,\sigalg{F})$, $\RV{V}:\Omega\to V$ and two pairs of sequences $(\RV{W},\RV{X}):=(\RV{W}_i,\RV{X}_i)_{i\in\mathbb{N}}$ and $(\RV{Y},\RV{Z}):=(\RV{Y}_i,\RV{Z}_i)_{i\in \mathbb{N}}$ with corresponding variables taking values in the same sets $W=Y$ and $X=Z$, if $(\prob{P}_C,\RV{W},\RV{X})$ and $(\prob{P}_C,\RV{Y},\RV{Z})$ are both local over $\RV{W}$ and
\begin{align}
    \prob{P}^{\RV{X}_{[n]}|\RV{V}\RV{W}_{[n]}} &= \prob{P}^{\RV{Z}_{[n]}|\RV{V}\RV{Y}_{[n]}}
\end{align}
for all $n\in\mathbb{N}$ then
\begin{align}
    \prob{P}^{\RV{X}|\RV{V}\RV{W}} &= \prob{P}^{\RV{Z}|\RV{V}\RV{Y}}
\end{align}
\end{corollary}

\begin{proof}
By assumption of locality
\begin{align}
    \prob{P}^{\RV{X}_{[n]}|\RV{V}\RV{W}_{[n]}}\otimes\mathrm{del}_{W^\mathbb{N}} &= \prob{P}^{\RV{X}|\RV{V}\RV{W}}(\mathrm{id}_{X^n}\otimes \mathrm{del}_{X^{\mathbb{N}}})\\
    \prob{P}^{\RV{Z}_{[n]}|\RV{V}\RV{Y}_{[n]}}\otimes\mathrm{del}_{W^\mathbb{N}} &= \prob{P}^{\RV{Z}|\RV{V}\RV{Y}}(\mathrm{id}_{X^n}\otimes \mathrm{del}_{X^{\mathbb{N}}})
\end{align}
hence for all $n,m>n$
\begin{align}
    \prob{P}^{\RV{X}_{[m]}|\RV{V}\RV{W}_{[m]}}(\mathrm{id}_{X^n}\otimes \mathrm{del}_{X^{m-n}}) &= \prob{P}^{\RV{Z}_{[m]}|\RV{V}\RV{Y}_{[m]}}(\mathrm{id}_{X^n}\otimes \mathrm{del}_{X^{m-n}})\\
    &= \prob{P}^{\RV{X}_{[n]}|\RV{V}\RV{W}_{[n]}}\otimes\mathrm{del}_{W^{m-n}}
\end{align}
and, in particular, by lemma \ref{lem:infinitely_extended_kernels}, $\prob{P}^{\RV{X}|\RV{V}\RV{W}}$ and $\prob{P}^{\RV{Z}|\RV{V}\RV{Y}}$ are the limits of the same sequence.
\end{proof}

\begin{lemma}[Alternative definition of exchange commutativity]
A sequential input-output model $(\prob{P}_C,\RV{D},\RV{Y})$ along with some $\RV{W}:\Omega\to W$ commutes with exchange over $\RV{W}$ if and only if for every $\alpha$, every finite permutation $\rho:\mathbb{N}\to\mathbb{N}$ and corresponding swap map $\mathrm{swap}_\rho:X^{\mathbb{N}}\kto X^{\mathbb{N}}$
\begin{align}
    \tikzfig{exch_com_lhs} &= \tikzfig{exch_com_rhs}
\end{align}
\end{lemma}

\begin{proof}
This follows from the fact that
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_\rho|\RV{W}\RV{D}_{\rho}} = \tikzfig{exch_com_rhs}
\end{align}
To see this, note that
\begin{align}
    &\tikzfig{exch_com_rhs}(\bigtimes_{i\in \mathbb{N}} A_i|w,(d_i)_\mathbb{N})\\
     &= \prob{P}_\alpha^{\RV{Y}|\RV{W}\RV{D}}(\bigtimes_{i\in \mathbb{N}}A_{\rho^{-1}(i)}|w,(d_{\rho^{-1}(i)})_\mathbb{N})\\
    &= \prob{P}_\alpha^{\RV{Y}_\rho|\RV{W}\RV{D}_\rho}(\bigtimes_{i\in \mathbb{N}}A_i|w,(d_i)_\mathbb{N})
\end{align}
\end{proof}

Theorem \ref{th:equal_of_condits} provides an alternative characterisation of causal contractibility in terms of the equality of the conditional distributions of sub-sequences.

\begin{theorem}[Equality of equally sized contractions]\label{th:equal_of_condits}
A sequential input-output model $(\prob{P}_C,\RV{D},\RV{Y})$ is causally contractible over $\RV{W}$ if and only if for and every subsequence $A=(A_i)_{i\in |A|}$ and $B=(B_i)_{i\in |A|}$ with $i\neq j\implies A_i\neq A_j$ and $B_i\neq B_j$ and every $\alpha$
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_A|\RV{WD}_{A\leftrightarrow [|A|]}} &= \prob{P}_\alpha^{\RV{Y}_B|\RV{WD}_{B\leftrightarrow [|A|]}}\\
    &= \prob{P}_\alpha^{\RV{Y}_A|\RV{WD}_A}\otimes \text{del}_{D^{|\mathbb{N}\setminus A}|}
\end{align}
where $[|A|]$ is the sequence $(1,2,...,|A|)$ for finite $A$ or $(1,2,...)$ for infinite $A$.
\end{theorem}

\begin{proof}
Only if:
If $A$ is finite, then by exchange commutativity
\begin{align}
        \prob{P}_\alpha^{\RV{Y}_A|\RV{WD_{A\leftrightarrow [n]}}} &= \prob{P}_\alpha^{\RV{Y}_{[n]}|\RV{WD}}\\
        &= \prob{P}_\alpha^{\RV{Y}_B|\RV{WD_{B\leftrightarrow [n]}}}
\end{align}
if $A$ is infinite, then we can take finite subsequences $A_m$ that are the first $m$ elements of $A$. Then
\begin{align}
            \prob{P}_\alpha^{\RV{Y}_{A_m}|\RV{WD_{A_m\leftrightarrow [m]}}} &= \prob{P}_\alpha^{\RV{Y}_{[m]}|\RV{WD}}\\
        &= \prob{P}_\alpha^{\RV{Y}_{B_m}|\RV{WD_{B_m\leftrightarrow [m]}}}
\end{align}
then by Corollary \ref{cor:equal_subconditionals}
\begin{align}
\prob{P}_\alpha^{\RV{Y}_A|\RV{WD_{A\leftrightarrow [n]}}}=\prob{P}_\alpha^{\RV{Y}_{B_m}|\RV{WD_{B_m\leftrightarrow [m]}}}
\end{align}

By locality
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_A|\RV{WD_{A\leftrightarrow [n]}}} &= \prob{P}_\alpha^{\RV{Y}_A|\RV{WD}_A}\otimes \text{del}_{D^{|\mathbb{N}\setminus A}}
\end{align}

If:
Taking $A=[n]$ for all $n$ establishes locality, and taking $A=(\rho(i))_{i\in \mathbb{N}}$ for arbitrary finite permutation $\rho$ establishes exchange commutativity.
\end{proof}

Theorem \ref{th:no_implication} shows that neither locality nor exchange commutativity is implied by the other.

\begin{theorem}\label{th:no_implication}
Exchange commutativity does not imply locality or vise versa.
\end{theorem}

\begin{proof}
First, a model that exhibits exchange commutativity but not locality. Suppose $D=Y=\{0,1\}$ and $\prob{P}_C^{\RV{Y}|\RV{D}}:D^{\mathbb{N}}\kto Y^{\mathbb{N}}$ is given by
\begin{align}
    \prob{P}_C^{\RV{Y}|\RV{D}}(\bigtimes_{i\in\mathbb{N}} A_i |(d_i)_{i\in\mathbb{N}}) &= \prod_{i\in \mathbb{N}} \delta_{\lim_{n\to\infty} \sum_{i}^{\mathbb{N}} \frac{d_i}{n}}(A_i)
\end{align}
then 
\begin{align}
    \prob{P}_C^{\RV{Y}_\rho|\RV{D}_\rho}(\bigtimes_{i\in\mathbb{N}} A_i |(d_i)_{i\in\mathbb{N}}) &= \prod_{i\in \mathbb{N}} \delta_{\lim_{n\to\infty} \sum_{i}^{\mathbb{N}} \frac{d_{\rho^{-1}(i)}}{n}}(A_{\rho^{-1}(i)})\\
    &= \prob{P}_C^{\RV{Y}|\RV{D}}(\bigtimes_{i\in\mathbb{N}} A_i |(d_i)_{i\in\mathbb{N}})
\end{align}
so $(\prob{P}_C,\RV{D},\RV{Y})$ commutes with exchange, but
\begin{align}
    \prob{P}_C^{\RV{Y}_1|\RV{D}}(\bigtimes_{i\in\mathbb{N}} A_i |0,1,1,1....) &= \delta_1(A_i)\\
    \prob{P}_C^{\RV{Y}_1|\RV{D}}(\bigtimes_{i\in\mathbb{N}} A_i |0,0,0,0....) &= \delta_0(A_i)
\end{align}
so $(\prob{P}_C,\RV{D},\RV{Y})$ is not local.

Next, a Markov kernel that satisfies locality but does not commute with exchange. Suppose again $D=Y=\{0,1\}$ and $\prob{P}_C^{\RV{Y}|\RV{D}}:D^{\mathbb{N}}\kto Y^{\mathbb{N}}$ is given by
\begin{align}
    \prob{P}_C^{\RV{Y}|\RV{D}}(\bigtimes_{i\in\mathbb{N}} A_i |(d_i)_{i\in\mathbb{N}}) &= \prod_{i\in \mathbb{N}} \delta_i(A_i)
\end{align}
then
\begin{align}
    \prob{P}_C^{\RV{Y}_\rho|\RV{D}_\rho}(\bigtimes_{i\in\mathbb{N}} A_i |(d_i)_{i\in\mathbb{N}}) &= \prod_{i\in \mathbb{N}} \delta_i(A_{\rho^{-1}(i)})\\
    &\neq \prod_{i\in \mathbb{N}} \delta_i(A_{i})\\
    =\prob{P}_C^{\RV{Y}|\RV{D}}(\bigtimes_{i\in\mathbb{N}} A_i |(d_i)_{i\in\mathbb{N}})
\end{align}
so $(\prob{P}_C,\RV{D},\RV{Y})$ does not commute with exchange but for all $n$
\begin{align}
    \prob{P}_C^{\RV{Y}_{[n]}|\RV{D}}(\bigtimes_{i\in[n]} A_i |(d_i)_{i\in\mathbb{N}}) &= \prod_{i\in [n]} \delta_i(A_{\rho^{-1}(i)})\\
    &= \prob{P}_C^{\RV{Y}_{[n]}|\RV{D}}(\bigtimes_{i\in[n]} A_i |(0)_{i\in\mathbb{N}})
\end{align}
so $(\prob{P}_C,\RV{D},\RV{Y})$ is local.
\end{proof}

Theorem \ref{th:no_implication} presents abstract counterexamples to show that the assumptions of exchange commutativity and locality are independent. For some more practical examples, a model of the treatment of several patients who are known to have different illnesses might satisfy consequence locality but not exchange commutativity. Patient B's treatment can be assumed not to affect patient A, but the same results would not be expected from giving patient A's treatment to patient B as from giving patient A's treatment to patient A.

A model of strategic behaviour might satisfy exchange commutativity but not locality. Suppose a decision maker is observing people playing a game where they press a red or green button, and (for reasons mysterious to the decision maker), receive a payout randomly of 0 or \$100. The decision maker might reason that the results should be the same no matter who presses a button, but also that people will be more likely to press the red button if the red button tends to give a higher payout. In this case, the decision maker's prediction for the payout of the $i$th attempt given the red button has been pressed will be higher if the proportion of red button presses in the entire dataset is higher. There are other reasons why exchange commutativity might hold but not locality -- \citet{dawid_causal_2000} offers the alternative example of herd immunity in vaccination campaigns. In this case, the overall proportion of the population vaccinated will affect the disease prevalence over and above an individual's vaccination status.

An aside: although locality could be described as an assumption that there is no interference between inputs and outputs of different indices, it actually allows for some models with certain kinds of interference between inputs and non-corresponding. For example: consider an experiment where I first flip a coin and record the results of this flip as the outcome $\RV{Y}_1$ of ``step 1''. Subsequently, I can either copy the outcome from step 1 to the result for ``step 2'' (this is the input $\RV{D}_1=0$), or flip a second coin use this as the input for step 2 (this is the input $\RV{D}_1=1$). $\RV{D}_2$ is an arbitrary single-valued variable. Then for all $d_1, d_2$
\begin{align}
    \prob{P}^{\RV{Y}_1|\RV{D}}(y_1|d_1,d_2) &= 0.5\\
    \prob{P}^{\RV{Y}_2|\RV{D}}(y_2|d_1,d_2) &= 0.5
\end{align}
Thus the marginal distribution of both experiments in isolation is $\text{Bernoulli}(0.5)$ no matter what choices I make, but the input $\RV{D}_1$ affects the joint distribution of the results of both steps, which is not ruled out by locality.

\subsection[Representation theorem]{A representation theorem for causally contractible input-output models}\label{sec:rep_theorem}

Theorem \ref{th:table_rep_kernel} shows that a causally contractible conditional distribution can be represented as the product of a column exchangeable probability distribution and a ``lookup function'' or ``switch''. This lookup function is also used in the representation of potential outcomes models (see, for example, \citet{rubin_causal_2005}), but in this case the probability distribution is not a distribution of potential outcomes. This theorem allows De Finetti's theorem to be applied to the column exchangeable probability distribution, which is a key step in proving the main result (Theorem \ref{th:ciid_rep_kernel}).

We reuse a number of concepts in the following work: models with sequences of inputs and outputs, ``tabulated'' representations of conditional probabilities and ``hypothesis'' or ``directing measures'' defined as the limit of relative frequencies.

\begin{definition}[Count-of-inputs variable]
Given a sequential input-output model $(\prob{P}_C,\RV{D},\RV{Y})$ on $(\Omega,\sigalg{F})$ with countable $D$, $\#_{\RV{D}_k=j}^i$ is the variable
\begin{align}
    \#_{\RV{D}_k=j}^i := \llbracket \sum_{i=1}^{k-1} \llbracket \RV{D}_i = j \rrbracket = i-1\rrbracket\llbracket \RV{D}_k = j \rrbracket
\end{align}
In particular, $\#_{\RV{D}_k=j}^i(\omega)=1$ is equal to $1$ if $\RV{D}_k(\omega)=j$ and there are exactly $i-1$ other indices $l<k$ such that $\RV{D}_l(\omega)=j$, and 0 otherwise.
\end{definition}

\begin{definition}[Tabulated conditional distribution]\label{def:tab_cd}
Given a sequential input-output model $(\prob{P}_C,\RV{D},\RV{Y})$ on $(\Omega,\sigalg{F})$, define the tabulated conditional distribution $\RV{Y}^D:\Omega\to Y^{\mathbb{N}\times D}$ by
\begin{align}
    \RV{Y}^D_{ij} = \sum_{k=1}^{\infty} \#_{\RV{D}_k=j}^i \RV{Y}_k
\end{align}
That is, the $(i,j)$-th coordinate of $\RV{Y}^D(\omega)$ is equal to the coordinate $\RV{Y}_k(\omega)$ for which the corresponding $\RV{D}_k(\omega)$ is the $i$th instance of the value $j$ in the sequence $(\RV{D}_1(\omega),\RV{D}_2(\omega),...)$, or 0 if there are fewer than $i$ instances of $j$ in this sequence.
\end{definition}

\begin{definition}[Measurable set of probability distributions]
Given a measurable set $(\Omega,\sigalg{F})$, the measurable set of distributions on $\Omega$, $\mathcal{M}_1(\Omega)$, is the set of all probability distributions on $\Omega$ equipped with the coarsest $\sigma$-algebra such that the evaluation maps $\eta_B:\nu\mapsto \nu(B)$ are measurable for all $B\in \sigalg{F}$.
\end{definition}

Note that we are only interested in the variable $\RV{H}$ when the limit exists almost surely, the part of the definition for where the limit does not exists is for completeness only.

\begin{definition}[Directing random measure]\label{def:dir_rand_meas}
Given a sequential input-output model $(\prob{P}_C,\RV{D},\RV{Y})$, we will say ``the'' directing random measure $\RV{H}:\Omega\to \mathcal{M}_1(Y^D)$ is the function
\begin{align}
    \RV{H} := \bigtimes_{j\in D} A_j \mapsto \begin{cases}
    \lim_{n\to \infty}\frac{1}{n} \sum_{i=1}^{\infty} \prod_{j\in D} \mathds{1}_{A_j}(\RV{Y}^D_{ij}) & \text{this limit exists}\\
    \llbracket \bigtimes_{j\in D} A_j = Y^D \rrbracket &\text{otherwise}
    \end{cases} 
\end{align}
See Definition \ref{def:tab_cd} for the definition of $\RV{Y}^D$.
\end{definition}

Weak data-independence is where outputs are independent of future inputs conditional on past inputs and the directing measure $\RV{H}$. Strong data independence is where this independence holds additionally without conditioning on $\RV{H}$.

\begin{definition}[Weakly data-independent]\label{def:weak_di}
A sequential input-output model $(\prob{P}_C,\RV{D},\RV{Y})$ is weakly data-independent if $\RV{Y}_i\CI^e_{\prob{P}_C} \RV{D}_{(i,\infty]}|(\RV{H},\RV{D}_{[1,i]},\RV{C})$.
\end{definition}

% \begin{definition}[Strongly data-independent]\label{def:strong_di}
% A sequential input-output model $(\prob{P}_C,\RV{D},\RV{Y})$ is strongly data-independent if it is weakly data-independent and $\RV{Y}_i\CI^e_{\prob{P}_C} \RV{D}_{(i,\infty]}|(\RV{D}_{[1,i]},\RV{C})$.
% \end{definition}

\begin{definition}[Permutation within columns]
Given a sequence of indices $(i,j)_{i\in \mathbb{N},j\in D}$ a finite permutation within the columns is a function $\eta:\mathbb{N}\times D\to \mathbb{N}\times D$ such that for each $j\in D$, $\eta_j:=\rho(\cdot,j)$ is a finite permutation $\mathbb{N}\to \mathbb{N}$.
\end{definition}

There is an assumption in the following theorem that the set of input sequences in which each value appears infinitely often has measure 1, which is needed in the main Theorem \ref{th:ciid_rep_kernel} to guarantee that the hypothesis $\RV{H}$ is a function of the observable variables.

\begin{theorem}\label{th:table_rep_kernel}
Suppose a sequential input-output model $(\prob{P}_C,\RV{D},\RV{Y})$ is given with $D$ countable and, letting $E\subset D^{\mathbb{N}}$ be the set of all sequences for which each $j\in D$ occurs infinitely often, $\prob{P}_\alpha^{\RV{D}}(E)=1$ for all $\alpha$. Then for some $\RV{W}$, $\prob{P}_\cdot^{\RV{Y}|\RV{WD}}$ is causally contractible if and only if for all $\alpha$
\begin{align}
    \prob{P}_\alpha^{\RV{Y}|\RV{WD}} &= \tikzfig{lookup_representation_kernel}\label{eq:lup_rep_kernel}\\
    &\iff\\
    \prob{P}_\alpha^{\RV{Y}|\RV{WD}}(\bigtimes_{i\in \mathbb{N}}A_i|w,(d_i)_{i\in \mathbb{N}}) &= \prob{P}_\alpha^{(\RV{Y}^D_{i d_i})_{i\in\mathbb{N}}|\RV{W}}(\bigtimes_{i\in \mathbb{N}}A_i|w)&\forall A_i\in \sigalg{Y}^{\mathbb{N}}, w\in W, d_i\in D
\end{align}
Where $\prob{F}_{\text{lu}}$ is the Markov kernel associated with the lookup map
\begin{align}
    \text{lu}:X^\mathbb{N}\times Y^{\mathbb{N}\times D}&\to Y\\
    ((x_i)_\mathbb{N},(y_{ij})_{i,j\in \mathbb{N}\times D})&\mapsto (y_{i d_i})_{i\in \mathbb{N}}
\end{align}
and for any finite permutation within columns $\eta:\mathbb{N}\times D\to \mathbb{N}\times D$
\begin{align}
    \prob{P}_\alpha^{(\RV{Y}^D_{ij})_{\mathbb{N}\times D}|\RV{W}}&= \prob{P}_\alpha^{(\RV{Y}^D_{\eta_j(i)j})_{\mathbb{N}\times D}|\RV{W}}\label{eq:col_exch}
\end{align}
\end{theorem}

\begin{proof}
Only if:
Note that $\#_{\RV{D}_k=j}^i$ and $\#_{\RV{D}_l=j}^i$ are mutually exclusive for $k\neq l$ and, by assumption, $\sum_{j\in D}\sum_{i\in \mathbb{N}} \#_{\RV{D}_k=j}^i=1$ almost surely (that is, there is some $i,j$ such that $\RV{D}_k$ is the $i$th occurrence of $j$). Define $\RV{R}_k:\omega \mapsto \argmax_{i\in\mathbb{N},j\in D} \#_{\RV{D}_k=j}^i(\omega)$ and $\RV{R}:k\mapsto \RV{R}_k$. $\RV{R}$ is almost surely bijective and 
\begin{align}
    \RV{Y}^D&:= (\RV{Y}^D_{ij})_{i\in \mathbb{N},j\in D}\\
    &= (\RV{Y}_{\RV{R}^{-1}(i,j)})_{i\in \mathbb{N},j\in D}\\
    &=: \RV{Y}_{\RV{R}^{-1}|\RV{W}}
\end{align}

By construction, $\RV{D}_{\RV{R}^{-1}(i,j)}=j$ almost surely; that is, $\RV{D}_{\RV{R}^{-1}}$ is a single-valued variable. In particular, it is almost surely equal to $e:=(e_{ij})_{i\in\mathbb{N},j\in D}$ such that $e_{ij}=j$ for all $i$. Hence
\begin{align}
    \prob{P}_\alpha^{\RV{Y}^D|\RV{W}\RV{D}_{\RV{R}^{-1}}}(A|w,d)&= \prob{P}_\alpha^{\RV{Y}_{\RV{R}^{-1}}|\RV{W}\RV{D}_{\RV{R}^{-1}}}(A|w,d)\\
    &\overset{\prob{P}_C}{\cong} \prob{P}_\alpha^{\RV{Y}_{\RV{R}^{-1}}|\RV{W}\RV{D}_{\RV{R}^{-1}}}(A|e,w)\label{eq:yd_is_indep}\\
    &= \prob{P}_\alpha^{\RV{Y}^D}(A|w)\label{eq:yd_dist}
\end{align}
for any $d\in D^{\mathbb{N}}$. Equation \ref{eq:yd_is_indep} implies $\RV{Y}^D\CI \RV{D}|(\RV{W},\RV{C})$.

Now,
\begin{align}
    \prob{P}^{\RV{Y}_{\RV{R}^{-1}}|\RV{W}\RV{D}_{\RV{R}^{-1}}}_\alpha(A|w,d) &= \int_R \prob{P}_\alpha^{\RV{Y}_\rho|\RV{W}\RV{D}_{\rho}}(A|d)\prob{P}_\alpha^{\RV{R}^{-1}|\RV{W}\RV{D}_{\RV{R}^{-1}}}(\mathrm{d}\rho|w,d)\label{eq:need_ccont}\\
\end{align}
For each $\rho$, define $\rho^n:\mathbb{N}\to \mathbb{N}$ as the finite permutation that agrees with $\rho$ on the first $n$ indices and is the identity otherwise. By causal contractibility, for $n\in \mathbb{N}$
\begin{align}
    \prob{P}^{\RV{Y}_{\rho^n([n])}|\RV{D}_{\rho^n([n])}} &= \prob{P}^{\RV{Y}_{\rho([n])}|\RV{D}_{\rho([n])}}\\
    &= \prob{P}^{\RV{Y}_{[n]}|\RV{D}_{[n]}}
\end{align}
By Corollary \ref{cor:equal_subconditionals}, it must therefore be the case that
\begin{align}
    \prob{P}^{\RV{Y}|\RV{D}} = \prob{P}^{\RV{Y}_{\rho}|\RV{D}_{\rho}}
\end{align}
Then from Equation \ref{eq:need_ccont}
\begin{align}
    \prob{P}^{\RV{Y}_{\RV{R}^{-1}}|\RV{W}\RV{D}_{\RV{R}^{-1}}}_\alpha(A|w,d) &\overset{\prob{P}_C}{\cong} \int_R \prob{P}_\alpha^{\RV{Y}_\rho|\RV{W}\RV{D}_{\rho}}(A|d)\prob{P}_\alpha^{\RV{R}^{-1}|\RV{W}\RV{D}_{\RV{R}^{-1}}}(\mathrm{d}\rho|w,d)\\
    &\overset{\prob{P}_C}{\cong} \int_R \prob{P}_C^{\RV{Y}|\RV{WD}}(A|w,d)\prob{P}_\alpha^{\RV{R}^{-1}|\RV{W}\RV{D}_{\RV{R}^{-1}}}(\mathrm{d}\rho|w,d)\\
    &\overset{\prob{P}_C}{\cong} \prob{P}_C^{\RV{Y}|\RV{WD}}(A|w,d)\label{eq:rotated_conditional}
\end{align}
 for all $i,j\in \mathbb{N}$. Then by Equation \ref{eq:yd_dist} and Equation \ref{eq:rotated_conditional}
\begin{align}
    \prob{P}_\alpha^{\RV{Y}^D|\RV{W}}(A|w) &= \prob{P}_\alpha^{\RV{Y}|\RV{WD}}(A|w,e)\label{eq:rel_bet_y_yd}
\end{align}

Take some $d\in D^{\mathbb{N}}$. From Equation \ref{eq:rel_bet_y_yd} and causal contractibility of $\prob{P}_C^{\RV{Y}|\RV{WD}}(A|e)$,
\begin{align}
    (\prob{P}_\alpha^{\RV{Y}^D|\RV{W}}\otimes \mathrm{id}_D)\kernel{F}_{lu}(A|w,d) &= \prob{P}_\alpha^{(\RV{Y}^D_{i d_i})_{\mathbb{N}}|\RV{W}}(A|d)\\
    &=\prob{P}_\alpha^{(\RV{Y}_{i d_i})_{\mathbb{N}}|\RV{WD}}(A|w,e)\\
    &= \prob{P}_\alpha^{(\RV{Y}_{i d_i})_{\mathbb{N}}|\RV{W}(\RV{D}_{i d_i})_{\mathbb{N}})}(A|w,(e_{i d_i})_{i\in \mathbb{N}})\\
    &= \prob{P}_\alpha^{\RV{Y}|\RV{WD}}(A|w,(e_{i d_i})_{i\in \mathbb{N}})\\
    &= \prob{P}_\alpha^{\RV{Y}|\RV{WD}}(A|w,(d_i)_{i\in\mathbb{N}})
\end{align}

Furthermore, for some finite permutation within columns $\eta:\mathbb{N}\times D\to \mathbb{N}\times D$, note that $\eta$ is a bijective function on $\mathbb{N}\times D$ that leaves all but a finite number of pairs unchanged. Thus, given some bijection $f:\mathbb{N}\to \mathbb{N}\times D$, $\rho:=\eta\circ f$ is a finite permutation on $\mathbb{N}$. From Equation \ref{eq:rel_bet_y_yd},
\begin{align}
    \prob{P}_\alpha^{(\RV{Y}^D_{\eta_j(i)j})_{\mathbb{N}\times D}|\RV{W}}(A|w) &= \prob{P}_\alpha^{\RV{Y}_\rho|\RV{WD}}(A|w,e)\\
    &= \prob{P}_\alpha^{\RV{Y}|\RV{WD}_{\rho^{-1}}}(A|w,e)&\text{by exchange commutativity}\\
    &= \prob{P}_\alpha^{\RV{Y}|\RV{WD}}(A|w,(e_{\rho^{-1}(i) d_i})_{i\in \mathbb{N}})\\
    &= \prob{P}_\alpha^{\RV{Y}|\RV{WD}}(A|w,e)\\
    &= \prob{P}_\alpha^{(\RV{Y}^D_{ij})_{\mathbb{N}\times D}|\RV{W}}(A|w)
\end{align}

If:
Suppose 
\begin{align}
    \prob{P}_\alpha^{\RV{Y}|\RV{WD}} &= \tikzfig{lookup_representation_kernel}
\end{align}
where $\prob{P}_\alpha^{\RV{Y}^D|\RV{W}}$ satisfies Equation \ref{eq:col_exch}.

Consider any two $d,d'\in D^{\mathbb{N}}$ such that for some $S,T\subset\mathbb{N}$ with $|S|=|T|=n$, $d_S=d'_T$. Let $S\leftrightarrow T$ be the transposition that swaps the $i$th element of $S$ with the $i$th element of $T$ for all $i$.
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_S|\RV{WD}}(\bigtimes_{i\in [n]} A_i|w,d) &= \prob{P}_\alpha^{(\RV{Y}^D_{i d_i})_{i\in S}|\RV{W}} (\bigtimes_{i\in [n]} A_i|w)\\
    &= \prob{P}_\alpha^{(\RV{Y}^D_{S\leftrightarrow T(i) d_i})_{i\in S}|\RV{W}} (\bigtimes_{i\in [n]} A_i|w)\\
    &= \prob{P}_\alpha^{(\RV{Y}^D_{i d_{S\leftrightarrow T(i)}})_{i\in T}|\RV{W}} (\bigtimes_{i\in [n]} A_i|w)\\
    &= \prob{P}_\alpha^{(\RV{Y}^D_{i d'_{i}})_{i\in T}|\RV{W}} (\bigtimes_{i\in [n]} A_i|w)\\
    &=  \prob{P}_\alpha^{\RV{Y}_T|\RV{WD}}(\bigtimes_{i\in [n]} A_i|w,d')
\end{align}
and, in particular, taking $T=[n]$
\begin{align}
    &= \prob{P}_\alpha^{\RV{Y}_{[n]}|\RV{WD}} (\bigtimes_{i\in [n]} A_i|w,d')
\end{align}
but $d'$ is an arbitrary sequence such that the $T$ elements match the $S$ elements of $d$, so this holds for any other $d''$ whose $T$ elements also match the $S$ elements of $d$. That is
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_S|\RV{WD}}(\bigtimes_{i\in [n]} A_i|w,d)&= (\prob{P}_\alpha^{\RV{Y}_{[n]}|\RV{WD}_{[n]}}\otimes \mathrm{del}_{D^{\mathbb{N}}}) (\bigtimes_{i\in [n]} A_i|w,d')
\end{align}
so $\kernel{K}$ is causally contractible by Theorem \ref{th:equal_of_condits}.
\end{proof}

\begin{lemma}
Suppose a sequential input-output model $(\prob{P}_C,\RV{D},\RV{Y})$ is such that $\prob{P}_\alpha^{\RV{Y}|\RV{WD}}$ is causally contractible over some $\RV{W}$, $\RV{Y}\CI^e_{\prob{P}_C} \RV{W}|(\RV{H},\RV{D},\RV{C})$ and for all $\alpha$
\begin{align}
    \prob{P}_\alpha^{\RV{Y}|\RV{WD}} &= \tikzfig{lookup_representation_kernel}\label{eq:lup_rep_kernel_2}
\end{align}
then
\begin{align}
    \prob{P}_{\alpha}^{\RV{Y}|\RV{HD}} &= \tikzfig{lookup_representation_kernel_h}
\end{align}
\end{lemma}

\begin{proof}
$\RV{Y}^D$ is a function of $\RV{Y}$ and $\RV{D}$ (see Definition \ref{def:tab_cd}) and $\RV{H}$ is a function of $\RV{Y}^D$. Say $f:Y\times D\to H$ is such that $\RV{H}=f(\RV{Y},\RV{D})$ (see Definition \ref{def:dir_rand_meas}). Then
\begin{align}
    \prob{P}_\alpha^{\RV{YH}|\RV{WD}} &= \tikzfig{lookup_representation_kernel_joint}\label{eq:luprep_joint}
\end{align}
For $d\in D^{\mathbb{N}}$, take $[d=j]_i$ to be the $i$th coordinate of $d$ equal to $j\in D$ and $\#_{[d=j]_i}$ to be the position in $d$ of $[d=j]_i$. Concretely, $f$ is given by
\begin{align}
    f(y,d) &= \bigtimes_{j\in D} A_j \mapsto \lim_{n\to \infty} \frac{1}{n}\sum_{i=1}^n \prod_{j\in D} \mathds{1}_{A_j}(y_{\#_{[d=j]_i}})
\end{align}
almost surely. Note that for $y^D\in Y^{D\times\mathbb{N}}$ we have
\begin{align}
    f\circ \mathrm{lu}(y^D,d) &= \bigtimes_{j\in D} A_j \mapsto \lim_{n\to \infty} \frac{1}{n}\sum_{i=1}^n \prod_{j\in D} \mathds{1}_{A_j}(y^D_{\#_{[d=j]_i} j})
\end{align}
Let $\RV{G}:=f\circ \mathrm{lu}(\RV{Y}^D,d)$ for some $d\in D^{\mathbb{N}}$.

Define $g:Y^{D\times\mathbb{N}}\to H$ by
\begin{align}
    g(y^D):= \bigtimes_{j\in D} A_j \mapsto \lim_{n\to \infty} \frac{1}{n}\sum_{i=1}^n \prod_{j\in D} \mathds{1}_{A_j}(y^D_{ij})
\end{align}
and note that $g(\RV{Y}^D)=\RV{H}$. For some $A\in \sigalg{Y}^D$ let $\RV{G}_A:=\RV{G}(A)$ and $\RV{H}_A:=\RV{H}(A)$.Consider 
\begin{align}
    \prob{P}_\alpha (\RV{G}_A\yields \RV{H}_A) &= \int_H \prob{P}_\alpha^{\RV{G}_A|\RV{H}} (\nu(A)| \nu)\prob{P}_\alpha^{\RV{H}}(\mathrm{d}\nu)\\
    &= \int_H \prob{P}_\alpha^{\RV{Y}^D|\RV{H}} (\lim_{n\to\infty}\frac{1}{n}\sum_{i=1}^n \prod_{j\in D} \mathds{1}_{A_j}(y_{\#_{[d=j]_i}})\yields \nu(A)| \nu)\prob{P}_\alpha^{\RV{H}}(\mathrm{d}\nu)
\end{align}
but by causal contractibility of $\prob{P}_\alpha^{\RV{Y}|\RV{WD}}$, conditional on $\RV{H}$ the $\RV{Y}^D_{ij}$ are mutually independent and $\RV{Y}^D_{ij}$ is identically distributed to $\RV{Y}^D_{lm}$ if $j=m$. Thus, by the law of large numbers, 
\begin{align}
    \prob{P}_\alpha^{\RV{Y}^D|\RV{H}} (\lim_{n\to\infty}\frac{1}{n}\sum_{i=1}^n \prod_{j\in D} \mathds{1}_{A_j}(y_{\#_{[d=j]_i}})\yields \nu(A)| \nu)= 1
\end{align}
In particular, by Eq. \ref{eq:luprep_joint}
\begin{align}
    \prob{P}_\alpha^{\RV{YH}|\RV{WD}}&=\tikzfig{lookup_representation_kernel_joint}\\
    &= \tikzfig{lookup_representation_kernel_joint_2}
\end{align}
noting that $\kernel{F}_g\otimes\mathrm{del}_W = \prob{P}_\alpha^{\RV{H}|\RV{Y}^D\RV{W}}$ (as $\RV{H}$ is by definition a function of $\RV{Y}^D$)
\begin{align}
    \prob{P}_\alpha^{\RV{YH}|\RV{WD}} &= \tikzfig{lookup_representation_kernel_joint_3}
\end{align}
and by the fact that $\RV{Y}\CI^e_{\prob{P}_C} \RV{W}|(\RV{H},\RV{D},\RV{C})$ and $\RV{Y}^D$ is a function of $\RV{Y}$ and $\RV{D}$, we have $(\RV{Y}^D,\RV{Y})\CI^e_{\prob{P}_C} \RV{W}|(\RV{H},\RV{C})$ by contraction. Then by decomposition, $\RV{Y}^D\CI^e_{\prob{P}_C} \RV{W}|(\RV{H},\RV{C})$ also, and so
\begin{align}
    \prob{P}_\alpha^{\RV{YH}|\RV{WD}} &= \tikzfig{lookup_representation_kernel_joint_4}
\end{align}
and so by higher order conditionals,
\begin{align}
    \prob{P}_{\alpha}^{\RV{Y}|\RV{HD}} &= \tikzfig{lookup_representation_kernel_h}
\end{align}
\end{proof}

Theorem \ref{th:ciid_rep_kernel} is the main result of this section. It shows that sequential input-output model $(\prob{P}_C,\RV{D},\RV{Y})$ is causally contractible over some $\RV{W}$ if and only if there is some hypothesis $\RV{H}$ such that the $\RV{Y}_i$s are related to the $\RV{D}_i$s by conditionally independent and identical response functions (subject to a support assumption).

Note that property (2) is equivalent to the conjunction of conditionally independent and identical response functions (Def \ref{def:cii_rf}) and weak data-independence (Def \ref{def:weak_di}).

\begin{theorem}[Representation of causally contractible models]\label{th:ciid_rep_kernel}
Suppose a sequential input-output model $(\prob{P}_C,\RV{D},\RV{Y})$ with sample space $(\Omega,\sigalg{F})$ is given with $D$ countable and, letting $E\subset D^{\mathbb{N}}$ be the set of all sequences for which each $j\in D$ occurs infinitely often, $\prob{P}_\alpha^{\RV{D}}(E)=1$ for all $\alpha$. Then the following are equivalent:
\begin{enumerate}
    \item There is some $\RV{W}$ such that $\prob{P}_\cdot^{\RV{Y}|\RV{WD}}$ is causally contractible and $\RV{Y}\CI^e_{\prob{P}_C} \RV{W} | (\RV{H},\RV{D},\RV{C})$
    \item For all $i$, $\RV{Y}_i\CI^e_{\prob{P}_C} (\RV{Y}_{\neq i},\RV{D}_{\neq i},\RV{C})|(\RV{H},\RV{D}_i)$ and for all $i,j$ $$\prob{P}_C^{\RV{Y}_i|\RV{H}\RV{D}_i}=\prob{P}_C^{\RV{Y}_j|\RV{H}\RV{D}_j}$$
    \item There is some $\kernel{L}:H\times X\kto Y$ such that for all $\alpha$, $$\prob{P}_\alpha^{\RV{Y}|\RV{HD}}= \tikzfig{do_model_representation_conditional}$$
\end{enumerate}
\end{theorem}

\begin{proof}
As a preliminary, we will show
\begin{align}
    \kernel{F}_{\mathrm{lu}} &= \tikzfig{lookup_rep_intermediate_kernel}\label{eq:ev_alternate_rep}
\end{align}
where  $\mathrm{lus}:D\times Y^D\to Y$ is the single-shot lookup function
\begin{align}
    ((y_i)_{i\in D},d)\mapsto y_d
\end{align}

Recall that $\mathrm{lu}$ is the function
\begin{align}
    ((d_i)_\mathbb{N},(y_{ij})_{i,j\in \mathbb{N}\times D})&\mapsto (y_{i d_i})_{i\in \mathbb{N}}
\end{align}
By definition, for any $\{A_i\in\sigalg{Y}|i\in \mathbb{N}\}$
\begin{align}
    \kernel{F}_{\mathrm{lu}}(\bigtimes_{i\in \mathbb{N}}A_i|(d_i)_\mathbb{N},(y_{ij})_{i\in \mathbb{N}\times D}) &= \delta_{(y_{i d_i})_{i\in \mathbb{N}}}(\bigtimes_{i\in \mathbb{N}}A_i)\\
        &= \prod_{i\in \mathbb{N}} \delta_{y_{i d_i}} (A_i)\\
        &= \prod_{i\in \mathbb{N}} \kernel{F}_{\text{evs}} (A_i|d_i,(y_{ij})_{j\in D})\\
        &= \left(\bigotimes_{i\in\mathbb{N}} \kernel{F}_{\mathrm{evs}} \right)(\bigtimes_{i\in \mathbb{N}}A_i|(d_i)_\mathbb{N},(y_{ij})_{i,j\in \mathbb{N}\times D})
\end{align}
which is what we wanted to show.

(1)$\implies$(3):
Let $H:=\sigalg{M}_1(Y^D)$ and define $\kernel{M}:H\kto Y^X$ by $\kernel{M}(A|h)=h(A)$ for all $A\in\sigalg{Y}^X$, $h\in H$. Define $\RV{Y}^D:\Omega\to Y^{\mathbb{N}\times D}$ as in Theorem \ref{th:table_rep_kernel}. Fix $w\in W$ and let $\prob{P}_{\alpha,w}^{\RV{Y}^D}:=\prob{P}_\alpha^{\RV{Y}^D|\RV{W}}(\cdot|w)$. By the column exchangeability of $\prob{P}_{\alpha,w}^{\RV{Y}^D}$, from \citet[Prop. 1.4]{kallenberg_basic_2005} there is a directing random measure $\RV{H}:Y^{X\times\mathbb{N}}\to H$ such that
\begin{align}
    \prob{P}_{\alpha,w}^{\RV{Y}^D|\RV{H}} &= \tikzfig{de_finetti_representation_conditional}
\end{align}
as the right hand side does not depend on $w$
\begin{align}
    \prob{P}_{\alpha}^{\RV{Y}^D|\RV{WH}} &= \tikzfig{de_finetti_representation_conditional_w}\label{eq:df_rep_mu}
\end{align}

By Theorem \ref{th:table_rep_kernel}, for each $w\in W$
\begin{align}
    \prob{P}_{\alpha}^{\RV{Y}|\RV{WD}} &= \tikzfig{lookup_representation_kernel}
\end{align}
we can also show
\begin{align}
    \prob{P}_{\alpha}^{\RV{Y}|\RV{HD}} &= \tikzfig{lookup_representation_kernel_h}\label{eq:lu_rep_h}
\end{align}

By the assumption $\RV{Y}\CI^e_{\prob{P}_C} \RV{W} | (\RV{H},\RV{D},\RV{C})$, we can substitute Equations \ref{eq:df_rep_mu} and \ref{eq:ev_alternate_rep} into \ref{eq:lu_rep_h} for
\begin{align}
    \prob{P}_{\alpha}^{\RV{Y}|\RV{HD}} &= \tikzfig{do_model_representation_conditional}
\end{align}

(3)$\implies$ (2):
If
\begin{align}
    \prob{P}_{\alpha}^{\RV{Y}|\RV{HD}} &= \tikzfig{do_model_representation_conditional}
\end{align}
then by the definition of higher order conditionals, for any $i\in \mathbb{N}$ and any $\alpha\in C$
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_i|\RV{HD}_i\RV{Y}_{\neq i}\RV{D}_{\neq i}} &\overset{\prob{P}_C}{\cong} \kernel{L}\otimes \text{del}_{Y^{\mathbb{N}}\times X^{\mathbb{N}}}
\end{align}
hence $\RV{Y}_i\CI^e_{\prob{P}_C} (\RV{Y}_{\neq i},\RV{D}_{\neq i},\RV{C})|(\RV{H},\RV{D}_i)$

(2)$\implies$ (1):
Take $\RV{W}:=\RV{H}$, which implies $\RV{Y}\CI^e_{\prob{P}_C} \RV{W} | (\RV{H},\RV{D},\RV{C})$ immediately. Take $\kernel{L}:= \prob{P}_C^{\RV{Y}_i|\RV{H}\RV{X}_i}$ for arbitrary $i$ (by assumption, they are all the same). Then, by assumption $\RV{Y}_i\CI^e_{\prob{P}_C} (\RV{Y}_{[1,i)},\RV{D}_{[1,i),\RV{C}})|(\RV{H},\RV{D}_i)$, for all $\alpha$
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_i|\RV{HD}_i\RV{Y}_{[1,i)}\RV{D}_{[1,i)}} &\overset{\prob{P}_C}{\cong} \kernel{L}\otimes \text{del}_{Y^{i-1}\times X^{i-1}}
\end{align}
and so by higher order conditionals
\begin{align}
    \prob{P}_{\alpha}^{\RV{Y}|\RV{HD}} &= \tikzfig{do_model_representation_conditional}\\
    &= \tikzfig{do_model_representation_conditional_permuted}
\end{align}
hence $(\prob{P}_C,\RV{D},\RV{Y})$ is exchange commutative over $\RV{H}$. Furthermore, take $A\subset \mathbb{N}$. Then
\begin{align}
    &\tikzfig{do_model_representation_conditional_deleted}\\
    =& \tikzfig{do_model_representation_conditional_deleted1}
\end{align}
so $(\prob{P}_C,\RV{D},\RV{Y})$ is also local over $\RV{H}$.
\end{proof}

\subsection[Data-independent inputs]{Conditionally independent and identical response functions with data-independent inputs}\label{sec:data_independent_actions}

Theorem \ref{th:ciid_rep_kernel} says that a sequential input-output model $(\prob{P}_C,\RV{D},\RV{Y})$ features conditionally independent and identical response functions $\prob{P}_C^{\RV{Y}_i|\RV{HD}_i}$ and is weakly data independent if and only if there is some $\RV{W}$ such that $\prob{P}_\cdot^{\RV{Y}|\RV{WD}}$ is causally contractible over $\RV{W}$, and $\RV{Y}\CI^E_{\prob{P}_C} \RV{W}|(\RV{H},\RV{C})$ (see Definition \ref{def:dir_rand_meas} for the definition of $\RV{H}$).

A simple special case to consider is when $\RV{W}$ is single valued -- that is, when $\prob{P}_C^{\RV{Y}||RV{D}}$. As Theorem \ref{th:data_ind_CC} shows, this corresponds to the models where the inputs $\RV{D}$ are strongly data-independent and everywhere independent of the hypothesis $\RV{H}$. We can also consider the case where $(\prob{P}_C, \RV{D},\RV{Y})$ is only exchange commutative over $*$. This corresponds to models where the inputs $\RV{D}$ are data-independent and the hypothesis $\RV{H}$ depends on a symmetric function of the inputs $\RV{D}$ (under some side conditions).

\begin{theorem}[Data-independent causal contractibility]\label{th:data_ind_CC}
Suppose a sequential input-output model $(\prob{P}_C,\RV{D},\RV{Y})$ with sample space $(\Omega,\sigalg{F})$ is given with $D$ countable and, letting $E\subset D^{\mathbb{N}}$ be the set of all sequences for which each $j\in D$ occurs infinitely often, $\prob{P}_\alpha^{\RV{D}}(E)=1$ for all $\alpha$. Then the following are equivalent:
\begin{enumerate}
    \item $\prob{P}_\cdot^{\RV{Y}|\RV{D}}$ is causally contractible
    \item For all $i$, $\RV{Y}_i\CI^e_{\prob{P}_C} (\RV{Y}_{\neq i},\RV{D}_{\neq i},\RV{C})|(\RV{H},\RV{D}_i)$, for all $i,j$ $$\prob{P}_C^{\RV{Y}_i|\RV{H}\RV{D}_i}=\prob{P}_C^{\RV{Y}_j|\RV{H}\RV{D}_j}$$, $\RV{H}\CI^e_{\prob{P}_C} \RV{D}|\RV{C}$ and for all $i$ $\RV{Y}_i\CI^e_{\prob{P}_C} \RV{D}_{(i,\infty]}) | (\RV{D}_{[1,i)},\RV{C})$
    \item There is some $\kernel{L}:H\times X\kto Y$ such that for all $\alpha$, $$\prob{P}_\alpha^{\RV{YH}|\RV{D}}= \tikzfig{do_model_representation_with_h}$$
\end{enumerate}
\end{theorem}

\begin{proof}
(1)$\implies$(3)
From Theorem \ref{th:table_rep_kernel}, $(\prob{P}_C,\RV{D},\RV{Y})$ causally contractible over $\RV{W}$ implies $\RV{Y}^D\CI^e_{\prob{P}_C} \RV{D}|(\RV{W},\RV{C})$ which in turn implies $\RV{H}\CI^e_{\prob{P}_C} \RV{D}|(\RV{W},\RV{C})$. If $\prob{P}_\cdot^{\RV{Y}|\RV{D}}$ is causally contractible, then $\RV{H}\CI^e_{\prob{P}_C}\RV{D}$.

From Theorem \ref{th:ciid_rep_kernel} we have $\RV{Y}_i\CI^e_{\prob{P}_C} (\RV{Y}_{[1,i)},\RV{D}_{[1,i),\RV{C}})|(\RV{H},\RV{D}_i)$ and $\prob{P}_C^{\RV{Y}_i|\RV{H}\RV{D}_i}=\prob{P}_C^{\RV{Y}_j|\RV{H}\RV{D}_j}$ and 
\begin{align}
 \prob{P}_\alpha^{\RV{Y}|\RV{HD}}= \tikzfig{do_model_representation_conditional}
\end{align}
Noting that $\RV{H}\CI^e_{\prob{P}_C}\RV{D}$, we can write
\begin{align}
    \prob{P}_\alpha^{\RV{YH}|\RV{D}}= \tikzfig{do_model_representation_with_h}
\end{align}

(3)$\implies$(2)
From 
\begin{align}
    \prob{P}_\alpha^{\RV{YH}|\RV{D}}= \tikzfig{do_model_representation_with_h}\label{eq:do_over_h}
\end{align}
we have
\begin{align}
 \prob{P}_\alpha^{\RV{Y}|\RV{HD}}= \tikzfig{do_model_representation_conditional}
\end{align}
and $\RV{H}\CI^e_{\prob{P}_C} \RV{D}|\RV{C}$, so we get all elements of (2) immediately except $\RV{D}_i\CI^e_{\prob{P}_C} (\RV{Y}_{[1,i)}) | (\RV{D}_{[1,i)},\RV{C})$. But marginalising Equation \ref{eq:do_over_h} over $\RV{H}$ yields
\begin{align}
     \prob{P}_\alpha^{\RV{Y}|\RV{D}}= \tikzfig{do_model_representation}
\end{align}
and, in particular, for any $A\subset\mathbb{N}$
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_A|\RV{D}_{(A,\mathbb{N}\setminus A)}} &= \prob{P}_\alpha^{\RV{Y}_A|\RV{D}_A}\otimes \text{del}_{D^{|\mathbb{N}\setminus A|}} 
\end{align}
hence, taking $A=\{i\}$, $\RV{D}_{\mathbb{N}\setminus \{i\}}\CI^e_{\prob{P}_C} (\RV{Y}_{i}) | (\RV{D}_{i},\RV{C})$ which implies $\RV{D}_{(i,\infty]}\CI^e_{\prob{P}_C} (\RV{Y}_{i}) | (\RV{D}_{[1,i]},\RV{C})$.
(2)$\implies$(1)
From Theorem \ref{th:ciid_rep_kernel} and $\RV{H}\CI^e_{\prob{P}_C} \RV{D}|\RV{C}$,
\begin{align}
    \prob{P}_\alpha^{\RV{Y}|\RV{D}}&= \tikzfig{do_model_representation}
\end{align}
and the argument from here is analogous to the section ``(2)$\implies$(1)'' from Theorem \ref{th:ciid_rep_kernel}. In particular
\begin{align}
    \prob{P}_{\alpha}^{\RV{Y}|\RV{D}} &= \tikzfig{do_model_representation_permuted}
\end{align}
hence $\prob{P}_C^{\RV{Y}|\RV{D}}$ is exchange commutative. Furthermore, take $A\subset \mathbb{N}$. Then
\begin{align}
    &\tikzfig{do_model_representation_deleted}\\
    =& \tikzfig{do_model_representation_deleted1}
\end{align}
\end{proof}

Given $\prob{P}_C^{\RV{Y}|\RV{D}}$ exchange commutative, we can show that  $\prob{P}_\cdot^{\RV{Y}|\RV{WD}}$ is causally contractible over where $\RV{W}$ is some symmetric function of $\RV{D}$. This $\RV{W}$ therefore satisfies $\RV{Y}\CI^e_{\prob{P}_C} \RV{W} | (\RV{H},\RV{D},\RV{C})$, and so exchange commutativity of $\prob{P}_C^{\RV{Y}|\RV{D}}$ is enough to establish $(\prob{P}_C,\RV{D},\RV{Y})$ has conditionally independent and identical response conditionals. 

\begin{theorem}\label{lem:exch_prod_ciid}
If $\prob{P}_C^{\RV{Y}|\RV{D}}$ is exchange commutative, and for each $\alpha$ $\prob{P}_\alpha^{\RV{D}}$ is absolutely continuous with respect to some exchangeable distribution in $\Delta(D^{\mathbb{N}})$ then there is some $\RV{W}$ symmetric over $\RV{D}$ such that $(\prob{P}_C,\RV{D},\RV{Y})$ is causally contractible over $\RV{W}$.
\end{theorem}

\begin{proof}
Take all sets in $\sigalg{D}^{\mathbb{N}}$ invariant under any finite permutation, and call this the \emph{exchangeable $\sigma$-algebra} $\sigalg{E}$ \citep[pg. 29]{kallenberg_basic_2005}. Let $\sigalg{E}_{\RV{D}}:=\RV{D}^{-1}(\sigalg{E})$.

Consider $\prob{P}_\alpha^{\RV{Y}_{[n]}|\RV{D}}$ for some $n\in \mathbb{N}$. By assumption, for any $\rho_{>n}$ that only affects indices after the $n$th, $\prob{P}_\alpha^{\RV{Y}_{[n]}|\RV{D}}=_\alpha^{\RV{Y}_{[n]}|\RV{D}_{\rho_{>n}}}$. That is, $\prob{P}_\alpha^{\RV{Y}_{[n]}|\RV{D}}$ is $\sigalg{E}_{\RV{D}_(n,\infty)}\vee\sigma(\RV{D}_{[n]}$-measurable.

Let $\sigalg{T}_{\RV{D}}$ be the tail $\sigma$-algebra generated by $\RV{D}$. $\sigalg{T}_{\RV{X}}$ is defined as the intersection $\cap_{i=1}^{\infty} \sigma(\RV{D}_{[i,\infty)})$. Note that $\sigalg{T}_{\RV{D}_{(n,\infty)}}=\sigalg{T}_{\RV{D}}$. By \citet[Corollary 1.6]{kallenberg_basic_2005}, $\sigalg{E}_{\RV{D}_(n,\infty)}=\sigalg{T}_{\RV{X}_{(n,\infty)}}$ almost surely (by assumption that $\prob{P}_\alpha$ is dominated by an exchangeable distribution), so $\sigalg{E}_{\RV{D}_(n,\infty)}=\sigalg{T}_{\RV{D}}$ almost surely and by \citet[Corollary 1.6]{kallenberg_basic_2005} again $\sigalg{E}_{\RV{D}_(n,\infty)}=\sigalg{E}_{\RV{D}}$ almost surely.

Thus $\prob{P}_\alpha^{\RV{Y}_{[n]}|\RV{D}}$ is $\sigalg{E}_{\RV{D}}\vee\sigma(\RV{D}_{[n]})$-measurable. By \citet[Corollary 1.6]{kallenberg_basic_2005} again, there is a random $\RV{J}$ taking values in the set of distributions on $D$ such that $\sigma(\RV{J})=\sigalg{E}_{\RV{X}}$ almost surely. Thus
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_{[n]}|\RV{D}\RV{J}} &= \prob{P}_\alpha^{\RV{Y}_{[n]}|\RV{D}_{[n]}\RV{J}}\otimes \text{erase}_{DX^{\mathbb{N}}}
\end{align}
That is, for all $\alpha$ $\RV{Y}_{[n]}\CI_{\prob{P}} \RV{D}_{(n,\infty)}|(\RV{D}_{[n]},\RV{J})$, implying $(\prob{P}_C,\RV{D},\RV{Y})$ is local over $\RV{J}$ and, because $\RV{J}$ is symmetric in $\RV{D}$, exchange commutative also.
\end{proof}

\begin{corollary}\label{th:ciid_rep_kernel_nolocal}
If $(\prob{P}_C,\RV{D},\RV{Y})$ is exchange commutative over $*$, and for each $\alpha$ $\prob{P}_\alpha^{\RV{D}}$ is absolutely continuous with respect to some exchangeable distribution in $\Delta(D^{\mathbb{N}})$ then
\begin{align}
    \prob{P}_\alpha^{\RV{Y}|\RV{HD}}= \tikzfig{do_model_representation_conditional}
\end{align}
\end{corollary}

\begin{proof}
By Theorem \ref{lem:exch_prod_ciid}, $\prob{P}_\cdot^{\RV{Y}|\RV{WD}}$ is causally contractible over $\RV{W}$, so the result follows immediately from Theorem \ref{th:ciid_rep_kernel}.
\end{proof}

\subsection{Examples and discussion}\label{sec:examples}

\paragraph{Passive observations and strategic behaviour}\label{pgph:passive_strategic}

Purely passive observations can be modeled with a ``probability set'' $\prob{P}_C$ where $|\prob{P}_C|=1$. In this case, a model that is exchangeable over the sequence of pairs $\RV{YD}:=(\RV{D}_i,\RV{Y}_i)_{i\in \mathbb{N}}$ has $(\prob{P}_C, \RV{D},\RV{Y})$ exchange commutative over $*$. This follows from the fact that
\begin{align}
    \prob{P}_C^{\RV{YD}} &= \prob{P}_C^{(\RV{Y}\RV{D})_\rho}\\
    \implies \prob{P}_C^{\RV{Y}|\RV{D}} &= \prob{P}_C^{\RV{Y}_\rho|\RV{D}_\rho}
\end{align}
thus by Corollary \ref{th:ciid_rep_kernel_nolocal}, $(\prob{P}_C, \RV{D},\RV{Y})$ features conditionally independent and identical response functions. Note that $\prob{P}_C^{\RV{Y}|\RV{D}}$ is not necessarily causally contractible. Suppose there is a machine with two arms $D=\{0,1\}$, one of which pays out \$100 and the other that pays out nothing. A decision maker (DM) doesn't know which is which, but the DM watches a sequence of people operate the machine who almost all do know which one is good. The DM is sure that they all want the money, and that they will pull the good arm $1-\epsilon$ of the time independent of every other trial. Set the hypotheses $\RV{H}$ to ``0 is good'' and ``1 is good'' (which we'll just refer to as $\{0,1\}$), with 50\% probability on each initially. Then
\begin{align}
    \prob{P}_C^{\RV{Y}_2|\RV{D}_2}(100|1) &= \sum_{0,1} \prob{P}_C^{\RV{Y}_2|\RV{D}_2\RV{H}}(100|1,0)\prob{P}_C^{\RV{H}|\RV{D}_2}(0|1) + \prob{P}_C^{\RV{Y}_2|\RV{D}_2\RV{H}}(100|1,1)\prob{P}_C^{\RV{H}|\RV{D}_2}(1|1)\\
    &= 1-\epsilon
\end{align}
but
\begin{align}
    \prob{P}_C^{\RV{Y}_2|\RV{D}_1\RV{D}_2}(100|0,1) &= \sum_{0,1} \prob{P}_C^{\RV{Y}_2|\RV{D}_1\RV{D}_2\RV{H}}(100|0,1,0)\prob{P}_C^{\RV{H}|\RV{D}_1\RV{D}_2}(0|0,1) + \prob{P}_C^{\RV{Y}_2|\RV{D}_1\RV{D}_2\RV{H}}(100|0,1,1)\prob{P}_C^{\RV{H}|\RV{D}_1\RV{D}_2}(1|0,1)\\
    &= 0.5
\end{align}

Consider the previous example, except instead of watching knowledgeable operators, the DM will pull each lever themselves, and they will decide in advance on the sequence of pulls. In this case, the dependence between $\RV{H}$ and $\RV{D}$ is removed and
\begin{align}
        \prob{P}_C^{\RV{Y}_2|\RV{D}_2}(100|1) &= \sum_{0,1} \prob{P}_C^{\RV{Y}_2|\RV{D}_2\RV{H}}(100|1,0)\prob{P}_C^{\RV{H}}(0) + \prob{P}_C^{\RV{Y}_2|\RV{D}_2\RV{H}}(100|1,1)\prob{P}_C^{\RV{H}}(1)\\
        &= 0.5\\
        \prob{P}_C^{\RV{Y}_2|\RV{D}_1\RV{D}_2}(100|0,1) &= \sum_{0,1} \prob{P}_C^{\RV{Y}_2|\RV{D}_1\RV{D}_2\RV{H}}(100|0,1,0)\prob{P}_C^{\RV{H}}(0) + \prob{P}_C^{\RV{Y}_2|\RV{D}_1\RV{D}_2\RV{H}}(100|0,1,1)\prob{P}_C^{\RV{H}}(1)\\
        &= 0.5
\end{align}
so here the decision maker has adopted a model where $\prob{P}_C^{\RV{Y}|\RV{D}}$ is causally contractible.

\paragraph{Two kinds of data}\label{pgph:two_kinds}

Theorem \ref{th:equal_of_condits} implies, for any $A,B\subset\mathbb{N}$ with $|A|=|B|$, $\prob{P}_\alpha^{\RV{Y}_A|\RV{WD}_A}=\prob{P}_\alpha^{\RV{Y}_B|\RV{WD}_B}$. A further consequence of this is, again for any $|A|=|B|$
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_i|\RV{D}_i\RV{W}\RV{Y}_A\RV{D}_A} &= \prob{P}_\alpha^{\RV{Y}_i|\RV{D}_i\RV{W}\RV{Y}_B\RV{D}_B}\label{eq:interchangeability_of_conditioning}
\end{align}

A decision maker might be in the position of having both observational and experimental data. Modify the machine from the previous example so that the good lever pays out \$100 $0.5+\epsilon$ of the time, and the bad lever pays out $0.5-\epsilon$ of the time and (as before) the prior probability that each lever is the good one is $0.5$. Suppose the DM from the previous examples observes a sequence of strangers operating the machine, the results associated with the sequence of pairs $(\RV{D}_i,\RV{Y}_i)_{i\in\mathbb{N}}$, and also operates the machine themselves according to a plan fixed in advance, the results associated with the sequence of pairs $(\RV{E},\RV{Z})_{i\in \mathbb{N}}$. Suppose they adopt a model $(\prob{P}_C,(\RV{D},\RV{E}),(\RV{Y},\RV{Z}))$ such that $\prob{P}_\cdot^{\RV{YZ}|\RV{DE}}$ is causally contractible (over $*$). By Equation \ref{eq:interchangeability_of_conditioning}, for some $n\in\mathbb{N}$ and any choice of actions by the DM $\alpha$
\begin{align}
    \prob{P}_\alpha^{\RV{Z}_i|\RV{E}_i\RV{D}_{[n]}\RV{Y}_{[n]}} &= \prob{P}_\alpha^{\RV{Y}_i|\RV{E}_i\RV{E}_{[n]}\RV{Z}_{[n]}}
\end{align}
However, this might not be appropriate. Suppose $(\proc{D}_1,\proc{D}_2,\proc{Y}_1,\proc{Y}_2)\yields (1,1,0,100)$ and $(\proc{E}_1,\proc{E}_2,\proc{Z}_1,\proc{Z}_2)\yields (1,1,0,100)$. One could reason as follows: while the consequences are no more likely under the hypothesis that lever 1 is good than under the hypothesis it is bad, the fact that the strangers (who may be knowledgeable) chose lever 1 twice provides some reason to believe that lever 1 is better than lever 0. On the other hand, the fact that DM chose lever 1 twice according to a plan fixed in advance does not support one hypothesis over another. This suggests that
\begin{align}
    \prob{P}_\alpha^{\RV{Z}_i|\RV{E}_i\RV{D}_{[2]}\RV{Y}_{[2]}}(100|1,1,1,0,100) > \prob{P}_\alpha^{\RV{Y}_i|\RV{E}_i\RV{E}_{[2]}\RV{Z}_{[2]}}(100|1,1,1,0,100)
\end{align}
That is, even when a DM accepts conditionally independent and identical response conditionals, causal contractibility of $(\prob{P}_C,\RV{D},\RV{Y})$ over $*$ can be too strong.

Causal contractibility of $\prob{P}_C^{\RV{Y}|\RV{D}}$ might be too strong for many purposes. Theorem \ref{th:ciid_rep_kernel} implies that, whenever $(\prob{P}_C, \RV{D},\RV{Y})$ has conditionally independent and identical response functions, causal contractibility over \emph{some} $\RV{W}$ must hold -- in the worst case over $\RV{H}$. $\RV{H}$ itself is a function of the limiting relative frequencies of $(\RV{D}_i,\RV{Y}_i)$ pairs.  We pose the following as an open question:
\begin{itemize}
    \item If $\prob{P}_\cdot^{\RV{Y}|\RV{HD}}$ is causally contractible, when is $\prob{P}_\cdot^{\RV{Y}_{(n,\infty)}|(\RV{D}_i,\RV{Y}_i)_{[n]}\RV{D}_{(n,\infty)}}$ approximately causally contractible?
\end{itemize}
In particular, if $\prob{P}_\cdot^{\RV{Y}_{(n,\infty)}|(\RV{D}_i,\RV{Y}_i)_{[n]}\RV{D}_{(n,\infty)}}$ is approximately causally contractible for sufficiently large $n$ then Equation \ref{eq:interchangeability_of_conditioning} may hold approximately whenever $|A|$ is sufficiently large.

\paragraph{Weaker assumptions}\label{pgph:weaker_assumptions}

Where observational data is mixed with experimental data, causal contractibility is rarely accepted. Accepting causal contractibility would mean that there is no reason not to mix both data types together and estimate response functions from the combined data. However, in practice when both data types are available they are not mixed like this -- \citet{eckles_bias_2021,gordon_comparison_2018,gordon_close_2022} all separate the observational and experimental data and \emph{test} whether the conditional probabilities derived from each appear to be equal, whereas the assumption of causal contractibility implies that one could just as well use the observational data to estimate conditional probabilities for unseen experimental pairs.

Given a mixture of observations and experiments, exchange commutativity holds that every experimental pair can be swapped with every observational pair without changing anything important. A weaker assumption than this is that every experimental pair can be swapped with \emph{some} unknown observational pairs without changing anything important. 

More specifically, suppose that there are $m$ observations $(\RV{D}_i,\RV{Y}_i)_{i\in [m]}$ followed by $\mathbb{N}$ experiments $(\RV{D}_i,\RV{Y}_i)_{i\in (m,\infty)}$ and there is a variable $\RV{R}$ taking values in the set of functions $\mathbb{N}\to [n]$. Then, supposing $\RV{R}(\omega)\yields r$, we might assume that for all $i\in [n]$, $(\prob{P}_C,\RV{D}_{r^{-1}(i)},\RV{Y}_{r^{-1}(i)})$ is causally contractible. Defining $r_\rho:=i\mapsto r(\rho(i))$, this amounts to the assumption that for some finite permutation $\rho:\mathbb{N}\to\mathbb{N}$ such that $r_\rho=r$: 
\begin{align}
    \prob{P}_\alpha^{\RV{Y}|\RV{D}\RV{R}}(A|d,r) &= \prob{P}_\alpha^{\RV{Y}_\rho|\RV{D}_\rho\RV{R}_\rho}(A|d,r)
\end{align}
That is, $\RV{R}$ divides the observations and the experiments into $n$ partitions such that swaps that keep all pairs in the same partition do not change the conditional probability of $\RV{Y}$ given $\RV{D}$. As an open question:
\begin{itemize}
    \item What other conditions are needed for $\prob{P}_\alpha^{\RV{Y}|\RV{RD}}$ to be exchange commutative?
\end{itemize}
The idea is that partial causal contractibility corresponds, under some conditions, to confounding by the components $\RV{R}_i$ of $\RV{R}$.

\paragraph{Backdoor adjustment}

An example of ``backdoor adjustment''. Suppose a sequential input-output model $(\prob{P}_C,(\RV{D},\RV{X}),\RV{Y})$ where $(\prob{P}_\cdot^{\RV{Y}|\RV{WDX}}$ is causally contractible, and:
\begin{itemize}
    \item $i>n\implies \RV{X}_{i}\CI^e_{\prob{P}_C}\RV{D}_{i}|(\RV{H},\RV{C})$
    \item $\prob{P}_\alpha^{\RV{X}_{i}|\RV{H}}\cong \prob{P}_\alpha^{\RV{X}_{1}|\RV{H}}$ for all $\alpha$
 \end{itemize}
Then the model exhibits a kind of ``backdoor adjustment''. Specifically, for $i>n$
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_{i}|\RV{D}_{i}\RV{H}}(A|d,h) &= \int_X \prob{P}_\alpha^{\RV{Y}_{i}|\RV{X}_{i}\RV{D}_{i}\RV{H}}(A|d,x,h)\prob{P}_\alpha^{\RV{X}_{i}|\RV{D}_{i}\RV{H}}(\mathrm{d}x|d,h)\\
    &= \int_X \prob{P}_\alpha^{\RV{Y}_{1}|\RV{X}_{1}\RV{D}_{1}\RV{H}}(A|d,x,h)\prob{P}_\alpha^{\RV{X}_{i}|\RV{H}}(\mathrm{d}x|h)\\
    &= \int_X \prob{P}_\alpha^{\RV{Y}_{1}|\RV{X}_{1}\RV{D}_{1}\RV{H}}(A|d,x,h)\prob{P}_\alpha^{\RV{X}_{1}|\RV{H}}(\mathrm{d}x|h)\label{eq:backdoor}
\end{align}

Equation \ref{eq:backdoor} is identical to the backdoor adjustment formula \citep[Chap. 1]{pearl_causality:_2009} for an intervention on $\RV{D}_1$ targeting $\RV{Y}_1$ where $\RV{X}_1$ is a common cause of both.

\paragraph{Choices should be under the decision maker's control}\label{sec:dm_control}

We have paid little attention to set of choices $C$. Causal contractibility is a helpful property of a model $\prob{P}_C$ that holds for all $\alpha\in C$, and many other independences we consider likewise held for all $\alpha$. Two points that might be a bit banal: first, it is $\alpha$ that is being chosen, not necessarily the sequence of inputs $\RV{D}$. There could be a meaningful distinction if, for example, $\prob{P}_\cdot^{\RV{Y}|\RV{D}}$ is causally contractible, but $\prob{P}_\alpha^{\RV{Y}|\RV{D}}$ is nevertheless different to $\prob{P}_\beta^{\RV{Y}|\RV{D}}$ for some $\alpha\neq\beta$. Secondly (and this is the point of the following example) whether or not it is reasonable to use a model where the choice variable $\RV{C}$ isn't directly relevant to many questions depends on the problem at hand and how the model is being applied to this problem. 

A particular concern arises when the choice variable $\RV{C}$ is not associated with the output of a decision procedure involving the model $\prob{P}_C$. In this situation, the value of $\RV{C}$ can affect the model in potentially unexpected ways. ``Potentially unexpected'' is a vague notion, and we can't say whether $\RV{C}$ being completely under the decision maker's control avoids ``unexpected'' dependence on $\RV{C}$, but it seems to be less problematic.

We set this up in terms of an ``analyst'' and an ``administrator'' who have responsibility for different parts of the procedure. They don't strictly need to be different people, but it helps make the issue clearer. The analyst's job is to construct a model $\prob{P}_C$, evaluate different options $\alpha\in C$ and offer advice regarding the choice. The administrator's job is to choose some $\alpha\in C$ satisfying the analyst's requirements and to carry out any procedure arising from this.

This separation of concerns gives the administrator a degree of freedom in their choice, and they can potentially use this to choose $\alpha$ with access to information that the analyst lacks.

In particular, suppose an experiment is modeled by a sequential input-output model $(\prob{P}_C,(\RV{D},\RV{U}),\RV{Y})$ and the set of choices $C=[0,1]^{\mathbb{N}}$ is a length $\mathbb{N}$ sequence of probability distributions in $\Delta(\{0,1\})$. The analyst, based on their knowledge of the experiment, constructs $\prob{P}_C$ such that $\prob{P}_C^{\RV{Y}_i|\RV{U}_i\RV{D}_i}(1|\cdot,\cdot)$ is given by:
\begin{center}
\begin{tabular}{ c | c | c }
  & $\RV{D}_i=0$ & $\RV{D}_i=1$ \\\hline 
 $\RV{U}_i=0$ & 0 & 0 \\ \hline 
 $\RV{U}_i=1$ & 1 & 1   
\end{tabular}
\end{center}
and the triples $(\RV{D}_i,\RV{U}_i,\RV{Y}_i)$ are mutually independent given $\RV{C}$. This makes $\prob{P}_C^{\RV{Y}|\RV{UD}}$ causally contractible over $*$. Suppose also 
\begin{align}
    \prob{P}_\alpha^{\RV{D}_i}(1) &= \alpha_i
\end{align}
where $\alpha=(\alpha_i)_{i\in\mathbb{N}}$. From the analyst's point of view, both before and after making their recommendations the $\RV{U}_i$ are also IID. This will be expressed with a probability distribution $\prob{Q}$ representing the analyst's prior knowledge:
\begin{align}
    \prob{Q}^{\RV{U}_i}(1) &= 0.5
\end{align}
one might be tempted to reason that, if $\prob{Q}$ is the analyst's state of knowledge after making any reccomendation, then we should take $\prob{P}_C^{\RV{U}}=\prob{Q}^{\RV{U}}$. Call the resulting model $\prob{P}_C'$. Together with the other assumptions above, this would imply
\begin{align}
    \prob{P}_C^{\prime \RV{Y}_i|\RV{D}_i}(1|d) &= 0.5 & \forall d\in \{0,1\}
\end{align}
Thus $\prob{P}_C^{\RV{Y}|\RV{D}}$ is also causally contractible.

However, the analyst's recommendation \emph{does not} fix the value of $\RV{C}$. Suppose analyst actually recommends any $\alpha$ such that $\lim_{n\to\infty} \sum_i^n \frac{\alpha_i}{n} = 0.5$ (acknowledging that, in this contrived example, there's no obvious reason to do so). Suppose that the administrator operates by the following rule: \emph{first} they observe the value of $\RV{U}_i$, then they choose $\alpha_i$ equal to whatever they saw with an $\epsilon$ sized step towards $0.5$. That is, if they see $\RV{U}_i\yields 1$, they choose $\alpha_i=1-\epsilon$, where $\epsilon < 0.5$.

Then the analyst should instead adopt the model
\begin{align}
    \prob{P}_\alpha^{\RV{U}_i}(1) &= \mathds{1}_{\alpha_i>0.5}
\end{align}
Take $\alpha$ such that $\alpha_i=1-\epsilon$ and $\alpha_j=\epsilon$. Then
\begin{align}
    \prob{P}_{\alpha}^{\RV{Y}_i|\RV{D}_i}(1|1) &= 1\\
    &\neq \prob{P}_\alpha^{\RV{Y}_j|\RV{D}_j}(1|1)\\
    &=0
\end{align}
everything has been assumed IID, so $\prob{P}_C^{\RV{Y}|\RV{HD}}$ is not causally contractible.

The original justification for having a set of choices $C$ is that $C$ is the set of things that, after deliberation aided by the model $\prob{P}_C$, the decision maker might select. The present example does not conform to this understanding of the meaning of the set $C$, and it suggests that one should be cautious when modelling ``decision problems'' with ``choices'' that are not actually the things that are being chosen.

\citet{kasy_why_2016} argues that ``randomised controlled trials are not needed for causal identifiability, only controlled trials'', and suggests that experiments should sometimes be designed with deterministic assignments of patients to treatment and control groups, optimised according to the experiment designer's criteria. \citet{banerjee_theory_2020} suggests that deterministic rules might falter when one can't pick a function to balance covariates in a way that satisfies everyone in a panel of reviewers, but this example suggests another reason one might want to avoid deterministic treatment assignments. If the choices $\alpha$ are a deterministic sequence of assignments for each index $i$, this means that there is an enormous set of possible choices, and many degrees of freedom if the choices ``aren't actually chosen'' in the sense of the example above. In contrast, if the set of choices is a single parameter in $[0,1]$ which is then used to assign all treatments according to a random procedure depending only on this parameter, there are many fewer degrees of freedom to exploit if the choice ``isn't actually chosen''.

% \subsubsection{Example: body mass index}

% Given a sequential just-do model $(\prob{P}_C,(\RV{B},\RV{I}),\RV{Y})$ with $\RV{B}:=(\RV{B}_i)_{i\in M}$ representing body mass index of individual $\RV{I}_i$ and $\RV{Y}:=(\RV{Y}_i)_{i\in M}$ representing health outcomes of interest for the same individual, \citet{hernan_does_2008} noted that there are multiple different choices that can influence an individual's body mass index $\RV{B}_i$ in the same way. Thus $\RV{YI}\CI^e_{\prob{P}_C} \RV{C}|\RV{B}$ might generally be rejected, and so there may be no uniform conditional $\prob{P}_C^{\RV{Y}|\RV{IB}}$. In this case, $\prob{P}_C^{\RV{Y}|\RV{IB}}$ cannot be causally contractible because it doesn't exist.

% Suppose instead a model $(\prob{P}_C,(\RV{D},\RV{I}),(\RV{B},\RV{Y}))$ is given, with $\RV{D}=(\RV{D}_i)_{i\in M}$ representing ``decisions'', appropriately fine-grained to satisfy
% \begin{align}
%     &\RV{YBI}\CI^e_{\prob{P}_C} \RV{C}|\RV{D}\\
%     &\RV{YBI}\CI^e_{\prob{P}_C} \RV{D}|\RV{C}
% \end{align}
% and $\prob{P}_C^{\RV{YB}|\RV{ID}}$ causally contractible. Then by Theorem \ref{th:cc_ind_treat} $\prob{P}_C^{\RV{Y}|\RV{BD}}$ is also causally contractible. In general, there may be some $U\subset H$ such that for any $h\in U$ 
% \begin{align}
%     \prob{P}_C^{\RV{Y}_i|\RV{B}_i\RV{D}_i\RV{H}}(y|b,d,h) &= \prob{P}_C^{\RV{Y}_i|\RV{B}_i\RV{H}}(y|b,h)\label{eq:conditional_conditional_independence}
% \end{align}
% then, \emph{conditioning on }$\RV{H}\in U$, the resulting $\prob{P}_{C,\RV{H}\in U}^{\RV{Y}|\RV{B}}$ is causally contractible.
% \todo[inline]{Defining conditioning}
% So it may be possible to derive the fact that there is a independent and identical response conditional $\prob{P}_{C,\RV{H}\in U}^{\RV{Y}_i|\RV{H}\RV{B}_i}$ if $\RV{H}\in U$ is implied by available data, even if it is not assumed outright.

\section[Data-dependent inputs]{Conditionally independent and identical response functions with data-dependent inputs}\label{sec:data_dependent}

The results of the previous section concern ``just-do'' models where actions have not dependence on previous data. Decision problems of interest actually have actions that depend on data -- what's really wanted are ``see-do'' models of some variety (see Definition \ref{def:see_do_model}). Here, Theorem \ref{th:data_ind_CC} is generalised to sequential see-do models with the use of \emph{probability combs}.

To begin with an example, consider a probability set $(\prob{P}_C,\RV{D},\RV{Y})$ with $\RV{D}:=(\RV{D}_i)_{i\in\mathbb{N}}$ and $\RV{Y}:=(\RV{Y}_i)_{i\in\mathbb{N}}$ as usual, and take a subsequence $(\RV{D}_i,\RV{Y}_i)_{i\in [2]}$ of length 2. Suppose $\prob{P}_C$ features conditionally independent and identical response functions -- that is, the following holds:
\begin{align}
    \RV{Y}_i&\CI^e_{\prob{P}_C} (\RV{Y}_{<i},\RV{D}_{<i},\RV{C})|\RV{H}\RV{D}_i&\forall i\in \mathbb{N}\\
    \land \prob{P}_C^{\RV{Y}_i|\RV{H}\RV{D}_i} &= \prob{P}_C^{\RV{Y}_0|\RV{H}\RV{D}_0} & \forall i\in \mathbb{N}
\end{align}

and, for simplicity, assume $\RV{H} \CI^e_{\prob{P}_C} (\RV{D}, \RV{C})$ also.

Then, for arbitrary $\alpha\in C$
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_{[2]}} &= \tikzfig{response_conditional_comb}
\end{align}
note that $\RV{D}_2$ depends on $\RV{Y}_1$ and $\RV{D}_1$. $\prob{P}_\alpha^{\RV{D}_2|\RV{Y}_1\RV{D}_1}$ has been ``inserted'' between the response conditionals $\prob{P}_C^{\RV{Y}_1|\RV{D}_1\RV{H}}$ and $\prob{P}_C^{\RV{Y}_2|\RV{D}_2\RV{H}}$.

Given $\prob{P}_C^{\RV{Y}_1|\RV{D}_1\RV{H}}$ and $\prob{P}_C^{\RV{Y}_2|\RV{D}_2\RV{H}}$, define the comb
\begin{align}
    \prob{P}_C^{\RV{Y}_{[2]}\combbreak \RV{D}_{[2]}} := \tikzfig{causally_contractible_comb}
\end{align}
then $\prob{P}_C^{\RV{Y}_{[2]}\combbreak \RV{D}_{[2]}}$ is causally contractible. $\prob{P}_C^{\RV{Y}_{[2]}\combbreak \RV{D}_{[2]}}$ is \emph{not} a uniform conditional probability; in general 
\begin{align}
    \prob{P}_\alpha^{\RV{D}_1\RV{D}_2} \prob{P}_C^{\RV{Y}_{[2]}\combbreak \RV{D}_{[2]}}\neq \prob{P}_\alpha^{\RV{Y}_1\RV{Y}_2}
\end{align}


\subsection{Combs}\label{sec:def_combs}

Combs generalise conditional probabilities in this sense: given a conditional distribution and a marginal distribution of the right type, joining them together (with the semidirect product\ref{def:copyproduct}) I get a marginal distribution of a different type. Define ``1-combs'' as conditional probabilities and ``0-combs'' as conditional distributions. Then the previous observation can be restated as: given a 1-comb and a 0-comb of the right type,  joining them together yields a 0-comb of a different type. Higher order combs generalise this: given an $n$-comb and an $n-1$-comb of the right type, joining them yields an $n-1$ comb.

Joining combs uses an ``insert'' operation (Definition \ref{def:insert_discrete}).  A graphical depiction of this operation gives some intuition for why it is called ``insert'':
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_{1}\RV{D}_2\RV{Y}_2|\RV{D}_1}&=\text{insert}(\prob{P}_\alpha^{\RV{D}_2|\RV{D}_1\RV{Y}_1},\prob{P}_C^{\RV{Y_{[2]}}\combbreak\RV{D}_{[2]}})\\
    &= \tikzfig{comb_insert_complicated}\label{eq:comb_insert_complicated}\\
    &= \tikzfig{comb_insert_gettingsimpler}\\
    &= \tikzfig{comb_insert_simple}\label{eq:comb_insert_simple}
\end{align}
While Equation \ref{eq:comb_insert_complicated} is a well-formed string diagram in the category of Markov kernels, Equation \ref{eq:comb_insert_simple} is not. In the case that all the underlying sets are discrete, Equation \ref{eq:comb_insert_simple} can be defined using an extended string diagram notation appropriate for the category of real-valued matrices \citep{jacobs_causal_2019}, though we do not introduce this extension here.

Formal definitions of combs and both notations follow. As with conditional probabilities, a \emph{uniform} $n$-comb $\prob{P}_C^{\RV{Y}_{[n]}\combbreak \RV{X}_{[n]}}$ is a Markov kernel that satisfies the definition of an $n$-comb for each $\alpha\in C$.

\begin{definition}[$n$-Comb]\label{def:uniform_comb}
Given a probability space $(\prob{P},\Omega,\sigalg{F})$ with variables $\RV{Y}_i:\Omega\to Y$, $\RV{D}_i:\Omega\to D$ for $i\in [n]$ and $\RV{W}:\Omega\to W$, the uniform $n$-comb $\prob{P}^{\RV{Y}_{[n]}\combbreak \RV{D}_{[n]}|\RV{W}}:W\times D^n\kto Y^n$ is the Markov kernel given by the recursive definition\todo{image alignment}
\begin{align}
    \prob{P}^{\RV{Y}_{1}\combbreak \RV{D}_{1}|\RV{W}} &= \prob{P}^{\RV{Y}_1|\RV{D}_1\RV{W}}\\
    \prob{P}^{\RV{Y}_{[m]}\combbreak \RV{D}_{[m]}|\RV{W}} &= \tikzfig{comb_inductive}
\end{align}
\end{definition}

\begin{definition}[$\mathbb{N}$-comb]
Given a probability space $(\prob{P},\Omega,\sigalg{F})$ with variables $\RV{Y}_i:\Omega\to Y$ and $\RV{D}_i:\Omega\to D$, for $i\in \mathbb{N}$ and $\RV{W}:\Omega\to W$, the $\mathbb{N}$-comb $\prob{P}^{\RV{Y}_{\mathbb{N}}\combbreak \RV{D}_{\mathbb{N}}|\RV{W}}:W\times D^\mathbb{N}\kto Y^\mathbb{N}$ is the Markov kernel such that for all $n\in \mathbb{N}$
\begin{align}
    \prob{P}^{\RV{Y}_{\mathbb{N}}\combbreak \RV{D}_{\mathbb{N}}|\RV{W}}[\mathrm{id}_{Y^{n}}\otimes \mathrm{del}_{Y^{\mathbb{N}}}] &= \prob{P}^{\RV{Y}_{[n]}\combbreak \RV{D}_{[n]}|\RV{W}}\otimes \mathrm{del}_{Y^{\mathbb{N}}}
\end{align}
\end{definition}

\begin{theorem}[Existence of $\mathbb{N}$-combs]
Given a probability set $\prob{P}$ with variables $\RV{Y}_i:\Omega\to Y$ and $\RV{D}_i:\Omega\to D$ for $i\in \mathbb{N}$ and $\RV{W}:\Omega\to W$, $D,Y,W$ standard measurable, a uniform $\mathbb{N}$-comb $\prob{P}^{\RV{Y}_{\mathbb{N}}\combbreak \RV{D}_{\mathbb{N}}|\RV{W}}:W\times D^\mathbb{N}\kto Y^\mathbb{N}$ exists.
\end{theorem}

\begin{proof}
For each $n\in \mathbb{N}$ $m<n$, we have
\begin{align}
    \prob{P}^{\RV{Y}_{[n]}\combbreak \RV{D}_{[n]}|\RV{W}}[\mathrm{id}_{Y^{n-m}}\otimes \mathrm{del}_{Y^m}] &= \prob{P}^{\RV{Y}_{[n-m]}\combbreak \RV{D}_{[n-m]}}\otimes \mathrm{del}_{Y^m}
\end{align}
and each $m$ and $n$ comb exists because the requisite conditional probabilities exist. Therefore the existence of $\prob{P}^{\RV{Y}_{\mathbb{N}}\combbreak \RV{D}_{\mathbb{N}}}$ is a consequence of Lemma \ref{lem:infinitely_extended_kernels}.
\end{proof}

For discrete sets, the insert operation has a compact definition:

\begin{definition}[Comb insert - discrete]\label{def:insert_discrete}
Given an $n$-comb $\prob{P}_\alpha^{\RV{Y}_{[n]}\combbreak \RV{D}_{[n]}}$ and an $n-1$ comb $\prob{P}_\alpha^{\RV{D}_{[2,n]}\combbreak \RV{Y}_{[n-1]}|\RV{D}_1}$ with $(D,\sigalg{D})$ and $(Y,\sigalg{Y})$ discrete, for all $y_i\in Y$ and $d_i\in D$
\begin{align}
    &\mathrm{insert}(\prob{P}_\alpha^{\RV{D}_{[2,n]}\combbreak \RV{Y}_{[n-1]}|\RV{D}_1},\prob{P}_\alpha^{\RV{Y}_{[n]}\combbreak \RV{D}_{[n]}})(y_{[n]},d_{[2,n]}|d_1)\\
     &= \prob{P}_\alpha^{\RV{Y}_{[n]}\combbreak \RV{D}_{[n]}}(y_{[n]}|d_{[n]})\prob{P}_\alpha^{\RV{D}_{[2,n]}\combbreak \RV{Y}_{[n-1]}|\RV{D}_1}(d_{[n]}|d_1,y_{[n-1]})
\end{align}
\end{definition}

Inserting a comb into a comb (of appropriate dimensions) yields a conditional probability.

\begin{theorem}
Given an $n$-comb $\prob{P}_\alpha^{\RV{Y}_{[n]}\combbreak \RV{D}_{[n]}}$ and an $n-1$ comb $\prob{P}_\alpha^{\RV{D}_{[2,n]}\combbreak \RV{Y}_{[n-1]}|\RV{D}_1}$, $(D,\sigalg{D})$ and $(Y,\sigalg{Y})$ discrete,
\begin{align}
    &\mathrm{insert}(\prob{P}_\alpha^{\RV{D}_{[2,n]}\combbreak \RV{Y}_{[n-1]}|\RV{D}_1},\prob{P}_\alpha^{\RV{Y}_{[n]}\combbreak \RV{D}_{[n]}})\\
     &= \prob{P}_\alpha^{\RV{Y}_{[n]}\RV{D}_{[2,n]}|\RV{D}_1}
\end{align}
\end{theorem}

\begin{proof}
Take $\RV{Y}_[0]=\RV{D}_{n+1}=*$, and
\begin{align}
    &\mathrm{insert}(\prob{P}_\alpha^{\RV{D}_{[2,n]}\combbreak \RV{Y}_{[n-1]}|\RV{D}_1},\prob{P}_\alpha^{\RV{Y}_{[n]}\combbreak \RV{D}_{[n]}})(y_{[n]},d_{[2,n]}|d_1)\\
     &= \prob{P}_\alpha^{\RV{Y}_{[n]}\combbreak \RV{D}_{[n]}}(y_{[n]}|d_{[n]})\prob{P}_\alpha^{\RV{D}_{[n]}\combbreak\RV{Y}_{[n-1]}|\RV{D}_1}(d_{[2,n]}|d_1,y_{[n-1]})\\
    &= \prod_{i=1}^n \prob{P}_\alpha^{\RV{Y}_{[i]}|\RV{D}_{[i]}\RV{Y}_{[i-1]}}(y_i|d_{[i]},y_{[i-1]})\prob{P}_\alpha^{\RV{D}_{i+1}|\RV{D}_{[i]}\RV{Y}_{[i-1]}}(d_i|d_{[i-1]},y_{[i-1]})\\
    &= \prob{P}^{\RV{Y}_{[n]}\RV{D}_{[2,n]}}(y_{[n]},d_{[n]}|d_1)
\end{align}
    
\end{proof}

\subsubsection{Aside: combs might be the output of the ``fix'' operation}

There is a relationship between combs and the ``fix'' operation defined in \citet{richardson_nested_2017}. In particular, suppose we have a probability $\prob{P}_\alpha$ and a comb $\prob{P}_\alpha^{\RV{Y}_{[2]}|\RV{D}_{[2]}}$. Then (assuming discrete sets)
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_{[2]}\combbreak \RV{D}_{[2]}}(y_1,y_2|d_1,d_2) &= \prob{P}_\alpha^{\RV{Y}_1|\RV{D}_1}(y_1|d_1)\prob{P}_\alpha^{\RV{Y}_2|\RV{D}_2}(y_2|d_2)\\
    &= \frac{\prob{P}_\alpha^{\RV{Y}_1|\RV{D}_1}(y_1|d_1)\prob{P}_\alpha^{\RV{D}_2|\RV{Y}_1\RV{D}_1}(d_2|y_1,d_1)\prob{P}_\alpha^{\RV{Y}_2|\RV{D}_2}(y_2|d_2)}{\prob{P}_\alpha^{\RV{D}_2|\RV{Y}_1\RV{D}_1}(d_2|y_1,d_1)}\\
    &= \frac{\prob{P}_\alpha^{\RV{Y}_{[2]}\RV{D}_2|\RV{D}_1}(y_1,y_2,d_2|d_1)}{\prob{P}_\alpha^{\RV{D}_2|\RV{Y}_1\RV{D}_1}(d_2|y_1,d_1)}
\end{align}
That is (at least in this case), the result of ``division by a conditional probability'' used in the fix operation is a comb. We speculate that the output of the fix operation is, in general, an $n$-comb, but we have not proven this.


\subsection[Representation of data-dependent inputs]{Representation of models with data dependent inputs}\label{sec:data_dependent_representation}

If we want to specify a ``see-do'' model where the input $\RV{D}_i$ might depend on inputs and outputs with indices lower than $i$, it might be substantially easier to talk about the comb $\prob{P}_\alpha^{\RV{Y}\combbreak \RV{D}}$ than about the conditional probability $\prob{P}_\alpha^{\RV{Y}|\RV{D}}$. The latter will have to account for possible dependence between outputs $\RV{Y}_i$ and \emph{future} inputs $\RV{D}_j$, which may not be straightforward, while by construction specification of the comb only requires the dependence of $\RV{Y}_i$ on past inputs and outputs.

The definitions of causal contractibility (Section \ref{sec:ccontracibility}) don't apply directly to the case of combs, because (for example) 
\begin{align}
    \text{swap}_{\rho} \prob{P}_C^{\RV{Y}\combbreak \RV{D}} \text{swap}_{\rho-1} \neq \prob{P}_C^{\RV{Y}_{\rho}\combbreak \RV{D}_\rho}
\end{align}

We can generalise causal contractibility to a notion that applies to generic Markov kernels, and do so in Section \ref{sec:ccontracibile_kernel}. The downside of this is that it's no longer easy to talk about what the transformations mean in terms of distributions of variables. In any case, Theorem \ref{th:response_is_cc_hdep} is an analogue of Theorem \ref{th:ciid_rep_kernel} for the case of a data-dependent model. There are two crucial differences between these theorems. First, while Theorem \ref{th:ciid_rep_kernel} constructs the hypothesis $\RV{H}$ as a function of the given variables, Theorem \ref{th:response_is_cc_hdep} extends the sample space to construct the corresponding hypothesis $\RV{G}$. If the ``given variables'' are observable, this means that $\RV{G}$ is not necessarily able to be constructed from observables. This leads to an open question:
\begin{itemize}
    \item Under what conditions is the hypothesis $\RV{G}$ (as defined in \ref{th:response_is_cc_hdep}) equal to a function of the given variables?
\end{itemize}
Secondly, Theorem \ref{th:response_is_cc_hdep} is proved without the ``auxiliary'' variable $\RV{W}$, and as a result it includes the additional assumption $\RV{H}\CI_{\prob{P}_C} (\RV{X},\RV{C})$.

\section[Causally contractible Markov kernels]{Causally contractible Markov kernels - definitions and explanation}\label{sec:ccontracibile_kernel}

The following definitions mirror the defintions Section \ref{sec:ccontracibility}, except they are stated in terms of kernel products instead of variables. This is so that they can be applied to combs, instead of limited to conditional probabilities.

\begin{definition}[kernel locality]\label{def:caus_cont_k}
A Markov kernel $\kernel{K}:X^{\mathbb{N}}\kto Y^{\mathbb{N}}$ is \emph{local} if for all $n\in \mathbb{N}$, $A_i\in \sigalg{Y}$, $(x_{[n]},x_{[n]^C})\in\mathbb{N}$ there exists $\kernel{L}:X^n\kto Y^n$ such that
\begin{align}
    \tikzfig{local_lhs} &= \tikzfig{local_rhs}\\
    &\iff\\
    \kernel{K}(\bigtimes_{i\in [n]} A_i\times Y^{\mathbb{N}}|x_{[n]},x_{[n]^C}) &= \kernel{L}(\bigtimes_{i\in [n]} A_i|x_{[n]})
\end{align}
\end{definition}

\begin{definition}[kernel exchange commutativity]\label{def:caus_exch_k}
A Markov kernel $\kernel{K}:X^{\mathbb{N}}\kto Y^{\mathbb{N}}$ \emph{commutes with exchange} if for all finite permutations $\rho:\mathbb{N}\to\mathbb{N}$, $A_i\in \sigalg{Y}$, $(x_{[n]},x_{[n]^C})\in\mathbb{N}$
\begin{align}
    \kernel{K}\mathrm{swap}_{\rho,Y} &=  \mathrm{swap}_{\rho,X} \kernel{K}\\
    &\iff\\
    \kernel{K}(\bigtimes_{i\in\mathbb{N}} A_{\rho(i)}|(x_i)_{i\in {\mathbb{N}}}) &= \kernel{K}(\bigtimes_{i\in\mathbb{N}} A_{i}|(x_{\rho(i)})_{i\in {\mathbb{N}}})
\end{align}
\end{definition}

Causal contractibility is the conjunction of both assumptions.
\begin{definition}[kernel causal contractibility]
A Markov kernel $\kernel{K}:X^{\mathbb{N}}\kto Y^{\mathbb{N}}$ is \emph{causally contractible} if it is local and commutes with exchange.
\end{definition}

\subsection[Representation of causally contractible kernels]{Representation of causally contractible Markov kernels}

The main theorem is proved in this section. Much of the work parallels work already done in Section \ref{sec:ccontracibility}.

Theorem \ref{th:equal_of_condits_k} is similar to Theorem \ref{th:equal_of_condits}, except it is stated in terms of transformations of a Markov kernel instead of in terms of conditional probabilities of variables.

\begin{theorem}[Equality of equally sized contractions]\label{th:equal_of_condits_k}
A Markov kernel $\kernel{K}:X^{\mathbb{N}}\kto Y^{\mathbb{N}}$ is \emph{causally contractible} if and only if for every $n\in \mathbb{N}$ and every $A\subset\mathbb{N}$ there exists some $\kernel{L}:X^n\kto Y^n$ such that
\begin{align}
    \kernel{K} \text{marg}_A &= \text{swap}_{[n]\leftrightarrow A} \kernel{L}\otimes \text{del}_{X^{\mathbb{N}}}
\end{align}
\end{theorem}

\begin{proof}
Only if:
By exchange commutativity
\begin{align}
    \text{swap}_{[n]\leftrightarrow A} \kernel{K} &= \kernel{K} \text{swap}_{[n]\leftrightarrow A}
\end{align}
multiply both sides by $\text{swap}_{[n]\leftrightarrow A}$ on the right and, because $\text{swap}_{[n]\leftrightarrow A}$ is its own inverse,
\begin{align}
        \text{swap}_{[n]\leftrightarrow A} \kernel{K}\text{swap}_{[n]\leftrightarrow A} &= \kernel{K}
\end{align}
so
\begin{align}
    \kernel{K}\text{marg}_A &= \text{swap}_{[n]\leftrightarrow A} \kernel{K}\text{swap}_{[n]\leftrightarrow A}\text{marg}_A\\
    &= \text{swap}_{[n]\leftrightarrow A} \kernel{K}\text{marg}_{[n]}
\end{align}
By locality, there exists some $\kernel{L}:X^n\kto Y^n$ such that
\begin{align}
    \kernel{K} \text{marg}_{[n]} &= \kernel{K}(\text{id}_{[n]}\otimes \text{del}_{X^{\mathbb{N}}})\\
     &= \kernel{L}\otimes \mathrm{del}_{X^{\mathbb{N}}}
\end{align}
If:
Taking $A=[n]$ for all $n$ establishes locality.

For exchange commutativity, note that for all $x\in X^{\mathbb{N}}$, $n\in\mathbb{N}$, we have
\begin{align}
    \text{swap}_{A\leftrightarrow [n]} \kernel{K} \text{marg}_A &= \text{swap}_{A\leftrightarrow [n]} \kernel{K} \text{swap}_{A\leftrightarrow [n]} (\text{id}_{[n]}\otimes \text{del}_{X^{\mathbb{N}}})\\
     &= \kernel{K} \text{marg}_{[n]}\\
     &= \kernel{K}(\text{id}_{[n]}\otimes \text{del}_{X^{\mathbb{N}}})
\end{align}
Then by Lemma \ref{lem:infinitely_extended_kernels}
\begin{align}
    \text{swap}_{A\leftrightarrow [n]} \kernel{K} \text{swap}_{A\leftrightarrow [n]} &= \kernel{K}
\end{align}
Consider an arbitrary finite permutation $\rho:\mathbb{N}\to \mathbb{N}$. $\rho$ can be decomposed into a finite set of cyclic permutations on disjoint orbits. Each cyclic permutation is simply the composition of some set of transpositions, and so $\rho$ itself can be written as a composition of a sequence of transpositions. Thus for any finite $\rho:\mathbb{N}\to\mathbb{N}$
\begin{align}
    \text{swap}_{\rho} \kernel{K} \text{swap}_{\rho} &= \kernel{K}
\end{align}
\end{proof}

Theorem \ref{th:table_rep_kernel_k} is similar to Theorem \ref{th:table_rep_kernel}, except the latter uses a variable $\RV{Y}^D$ explicitly defined on the sample space, while Theorem \ref{th:table_rep_kernel_k} simply says an appropriate probability distribution exists. 

\begin{theorem}\label{th:table_rep_kernel_k}
A Markov kernel $\kernel{K}:X^{\mathbb{N}}\kto Y^{\mathbb{N}}$ is causally contractible if and only if there exists a column exchangeable probability distribution $\mu \Delta(Y^{|X|\times \mathbb{N}})$ such that
\begin{align}
    \kernel{K} &= \tikzfig{lookup_representation_kernel}\label{eq:lup_rep_kernel_k}\\
    &\iff\\
    \kernel{K}(A|(x_i)_{i\in \mathbb{N}}) &= \mu \Pi_{(x_i i)_{i\in\mathbb{N}}}(A)\forall A\in \sigalg{Y}^{\mathbb{N}}
\end{align}
Where $\Pi_{(d_i i)_{i\in\mathbb{N}}}:Y^{|X|\times \mathbb{N}}\to Y^{\mathbb{N}}$ is the function 
\begin{align}
    (y_{j i})_{j,i \in X\times  \mathbb{N}}\mapsto (y_{d_i i})_{i\in \mathbb{N}}
\end{align}
that projects the $(x_i,i)$ indices of $y$ for all $i\in \mathbb{N}$, and $\prob{F}_{\text{ev}}$ is the Markov kernel associated with the evaluation map
\begin{align}
    \text{ev}:X^\mathbb{N}\times Y^{X\times \mathbb{N}}&\to Y\\
    ((x_i)_\mathbb{N},(y_{ji})_{j,i\in X\times \mathbb{N}})&\mapsto (y_{x_i i})_{i\in \mathbb{N}}
\end{align}
\end{theorem}

\begin{proof}
Only if:
Choose $e:=(e_i)_{i\in\mathbb{N}}$ such that $e_{i+|X|j}$ is the $i$th element of $X$ for all $i,j\in \mathbb{N}$.

Define
\begin{align}
    \mu(\bigtimes_{(i,j)\in X\times \mathbb{N}} A_{ij}):=\kernel{K}(\bigtimes_{(i,j)\in X\times \mathbb{N}} A_{ij}|e)& \forall A_{ij}\in \sigalg{Y}
\end{align}

Now consider any $x:=(x_i)_{i\in \mathbb{N}}\in X^{\mathbb{N}}$. By definition of $e$, $e_{x_i i}=x_i$ for any $i,j\in \mathbb{N}$.

Define
\begin{align}
    \prob{Q}:X^{\mathbb{N}}\kto Y^{\mathbb{N}}\\
    \prob{Q}:= \tikzfig{lookup_representation_kernel}
\end{align}
and consider some $A\subset \mathbb{N}$, $|A|=n$ and $B:= (x_i,i))_{i\in A}$. Note that the subsequence of $e$ indexed by $B$, $e_B:=(e_{x_i i})_{i\in A}=x_A$. Thus given the swap map $\mathrm{swap}_{A\leftrightarrow B}:\mathbb{N}\to\mathbb{N}$ that sends the first element of $A$ to the first element of $B$ and so forth, $\mathrm{swap}_{A\leftrightarrow B} (e_B) = x_A$. For arbitrary $\{C_i\in \sigalg{Y}|i\in A\}$, define $C_A:=\mathrm{swap}_{[n]\leftrightarrow A} (\times_{i\in [n]} C_i\times Y^{\mathbb{N}})$. Then, for arbitrary $x\in X^{\mathbb{N}}$
\begin{align}
    \prob{Q}(C_A|x) &= \mu (\mathrm{ev}_x^{-1}(C_A))\label{eq:q_mu_rel_k}
\end{align}

The argument of $\mu$ is
\begin{align}
    \mathrm{ev}_x^{-1}(C_A)&=\{(y_{ji})_{j,i\in X\times\mathbb{N}}|(y_{x_i i})_{i\in\mathbb{N}}\in C_A\}\\
    &= \bigtimes_{i\in \mathbb{N}} \bigtimes_{j\in X} D_{ji}
\end{align}
where
\begin{align}
    D_{ji} = \begin{cases}
        C_i & (j,i)\in B\\
        Y & \text{otherwise}
    \end{cases}
\end{align}
and so
\begin{align}
    \text{swap}_{A\leftrightarrow B} (\mathrm{ev}_x^{-1}(C_A)) &= C_A\label{eq:swap_select_relation_k}
\end{align}

Substituting Equation \ref{eq:swap_select_relation_k} into \ref{eq:q_mu_rel_k}
\begin{align}
    \prob{Q}(C_A|x) &= \mu \text{swap}_{A\leftrightarrow B} (C_A)\\
    &= \kernel{K} \text{swap}_{A\leftrightarrow B} (C_A|e)\\
    &= \kernel{K}\text{swap}_{A\leftrightarrow B} (C_A|e_B,\text{swap}_{B\leftrightarrow A}(x)_B^C)&\text{by locality}\\
    &= \kernel{K}\text{swap}_{A\leftrightarrow B} (C_A|\text{swap}_{B\leftrightarrow A}(x))\\
    &= \text{swap}_{B\leftrightarrow A} \kernel{K}\text{swap}_{A\leftrightarrow B} (C_A|x)\\
    &= \kernel{K}(C_A|x)&\text{by commutativity of exchange}
\end{align}

Because this holds for all $x$, $A\subset\mathbb{N}$, by Lemma \ref{lem:infinitely_extended_kernels}

\begin{align}
    \prob{Q} &= \kernel{K}
\end{align}

Next we will show $\mu$ is column exchangeable. Consider any column swap $\text{swap}_{c}:X\times \mathbb{N}\to X\times \mathbb{N}$ that acts as the identity on the $X$ component and a finite permutation on the $\mathbb{N}$ component. From the definition of $e$, $\text{swap}_c(e)=e$. Thus by commutativity of exchange, for any $A\in \sigalg{Y}^{\mathbb{N}}$
\begin{align}
 \kernel{K}(A|e) &= \text{swap}_c\kernel{K}\text{swap}_c(A|e)\\
 &= \kernel{K}\text{swap}_c(A|\text{swap}_c(e))\\
 &= \kernel{K}\text{swap}_c(A|e)
\end{align}


If:
Suppose 
\begin{align}
    \kernel{K} &= \tikzfig{lookup_representation_kernel}
\end{align}
where $\mu$ is column exchangeable, and consider any two $x,x'\in X^{\mathbb{N}}$ such that some subsequences are equal $x_S=x'_T$ with $S,T\subset \mathbb{N}$ and $|S|=|T|=[n]$.

For any $\{A_i\in\sigalg{Y}|i\in S\}$, let $A_S = \text{swap}_{[n]\leftrightarrow S} \times_{i\in [n]} A_i\times Y^{\mathbb{N}}$, $A_T = \text{swap}_{S\leftrightarrow T} (A_S)$, $B=(x_i i)_{i\in S}$ and $C=(x_i i)_{i\in T}=(x_{\text{swap}_{S\leftrightarrow T}}(i) i)_{i\in S}$. By Equations \ref{eq:q_mu_rel_k} and \ref{eq:swap_select_relation_k}
\begin{align}
    \kernel{K}(A_S|x) &= \mu \text{swap}_{S\leftrightarrow B} (A_S)\\
    &= \mu \text{swap}_{T\leftrightarrow C} (A_T)&\text{ by column exchangeability of }\mu\\
    &= \kernel{K}(A_T|\text{swap}_{S\leftrightarrow T}(x))\\
    &=  \text{swap}_{S\leftrightarrow T}\kernel{K}(A_T| x)\\
    &= \text{swap}_{S\leftrightarrow T} \kernel{K} \text{swap}_{S\leftrightarrow T} (A_S| x)
\end{align}
so $\kernel{K}$ is causally contractible by Theorem \ref{th:equal_of_condits_k}.
\end{proof}



\begin{lemma}[Exchangeable table to response functions]\label{lem:extabl_to_respf_k}
Given $\kernel{K}:X^{\mathbb{N}}\kto Y^{\mathbb{N}}$, $X$ and $Y$ standard measurable, if
\begin{align}
    \kernel{K} &= \tikzfig{lookup_representation_kernel_k}
\end{align}
for $\mu\in \Delta(Y^{X\times\mathbb{N}})$ column exchangeable, then defining $(H,\sigalg{H}):=\mathcal{M}_1(Y^{X\times\mathbb{N}})$ there is some $\RV{H}:Y^{X\times\mathbb{N}}\to H$ and $\kernel{L}:H\times X\kto Y$ such that
\begin{align}
    \kernel{K} &= \tikzfig{do_model_representation_kernel_k}
\end{align}
\end{lemma}

\begin{proof}
As a preliminary, we will show
\begin{align}
    \kernel{F}_{\mathrm{ev}} &= \tikzfig{lookup_rep_intermediate_kernel}\label{eq:ev_alternate_rep_k}
\end{align}
where  $\mathrm{evs}_{Y^X\times X}:Y^X\times X\to Y$ is the single-shot evaluation function
\begin{align}
    (x,(y_i)_{i\in X})\mapsto y_x
\end{align}

Recall that $\mathrm{ev}$ is the function
\begin{align}
    ((x_i)_\mathbb{N},(y_{ji})_{j,i\in X\times \mathbb{N}})&\mapsto (y_{x_i i})_{i\in \mathbb{N}}
\end{align}
By definition, for any $\{A_i\in\sigalg{Y}|i\in \mathbb{N}\}$
\begin{align}
    \kernel{F}_{\mathrm{ev}}(\bigtimes_{i\in \mathbb{N}}A_i|(x_i)_\mathbb{N},(y_{ji})_{i\in X\times \mathbb{N}}) &= \delta_{(y_{x_i i})_{i\in \mathbb{N}}}(\bigtimes_{i\in \mathbb{N}}A_i)\\
        &= \prod_{i\in \mathbb{N}} \delta_{y_{x_i i}} (A_i)\\
        &= \prod_{i\in \mathbb{N}} \kernel{F}_{\text{evs}} (A_i|x_i,(y_{ji})_{j\in X})\\
        &= \left(\bigotimes_{i\in\mathbb{N}} \kernel{F}_{\mathrm{evs}} \right)(\bigtimes_{i\in \mathbb{N}}A_i|(x_i)_\mathbb{N},(y_{ji})_{j\in X\times \mathbb{N}})
\end{align}
which is what we wanted to show.

Define $\kernel{M}:H\kto Y^X$ by $\kernel{M}(A|h)=h(A)$ for all $A\in\sigalg{Y}^X$, $h\in H$. By the column exchangeability of $\mu$, from \citet[Prop. 1.4]{kallenberg_basic_2005} there is a directing random measure $\RV{H}:Y^{X\times\mathbb{N}}\to H$ such that
\begin{align}
    \mu(\kernel{F}_{\RV{H}}\otimes \mathrm{id}_{Y^{X\times\mathbb{N}}}) &= \tikzfig{de_finetti_representation_kernel}\label{eq:df_rep_mu_k}\\
    &\iff\\
    \mu(\bigtimes_{i\in \mathbb{N}} A_i\times B) &= \int_B \prod_{i\in \mathbb{N}} \kernel{M}(A_i|h) \mu\kernel{F}_{\RV{H}}(\mathrm{d}h)&\forall A_i\in\sigalg{Y}^X
\end{align}

By Equations \ref{eq:lookup_representation_k} and \ref{eq:ev_alternate_rep_k}
\begin{align}
    \kernel{K} &= \tikzfig{do_model_representation_kernel_pre}\\
    &:= \tikzfig{do_model_representation_kernel_k}\label{eq:lup_rep_combined_k}
\end{align}
Where we can connect the copied outputs of $\mu\kernel{F}_{\RV{H}}$ to the inputs of each $\kernel{M}$ ``inside the plate'' as the plates in Equations \ref{eq:ev_alternate_rep} and \ref{eq:df_rep_mu} are equal in number and each connected wire represents a single copy of $Y^D$.
\end{proof}

Theorem \ref{th:ciid_rep_kernel_k} is similar to Theorem \ref{th:ciid_rep_kernel}, but it is stated without the use of variables.

\begin{theorem}\label{th:ciid_rep_kernel_k}
Given a kernel $\kernel{K}:X^{\mathbb{N}}\kto Y^{\mathbb{N}}$, let $(H,\sigalg{H}):=\mathcal{M}_1(Y^X)$ be the set of probability distributions on $(Y^X,\sigalg{Y}^X)$. $\kernel{K}$ is causally contractible if and only if there is some $\mu\in \Delta(H)$ and $\kernel{L}:H\times X\kto Y$ such that 
\begin{align}
    \kernel{K} &= \tikzfig{do_model_representation_kernel}\\
    &\iff\\
    \kernel{K}(\bigtimes_{i\in\mathbb{N}}A_i|(x_i)_{i\in\mathbb{N}}) &= \int_H \prod_{i\in\mathbb{N}} \kernel{L}(A_i|h,x_i)\mu(\mathrm{d}h)
\end{align}
\end{theorem}

\begin{proof}
Only if:
By Theorem \ref{th:table_rep_kernel_k}, we can represent the conditional probability $\kernel{K}$ as
\begin{align}
        \kernel{K} &= \tikzfig{lookup_representation_kernel}\label{eq:lookup_representation_k}
\end{align}
where $\nu\in \Delta(Y^{X\times\mathbb{N}})$ is column exchangeable.

Applying Lemma \ref{lem:extabl_to_respf_k} yields the desired result.

If:
By assumption, for any $\{A_i\in \sigalg{Y}|i\in\mathbb{N}\}$, $x:=(x_i)_{i\in\mathbb{N}}\in X^{\mathbb{N}}$
\begin{align}
    \kernel{K}(\bigtimes_{i\in \mathbb{N}} A_i|x) &= \int_H \prod_{i\in \mathbb{N}}\kernel{L}(A_i|h,x_i)\mu(\mathrm{d}h)
\end{align}

Consider any $S,T\subset\mathbb{N}$ with $|S|=|T|$, and define $A_S:=\times_{i\in\mathbb{N}} B_i$ where $B_i=Y$ if $i\not\in S$, otherwise $A_i$ is an arbitrary element of $\sigalg{Y}$. Define $A_T:=\times_{i\in\mathbb{N}} B_{\mathrm{swap}_{S\leftrightarrow T}(i)}$.

\begin{align}
    \kernel{K}(A_S|x) &= \int_H \prod_{i\in S}\kernel{L}(A_i|h,x_i)\mu(\mathrm{d}h)\\
                      &= \int_H\prod_{i\in T}\kernel{L}(A_i|h,x_{\mathrm{swap}_{S\leftrightarrow T}(i)})\mu(\mathrm{d}h)\\
                      &= \mathrm{swap}_{S\leftrightarrow T}\kernel{K}(A_T|x)\\
                      &= \mathrm{swap}_{S\leftrightarrow T}\kernel{K}\mathrm{swap}_{S\leftrightarrow T}(A_S|x)
\end{align}
So by Theorem \ref{th:equal_of_condits}, $\kernel{K}$ is causally contractible.
\end{proof}

Theorem \ref{th:ciid_rep_kernel} is the main result of this section. It shows that a causally contractible Markov kernel $X^{\mathbb{N}}\kto Y^{\mathbb{N}}$ is representable as a ``prior'' $\mu\in \Delta(H)$ and a ``parallel product'' of Markov kernels $H\times X\kto Y$. These will be the response conditionals when Theorem \ref{th:ciid_rep_kernel} is applied to probability set models. It is a data-dependent approximate analogue of Theorem \ref{th:ciid_rep_kernel}.

\begin{theorem}\label{th:response_is_cc_hdep}
Given a sequential input-output model $(\prob{P}_C',\RV{D}',\RV{Y}')$ on $(\Omega,\sigalg{F})$, then $\prob{P}_C^{\prime \RV{Y}'\combbreak \RV{D}'}$ is causally contractible if and only if there is a latent extension $\prob{P}_C$ of $\prob{P}_C'$ to $(\Omega\times H,\sigalg{F}\otimes\sigalg{Y}^{D\times\mathbb{N}})$ with hypothesis $\RV{H}:\Omega\times H\to H$ such that $\RV{Y}_i\CI^e_{\prob{P}_C'} (\RV{Y}_{<i},\RV{X}_{<i},C)|(\RV{X}_i,\RV{H})$ and $\prob{P}_C^{\RV{Y}_i|\RV{X}_i\RV{H}}=\prob{P}_C^{\RV{Y}_j|\RV{X}_j\RV{H}}$ for all $i,j\in \mathbb{N}$ and $\RV{H}\CI_{\prob{P}_C} (\RV{X},\RV{C})$.
\end{theorem}

\begin{proof}
If:
By assumption, there is some $\kernel{L}:H\times D\kto Y$ such that
\begin{align}
    \prob{P}_C^{\RV{Y}_i|\RV{H}\RV{D}_i} &= \kernel{L}
\end{align}
and $\RV{Y}_i\CI^e_{\prob{P}_C} (\RV{Y}_{<i},\RV{D}_{<i})|(\RV{D}_i,\RV{H})$. Thus
\begin{align}
    \prob{P}_C^{\RV{Y}_i|\RV{H}\RV{D}_i\RV{Y}_{<i}\RV{D}_{<i}} &= \kernel{L}\otimes \text{erase}_{Y^{i-1}\times D^{i-1}}
\end{align}
and so
\begin{align}
    \prob{P}_C^{\RV{Y}\combbreak \RV{D}} &= \tikzfig{do_model_representation}\label{eq:comb_representation_w_CI_k}
\end{align}
and so by Theorem \ref{th:ciid_rep_kernel_k}, $\prob{P}_C^{\RV{Y}\combbreak \RV{D}}$ is causally contractible.

Only if:
First, define the extension $\prob{P}_C$. From Theorem \ref{th:ciid_rep_kernel_k} and causal contractibility of $\prob{P}_C^{\prime \RV{Y}'\combbreak \RV{D}'}$ there is some $H$, $\mu\in \Delta(H)$ and $\kernel{L}:H\times D\kto Y$ such that
\begin{align}
    \prob{P}_C^{\prime \RV{Y}'\combbreak\RV{D}'} &= \tikzfig{do_model_representation_mu_k} 
\end{align}
thus, by the definition of the comb insert operation
\begin{align}
    \prob{P}_\alpha^{\prime\RV{D}'_{[n]} \RV{Y}'_{[n]}} &= \prob{P}_\alpha^{\RV{D}_1}\odot \text{insert}(\prob{P}_\alpha^{\prime \RV{D}'_{[2,n]}\combbreak\RV{Y}'_{[n-1]}}, \prob{P}_C^{\prime \RV{Y}'_{[n]}\combbreak\RV{D}'_{[n]}}) 
\end{align}
Let
\begin{align}
    \prob{P}_C^{\RV{Y}_i|\RV{H}\RV{D}_i} &= \kernel{L}\label{eq:identical_response_assumption_k}
\end{align}
and let $\RV{Y}_i\CI^e_{\prob{P}_C} (\RV{Y}_{<i},\RV{D}_{<i})|(\RV{D}_i,\RV{H})$, and for all $\alpha$ set $\prob{P}_\alpha^{\RV{W}|\RV{DY}}=\prob{P}_\alpha^{\prime \RV{W}'|\RV{D'Y'}}$ for all $\RV{W}':\Omega\to W$ and $\prob{P}_\alpha^{\RV{D}_i|\RV{Y}_{<i}\RV{D}_{<i}}=\prob{P}_\alpha^{\prime \RV{D}_i'|\RV{Y}_{<i}'\RV{D}_{<i}''}$.

It remains to be shown that $\prob{P}_\alpha^{\RV{DY}}=\prob{P}_\alpha^{\prime \RV{DY}}$.

By Equation \ref{eq:identical_response_assumption_k} and $\RV{Y}_i\CI^e_{\prob{P}_C} (\RV{Y}_{<i},\RV{D}_{<i})|(\RV{D}_i,\RV{H})$, it follows (for identical reasons as Equation \ref{eq:comb_representation_w_CI_k}) that
\begin{align}
    \prob{P}_C^{\RV{Y}\combbreak \RV{D}} &= \tikzfig{do_model_representation}\\
    &= \tikzfig{do_model_representation_mu}\\
    &= \prob{P}_C^{\prime \RV{Y}'\combbreak\RV{D}'}
\end{align}

And so for all $n\in \mathbb{N}$
\begin{align}
    \prob{P}_\alpha^{\RV{D}_{[n]} \RV{Y}_{[n]}} &=  \prob{P}_\alpha^{\RV{D}_1}\odot \text{insert}(\prob{P}_\alpha^{ \RV{D}_{[2,n]}\combbreak\RV{Y}_{[n-1]}}, \prob{P}_C^{\prime \RV{Y}_{[n]}\combbreak\RV{D}_{[n]}}) \\
    &= \prob{P}_\alpha^{\RV{D}_1}\odot \text{insert}(\prob{P}_\alpha^{\prime \RV{D}'_{[2,n]}\combbreak\RV{Y}'_{[n-1]}}, \prob{P}_C^{\prime \RV{Y}'_{[n]}\combbreak\RV{D}'_{[n]}}) \\
    &= \prob{P}_\alpha^{\prime\RV{D}'_{[n]} \RV{Y}'_{[n]}}
\end{align}
\end{proof}

\section{Discussion}\label{sec:discussion}

The work in this chapter is motivated by the aim of better understanding the role of repeatability or symmetry in causal inference problems. It's technical contributions are the introduction of the structural assumption of conditionally independent and identical response functions and the symmetry of causal models we call causal contractibility, and relating these two ideas. 

Causal contractibility is a complicated symmetry, and in particular the relevant symmetry might hold only after conditioning on an auxiliary variable $\RV{W}$. One open questions we've posed is whether this $\RV{W}$ can generally be taken to be approximated by a suitable subsequence of the available data (see \ref{pgph:two_kinds}). If it can, this might help to understand the role of the variable $\RV{W}$, and also the applicability of conditions like data interchangeability (see Equation \ref{eq:interchangeability_of_conditioning}).

The theorem for data-dependent models (Theorem \ref{th:response_is_cc_hdep}) has a number of gaps compared to the data-independent theorem -- in particular, it demands that the inputs are independent of the hypothesis and defines the hypothesis in terms of a latent extension of the sample space. The fact that probability combs play a key role in this theorem, and seem to play important roles in other important theorems in causal inference suggests that a deeper understanding of combs might be broadly useful in causal inference.

Like identifiability in other causal frameworks, causal contractibility is an ideal that is not always applicable. A key question is what weaker assumptions can be made that are more widely applicable. In \ref{pgph:weaker_assumptions}, we suggest that ``partial causal contractibility'' might be one such assumption to explore.

% \section{Assessing decision problems for exchange commutativity}

% Exchange commutativity is a condition that, if it holds, allows a decision maker to use the map $D\kto Y$ calculated from relative frequencies to determine the optimal course of action. The question is: when should a decision maker consider this assumption reasonable?, confronted with a decision problem actually adopt a causally contractible model $\prob{P}_C$ to help them make their decision? This is not an easy question for several reasons. Two of these are:
% \begin{itemize}
%     \item The kind of symmetry required by exchange commutativity seems to us much harder to intuit than the kind of symmetry required by regular exchangeability
%     \item The conditions of exchange commutativity and locality must hold for each choice in $C$
% \end{itemize}

% ``Ordinary'' exchangeability is often considered to be appropriate when modelling a measurement procedure that consists of a sequence of indistinguishable sub-procedures. A common example is a sequence of coin flips -- there is (usually) no reason to consider any coin flip to differ in any important way from any other. Thus, one can reason, swapping the labels of the coin flips yields a measurement procedure that is effectively identical. It follows that the model should be unchanged under a permutation of the variables representing the sequence of flips -- that is, it should be exchangeable\footnote{As \citet[pg. 461]{walley_statistical_1991} points out, the conclusion of exchangeability also requires the assumption that the measurement procedure should be modeled with a single probability distribution, which is an assumption that is being made in this chapter}. The basic judgement call is then: the subprocedures for each coin flip are effectively identical.

% Exchange commutativity requires a different kind of judgement. A common causal inference example features a decision procedure that yields a sequence of (treatment, outcome). Exchange commutativity asks us to compare the original procedure with an arbitrary procedure that shuffles the pairs. Then, \emph{given any fixed vector of treatment values}, the resulting pair of procedures must be effectively indistinguishable. Full causal contractibility adds the requirement that, comparing two procedures of this type and restricting our attention to a subsequence of outcomes, we can ignore any differences between treatment vectors that do not correspond to the subsequence of interest.

% This is not particularly easy to think about! \citet{greenland_identifiability_1986} mention the condition that the treatments of different patients could be swapped without changing the distribution over outcomes. This can be interpreted as saying: given two choices that induce deterministic treatment vectors, if the vector induced by the first is a permutation of the vector induced by the second, the resulting distributions of outcomes (appropriately permuted) should be identical. This is a consequence of exchange commutativity, but it is not equivalent: treatments (or ``inputs'') may not be deterministic for all choices, in which case it's not clear what ``swapping treatments'' means. If it's a hypothetical action that swaps treatments (see the discussion at the end of \ref{sec:whats_the_point}), it seems that some theory is needed to say what equivalence under such hypothetical actions imply for the actual choices to be evaluated.

% A further complication is due to the fact that, by necessity, a probability set $\prob{P}_C$ models a measurement procedure for each of a set of choices $C$. Someone constructing a model $\prob{P}_C$ to help them deal with decision problem may want to reason that their state of knowledge after selecting some choice $\alpha\in C$ is the same as their state of knowledge when they are constructing $\prob{P}_C$. That is, they don't want to worry about whether their choice ``depends on anything''. The fact that they don't want to worry about this doesn't mean that they don't have to! The theory of probability sets is formal, and it can be augmented with decision rules to yield a formal theory of making decisions, but the correspondence between $\prob{P}_C$ and the ``real things that constitute the decision problem'' is a judgement call, and it is possible to make poor calls. Example \ref{ex:confounding} is an example illustrating this. There are ways to deal with actions that ``depend on things'', see for example \citet{gallow_causal_2020}'s discussion of ``managing the news'', but the question of constructing appropriate models seems hard enough without the extra complication.

% Individual-level causal contractibility is an attempt to specify a method for model construction that involves judgements that are (mostly) easier to think about than regular causal contractibility and that may sometimes yield regular causal contractibility as a result (Theorem \ref{th:cc_ind_treat}). Notably, the assumption of individual-level causal contractibility can, under certain conditions, imply that a model is causally contractible conditional on an ``unobserved'' variable, analogous to the familiar assumption of hidden confounding. 