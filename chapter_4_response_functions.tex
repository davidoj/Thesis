%!TEX root = main.tex

\chapter{Decision problems with repeatable phenomena}\label{ch:evaluating_decisions}

Chapter \ref{ch:tech_prereq} introduced probability sets as generic tools for causal modelling, while Chapter \ref{ch:2p_statmodels} examined how probability set models fit into existing work addressing the question of how to construct mathematical models of decision problems. Both chapters deal with probability sets with minimal amounts of structure. Section \ref{sec:cons_to_sdp} introduced \emph{see-do} models, which feature observations, consequences, decisions and hypotheses and some assumptions relating these different elements, but no particular assumptions are made about (for example) the form of the observations. Causal models that solve ``practical problems'' typically have a lot more structure than this.

This chapter is concerned with examining the basic assumptions that underpin practical causal models. Causal and non-causal statistical problems usually concern repeatable phenomena -- that is, the measurement procedure that is being modeled is assumed to break down into a sequence of subprocedures that are, in some sense, all similar to one another. In non-causal problems, the sense of similarity is sometimes expressed by the assumption of \emph{exchangeability}, which is the assumption that swapping the subprocedures does not have any effect on the appropriate model of the phenomenon. This assumption is not applicable to causal problems, because generally different choices will lead to different actions being taken for different subprocedures. The key idea in this chapter is the idea of a \emph{repeatable response function} -- this is the idea that (roughly speaking) each subprocedure features ``input'' and ``output'' variables such that the mapping from input to output is interchangeable.

The key result is the representation of a sequence of subprocedures as a mixture of repeatable response functions is equivalent to the assumption of \emph{causal contractibility}, a generalisation of exchangeability. In the general case, where the input for the $i$th subprocedure can depend on the results of the subprocedures prior to $i$, causal contractibility is required of a \emph{comb} induced by the probability set, a generalisation of the notion of a conditional probability. If inputs to each subprocedure are independent of data produced prior, then the comb reduces to a conditional probability. Causal contractibility is the union of an assumption of a particular kind of exchangeability, and a conditional independence assumption, both of which can be found separately in prior literature.

Causal contractibility is a more complicated assumption than exchangeability, and it is not obvious how to assess when it is appropriate or not. One implications of causal contractibility is the equality of reduced conditionals (Theorem \ref{th:equal_of_condits}), which is argued to rule out causal contractibility in some situations where passive observations are mixed with data from active intervention. A second contribution is to show that when subprocedures are associated with \emph{interchangeable unique identifiers} and the inputs are deterministically controlled, then causal contractibility holds.

Causal contractibility is a very strong assumption, and it appears to generally be unacceptable assumption when data from passive observations is mixed with data from active intervention. A weaker but more widely applicable assumption is examined: ``what will be done has been done already, and will be done again'' \todo{Ecclesiastes' assumption?}. This is the assumption that, no matter what choice is made, some subsequence of observations can be found that are distributed identically to the consequences. This assumption leads to a certain kind of ``unobserved confounder''.

\section{Relevance to previous work}

This chapter draws on three different lines of work. The first is the study of representations of symmetric of probability models. The equivalence between infinite exchangeable probability models and mixtures of independent and identically distributed models was shown by \cite{de_finetti_foresight_1992}. This result has been extended in many ways, including to finite sequences \citet{kerns_definettis_2006,diaconis_finite_1980} and for partially exchangeable arrays \citet{aldous_representations_1981}. A comprehensive overview of results is presented in \citet{kallenberg_probabilistic_2005}. This work is only engaged shallowly with this literature, but the idea that symmetries of probabilistic models may imply representability as mixtures of ``fixed but unknown'' models is crucial, as is basic result of De Finetti.

The second line of work is the study of exchangeability-like assumptions in causal models in particular. \citet{dawid_decision-theoretic_2020} defines \emph{post-treatment exchangeability}, closely related to \emph{exchange commutativity} (Definition \ref{def:caus_exch}), one of the two conditions constituting causal contractibility. \citet{greenland_identifiability_1986} discusses the assumption of ``exchangeability of individuals'' in a medical experiment that also suggests the key idea of exchange commutativity. \citet{banerjee_chapter_2017} also mention the condition that ``subjects are exchangeable conditional on covariates, so that experiments identical up to a permutation of labels are equivalent from the perspective of the experimenter'', which is similarly suggestive of exchange commutativity. While both of these are suggestive of exchange commutativity, exchanging \emph{individuals} is a transformation of measurement procedures, not an operation defined on a probability model, and so it does not automatically imply any particular properties of the model. Exchange commutativity, on the other hand, is a symmetry of Markov kernels which might be appealing in situations where measurement procedures with individuals exchanged are regarded as essentially the same. \citet{rubin_causal_2005} discusses the assumption of the exchangeability of potential outcomes.

The other component of causal contractibility is \emph{consequence locality} (Definition \ref{def:caus_cont}). This is also suggested by existing work -- in particular, the stable unit treatment distribution assumption (SUTDA) in \citet{dawid_decision-theoretic_2020}, and the stable unit treatment value assumption (SUTVA) in \citep{rubin_causal_2005}:
\begin{blockquote}
(SUTVA) comprises two sub-assumptions. First, it assumes that \emph{there is no interference between units (Cox 1958)}; that is, neither $Y_i(1)$ nor $Y_i(0)$ is affected by what action any other unit received. Second, it assumes that \emph{there are no hidden versions of treatments}; no matter how unit $i$ received treatment $1$, the outcome that would be observed would be $Y_i(1)$ and similarly for treatment $0$.
\end{blockquote}

Finally, the idea of \emph{combs} in probabilistic models was first proposed by \citet{chiribella_quantum_2008} and an application to causal models was developed by \citet{jacobs_causal_2019}.

\section{Repeatable Response Functions}\label{sec:response_functions}

Start with a sequence of variable pairs $(\RV{X}_i,\RV{Y}_i)_{i\in \mathbb{N}}$ where $\RV{X}_i$ is the $i$th ``input'' and $\RV{Y}_i$ is the corresponding ``output'', each taking values in $X$ and $Y$ respectively. A ``repeatable response function'' is a probabilistic mapping $X\kto Y$ that is identical for all $(\RV{X}_i,\RV{Y}_i)$ pairs. Repeatable response functions are in general be unknown, in which case (under a Bayesian model $\prob{P}_C$), the response function is \emph{not} the conditional $\prob{P}_C^{\RV{Y}_i|\RV{X}_i}$ but rather the conditional $\prob{P}_C^{\RV{Y}_i|\RV{X}_i\RV{H}}$ where $\RV{H}$ is an unobserved hypothesis variable. ``Repeatability'' also means that the same response function can be obtained no matter which actions have already been taken. That is, $\RV{Y}_i$ is independent of previous input-output pairs when conditioned on $\RV{H}$ and $\RV{X}_i$.

The result of this section is that the pairs in a sequence $(\RV{X},\RV{Y}):=(\RV{X}_i,\RV{Y}_i)_{i\in \mathbb{N}}$ modeled by $\prob{P}_C$ are related by repeatable repeatable response functions if and only if there is a causally contractible \emph{uniform comb} $\prob{P}_C^{\RV{Y}\combbreak\RV{X}}$. Combs are a generalisation of conditional probabilities and, and if we assume that actions are independent of previous data ($\RV{X}_i\CI_{\prob{P}_C}^e \RV{Y}_{<i} C|\RV{X}_{<i}$), this reduces to the assumption that the uniform conditional distribution $\prob{P}_C^{\RV{Y}|\RV{X}}$ is causally contractible.

Because combs are unfamiliar, this section is structured so that the ``data-independent actions'' case is introduced first. Specifically, the representation theorem is proven for general Markov kernels in Section \ref{sec:ccontracibility}, and applied to models $\prob{P}_C$ with data-independent actions in Section \ref{sec:data_independent_actions}. Subsequently, combs are introduced in Section \ref{sec:data_dependent}, and the general result applied to models with data-dependent actions. The following section, Section \ref{sec:assessing}, discusses questions related to when the assumption of causal contractibility might be held to apply to a particular problem, as well as introducing ``Ecclesiastes' assumption'', a weaker assumption than causal contractibility which could be applied to problems where observations and active interventions are mixed.

% Another way to see where we are going is to consider graphical statements of our and De Finetti's result.

% Take $S=\{0,1\}$ and identify the space $\Delta(S)$ of probability measures on $S$ with the interval $[0,1]$. De Finetti showed that any infinite exchangeable probability measure $\prob{P}_\alpha$ on $\{0,1\}^\mathbb{N}$ can be represented by a prior $\prob{P}_\alpha^{\RV{H}}\in [0,1]$ for some $\RV{H}:\Omega\to H$ and a conditional probability $\prob{P}^{\RV{S}_0|\RV{H}}:[0,1]\kto \{0,1\}$ such that

% \begin{align}
%     \prob{P}_\alpha &= \tikzfig{de_finetti_rep0}\label{eq:definettirep}
% \end{align}

% Here $\prob{P}^{\RV{S}_0|\RV{H}}$ can be defined concretely by $\prob{P}^{\RV{S}_0|\RV{H}}(1|h)=h$. Equivalently, the probability gap model on $S^\mathbb{N}$ defined by the assumption of exchangeability is equivalent to the probability gap model defined by the conditional probability

% \begin{align}
%     \prob{P}^{\RV{S}|\RV{H}} = \tikzfig{de_finetti_conditional}
% \end{align}

% That is, there is some hypothesis $\RV{H}$ and conditional on $\RV{H}$ the measurements are independent and identically distributed. The proof of this is constructive -- $\RV{H}$ is a function of $\RV{S}$.


\subsection{Representation of causally contractible Markov kernels}\label{sec:ccontracibility}

Here a representation theorem is proved for causally contractible Markov kernels. First, causal contractibility is defined, which is the conjunction of the assumptions of \emph{locality} and \emph{exchange commutativity}. 

The intuitive basis of the assumptions is easier to see for Markov kernels with just two input-output pairs. In that simplified case, exchange commutativity for two inputs and outputs is given by the following equality:
\begin{align}
    \tikzfig{commutativity_of_exchange}
\end{align}
It expresses the idea that swapping the inputs is equivalent to swapping the outputs. Locality is given by the following pair of equalities:
\begin{align}
    \tikzfig{cons_locality_1}\\
    \tikzfig{cons_locality_2}
\end{align}
and expresses the notion that the outputs are independent of the non-corresponding input, conditional on the corresponding input.

The general definitions follow.
\begin{definition}[Locality]\label{def:caus_cont}
A Markov kernel $\kernel{K}:X^{\mathbb{N}}\kto Y^{\mathbb{N}}$ is \emph{local} if for all $n\in \mathbb{N}$, $A_i\in \sigalg{Y}$, $(x_{[n]},x_{[n]^C})\in\mathbb{N}$ there exists $\kernel{L}:X^n\kto Y^n$ such that
\begin{align}
    \tikzfig{local_lhs} &= \tikzfig{local_rhs}\\
    &\iff\\
    \kernel{K}(\bigtimes_{i\in [n]} A_i\times Y^{\mathbb{N}}|x_{[n]},x_{[n]^C}) &= \kernel{L}(\bigtimes_{i\in [n]} A_i|x_{[n]})
\end{align}
\end{definition}

\begin{definition}[Exchange commutativity]\label{def:caus_exch}
A Markov kernel $\kernel{K}:X^{\mathbb{N}}\kto Y^{\mathbb{N}}$ \emph{commutes with exchange} if for all finite permutations $\rho:\mathbb{N}\to\mathbb{N}$, $A_i\in \sigalg{Y}$, $(x_{[n]},x_{[n]^C})\in\mathbb{N}$
\begin{align}
    \kernel{K}\mathrm{swap}_{\rho,Y} &=  \mathrm{swap}_{\rho,X} \kernel{K}\\
    &\iff\\
    \kernel{K}(\bigtimes_{i\in\mathbb{N}} A_{\rho(i)}|(x_i)_{i\in {\mathbb{N}}}) &= \kernel{K}(\bigtimes_{i\in\mathbb{N}} A_{i}|(x_{\rho(i)})_{i\in {\mathbb{N}}})
\end{align}
\end{definition}

Causal contractibility is the conjunction of both assumptions.
\begin{definition}[Causal contractibility]
A Markov kernel $\kernel{K}:X^{\mathbb{N}}\kto Y^{\mathbb{N}}$ is \emph{causally contractible} if it is local and commutes with exchange.
\end{definition}

All swaps can be written as a product of transpositions, so proving that a property holds for all finite transpositions is enough to show it holds for all finite swaps. It's useful to define a notation for transpositions.
\begin{definition}[Finite transposition]
Given two equally sized sequences $A=(a_i)_{i\in [n]}$, $B=(b_i)_{i\in [n]}$, ${A\leftrightarrow B}:\mathbb{N}\to \mathbb{N}$ is the permutation that sends the $i$th element of $A$ to the $i$th element of $B$ and vise versa. Note that $A\leftrightarrow B$ is its own inverse.
\end{definition}

The name \emph{causal contractibility} is motivated by the fact that the assumption is equivalent to the assumption that all subsequences of input-output pairs have the same overall input-output relationship. This is the content of Theorem \ref{th:equal_of_condits}. See also Theorem \ref{th:equal_of_reduced_condits} for a version of this statement applied to models with data-independent actions that might be easier to understand.

\begin{theorem}[Equality of equally sized contractions]\label{th:equal_of_condits}
A Markov kernel $\kernel{K}:X^{\mathbb{N}}\kto Y^{\mathbb{N}}$ is \emph{causally contractible} if and only if for every $n\in \mathbb{N}$ there exists there exists $\kernel{L}_n:X^n\kto Y^n$ such that for all $A\subset \mathbb{N}$ where $|A|=n$
\begin{align}
    \text{swap}_{[n]\leftrightarrow A} \kernel{K} \text{swap}_{[n]\leftrightarrow A} (\text{id}_{[n]}\otimes \text{del}_{[n]^C}) &= \kernel{L}_n\otimes \mathrm{del}_{[n]^C}
\end{align}
\end{theorem}

\begin{proof}
Only if:
By exchange commutativity
\begin{align}
    \text{swap}_{[n]\leftrightarrow A} \kernel{K} &= \kernel{K} \text{swap}_{A\leftrightarrow n}
\end{align}
multiply both sides by $\text{swap}_{A\leftrightarrow n}$ on the right and we have (because $\text{swap}_{[n]\leftrightarrow A}$ is its own inverse)
\begin{align}
        \text{swap}_{[n]\leftrightarrow A} \kernel{K}\text{swap}_{A\leftrightarrow n} &= \kernel{K}
\end{align}
Then by locality, there exists some $\kernel{L}:X^n\kto Y^n$ such that
\begin{align}
    \text{swap}_{[n]\leftrightarrow A} \kernel{K} \text{swap}_{A\leftrightarrow n} (\text{id}_{[n]}\otimes \text{del}_{[n]^C}) &= \kernel{K} (\text{id}_{[n]}\otimes \text{del}_{[n]^C})\\
    &=\kernel{L}\otimes \mathrm{del}_{[n]^C}
\end{align}
If:
Taking $A=[n]$ establishes locality immediately.

For exchange commutativity, note that for all $x\in X^{\mathbb{N}}$, $n\in\mathbb{N}$, we have
\begin{align}
    \text{swap}_{A\leftrightarrow [n]} \kernel{K} \text{swap}_{A\leftrightarrow [n]} (\text{id}_{[n]}\otimes \text{del}_{[n]^C}) &= \kernel{K}  (\text{id}_{[n]}\otimes \text{del}_{[n]^C})
\end{align}
Then by Lemma \ref{lem:infinitely_extended_kernels}
\begin{align}
    \text{swap}_{A\leftrightarrow [n]} \kernel{K} \text{swap}_{A\leftrightarrow [n]} &= \kernel{K}
\end{align}
Consider an arbitrary finite permutation $\rho:\mathbb{N}\to \mathbb{N}$. $\rho$ can be decomposed into a finite set of cyclic permutations on disjoint orbits. Each cyclic permutation is simply the composition of a sequence of 1-cycles of the form $A\leftrightarrow [n]$, and so $\rho$ itself can be written as a composition of a sequence of 1-cycles. Thus for any finite $\rho:\mathbb{N}\to\mathbb{N}$
\begin{align}
    \text{swap}_{\rho} \kernel{K} \text{swap}_{\rho} &= \kernel{K}
\end{align}
\end{proof}

Theorem \ref{th:table_rep_kernel} shows that a causally contractible Markov kernel can be represented as the product of a column exchangeable probability distribution and a ``lookup function''. This representation is identical to the representation of potential outcomes models (see, for example, \citet{rubin_causal_2005}), but Theorem \ref{th:table_rep_kernel} applies to arbitrary kernels and the resulting representation will usually not be interpretable as a potential outcomes models. This theorem allows De Finetti's theorem to be applied to the column exchangeable probability distribution, which is a key step in proving the main result (Theorem \ref{th:ciid_rep_kernel}).

\begin{theorem}\label{th:table_rep_kernel}
A Markov kernel $\kernel{K}:X^{\mathbb{N}}\kto Y^{\mathbb{N}}$ is causally contractible if and only if there exists a column exchangeable probability distribution $\mu \Delta(Y^{|X|\times \mathbb{N}})$ such that
\begin{align}
    \kernel{K} &= \tikzfig{lookup_representation_kernel}\label{eq:lup_rep_kernel}\\
    &\iff\\
    \kernel{K}(A|(x_i)_{i\in \mathbb{N}}) &= \mu \Pi_{(x_i i)_{i\in\mathbb{N}}}(A)\forall A\in \sigalg{Y}^{\mathbb{N}}
\end{align}
Where $\Pi_{(d_i i)_{i\in\mathbb{N}}}:Y^{|X|\times \mathbb{N}}\to Y^{\mathbb{N}}$ is the function 
\begin{align}
    (y_{j i})_{j,i \in X\times  \mathbb{N}}\mapsto (y_{d_i i})_{i\in \mathbb{N}}
\end{align}
that projects the $(x_i,i)$ indices of $y$ for all $i\in \mathbb{N}$, and $\prob{F}_{\text{ev}}$ is the Markov kernel associated with the evaluation map
\begin{align}
    \text{ev}:X^\mathbb{N}\times Y^{X\times \mathbb{N}}&\to Y\\
    ((x_i)_\mathbb{N},(y_{ji})_{j,i\in X\times \mathbb{N}})&\mapsto (y_{x_i i})_{i\in \mathbb{N}}
\end{align}
\end{theorem}

\begin{proof}
Only if:
Choose $e:=(e_i)_{i\in\mathbb{N}}$ such that $e_{i+|X|j}$ is the $i$th element of $X$ for all $i,j\in \mathbb{N}$.

Define
\begin{align}
    \mu(\bigtimes_{(i,j)\in X\times \mathbb{N}} A_{ij}):=\kernel{K}(\bigtimes_{(i,j)\in X\times \mathbb{N}} A_{ij}|e)& \forall A_{ij}\in \sigalg{Y}
\end{align}

Now consider any $x:=(x_i)_{i\in \mathbb{N}}\in X^{\mathbb{N}}$. By definition of $e$, $e_{x_i i}=x_i$ for any $i,j\in \mathbb{N}$.

Define
\begin{align}
    \prob{Q}:X^{\mathbb{N}}\kto Y^{\mathbb{N}}\\
    \prob{Q}:= \tikzfig{lookup_representation_kernel}
\end{align}
and consider some $A\subset \mathbb{N}$, $|A|=n$ and $B:= (x_i,i))_{i\in A}$. Note that the subsequence of $e$ indexed by $B$, $e_B:=(e_{x_i i})_{i\in A}=x_A$. Thus given the swap map $\mathrm{swap}_{A\leftrightarrow B}:\mathbb{N}\to\mathbb{N}$ that sends the first element of $A$ to the first element of $B$ and so forth, $\mathrm{swap}_{A\leftrightarrow B} (e_B) = x_A$. For arbitrary $\{C_i\in \sigalg{Y}|i\in A\}$, define $C_A:=\mathrm{swap}_{[n]\leftrightarrow A} (\times_{i\in [n]} C_i\times Y^{\mathbb{N}})$. Then, for arbitrary $x\in X^{\mathbb{N}}$
\begin{align}
    \prob{Q}(C_A|x) &= \mu (\mathrm{ev}_x^{-1}(C_A))\label{eq:q_mu_rel}
\end{align}

The argument of $\mu$ is
\begin{align}
    \mathrm{ev}_x^{-1}(C_A)&=\{(y_{ji})_{j,i\in X\times\mathbb{N}}|(y_{x_i i})_{i\in\mathbb{N}}\in C_A\}\\
    &= \bigtimes_{i\in \mathbb{N}} \bigtimes_{j\in X} D_{ji}
\end{align}
where
\begin{align}
    D_{ji} = \begin{cases}
        C_i & (j,i)\in B\\
        Y & \text{otherwise}
    \end{cases}
\end{align}
and so
\begin{align}
    \text{swap}_{A\leftrightarrow B} (\mathrm{ev}_x^{-1}(C_A)) &= C_A\label{eq:swap_select_relation}
\end{align}

Substituting Equation \ref{eq:swap_select_relation} into \ref{eq:q_mu_rel}
\begin{align}
    \prob{Q}(C_A|x) &= \mu \text{swap}_{A\leftrightarrow B} (C_A)\\
    &= \kernel{K} \text{swap}_{A\leftrightarrow B} (C_A|e)\\
    &= \kernel{K}\text{swap}_{A\leftrightarrow B} (C_A|e_B,\text{swap}_{B\leftrightarrow A}(x)_B^C)&\text{by locality}\\
    &= \kernel{K}\text{swap}_{A\leftrightarrow B} (C_A|\text{swap}_{B\leftrightarrow A}(x))\\
    &= \text{swap}_{B\leftrightarrow A} \kernel{K}\text{swap}_{A\leftrightarrow B} (C_A|x)\\
    &= \kernel{K}(C_A|x)&\text{by commutativity of exchange}
\end{align}

Because this holds for all $x$, $A\subset\mathbb{N}$, by Lemma \ref{lem:infinitely_extended_kernels}

\begin{align}
    \prob{Q} &= \kernel{K}
\end{align}

Next we will show $\mu$ is column exchangeable. Consider any column swap $\text{swap}_{c}:X\times \mathbb{N}\to X\times \mathbb{N}$ that acts as the identity on the $X$ component and a finite permutation on the $\mathbb{N}$ component. From the definition of $e$, $\text{swap}_c(e)=e$. Thus by commutativity of exchange, for any $A\in \sigalg{Y}^{\mathbb{N}}$
\begin{align}
 \kernel{K}(A|e) &= \text{swap}_c\kernel{K}\text{swap}_c(A|e)\\
 &= \kernel{K}\text{swap}_c(A|\text{swap}_c(e))\\
 &= \kernel{K}\text{swap}_c(A|e)
\end{align}


If:
Suppose 
\begin{align}
    \kernel{K} &= \tikzfig{lookup_representation_kernel}
\end{align}
where $\mu$ is column exchangeable, and consider any two $x,x'\in X^{\mathbb{N}}$ such that some subsequences are equal $x_S=x'_T$ with $S,T\subset \mathbb{N}$ and $|S|=|T|=[n]$.

For any $\{A_i\in\sigalg{Y}|i\in S\}$, let $A_S = \text{swap}_{[n]\leftrightarrow S} \times_{i\in [n]} A_i\times Y^{\mathbb{N}}$, $A_T = \text{swap}_{S\leftrightarrow T} (A_S)$, $B=(x_i i)_{i\in S}$ and $C=(x_i i)_{i\in T}=(x_{\text{swap}_{S\leftrightarrow T}}(i) i)_{i\in S}$. By Equations \ref{eq:q_mu_rel} and \ref{eq:swap_select_relation}
\begin{align}
    \kernel{K}(A_S|x) &= \mu \text{swap}_{S\leftrightarrow B} (A_S)\\
    &= \mu \text{swap}_{T\leftrightarrow C} (A_T)&\text{ by column exchangeability of }\mu\\
    &= \kernel{K}(A_T|\text{swap}_{S\leftrightarrow T}(x))\\
    &=  \text{swap}_{S\leftrightarrow T}\kernel{K}(A_T| x)\\
    &= \text{swap}_{S\leftrightarrow T} \kernel{K} \text{swap}_{S\leftrightarrow T} (A_S| x)
\end{align}
so $\kernel{K}$ is causally contractible by Theorem \ref{th:equal_of_condits}.
\end{proof}

Theorem \ref{th:ciid_rep_kernel} is the main result of this section. It shows that a causally contractible Markov kernel $X^{\mathbb{N}}\kto Y^{\mathbb{N}}$ is representable as a ``prior'' $\mu\in \Delta(H)$ and a ``parallel product'' of Markov kernels $H\times X\kto Y$. These will be the response conditionals when Theorem \ref{th:ciid_rep_kernel} is applied to probability set models.

\begin{theorem}\label{th:ciid_rep_kernel}
A kernel $\kernel{K}:X^{\mathbb{N}}\kto Y^{\mathbb{N}}$ is causally contractible if and only if there exists some set $H$, $\mu\in \Delta(H)$ and $\kernel{L}:H\times X\kto Y$ such that
\begin{align}
    \kernel{K} &= \tikzfig{do_model_representation_kernel}\\
    &\iff\\
    \kernel{K}(\bigtimes_{i\in\mathbb{N}}A_i|(x_i)_{i\in\mathbb{N}}) &= \int_H \prod_{i\in\mathbb{N}} \kernel{L}(A_i|h,x_i)\mu(\mathrm{d}h)
\end{align}
\end{theorem}

\begin{proof}
By Theorem \ref{th:table_rep_kernel}, we can represent the conditional probability $\kernel{K}$ as
\begin{align}
        \kernel{K} &= \tikzfig{lookup_representation_kernel}\label{eq:lookup_representation}
\end{align}
where $\mu$ is column exchangeable.

As a preliminary, we will show
\begin{align}
    \kernel{F}_{\mathrm{ev}} &= \tikzfig{lookup_rep_intermediate_kernel}\label{eq:ev_alternate_rep}
\end{align}
where  $\mathrm{evs}_{Y^D\times D}:Y^D\times D\to Y$ is the single-shot evaluation function
\begin{align}
    (x,(y_i)_{i\in X})\mapsto y_x
\end{align}

Recall that $\mathrm{ev}$ is the function

\begin{align}
    ((x_i)_\mathbb{N},(y_{ji})_{j,i\in X\times \mathbb{N}})&\mapsto (y_{x_i i})_{i\in \mathbb{N}}
\end{align}

By definition, for any $\{A_i\in\sigalg{Y}|i\in \mathbb{N}\}$

\begin{align}
    \kernel{F}_{\mathrm{ev}}(\bigtimes_{i\in \mathbb{N}}A_i|(x_i)_\mathbb{N},(y_{ji})_{i\in X\times \mathbb{N}}) &= \delta_{(y_{x_i i})_{i\in \mathbb{N}}}(\bigtimes_{i\in \mathbb{N}}A_i)\\
        &= \prod_{i\in \mathbb{N}} \delta_{y_{x_i i}} (A_i)\\
        &= \prod_{i\in \mathbb{N}} \kernel{F}_{\text{evs}} (A_i|x_i,(y_{ji})_{j\in X})\\
        &= \left(\bigotimes_{i\in\mathbb{N}} \kernel{F}_{\mathrm{evs}} \right)(\bigtimes_{i\in \mathbb{N}}A_i|(x_i)_\mathbb{N},(y_{ji})_{j\in X\times \mathbb{N}})
\end{align}

Which is what we wanted to show.

Only if:
By the column exchangeability of $\mu$, from \citet{kallenberg_basic_2005} we have some $H$, a directing random measure $\nu\in \Delta(H)$ and a kernel $\kernel{L}:H\kto Y^X$ such that
\begin{align}
    \mu &= \tikzfig{de_finetti_representation_kernel}\label{eq:df_rep_mu}\\
    &\iff\\
    \mu(\bigtimes_{i\in \mathbb{N}} A_i) &= \int_H \prod_{i\in \mathbb{N}} \kernel{L}(A_i|h)\mu(\mathrm{d}h)&\forall A_i\in\sigalg{Y}^X
\end{align}

By Equations \ref{eq:lookup_representation} and \ref{eq:ev_alternate_rep}
\begin{align}
    \kernel{K} &= \tikzfig{do_model_representation}\label{eq:lup_rep_combined}
\end{align}
Where we can connect the outputs of $\mu$ to the inputs of $\kernel{F}_{\mathrm{evs}}$ ``inside the plate'' as the plates in Equations \ref{eq:ev_alternate_rep} and \ref{eq:df_rep_mu} are equal in number and each connected wire represents a single copy of $Y^D$.

If:
By assumption, for any $\{A_i\in \sigalg{Y}|i\in\mathbb{N}\}$, $x:=(x_i)_{i\in\mathbb{N}}\in X^{\mathbb{N}}$
\begin{align}
    \kernel{K}(\bigtimes_{i\in \mathbb{N}} A_i|x) &= \int_H \prod_{i\in \mathbb{N}}\kernel{L}(A_i|h,x_i)\mu(\mathrm{d}h)
\end{align}

Consider any $S,T\subset\mathbb{N}$ with $|S|=|T|$, and define $A_S:=\times_{i\in\mathbb{N}} B_i$ where $B_i=Y$ if $i\not\in S$, otherwise $A_i$ is an arbitrary element of $\sigalg{Y}$. Define $A_T:=\times_{i\in\mathbb{N}} B_{\mathrm{swap}_{S\leftrightarrow T}(i)}$.

\begin{align}
    \kernel{K}(A_S|x) &= \int_H \prod_{i\in S}\kernel{L}(A_i|h,x_i)\mu(\mathrm{d}h)\\
                      &= \int_H\prod_{i\in T}\kernel{L}(A_i|h,x_{\mathrm{swap}_{S\leftrightarrow T}(i)})\mu(\mathrm{d}h)\\
                      &= \mathrm{swap}_{S\leftrightarrow T}\kernel{K}(A_T|x)\\
                      &= \mathrm{swap}_{S\leftrightarrow T}\kernel{K}\mathrm{swap}_{S\leftrightarrow T}(A_S|x)
\end{align}
So by Theorem \ref{th:equal_of_condits}, $\kernel{K}$ is causally contractible.
\end{proof}

\subsection{Causal contractibility with data-independent actions}

Given a sequence of variables $(\RV{D}_i,\RV{Y}_i)_{i\in \mathbb{N}}$ where the ``inputs'' are $\RV{D}:=(\RV{D}_i)_{i\in\mathbb{N}}$ and the ``outputs'' are $\RV{Y}=(\RV{Y}_i)_{i\in\mathbb{N}}$, say the inputs are independent of previous observations if $\RV{D}_i\CI^e_{\prob{P}_C} (\RV{Y}_{<i},C)|\RV{D}_{<i}$ for all $i\in\mathbb{N}$. This models an experiment where it may be possible to choose different inputs $\RV{D}$, but all the inputs are determined before the outputs $\RV{Y}$ are known. If a model satisfies this, then the dependence of $\RV{Y}$ on $\RV{D}$ is a sequence of repeatable response functions if and only if the uniform conditional $\prob{P}_C^{\RV{Y}|\RV{D}}$ exists (Definition \ref{def:cprob_pset}) and is causally contractible.

We call a model $\prob{P}_C$ with sequential outputs $\RV{Y}$ and a corresponding sequence of data-independent inputs $\RV{D}$ a ``sequential just-do model''.

\begin{definition}[Sequential just-do model]
A \emph{sequetntial just-do model} is a triple $(\prob{P}_C,\RV{D},\RV{Y})$ where $\prob{P}_C$ is a probability set on $(\Omega,\sigalg{F})$, $\RV{D}$ is a sequence of ``inputs'' $\RV{D}:=(\RV{D}_i)_{i\in\mathbb{N}}$ and $\RV{Y}$ is a corresponding sequence of ``outputs'' $\RV{Y}=(\RV{Y}_i)_{i\in\mathbb{N}}$ where $\RV{D}_i':\Omega\to D$ and $\RV{Y}_i':\Omega\to Y$. Furthermore, it is required that $\RV{X}_i\CI^e_{\prob{P}_C} (\RV{Y}_{<i},C)|\RV{X}_{<i}$ for all $i\in \mathbb{N}$, and $\RV{Y}'\CI^e_{\prob{P}_C} C|\RV{D}'$.
\end{definition}

To apply Theorem \ref{th:ciid_rep_kernel} to a sequential just-do model, it is necessary to extend the model to a larger sample space including ``latent variables'' taking values in $Y^D$. A latent extension is a model over a larger collection of variables that reduces to the original model when we restrict our attention to the original collection of variables.

\begin{definition}[Latent extension]
Given a probability set $\prob{P}_C'$ on $(\Omega,\sigalg{F})$ and some measurable set $(G,\sigalg{G})$, a probability set $\prob{P}_C$ is a \emph{latent extension} of $\prob{P}_C'$ to $(\Omega\times G,\sigalg{F}\otimes \sigalg{G})$ if $\prob{P}_C \kernel{F}_{\Pi_{\Omega}} = \prob{P}_C'$.
\end{definition}

\begin{convention}[Variables on a latent extension]
Given a probability set $\prob{P}_C'$ on $(\Omega,\sigalg{F})$ and a latent extension $\prob{P}_C$ on $(\Omega\times G,\sigalg{F}\otimes \sigalg{G})$, every variable on the original sample space is given a primed name $\RV{X}'$, $\RV{Y}'$ etc., and corresponds to an unprimed variable on the larger space $\RV{X}:=\Pi_\Omega\circ \RV{X}'$.
\end{convention}

Theorem \ref{th:data_ind_CC} applies Theorem \ref{th:ciid_rep_kernel} to the case of a model with data-independent actions and derives the required conditional independences and equalities to show that a sequential just-do model $(\prob{P}_C,\RV{D},\RV{Y})$ with causally contractible $\prob{P}_C^{\RV{Y}|\RV{X}}$ satisfies the required conditional independences and equalities of conditional distributions for $\RV{X}$ and $\RV{Y}$ to be related by repeatable response functions.

\begin{theorem}[Data-independent causal contractibility]\label{th:data_ind_CC}
Given a sequential just-do model $(\prob{P}_C',\RV{D}',\RV{Y}')$ on $(\Omega,\sigalg{F})$, then $\prob{P}_C^{\prime \RV{Y}'|\RV{D}'}$ is causally contractible if and only if there is a latent extension $\prob{P}_C$ of $\prob{P}_C'$ to $(\Omega\times Y^D,\sigalg{F}\otimes\sigalg{Y}^D)$ with some hypothesis $\RV{H}:\Omega\times Y^D\times H\to H$ such that $\RV{Y}_i\CI^e_{\prob{P}_C'} (\RV{Y}_{<i},\RV{X}_{<i},C)|(\RV{X}_i,\RV{H})$ and $\prob{P}_C^{\RV{Y}_i|\RV{X}_i\RV{H}}=\prob{P}_C^{\RV{Y}_j|\RV{X}_j\RV{H}}$ for all $i,j\in \mathbb{N}$.
\end{theorem}

\begin{proof}
If:
First, define the extension $\prob{P}_C$. From Theorem \ref{th:table_rep_kernel} and causal contractibility of $\prob{P}_C^{\prime \RV{Y}'|\RV{D}'}$ there is some $\mu\in \Delta(Y^D)$ such that
\begin{align}
    \prob{P}_C^{\prime \RV{Y}'|\RV{D}'} &= \tikzfig{lookup_representation_variablised}\label{eq:lup_rep_varb}
\end{align}
Let $\prob{P}_C^{\RV{Y}^D}=\mu$, $\prob{P}_C^{\RV{Y}|\RV{Y}^D\RV{D}}=\kernel{F}_{\mathrm{ev}}$ and $\RV{Y}^D:=\Pi_{Y^D}$, the projection $\Omega\times Y^D\to \Omega$. Let $\RV{W}=\Pi_{\Omega}$ and for each $\alpha\in C$, set 
\begin{align}
    \prob{P}_\alpha^{\RV{W}|\RV{Y}^D\RV{D}} &= \tikzfig{augmented_ccontracible}
\end{align}
Then 
\begin{align}
    \prob{P}_\alpha^{\RV{W}} &= \tikzfig{augmented_ccon2}\\
    &= \tikzfig{augmented_ccon3}\\
    &= \prob{P}_\alpha^{\prime \text{id}_\Omega}
    &= \prob{P}_\alpha
\end{align}

Before going further, it's necessary to check that there is some nonempty probability set $\prob{P}_C$ with these conditionals. By Theorem \ref{lem:valid_extendability}, because $\prob{P}_\alpha^{\RV{W}}&=\prob{P}_{\alpha}'$ it is sufficient to show that $\prob{P}_\alpha^{\RV{Y}^D|\RV{W}}$ is valid. Because 
\begin{align}
    (\RV{W},\RV{Y}^D)(\Omega\times Y^D)&=\Omega\times Y^D\\
    &=\RV{W}(\Omega\times Y^D)\times \RV{Y}^D(\Omega\times Y^D)
\end{align}
Validity is guaranteed for any $\prob{P}_\alpha^{\RV{Y}^D|\RV{W}}$, because there are no jointly impossible events.

Thus $\prob{P}_C$ is a latent extension of $\prob{P}_C'$, and so $\prob{P}_C^{\RV{Y}|\RV{D}}$ is also causally contractible.


Only if:
If $\prob{P}_C$ is a latent extension of $\prob{P}_C'$, then $\prob{P}_C^{\RV{Y}|\RV{D}}$ is causally contractible if and only if $\prob{P}_C^{\prime \RV{Y}'|\RV{D}'}$ is causally contractible. Thus it is sufficient to show $\prob{P}_C^{\RV{Y}|\RV{D}}$ is causally contractible.









\end{proof}


Theorem \ref{th:no_implication} shows that neither condition implies the other. 

\begin{theorem}\label{th:no_implication}
Exchange commutativity does not imply locality of consequences or vise versa.
\end{theorem}

\begin{proof}
Appendix \ref{sec:ccontract_appendix}.
\end{proof}

If we are modelling the treatment of several patients whom who have already been examined, we might assume consequence locality -- patient B's treatment does not affect patient A -- but not exchange commutativity -- we don't expect the same results from giving patient A's treatment to patient B as we would from giving patient A's treatment to patient A. 

A model of stimulus payments might exhibit exchange commutativity but not consequence locality. If exactly $n$ payments of \$10 000 are made, we might suppose that it doesn't matter much exactly who receives the payments, but the amount of inflation induced depends on the number of payments made; making 100 such payments will have a negligible effect on inflation, while making payments to everyone in the country will have a substantial effect. \citet{dawid_causal_2000} offers the example of herd immunity in vaccination campaigns as a situation where post-treatment exchangeability holds but locality of consequences does not.

Although locality of consequences seems to intuitively encompass an assumption of non-interference, it still allows for some models in which exhibit certain kinds of interference between actions and outcomes of different indices. For example: I have an experiment where I first flip a coin and record the results of this flip as the outcome of the first step of the experiment, but I can choose either to record this same outcome as the provisional result of the second step (this is the choice $\RV{D}_1=0$), or choose to flip a second coin and record the result of that as the provisional result of the second step of the experiment (this is the choice $\RV{D}_1=1$). At the second step, I may further choose to copy the provisional results ($\RV{D}_2=0$) or invert them ($\RV{D}_2=1$). Then

\begin{align}
    \prob{P}_S^{\RV{Y}_1|\RV{D}}(y_1|d_1,d_2) &= 0.5\\
    \prob{P}_S^{\RV{Y}_2|\RV{D}}(y_2|d_1,d_2) &= 0.5
\end{align}
\begin{itemize}
    \item The marginal distribution of both experiments in isolation is $\text{Bernoulli}(0.5)$ no matter what choices I make, so a model of this experiment would satisfies Definition \ref{def:caus_cont}
    \item Nevertheless, the choice for the first experiment affects the result of the second experiment
\end{itemize}

\begin{theorem}[Equality of reduced conditionals]\label{th:equal_of_reduced_condits}
A probability set $\prob{P}_C$ that is $(\RV{D};\RV{Y})$-causally contractible has, for any $A,B\subset M$ with $|A|=|B|$
\begin{align}
    \prob{P}_C^{\RV{Y}_A|\RV{D}_A} \overset{\prob{P}_C}{\cong} \prob{P}_C^{\RV{Y}_B|\RV{D}_B}
\end{align}
\end{theorem}

\begin{proof}
Only if:
For any $A,B\subset M$, let $\text{s}_{BA}:D^M\kto D^M$ be the swap map that sends the $B$ indices to $A$ indices and $\text{s}_{AB}:Y^M\kto Y^M$ be the swap map that sends $A$ indices to $B$ indices.
\begin{align}
    \tikzfig{consequence_locality} &= \tikzfig{contractibility_from_exchange_1}\\
    &= \tikzfig{contractibility_from_exchange_2}\\
    &= \tikzfig{contractibility_from_exchange_3}
\end{align}
Thus
\begin{align}
    \prob{P}_C^{\RV{Y}_A|\RV{D}_A\RV{D}_{M\setminus A}} &\overset{\prob{P}_C}{\cong} \prob{P}_C^{\RV{Y}_B|\RV{D}_B\RV{D}_{M\setminus B}}\\
    &\overset{\prob{P}_C}{\cong} \tikzfig{consequence_locality}\label{eq:cons_local}\\
    \implies \prob{P}_C^{\RV{Y}_A|\RV{D}_A} \overset{\prob{P}_C}{\cong} \prob{P}_C^{\RV{Y}_B|\RV{D}_B}
\end{align}
\end{proof}



% \begin{proposition}[Representation of do-models that commute with exchange]
% Suppose we have a fundamental probability set $\Omega$ and a do model $(\prob{P},\RV{D},\RV{Y},R)$ such that $\RV{D}:=(\RV{D}_i)_{i\in \mathbb{N}}$ and $\RV{Y}:=(\RV{Y}_i)_{i\in\mathbb{N}}$ where $\prob{P}$ commutes with exchange and there is some $\alpha^*\in R$ such that $\prob{P}^{\alpha^*}\gg\prob{P}_\beta$ for all $\beta in R$. Then there exists a symmetric function $\RV{H}:(Y\times D)^\mathbb{N}\to H$ such that  $\prob{P}^{\RV{Y}|\RV{DH}}$ exists and $\RV{Y}_i\CI_{\prob{P}}(\RV{D}_j,\RV{Y}_j)_{j\in \mathbb{N}}\setminus \{i\}|\RV{H}\RV{D}_i$, or equivalently 
% \begin{align}
%     \prob{P}^{\RV{Y}} &= \tikzfig{do_model_representation}
% \end{align}
% \end{proposition}

% % \begin{lemma}[Contraction and independence]
% % Let $\RV{J}$, $\RV{K}$ and $\RV{L}$ be variables on $\Omega$ and $\prob{Q}\in \Delta(\Omega)$ a base measure such that $\prob{Q}^{\RV{JK}}=\prob{Q}^{\RV{JL}}$ and $\sigma{K}\subset \sigma{L}$. Then $\RV{J}\CI\RV{L}|\RV{K}$. 
% % \end{lemma}

% % \begin{proof}
% % From Lemma 1.3 in \citet{kallenberg_basic_2005}.
% % \end{proof}

% \begin{proof}
% If $\prob{P}$ commutes with exchange, then for any $\alpha\in R$ such that $\prob{P}_\alpha^{\RV{D}}$ is exchangeable then $\prob{P}_\alpha$ is also exchangeable. Then there exists $\RV{H}$ a symmetric function of $(\RV{Y}_i,\RV{D}_i)_{i\in\mathbb{N}}$ such that $\RV{Y}_i\CI_{\prob{P}}(\RV{D}_j,\RV{Y}_j)_{j\in \mathbb{N}}\setminus \{i\}|\RV{H}\RV{D}_i$. This is De Finetti's representation theorem, and many proofs exists, see for example \citep{kallenberg_basic_2005}.

% In particular, let 

% \begin{align}
%     \RV{H}:=A\times B\mapsto \lim_{n\to\infty} \frac{1}{n}\sum_{i\in n} \mathds{1}_{A\times B}((\RV{Y}_i, \RV{D}_i))
% \end{align}

% Then for all $\alpha\in R$,
% \begin{align}
%     \prob{P}_\alpha^{(\RV{Y}_i,\RV{D}_i)_{i\in\mathbb{N}}|\RV{H}}(A\times B|h) \overset{a.s.}{=} h(A\times B)\label{eq:given_h}
% \end{align}

% The proof that the limit exists and the above equality holds can again be found int \citep{kallenberg_basic_2005}.
% \end{proof}

\subsection{Existence of response conditionals}

The main result in this section is Theorem \ref{th:iid_rep} which shows that a probability set $\prob{P}_C$ is causally contractible if and only if it can be represented as the product of a distribution over hypotheses $\prob{P}_\square^{\RV{H}}$ and a collection of identical uniform conditionals $\prob{P}_C^{\RV{Y}_1|\RV{D}_1\RV{H}}$. Note the hypothesis $\RV{H}$ that appears in this conditional; it can be given the interpretation of a random variable that expresses the ``true but initially unknown'' $\RV{Y}_1|\RV{D}_1$ conditional probability.

\begin{theorem}\label{th:table_rep}
Given a probability set $\prob{P}_C$ and variables $\RV{D}:=(\RV{D}_i)_{i\in \mathbb{N}}$ and $\RV{Y}:=(\RV{Y}_i)_{i\in \mathbb{N}}$, $\prob{P}_C$ is  $(\RV{D};\RV{Y})$-causally contractible if and only if there exists a column exchangeable probability distribution $\mu^{\RV{Y}^D}\in \Delta(Y^{|D|\times \mathbb{N}})$ such that
\begin{align}
    \prob{P}_C^{\RV{Y}|\RV{D}} &= \tikzfig{lookup_representation}\label{eq:lup_rep}\\
    &\iff\\
    \prob{P}_C^{\RV{Y}|\RV{D}}(y|(d_i)_{i\in \mathbb{N}}) &= \mu^{\RV{Y}^D} \kernel{F}_{\Pi_{(d_i i)_{i\in\mathbb{N}}}}(y)
\end{align}
Where $\kernel{F}_{\Pi_{(d_i i)_{i\in\mathbb{N}}}}:Y^{|D|\times \mathbb{N}}\kto Y^{\mathbb{N}}$ is the Markov kernel associated with the function that projects the $(d_i,i)$ indices for all $i\in \mathbb{N}$ and $\prob{F}_{\text{ev}}$ is the Markov kernel associated with the evaluation map
\begin{align}
    \text{ev}:D^\mathbb{N}\times Y^{D\times \mathbb{N}}&\to Y\\
    ((d_i)_\mathbb{N},(y_{ij})_{i\in D,j\in \mathbb{N}})&\mapsto (y_{d_i i})_{i\in \mathbb{N}}\label{eq:ev_function2}
\end{align}
\end{theorem}

\begin{proof}
Appendix \ref{sec:ccontract_appendix}.
\end{proof}

We would prefer to talk about $\RV{Y}^D$ as a latent variable, rather than needing to refer to the the factorisation of a model in terms of $\mu^{\RV{Y}^D}$ in Equation \ref{eq:lup_rep}. This motivates the definition of an \emph{augmented} causally contractible model.

An augmented causally contractible model looks in some respects similar to a potential outcomes model - both have a distribution over an unobserved ``tabular'' variable $\RV{Y}^D$, and the value of $\RV{Y}_i$ given $\RV{D}$ is deterministically equal to the $\RV{Y}_i^\RV{D}$ (abusing notation). However, the $\RV{Y}^D$ in an augmented causally contractible model usually can't be interpreted as potential outcomes. For example, consider a series of bets on fair coin flips. Model the consequence $\RV{Y}_i$ as uniform on $\{0,1\}$ for any decision $\RV{D}_i$, for all $i$. Specifically, $D=Y=\{0,1\}$ and $\prob{P}_\alpha^{\RV{Y}_n}(y)=\prod_{i\in [n]} 0.5$ for all $n$, $y\in Y^n$, $\alpha\in R$. Then the construction of $\prob{P}^{\RV{Y}^D}$ following the method in Lemma \ref{th:table_rep} yields $\prob{P}^{Y^D_i}(y^D_i)=\prod_{j\in D} 0.5$ for all $y^D_i\in Y^D$. In this model $\RV{Y}^0_i$ and $\RV{Y}^1_i$ are independent and uniformly distributed. However, if we wanted $\RV{Y}^0_i$ to be interpretable as ``what would happen if I bet on outcome 0 on turn $i$'' and $\RV{Y}^1$ to represent ``what would happen if I bet on outcome 1 on turn $i$'', then we ought to have $\RV{Y}^0_i = 1-\RV{Y}^1_i$.

The following is the main theorem of this section, that establishes the equivalence between causal contractibility and the existence of response conditionals. The argument in outline is: because $\prob{P}_C^{\RV{Y}^D}$ is a column exchangeable probability distribution we can apply De Finetti's theorem to show $\prob{P}_C^{\RV{Y}^D}$ is representable as a product of identical parallel copies of $\prob{P}_C^{\RV{Y}_1^D|\RV{H}}$ and a common prior $\prob{P}_C^{\RV{H}}$. This in turn can be used to show that $\prob{P}_C^{\RV{Y}|\RV{D}}$ can be represented as a product of identical parallel copies of $\prob{P}_C^{\RV{Y}_1|\RV{D}_1\RV{H}}$ and the same common prior $\prob{P}_C^{\RV{H}}$.

\begin{theorem}\label{th:iid_rep}
Suppose we have a sample space $(\Omega,\sigalg{F})$ and a probability set $\prob{P}_C$ and variables $\RV{D}:=(\RV{D}_i)_{i\in \mathbb{N}}$ and $\RV{Y}:=(\RV{Y}_i)_{i\in \mathbb{N}}$. Suppose also that  $\prob{P}_C$ is $(\RV{D};\RV{Y})$-causally contractible if and only if there exists some $\RV{H}:\Omega\to H$ such that $\prob{P}^{\RV{H}}_C$ and $\prob{P}_C^{\RV{Y}_i|\RV{H}\RV{D}_i}$ exist for all $i\in \mathbb{N}$ and
\begin{align}
    \prob{P}_C^{\RV{Y}|\RV{D}} &= \tikzfig{do_model_representation}\\
    &\iff\\
    \RV{Y}_i&\CI^e_{\prob{P}_C} \RV{Y}_{\mathbb{N}\setminus i},\RV{D}_{\mathbb{N}\setminus i}C|\RV{H}\RV{D}_i&\forall i\in \mathbb{N}\\
    \land \RV{H} &\CI^e_{\prob{P}_C} \RV{D} C\\
    \land \RV{D}_i&\CI_{\prob{P}_C}^e \RV{Y}_{<i>} C|\RV{D}_{<i}\\
    \land \prob{P}_C^{\RV{Y}_i|\RV{H}\RV{D}_i} &= \prob{P}^{\RV{Y}_0|\RV{H}\RV{D}_0} & \forall i\in \mathbb{N}
\end{align}
Where $\Pi_{D,i}:D^\mathbb{N}\kto D$ is the $i$th projection map.
\end{theorem}

\begin{proof}
Appendix \ref{sec:ccontract_appendix}.
\end{proof}

\subsection{Data-independent actions}\label{sec:data_independent_actions}

\subsection{Combs and response conditionals with data-dependent actions}\label{sec:data_dependent}

\subsection{Elaborations and examples}

Theorem \ref{th:iid_rep} requires an infinite sequence of causally contractible pairs. In practice we only want to model finite sequences of variables, but this theorem applies as long as it is possible to extend the finite model to an infinite model maintaining causal contractibility.


Theorem \ref{th:iid_rep} applies whatever procedure we use to obtain the $(\RV{D}_i,\RV{Y}_i)$ pairs -- the $\RV{D}_i$s may be randomised, passive observations or active choices. Purely passive observations can be modeled with a probability set of size 1, and in this case an exchangeable sequence of $(\RV{D}_i,\RV{Y}_i)$ will also be causally contractible.

If we are modelling $M$ passive observations followed by $N$ active choices, then we will have a model $\prob{P}_C$ with $\RV{D}_{[M]}\CI_{\prob{P}_C}^e C$ (because these are passive observations). If this model is $(\RV{D};\RV{Y})$-causally contractible, then one consequence of this is an ``observational imitation'' condition: any choice $\alpha$ that makes $\prob{P}_\alpha^{\RV{D}_{[M+N]}}$ exchangeable also makes $\prob{P}_\alpha^{\RV{Y}_{[M+N]}}$ exchangeable. That is, if for some permutation $\mathrm{swap}_\rho$
\begin{align}
    \prob{P}_\alpha^{\RV{D}_{[M+N]}}\mathrm{swap}_\rho &= \prob{P}_\alpha^{\RV{D}_{[M+N]}}
\end{align}
then by commutativity of exchange
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_{[M+N]}} &= \prob{P}_\alpha^{\RV{D}_{[M+N]}} \prob{P}_C^{\RV{Y}_{[M+N]}|\RV{D}_{[M+N]}}\\
    &=  \prob{P}_\alpha^{\RV{D}_{[M+N]}}\mathrm{swap}_\rho \prob{P}_C^{\RV{Y}_{[M+N]}|\RV{D}_{[M+N]}}\\
    &= \prob{P}_\alpha^{\RV{D}_{[M+N]}} \prob{P}_C^{\RV{Y}_{[M+N]}|\RV{D}_{[M+N]}}\mathrm{swap}_\rho\\
    &= \prob{P}_\alpha^{\RV{Y}_{[M+N]}}\mathrm{swap}_\rho
\end{align}

If we assume a probability set $\prob{P}_C$ is $(\RV{D},\RV{X};\RV{Y})$-causally contractible and $\RV{X}_{i}\CI^e_{\prob{P}_C}\RV{D}_{i}C|\RV{H}$ -- that is, $\RV{D}_i$ is must be independent of $\RV{X}_i$ conditional on $\RV{H}$ -- then we get a version of the ``backdoor adjustment'' formula. Specifically
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_{i}|\RV{D}_{i}\RV{H}}(A|d,h) &= \int_X \prob{P}_\alpha^{\RV{Y}_{i}|\RV{X}_{i}\RV{D}_{i}\RV{H}}(A|d,x,h)\prob{P}_\alpha^{\RV{X}_{i}|\RV{D}_{i}\RV{H}}(\mathrm{d}x|d,h)\\
    &= \int_X \prob{P}_C^{\RV{Y}_{1}|\RV{X}_{1}\RV{D}_{1}\RV{H}}(A|d,x,h)\prob{P}_C^{\RV{X}_{i}|\RV{H}}(\mathrm{d}x|h)
\end{align}

If we additionally assume $\prob{P}_C^{\RV{X}_{i}|\RV{H}}\cong \prob{P}_C^{\RV{X}_{1}|\RV{H}}$ then 
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_{i}|\RV{D}_{i}\RV{H}}(A|d,h) &= \int_X \prob{P}_C^{\RV{Y}_{1}|\RV{X}_{1}\RV{D}_{1}\RV{H}}(A|d,x,h)\prob{P}_C^{\RV{X}_{1}|\RV{H}}(\mathrm{d}x|h)\label{eq:backdoor}
\end{align}

Equation \ref{eq:backdoor} is identical to the backdoor adjustment formula for an intervention on $\RV{D}_1$ targeting $\RV{Y}_1$ where $\RV{X}_1$ is a common cause of both.

While it is formally possible to use a causally contractible model for a decision procedure that involves both passive observations and active choices, causal contractibility is a very strong assumption. Suppose we have a decision procedure in which $M$ passive observations are made $(\RV{D}_M,\RV{Y}_M)$, followed by $M$ active choices $(\RV{D}_{(M,2M]},\RV{Y}_{(M,2M]})$. If a model $\prob{P}_C$ of this procedure is $(\RV{D}_{2M},\RV{Y}_{2M})$-causally contractible model then the following holds (see corollary \ref{th:equal_of_condits}):
\begin{align}
    \prob{P}^{\RV{Y}_{[2,M+1]}|\RV{D}_{[2,M+1]}}_C &= \prob{P}^{\RV{Y}_{(M,2M]}|\RV{D}_{(M,2M]}}\\
    \implies \prob{P}^{\RV{Y}_{M+1}|\RV{D}_{[2,M+1]}\RV{Y}_{[2,M]}}_C &= \prob{P}^{\RV{Y}_{M+1}|\RV{D}_{(M,2M]}\RV{Y}_{(M+1,2M]}}
\end{align}
That is, causal contractibility implies that there is no difference between conditioning on observational results or on the results of active choices; active choices are as good for predicting observations as vise-versa. Normally one might consider randomised experimental results to be ``better'' than passive observations, but this is not compatible with the assumption of causal contractibility.

\section{Assessing causal contractibility}\label{sec:assessing}

Assessing when a particular sequence of experiments should be modeled with a causally contractible model can be difficult. One way to justify the assumption is in two steps: first, all the repetitions of the experiment that yield the values of each of the $(\RV{D}_i,\RV{Y}_i)$ pairs are indistinguishable ``at the time of model construction'', and and they are still indistinguishable after learning the value of $\RV{D}$ -- because, for example, $\RV{D}$ is deterministic for each choice $\alpha\in C$. 

Two step justifications of this form are common in literature on causal identifiability. For example, \citet{greenland_identifiability_1986} explain

\begin{quote}
    Equivalence of response type may be thought of in terms of exchangeability of individuals: if the exposure states of the two individuals had been exchanged, the same data distribution would have resulted.
\end{quote}

Note that exchanging individuals involved in an experiment and exchanging the individuals' exposure states are two different things, and the former doesn't imply the latter. We may consider a model that is symmetric to permutations of individual identifiers but is not symmetric to permuting individual identifiers and leaving exposure states fixed.

\citet{dawid_decision-theoretic_2020} suggests (with many qualifications) that ``post-treatment exchangeability'' for a decision problem regarding taking aspirin to treat a headache may be acceptable if the data are from

\begin{quote}
    A group of individuals whom I can regard, in an intuitive sense, as similar to myself, with headaches similar to my own.
\end{quote}

Dawid points to the ``first step'' in our two step justification for causal contractibility: that the people involved are ``similar'' in an appropriate sense. However, under Dawid's approach there is a background assumption here that whether or not I take the aspirin is deterministic given the choice I end up making, which is the second step in our two step justification.

Finally, \citet{rubin_causal_2005} explicitly discusses two separate assumptions to justify causal identifiability:

\begin{quote}
    indexing of the units is, by definition, a random permutation of $1,..., N$, and thus any distribution on the science must be row-exchangeable [...] The second critical fact is that if the treatment assignment mechanism is ignorable (e.g., randomized), then when the expression for the assignment mechanism (2) is evaluated at the observed data, it is free of dependence on $Y_{mis}$
\end{quote}

Here we have a more abstract statement about the row-exchangeability of ``the science'', rather than individual people involved in an experiment, but we regard it as similar in spirit to assumptions that people involved in the experiment are ``similar''. Rubin explicitly mentions a second condition: that the treatment assignment is randomized.

Theorem \ref{th:cc_ind_treat} formalises these ideas. As an example of its application, consider an experiment where $N$ patients, each with an individual identifier $\RV{I}_i$, receive treatment $\RV{D}_i$ and experience outcome $\RV{Y}_i$. We assume a $((\RV{D},\RV{I});\RV{Y})$-causally contractible model $\prob{P}_C$ is appropriate. This reflects two judgments; firstly, that treatment $\RV{D}_i$ and identifiers $\RV{I}_i$ screen off all other variables from $\RV{Y}_i$ (Definition \ref{def:caus_cont}), and secondly that the order in which the individuals appear and the treatments are received does not alter the consequences we expect to see (Definition \ref{def:caus_exch}). The fact that we need a preliminary assumption of causal contractibility is similar to how, in the potential outcomes framework, a preliminary assumption of SUTVA is required in order to justify the use of potential outcomes.

Next, we assume that, no matter which choice $\alpha\in C$ is decided on, all identifiers can be swapped without altering the distribution over consequences, and finally that for each choice $\alpha\in C$ the treatment vector $\RV{D}$ is deterministic. Then, according to Theorem \ref{th:cc_ind_treat}, $\prob{P}_C$ is also $(\RV{D};\RV{Y})$-causally contractible. This can be extended to the case where $\RV{D}$ is a function of a ``random signal'' $\RV{R}$.

\begin{lemma}\label{lem:ind_to_cc}
If $\prob{P}_C$ is $((\RV{D},\RV{I});\RV{Y})$-causally contractible and $\RV{Y}\CI_{\prob{P}_C}^e \RV{I}C|\RV{D}$, then $\prob{P}_C$ is also $(\RV{D};\RV{Y})$-causally contractible.
\end{lemma}

\begin{proof}
For arbitrary $\nu\in \Delta(I^{\mathbb{N}})$, by assumption of $((\RV{D},\RV{I});\RV{Y})$-causal contractibility and Theorem \ref{th:iid_rep}
\begin{align}
    \prob{P}_C^{\RV{Y}|\RV{DI}} &= \tikzfig{index_independence_1}\\
    &= \tikzfig{index_independence_2}\\
    &= \tikzfig{index_independence_3}\\
    \implies \prob{P}_C^{\RV{Y}|\RV{D}} &= \tikzfig{index_independence_4}
\end{align}
Applying Theorem \ref{th:iid_rep} in reverse, we get $\prob{P}_C$ is $(\RV{D};\RV{Y})$-causally contractible.
\end{proof}

\begin{definition}[Index variable]
Suppose we have a probability set $\prob{P}_C$ and a variable $\RV{I}:\Omega\to \mathbb{N}^{\mathbb{N}}$, such that, defining $A\subset\mathscr{P}(\mathbb{N})$ to be the set of all permutations of $\mathbb{N}$, $\prob{P}_\alpha^{\RV{I}}(A)=1$. Then $\RV{I}$ is an \emph{index variable}.
\end{definition}

\begin{lemma}\label{lem:ind}
Suppose we have a probability set $\prob{P}_C$ where $\RV{Y}\CI_{\prob{P}_C}^e C|(\RV{D},\RV{I})$ and $\RV{I}$ is an index variable. If for each permutation $\rho:\mathbb{N}\to \mathbb{N}$
\begin{align}
    \prob{P}_\alpha^{\RV{Y}|\RV{ID}} &= (\text{swap}_{\rho(I)}\otimes \text{Id}_X )\prob{P}_\alpha^{\RV{Y}|\RV{ID}}
\end{align}
then $\RV{Y}\CI_{\prob{P}_C}^e \RV{I}C|\RV{D}$.
\end{lemma}

\begin{proof}
Taking $A\subset \mathbb{N}^{\mathbb{N}}$ to be the set of permutations of $\mathbb{N}$, note that for every $i\in A$, $B\in\sigalg{Y}^{\mathbb{N}}$, $d\in D^{\mathbb{N}}$ we can take $\rho_i:\mathbb{N}\to \mathbb{N}$ such that the image of $i$ under $\rho$ is $\mathbb{N}$; that is, $\rho_i(i)=\text{id}_{\mathbb{N}}$. Then
\begin{align}
    \prob{P}_C^{\RV{Y}|\RV{ID}}(B|i,d) &= (\text{swap}_{\rho_i(I)}\otimes \text{Id}_X )\prob{P}_C^{\RV{Y}|\RV{ID}}(B|i,d)\\
    &= \prob{P}_C^{\RV{Y}|\RV{ID}}(B|\text{id}_{\mathbb{N}},d)
\end{align}
Therefore
\begin{align}
    \prob{P}_C^{\RV{Y}|\RV{ID}} &\overset{\prob{P}_C}{\cong} \text{erase}_{\mathbb{N}^{\mathbb{N}}}\otimes \kernel{K} 
\end{align}
where $\kernel{K}:D^{\mathbb{N}}\kto Y^{\mathbb{N}}$ is the kernel
\begin{align}
    (B|d)\mapsto \prob{P}_C^{\RV{Y}|\RV{ID}}(B|\text{id}_{\mathbb{N}},d)
\end{align}
\end{proof}


\begin{theorem}\label{th:cc_ind_treat}
Suppose we have a probability set $\prob{P}_C$, $C$ countable, that $((\RV{D},\RV{I});\RV{Y})$-causally contractible for variables $\RV{Y}:\Omega\to Y^{\mathbb{N}}$, $D:\Omega\to D^{\mathbb{N}}$ and index variable $\RV{I}:\Omega\to \mathbb{N}^{\mathbb{N}}$. If for each $\alpha\in C$, $\rho:\mathbb{N}\to \mathbb{N}$
\begin{align}
    \prob{P}_\alpha^{\RV{Y}|\RV{I}} &= \text{swap}_{\rho(I)}\prob{P}_\alpha^{\RV{Y}|\RV{I}}
\end{align}
and there is an invertible function $f:C\to D$ such that
\begin{align}
    \prob{P}_\alpha^{\RV{D}} = \kernel{F}_f
\end{align}
then $\prob{P}_C$ is $(\RV{D};\RV{Y})$-causally contractible.
\end{theorem}

\begin{proof}
The map $\kernel{Q}:(B|i,\alpha)\mapsto \prob{P}_\alpha^{\RV{Y}|\RV{I}}(B|i)$ is itself a Markov kernel. By assumption, for any $\alpha$,
\begin{align}
    \kernel{Q} \overset{\prob{P}_\alpha}{\cong} (\text{id}_{\mathbb{N}^\mathbb{N}}\otimes\kernel{F}_f) \prob{P}_\alpha^{\RV{Y}|\RV{ID}}
\end{align}
Furthermore, by assumption
\begin{align}
    (\text{swap}_{\rho(I)}\otimes \text{Id}_{C} )\kernel{Q} = \kernel{Q}
\end{align}
Therefore
\begin{align}
    (\text{swap}_{\rho(I)}\otimes \text{Id}_{C} ) \prob{P}_\alpha^{\RV{Y}|\RV{ID}}&= (\text{id}_{\mathbb{N}^\mathbb{N}}\otimes\kernel{F}_{f})(\text{id}_{\mathbb{N}^\mathbb{N}}\otimes\kernel{F}_{f^-1})(\text{swap}_{\rho(I)}\otimes \text{Id}_{C} ) \prob{Q}\\
     &= (\text{id}_{\mathbb{N}^\mathbb{N}}\otimes\kernel{F}_{f^{-1}}) \prob{Q}\\
     &= \prob{P}_\alpha^{\RV{Y}|\RV{ID}}
\end{align}
Then by Lemma \ref{lem:ind} we have $\RV{Y}\CI_{\prob{P}_C}^e \RV{I}C|\RV{D}$ and by Lemma \ref{lem:ind_to_cc} we have $(\RV{D};\RV{Y})$-causal contractibility.
\end{proof}

If we suppose Theorem \ref{th:cc_ind_treat} applies to $C'\subset C$ such that $\RV{D}$ is deterministic for all $\alpha\in C'$, while $C$ consists of $C'$ and all ``random choices between elements of $C'$''; that is for all $\beta\in C$
\begin{align}
    \prob{P}_\beta &= \sum_{c\in C} k_c \prob{P}_c 
\end{align} 

Then it follows that
\begin{align}
    \prob{P}_\beta^{\RV{Y}|\RV{D}} &= \sum_{c\in C} k_c \prob{P}_c^{\RV{Y}|\RV{D}}
\end{align}
and hence $\prob{P}_C$ is still $(\RV{D};\RV{Y})$-causally contractible. Note that in order to actually implemented a random choice, we would typically consult a known random source $\RV{R}$ and set $\RV{D}$ deterministically on the basis of the value of $\RV{R}$.

\subsection{Body mass index revisited}

If we have a probability set $\prob{P}_C$ with $\RV{B}:=(\RV{B}_i)_{i\in M}$ representing body mass index and $\RV{Y}:=(\RV{Y}_i)_{i\in M}$ representing health outcomes of interest, the previous considerations don't support a judgement of causal contractibility for $(\RV{B};\RV{Y})$, because the choices we imagine we might have available do not allow $\RV{B}$ to be a deterministic invertible function of the choice. Note that we haven't established that causal contractibility cannot be appropriate, merely that we have no reason to accept it on the basis of arguments so far.

Causal contractibility is the a priori assumption that there is a response function relating each pair from a sequence of pairs of variables. However, we could also consider the possibility that we conclude that there is such a response function after reviewing the data.

Suppose we are in possession of a $(\RV{D};(\RV{B},\RV{Y}))$-causally contractible probability set $\prob{P}_C$, such that each $(\RV{D}_i,\RV{B}_i,\RV{Y}_i)$ is related by the response conditional $\prob{P}_C^{\RV{Y}_1\RV{B}_1|\RV{D}_1\RV{H}}$. Suppose that we also have an ``oracle'' available that performs an infinite number of samples under appropriate conditions and reveals the value $h\in H$ yielded by the variable $\RV{H}$. Then we can consider the new probability set $\prob{P}_{C,h}$ where for arbitrary $\RV{Z}:\Omega\to Z$, $A\in \sigalg{Z}$
\begin{align}
    \prob{P}_{C,h}^{\RV{Z}}(A) &= \prob{P}_C^{\RV{Z}|\RV{H}}(A|h)
\end{align}

Note that $\prob{P}_{C,h}$ remains $(\RV{D};(\RV{B},\RV{Y}))$-causally contractible with response conditionals $\prob{P}_{C,h}^{\RV{Y}_1\RV{B}_1|\RV{D}_1}$. Furthermore that by Theorem \ref{th:higher_order_conditionals}, we have $((\RV{D},\RV{B});\RV{Y})$-causal contractibility with response conditionals $\prob{P}_{C,h}^{\RV{Y}_1|\RV{D}_1\RV{B}_1}$. In this case, we could find that $\RV{Y}_i\CI_{\prob{P}_{C,h}}^e \RV{D}_i|\RV{B}_i$, and so by Lemma \ref{lem:ind_to_cc} $\prob{P}_{C,h}$ is also $(\RV{B};\RV{Y})$-causally contractible.

In this case, it seems much more reasonable to describe the fact that $\RV{B}$ has a causal effect on $\RV{Y}$ as a \emph{finding}, rather than an \emph{assumption}. 

\section{Allowing dependence on observations}

We want to remove the assumption $\RV{D}_i\CI_{\prob{P}_C}^e \RV{Y}_{<i} C|\RV{D}_{<i}$. It turns out that this is fairly straightforward to do -- essentially, we require that causal contractibility holds for some $C'$ such that $\RV{D}_i\CI_{\prob{P}_{C'}}^e \RV{Y}_{<i} C|\RV{D}_{<i}$, and then we require that $\prob{P}_C$ extends $\prob{P}_{C'}$ in the appropriate way. Before we do this, however, we will introduce the idea of a \emph{comb}, which is an important generalisation of the idea of a conditional probability. The result we will eventually arrive at is that a model features response conditionals if it has a causally contractible comb of the appropriate type.

To begin with an example, consider a probability set $\prob{P}_C$, variables $\RV{D}_{\mathbb{N}}$ and $\RV{Y}_{\mathbb{N}}$ and a subsequence $(\RV{D}_i,\RV{Y}_i)_{i\in [2]}$ of length 2. Lifting the assumption $\RV{D}_i\CI_{\prob{P}_C}^e \RV{Y}_{<i} C|\RV{D}_{<i}$ means that only the following holds:
\begin{align}
    \RV{Y}_i&\CI^e_{\prob{P}_C} \RV{Y}_{[\mathbb{N}]\setminus i},\RV{D}_{\mathbb{N}\setminus i}C|\RV{H}\RV{D}_i&\forall i\in \mathbb{N}\\
    \land \RV{H} &\CI^e_{\prob{P}_C} \RV{D} C\\
    \land \prob{P}_C^{\RV{Y}_i|\RV{H}\RV{D}_i} &= \prob{P}^{\RV{Y}_0|\RV{H}\RV{D}_0} & \forall i\in \mathbb{N}
\end{align}

In this case, for arbitrary $\alpha\in C$
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_{[2]}} &= \tikzfig{response_conditional_comb}
\end{align}
we still have response conditionals $\prob{P}_C^{\RV{Y}_i|\RV{D}_i\RV{H}}$ but now the $\RV{D}_i$'s can depend on the previous $\RV{Y}_i$'s.

Given $\prob{P}_C^{\RV{Y}_1|\RV{D}_1\RV{H}}$ and $\prob{P}_C^{\RV{Y}_2|\RV{D}_2\RV{H}}$, we can define
\begin{align}
    \prob{P}_C^{\RV{Y}_{[2]}\combbreak \RV{D}_{[2]}} := \tikzfig{causally_contractible_comb}
\end{align}
and $\prob{P}_C^{\RV{Y}_{[2]}\combbreak \RV{D}_{[2]}}$ is still ``$(\RV{D};\RV{Y})$-causally contractible'', but it is no longer a uniform conditional probability. It is is a \emph{uniform 2-comb}.

\subsection{Combs}

Where uniform conditional probabilities map probability distributions to probability distributions via the semidirect product, 2-combs map conditional probabilities to conditional probabilities via an ``insert'' operation. Similarly, higher order combs map  Graphically, the semidirect product looks like
\begin{align}
    \prob{P}_\alpha^{\RV{XY}}&=\prob{P}_\alpha^{\RV{X}}\cprod\prob{P}_C^{\RV{Y}|\RV{X}}\\
    &= \tikzfig{conditional_semidirect_product}
\end{align}
and the insert operation looks like
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_{1}\RV{D}_2\RV{Y}_2|\RV{D}_1}&=\text{insert}(\prob{P}_\alpha^{\RV{D}_2|\RV{D}_1\RV{Y}_1},\prob{P}_C^{\RV{Y_{[2]}}\combbreak\RV{D}_{[2]}})\\
    &= \tikzfig{comb_insert_complicated}\label{eq:comb_insert_complicated}\\
    &= \tikzfig{comb_insert_gettingsimpler}\\
    &= \tikzfig{comb_insert_simple}\label{eq:comb_insert_simple}
\end{align}

While Equation \ref{eq:comb_insert_complicated} is a well-formed string diagram in the category of Markov kernels, Equation \ref{eq:comb_insert_simple} is not. In the case that all the underlying sets are discrete, Equation \ref{eq:comb_insert_simple} can be defined using an extended string diagram notation appropriate for the category of real-valued matrices \citep{jacobs_causal_2019}. We're not going to go to introduce the extension here, but we will give an algebraic definition of the insert operation for discrete sets.

\begin{definition}[Uniform $n$-Comb]
Given a probability set $\prob{P}_C$ with variables $\RV{Y}_i:\Omega\to Y$ and $\RV{D}_i:\Omega\to D$ for $i\in [n]$ an uniform conditional probabilities $\{\RV{P}_C^{\RV{Y}_i|\RV{D}_{[i]}\RV{Y}_{[i-1]}}|i\in [n]\}$, the uniform $n$-comb $\prob{P}_C^{\RV{Y}_{[n]}\combbreak \RV{D}_{[n]}}:D^n\kto Y^n$ is the Markov kernel given by the recursive definition
\begin{align}
    \prob{P}_C^{\RV{Y}_{1}\combbreak \RV{D}_{1}} &= \prob{P}_C^{\RV{Y}_1|\RV{D}_1}\\
    \prob{P}_C^{\RV{Y}_{[m]}\combbreak \RV{D}_{[m]}} &= \tikzfig{comb_inductive}
\end{align}
\end{definition}

\begin{definition}[Uniform $\mathbb{N}$-comb]
Given a probability set $\prob{P}_C$ with variables $\RV{Y}_i:\Omega\to Y$ and $\RV{D}_i:\Omega\to D$ for $i\in \mathbb{N}$ and uniform conditional probabilities $\{\RV{P}_C^{\RV{Y}_i|\RV{D}_{[i]}\RV{Y}_{[i-1]}}|i\in \mathbb{N}\}$, the uniform $\mathbb{N}$-comb $\prob{P}_C^{\RV{Y}_{\mathbb{N}}\combbreak \RV{D}_{\mathbb{N}}}:D^\mathbb{N}\kto Y^\mathbb{N}$ is the Markov kernel such that for all $n\in \mathbb{N}$
\begin{align}
    \prob{P}_C^{\RV{Y}_{\mathbb{N}}\combbreak \RV{D}_{\mathbb{N}}}(\mathrm{id}_{Y^{n}}\otimes \mathrm{del}_{Y^{\mathbb{N}}}) &= \prob{P}_C^{\RV{Y}_{[n]}\combbreak \RV{D}_{[n]}}\otimes \mathrm{del}_{Y^{\mathbb{N}}}
\end{align}
\end{definition}

\begin{theorem}[Existence of $\mathbb{N}$-combs]
Given a probability set $\prob{P}_C$ with variables $\RV{Y}_i:\Omega\to Y$ and $\RV{D}_i:\Omega\to D$ for $i\in \mathbb{N}$ and uniform conditional probabilities $\{\RV{P}_C^{\RV{Y}_i|\RV{D}_{[i]}\RV{Y}_{[i-1]}}|i\in \mathbb{N}\}$, a uniform $\mathbb{N}$-comb $\prob{P}_C^{\RV{Y}_{\mathbb{N}}\combbreak \RV{D}_{\mathbb{N}}}:D^\mathbb{N}\kto Y^\mathbb{N}$ exists.
\end{theorem}

\begin{proof}
For each $n\in \mathbb{N}$ $m<n$, we have
\begin{align}
    \prob{P}_C^{\RV{Y}_{[n]}\combbreak \RV{D}_{[n]}}(\mathrm{id}_{Y^{n-m}})\otimes \mathrm{del}_{Y^m}) &= \prob{P}_C^{\RV{Y}_{[n-m]}\combbreak \RV{D}_{[n-m]}}\otimes \mathrm{del}_{Y^m}
\end{align}

Therefore the existence of $\prob{P}_C^{\RV{Y}_{\mathbb{N}}\combbreak \RV{D}_{\mathbb{N}}}$ is a consequence of Lemma \ref{lem:infinitely_extended_kernels}.
\end{proof}

We now define the insert operation for discrete sets only

\begin{definition}[Comb insert]
Given an $n$-comb $\prob{P}_C^{\RV{Y}_{[n]}\combbreak \RV{D}_{[n]}}$ and an $n-1$ comb $\prob{P}_\alpha^{\RV{D}_{[1,n]}|\RV{Y}_{[n-1]}}$, $(D,\sigalg{D})$ and $(Y,\sigalg{Y})$ discrete, for all $y_i\in Y$ and $d_i\in D$
\begin{align}
    \mathrm{insert}(\prob{P}_\alpha^{\RV{D}_{[1,n]}\combbreak \RV{Y}_{[n-1]}},\prob{P}_C^{\RV{Y}_{[n]}\combbreak \RV{D}_{[n]}})(y_{[n]},d_{[n-1]}|d_1) &= \prob{P}_C^{\RV{Y}_{[n]}\combbreak \RV{D}_{[n]}}(y_n|d_{[1,n]},d_1)\prob{P}_\alpha^{\RV{D}_{[1,n]}\combbreak\RV{Y}_{[n-1]}}(d_{[1,n]}|y_{[n-1]})
\end{align}
\end{definition}

\subsection{Response conditionals in models with history dependence}

The main theorem of this section, Theorem \ref{th:response_hdep} is an almost trivial extension of Theorem \ref{th:iid_rep}. Instead of starting with a probability set $\prob{P}_C$ with a uniform conditional $\prob{P}_C^{\RV{Y}|\RV{D}}$, we assume instead a collection of uniform conditionals $\prob{P}_C^{\RV{Y}_i|\RV{D}_{\leq i}\RV{Y}_{<i}}$ (this is a strictly weaker condition, as we can derive these as higher order conditionals from the assumption of $\prob{P}_C^{\RV{Y}|\RV{D}}$). Theorem \ref{th:response_hdep} then says if the $\mathbb{N}$-comb $\prob{P}_C^{\RV{Y}\combbreak\RV{D}}$ is causally contractible, then we have response conditionals $\prob{P}_C^{\RV{Y}_i|\RV{D}_i\RV{H}}$. Because $\prob{P}_C^{\RV{Y}\combbreak\RV{D}}$ is a Markov kernel with the same signature as $\prob{P}_C^{\RV{Y}|\RV{D}}$, the reasoning from Theorem \ref{th:response_hdep} carries over almost directly..

Unlike causal contractibility in the history independent case, we are not aware of an equivalent condition to the causal contractibility of $\prob{P}_C^{\RV{Y}\combbreak\RV{D}}$ in terms of relabeled random variables. The difficulty is, because $\RV{D}_i$ can depend on $\RV{Y}_{<i}$, we usually don't have symmetry with respect to relabeling variables.

\begin{theorem}[]\label{th:response_hdep}
Suppose we have a sample space $(\Omega,\sigalg{F})$, a probability set $\prob{P}_C$ and variables $\RV{D}:=(\RV{D}_i)_{i\in \mathbb{N}}$ and $\RV{Y}:=(\RV{Y}_i)_{i\in \mathbb{N}}$ with uniform $\mathbb{N}$-comb $\prob{P}_C^{\RV{Y}\combbreak \RV{D}}$. $\prob{P}_C^{\RV{Y}\combbreak \RV{D}}$ is causally contractible if and only if $\Omega$ can be extended with some $\RV{H}:\Omega\times H\to H$ such that $\prob{P}^{\RV{H}}_C$ and $\prob{P}_C^{\RV{Y}_i|\RV{H}\RV{D}_i}$ exist for all $i\in \mathbb{N}$ and
\begin{align}
    \prob{P}_C^{\RV{Y}\combbreak\RV{D}} &= \tikzfig{do_model_representation}\\
    &\iff\\
    \RV{Y}_i&\CI^e_{\prob{P}_C} \RV{Y}_{\mathbb{N}\setminus i},\RV{D}_{\mathbb{N}\setminus i}C|\RV{H}\RV{D}_i&\forall i\in \mathbb{N}\\
    \land \RV{H} &\CI^e_{\prob{P}_C} \RV{D} C\\
    \land \prob{P}_C^{\RV{Y}_i|\RV{H}\RV{D}_i} &= \prob{P}^{\RV{Y}_0|\RV{H}\RV{D}_0} & \forall i\in \mathbb{N}
\end{align}
Where $\Pi_{D,i}:D^\mathbb{N}\kto D$ is the $i$th projection map.
\end{theorem}

\begin{proof}
Apply Lemma \todo{lemma I've yet to prove} to $\prob{P}_C^{\RV{Y}\combbreak \RV{D}}$.
\end{proof}

\subsection{Validity}

\subsection{Combs are the output of the ``fix'' operation}

There is a relationship between combs and the ``fix'' operation defined in \citet{richardson_nested_2017}. In particular, suppose we have a probability $\prob{P}_\alpha$ and a comb $\prob{P}_\alpha^{\RV{Y}_{[2]}|\RV{D}_{[2]}}$. Then (assuming discrete sets)
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_{[2]}|\RV{D}_{[2]}}(y_1,y_2|d_1,d_2) &= \prob{P}_\alpha^{\RV{Y}_1|\RV{D}_1}(y_1|d_1)\prob{P}_\alpha^{\RV{Y}_2|\RV{D}_2}(y_2|d_2)\\
    &= \frac{\prob{P}_\alpha^{\RV{Y}_1|\RV{D}_1}(y_1|d_1)\prob{P}_\alpha^{\RV{D}_2|\RV{Y}_1\RV{D}_1}(d_2|y_1,d_1)\prob{P}_\alpha^{\RV{Y}_2|\RV{D}_2}(y_2|d_2)}{\prob{P}_\alpha^{\RV{D}_2|\RV{Y}_1\RV{D}_1}(d_2|y_1,d_1)}\\
    &= \frac{\prob{P}_\alpha^{\RV{Y}_[2]\RV{D}_2|\RV{D}_1}(y_1,y_2,d_2|d_1)}{\prob{P}_\alpha^{\RV{D}_2|\RV{Y}_1\RV{D}_1}(d_2|y_1,d_1)}
\end{align}
This is precisely the ``division by a conditional probability'' used in the fix operation. We speculate that the fix operation is precisely an alternative definition of an $n$-comb, but we have not proven this.


\section{Weaker assumptions than causal contractibility}

