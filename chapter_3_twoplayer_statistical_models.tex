
%!TEX root = main.tex

\chapter{Two player statistical models and see-do models}

\todo[inline]{These are ``todo'' notes. All such notes that involve theoretical development are also collected in an unordered list of outstanding theoretical questions}

In this chapter I introduce two types of model. Models of the first type are called \emph{two player statistical models} and the second type are a special class of the first called \emph{see-do models}. Fundamentally, each of these is just a particular kind of stochastic function. The reason we are interested in these kinds of stochastic functions is that is that almost all causal models are instances of see-do models. Before introducing two player models and discussing what makes them causal, it is worth briefly considering models in statistics and machine learning generally.

A \emph{world model} is something I will informally define as a family of ``descriptions'' indexed by hypotheses $\{R_h|h\in H\}$. The set $H$ represents hypotheses or proposals for how the world ought to be described, and each proposal $h\in H$ entails some description of the world $\prob{R}_h$. Some examples of world models:

\begin{itemize}
    \item A linear regressor may take some data $\mathbf{x}$ and $\mathbf{y}$ and returns a parameter $\beta\in B$ with the property that $(\mathbf{y}-\mathbf{x}^T \beta)^2$ is small. A normal way to interpret the parameter $\beta$ is to consider it to be a proposal about how some phenomenon of interest should be described, with this description explicitly given by the function $f:x\mapsto \beta x$.
    \item A neural network used in classification may take data $\mathbf{x}$ and labels $\mathbf{y}$ and returns parameters $\mathbf{w}\in W$ with the property that $-\mathbf{y} \log \mathbf{x}+(1-y)\log(1-\mathbf{x})$ is small. Each $\mathbf{w}$ is a proposal for how to classify data and the classification rule associated with each $\mathbf{w}$ is a function $x\mapsto f(\mathbf{w},x)$.
    \item A crude description of a general election pre-poll result can be given by the ``true fraction'' $\theta$ of voters for each candidate and, under some unreasonably strong sampling assumptions, and the results of the survey for each $\theta$ can be described by $\prod_N\prob{P}_\theta^\RV{X}$ where $N$ is the number of voters surveyed and $\RV{X}$ is the vote choice of each.
\end{itemize}

In the first two examples the ``description'' that goes with each hypothesis is a function, while in the third example the descriptions are probability measures. In almost all practical cases, these descriptions of the world do not tell us exactly how the world will turn out under each hypothesis, but at best offer us a prediction that is as good as we can hope for. Probability is the tool that is very widely used to formalise such ``descriptions with uncertainty''. Say I have two different linear regressors: one which minimises squared error on the training data and one that always returns $\beta=10$. I want to ask which one produces descriptions that are more fit for my purpose. It is pointless to ask which one is correct because, in general, I cannot know that either will offer a description that is even approximately correct. However, I can consider a second level world model $\{\prob{P}^{\RV{X}\RV{Y}}_\alpha|\alpha\in A\}$ in which the phenomenon of interest is described by a family of probability measures, and then I can ask, given an $\alpha$, which $\beta$ is my regressor likely to return and how closely will $x\mapsto \beta x$ be to $\mathbb{E}_{\prob{P}_\alpha}[\RV{Y}|\RV{X}]$ for each likely choice. Generally, if I need to model a world with uncertainty I will need a world model that is an indexed family of probability measures.

A world model that consists of a family of probability measures $\{\prob{P}_h|h\in H\}$ is a \emph{statistical model} or \emph{statistical experiment}. Because I almost always need to Statistical models can be found everywhere in theoretical statistics and machine learning \cite{fisher_statistical_1992,le_cam_comparison_1996,freedman_asymptotic_1963,de_finetti_foresight_1992,vapnik_nature_2013,wald_statistical_1950}. A key point about statistical models -- even if I can only state it somewhat vaguely -- is that the truth of any hypothesis $h\in H$ has no dependence on what I might want to be true. As a user of statistical models, I have no authority to choose a hypothesis -- this is Nature's choice alone. 

I can sometimes make choices that will affect the way that the future turns out. I might have some set $D$ of choices I can make, and for each $d\in D$ I require a description of the results of my choice. Just as the results of hypotheses are often uncertain, so are the results of choices. I might be motivated to choose a probability measure $\prob{P}_d$ to describe them, maybe because it is common to do so or because I find arguments for subjective expected utility theory compelling \citep{steele_decision_2020}. A family of probability measures indexed by a set of choices $\{\prob{P}_d|d\in D\}$ will be called a \emph{consequence model}.

Statistical models and consequence models are both families of probability measures indexed by arbitrary sets, which we have called hypotheses $H$ and choices $D$ respectively. These sets are distinguished by how they are interpreted when a two-player statistical model is used in the course of solving some kind of problem. The difference can be informally summarised in this manner: I do not get to tell Nature what choice $h\in H$ she makes, and Nature does not get to tell me what choice $d\in D$ I make. It will often be the case that I have multiple choices that can affect how the world turns out \emph{and} I have multiple hypotheses about how each choice will affect the world. In this case, I will have a \emph{two-player statistical model} $\{\prob{P}_{h,d}|h\in H,d\in D\}$. 

So far I have explained the distinction between ``player 1'' and ``player 2'' in vague metaphorical terms. If I am using a two-player statistical model in the context of a well defined problem such as ``given data, what choice should I make?'' then we can say precisely what $H$ and $D$ are and what role each plays in the problem. However, the field of causal inference includes other types of problem such as counterfactual problems which involve a choice set $D$ that plays a different role to the choice set in decision problems. Thus, while I will argue that causal models are two-player statistical models, and the second player is what distinguishes them from ordinary statistical models, the same kind of model can be used with different interpretations of what the second player's choices represent. This will be explored in more detail in the coming chapters.

\todo[inline]{Note to proof readers: I moved the discussion of decomposability to the next chapter so I can introduce it alongside the result that uses it}

\section{Two player statistical models and see-do models}

Two player statistical models were introduced as doubly indexed sets of probability measures $\{\prob{P}_{h,d}|h\in H,d\in D\}$. If each $\prob{P}_{h,d}\in \Delta(\sigalg{E})$ for some measurable space $(E,\sigalg{E})$, the indexed set is equivalent to a function $H\times D\to \Delta(\sigalg{E})$. In the following work, we will make two simplifying assumptions:

\begin{enumerate}
    \item A two player statistical model can be represented by a \emph{Markov kernel} $\kernel{T}:H\times D\to \Delta(\sigalg{E})$
    \item The kernel space $(\kernel{T},(H\times D,\sigalg{H}\otimes\sigalg{D}),(E,\sigalg{E}))$ admits disintegrations $\kernel{T}^{\RV{Y}|\RV{XDH}}$ for arbitrary random variables $\RV{X},\RV{Y}$ on $H\times D\times E$ and domain variable $\RV{D}\utimes\RV{H}$
\end{enumerate}

The first condition amounts to the additional requirement that $(h,d)\mapsto \kernel{T}_{h,d}(A)$ is measurable for every $A\in \sigalg{H}\otimes\sigalg{D}\otimes\sigalg{E}$, and sufficient for the second condition is that $D\times H$ is countable and $X\times Y$ standard measurable (though this is not necessary, see Theorem \ref{th:existence_continous}).

\begin{definition}[Two player statistical model]\label{def:2p_stat}
A \emph{two-player statistical model} $(\kernel{T},\RV{H},\RV{D},\RV{O})$ is a Markov kernel $\kernel{T}:H\times D\to \Delta(\sigalg{O})$ such that, for any random variables $\RV{X}: H\times D\times O\to X$ and $\RV{Y}:H\times D\times O\to Y$, a disintegration $\kernel{K}^{\RV{Y}|\RV{XDH}}:X\times D\times H\to \Delta(\sigalg{Y})$ exists along with three distinguished random variables: the \emph{hypothesis} $\RV{H}:H\times D\times O\to H$ given by $(h,d,o)\mapsto h$ (forgetting the choice and outcome) and the \emph{choice} $\RV{D}:H\times D\times O\to D$ given by $(h,d,o)\mapsto d$ (forgetting the hypothesis and outcome) and the \emph{outcome} $\RV{O}:H\times D\times O\to O$ given by $(h,d,o)\mapsto o$ (forgetting the choice and hypothesis).
\end{definition}

Decision problems involving often involve some data $\RV{X}$ is observed, then a choice is made, then the consequences $\RV{Y}$ are observed. In such a model, the observed data $\RV{X}$ cannot be affected by the choice. These models will be called \emph{see-do} models to capture the assumption that there is an order in which seeing and doing happen.

\begin{definition}[See-Do model]\label{def:seedo}
A \emph{see-do model} $(\kernel{T},\RV{H},\RV{D},\RV{X},\RV{Y})$ is a two-player statistical model $(\kernel{T},\RV{H},\RV{D},\RV{O})$ with two additional distinguished random variables: the \emph{observation} $\RV{X}: H\times D\times O\to X$ and the \emph{consequence} $\RV{Y}:H\times D\times O\to Y$ such that the outcome is the coupled product of the observation and the consequence $\RV{O}=\RV{X}\utimes\RV{Y}$. A see-do model must observe the conditional independence $\RV{X}\CI_\kernel{T} \RV{D}|\RV{H}$, i.e. the observation is independent of the choice conditional on the hypothesis.

Because $\RV{O}=\RV{X}\utimes \RV{Y}$, we do not need to explicitly define $\RV{O}$ when specifying a see-do model.
\end{definition}


\section{Frequentist random variables and Bayesian forecasts}

We've chosen to represent the ``description of the world'' using probability. This is an overwhelmingly common choice for describing things with uncertainty, but it is worth asking precisely what is being described here. It's well-known that probability is suitable for representing a number of different things. Two common choices are:

\begin{enumerate}
    \item The long run convergence of relative frequencies of sequences or ensembles of observations of certain types of systems (\emph{frequentist probability})
    \item Forecasts of observations that will take place in the future (\emph{Bayesian forecasts})
\end{enumerate}

The first view is a very common interpretation of probability as it is used in statistical models. One can view each hypotheses in a statistical model as representing the proposition that the system will tend to produce long sequences of observations with relative frequencies consistent with the associated probability measure. For example, given a possibly loaded die, we might entertain hypotheses a) it is a system that produces a 6 $\frac{1}{6}$ of the time, b) it is a system that produces a 6 $\frac{1}{4}$ of the time, and many other hypotheses besides. 

On the other hand, if I view a sequence of random variables as a sequence of \emph{Bayesian forecasts} then I do not strictly need to entertain a set of hypotheses regarding the long run relative frequencies of observations. If $\RV{X}_1,\RV{X}_2,\RV{X}_3,...$ are rolls of a possibly loaded die, then I can make forecasts from a single probability distribution over $\RV{X}_{[n]}$ (sometimes called the ``posterior distribution''). Furthermore, this distribution need not have any particular frequentist properties: I can forecast the probability of a $6$ is, for each roll, $1,0,1,1,0,0,0,0,1,1,1,1,1,1,1,1,1$ with no dependence on previusly seen rolls, such that relative frequencies never converge. Provided my forecast is a valid probability measure, it may seem unwise but it is still a well formulated Bayesian forecast.

\citet{hajek_interpretations_2019} has a detailed discussion of various interpretations of probability along with strengths and criticisms of each.

For our purposes, however, an important observation is this: two player statistical models and see-do models are defined by analogy with statistical models. While statistical models typically represent \emph{frequentist probability}, the random variable $\RV{Y}$ represents the consequences of a choice $\RV{D}$. We often want to use the consequence map from $\RV{D}$ to $\RV{Y}$ to help us select a choice - intuitively, this suggests that $\RV{Y}$ is actually a forecast of the consequences of a choice that we might make.

It is similarly not entirely clear what interpretation of probability is being used in Causal Bayesian Networks. While probability is discussed from a Bayesian point of view in chapter 1 of \citet{pearl_causality:_2009}, this chapter also mentions

\begin{quote}
[...] causal relationships are ontological, describing objective physical constraints in our world, whereas probabilistic relationships are epistemic, reflecting what we know or believe about the world.
\end{quote}

Causal relationships in the Causal Bayesian Network framework are expressed by terms like $\prob{P}(\RV{Y}|do(\RV{X}))$ which represent Markov kernels mapping values of $X$ to distributions on $Y$. Presumably a ``correct'' choice for such a Markov kernel should somehow be compatible with the ``objective physical constraints'' imposed by causal relationships. While this compatibility need not be given in terms of a relathionship between the relative frequency limits of $\RV{Y}$ under $\prob{P}(\RV{Y}|do(\RV{X}))$ and some ``actual relative frequency of repeated observations of the world'', this is a reasonably obvious choice to make and alternatives are not to be found in \citet{pearl_causality:_2009}.

Representation theorems provide a bridge between the frequentist and Bayesian perspectives and can help us to clarify what exactly we are using probabilities to describe. In particular, forecasts that satisfy an assumption known as \emph{exchangeability}, which informally means that the \emph{the order of observations does not matter}, induce statistical models in which each hypothesis stands for a frequentist model of the world. Similarly, ``see-do forecasts'' (defined below) that satisfy \emph{double exchangeability}, which adds to exchangeability the assumption that \emph{the order of consequences also doesn't matter}, induces a see-do model where each hypothesis again stands for a frequentist model.

This is not a complete solution to the problem of what probability represents in see-do models, or causal models generally, but it is a start. In addition, the following work introduces some key ideas which will be revisited in later discussion of counterfactuals and identifiability.

\begin{definition}[Forecasts, see-do forecast]
A \emph{do forecast} $(\kernel{F},\RV{D},\RV{O})$ is a Markov kernel $\kernel{F}:D\to \Delta(\sigalg{O})$ for some set of choices $D$ and outcomes $O$. The choice variable $\RV{D}:D\times O\to D$ is the map $(d,e)\mapsto d$ that forgest the outcome and and the outcome variable $\RV{O}:D\times O\to O$ is the map $(d,o)\to o$ that forgets the choice.

A \emph{see-do forecast} is a forecast $(\kernel{F},\RV{D},\RV{X},\RV{Y})$ with an \emph{observation variable} $\RV{X}:D\times O\to X$ and a \emph{consequence variable} $\RV{Y}:D\times O\to Y$ such that $\RV{O}=\RV{X}\utimes\RV{Y}$ and $\RV{X}\CI\RV{D}$.

A \emph{forecast} $(\kernel{F},\RV{O})$ is a probability measure $\prob{F}\in\Delta(\sigalg{O})$ and an outcome variable $\RV{O}:O\to O$.
\end{definition}

We can go from a see-do model to a see-do forecast by adding a prior to the model.

\begin{theorem}
Given a two player statistical model $(\kernel{K},\RV{H},\RV{D},\RV{O})$ and any prior $\mu\in \Delta(\sigalg{H})$, defining $\kernel{L}:=(\mu\otimes \mathrm{Id}_D)\kernel{K}$ we have $(\kernel{L},\RV{D},\RV{O})$ is a do-forecast. 

If $(\kernel{K},\RV{H},\RV{D},\RV{X},\RV{Y})$ is a see-do model then, defining $\kernel{L}$ as before, $(\kernel{L},\RV{D},\RV{X},\RV{Y})$ is a see-do forecast.
\end{theorem}


\begin{proof}
The first part is trivial: $(\mu\otimes \mathrm{Id}_D)\kernel{K}$ is a Markov kernel $D\to \Delta(\sigalg{O})$ by construction, and $\RV{D}$ and $\RV{O}$ are choice and outcome variables by definition of the original two player statistical model.

For the second part $\RV{X}\CI_{\kernel{L}}\RV{D}$ is required. By Theorem \ref{th:iterated_disint} we have

\begin{align}
    \kernel{K} &= \kernel{K}^{\RV{X}\RV{Y}|\RV{HD}}\\
    &= \begin{tikzpicture}
        \path (0,0) node (H) {$\RV{H}$}
        + (0,-1.15) node (D) {$\RV{D}$}
        ++ (0.5,0) node[copymap] (copy0) {}
        ++ (0.7,0) node[kernel] (X) {$\kernel{K}^{\RV{X}|\RV{H}}$}
        ++ (0.5,0) node[copymap] (copy1) {}
        +  (0.8,-1) node[kernel] (Y) {$\kernel{K}^{\RV{Y}|\RV{XHD}}$}
        ++ (2,0) node (Xr) {$\RV{X}^{sd}$}
        +  (0,-1) node (Yr) {$\RV{Y}^{sd}$};
        \draw (H) -- (X) -- (Xr);
        \draw (copy0) to [out=-90,in=180] ($(Y.west) + (0,0)$) 
              (copy1) to [out=-90,in=180] ($(Y.west) + (0,0.15)$)
              (D) -- ($(Y.west) + (0,-0.15)$);
        \draw (Y) -- (Yr);
    \end{tikzpicture}
\end{align}

It follows that

\begin{align}
    \kernel{L}^{\RV{X}\RV{Y}|\RV{D}} &= \kernel{L}\\
    &= (\mu\otimes \mathrm{Id}_D)\kernel{K}\\
    &=  \begin{tikzpicture}
        \path (0,0) node[dist] (H) {$\mu$}
        + (0,-1.15) node (D) {$\RV{D}$}
        ++ (0.5,0) node[copymap] (copy0) {}
        ++ (0.7,0) node[kernel] (X) {$\kernel{K}^{\RV{X}|\RV{H}}$}
        ++ (0.5,0) node[copymap] (copy1) {}
        +  (0.8,-1) node[kernel] (Y) {$\kernel{K}^{\RV{Y}|\RV{XHD}}$}
        ++ (2,0) node (Xr) {$\RV{X}$}
        +  (0,-1) node (Yr) {$\RV{Y}$};
        \draw (H) -- (X) -- (Xr);
        \draw (copy0) to [out=-90,in=180] ($(Y.west) + (0,0)$) 
              (copy1) to [out=-90,in=180] ($(Y.west) + (0,0.15)$)
              (D) -- ($(Y.west) + (0,-0.15)$);
        \draw (Y) -- (Yr);
    \end{tikzpicture}
\end{align}

Then

\begin{align}
    \kernel{L}^{\RV{X}|\RV{D}} &= \begin{tikzpicture}
        \path (0,0) node[dist] (H) {$\mu$}
        + (0,-1.15) node (D) {$\RV{D}$}
        ++ (0.5,0) node[copymap] (copy0) {}
        ++ (0.7,0) node[kernel] (X) {$\kernel{K}^{\RV{X}|\RV{H}}$}
        ++ (0.5,0) node[copymap] (copy1) {}
        +  (0.8,-1) node[kernel] (Y) {$\kernel{K}^{\RV{Y}|\RV{XHD}}$}
        ++ (2,0) node (Xr) {$\RV{X}$}
        +  (0,-1) node (Yr) {};
        \draw (H) -- (X) -- (Xr);
        \draw (copy0) to [out=-90,in=180] ($(Y.west) + (0,0)$) 
              (copy1) to [out=-90,in=180] ($(Y.west) + (0,0.15)$)
              (D) -- ($(Y.west) + (0,-0.15)$);
        \draw[-{Rays[n=8]}] (Y) -- (Yr);
    \end{tikzpicture}\\
    &= \begin{tikzpicture}
        \path (0,0) node[dist] (H) {$\mu$}
        + (0,-1.) node (D) {$\RV{D}$}
        ++ (1.2,0) node[kernel] (X) {$\kernel{K}^{\RV{X}|\RV{H}}$}
        ++ (2.5,0) node (Xr) {$\RV{X}$}
        +  (0,-1) node (Yr) {};
        \draw (H) -- (X) -- (Xr);
        \draw[-{Rays[n=8]}] (D) -- (Yr);
    \end{tikzpicture}
\end{align}

And so $\RV{X}\CI_{\kernel{L}}\RV{D}$.
\end{proof}

In addition, any see-do forecast can be interpreted as a see-do model with a single hypothesis. Recalling the discussion of the indiscrete space $\{*\}$ in \ref{sec:string_diagram_elements}, we can identify a Markov kernel $\kernel{F}:D\to \Delta(\sigalg{E})$ with a Markov kernel $\kernel{T}:\{*\}\times D\to \Delta(\sigalg{E})$ where $\kernel{T}_{*,d}=\kernel{F}_d$ for all $d\in D$. Defining the hypothesis $\RV{H}:\{*\}\times D\times E\to H$ given by the constant fuction $(*,d,e)\mapsto *$, we can create from any see-do forecast $(\kernel{F},\RV{D},\RV{X},\RV{Y})$ a see-do model $(\RV{T},\RV{H},\RV{D},\RV{X},\RV{Y})$ (the required conditional independence is observed by construction in the single hypothesis $*$). However, this single hypothesis model is typically not a \emph{frequentist model}.

\citet{de_finetti_foresight_1992} has shown how frequentist models in particular can be recovered from exchangeable forecasts. Informally speaking, if and only if a forecast $(\prob{P},\RV{O})$ has the property that distribution of a sequence of random variables $\prob{P}^{\RV{X}_1\RV{X}_2\RV{X}_3}$ is identical to the distribution of any permutation of the sequence $\prob{P}^{\RV{X}_2\RV{X}_1\RV{X}_3}$ (an assumption known as \emph{exchangeability}), and this sequence can be extended infinitely, then there exists a hypothesis class $(H,\sigalg{H})$, a Markov kernel $\kernel{Q}:H\to \Delta(\sigalg{O})$ and a \emph{prior} $\mu\in \Delta(\sigalg{H})$ such that

\begin{align}
    \prob{P}^{\RV{X}_1\RV{X}_2\RV{X}_3} = \begin{tikzpicture}
        \path (0,0) node[dist] (P) {$\mu$}
        ++ (0.7,0) node[copymap] (copy0) {}
        ++ (0.5,0.5) node[kernel] (Q1) {$\kernel{Q}$}
        +  (0,-0.5) node[kernel] (Q2) {$\kernel{Q}$}
        +  (0,-1) node[kernel] (Q3) {$\kernel{Q}$}
        ++ (1,0) node (X1) {$\RV{X}_1$}
        + (0,-0.5) node (X2) {$\RV{X}_2$}
        + (0,-1) node (X3) {$\RV{X}_3$};
        \draw (P) -- (copy0);
        \draw (copy0) to [out=45,in=180] (Q1) (copy0) to [out=0, in=180] (Q2) (copy0) to [out=-45,in=180] (Q3);
        \draw (Q1) -- (X1) (Q2) -- (X2) (Q3) -- (X3);
    \end{tikzpicture}
\end{align}

Defining the hypothesis $\RV{H}:O\mapsto H$ such that $\prob{P}^{\RV{H}}=\mu$ and $\prob{P}^{\RV{O}|\RV{H}}=\kernel{Q}$, $(\kernel{Q},\RV{H},\RV{O})$ is a statistical model.

In the following section, we extend this result to the case of see-do forecasts. We first consider a model that is exchangeable in the observations only, and then introduce the notion of functional exchangeability which is a generalisation of exchangeability to Markov kernels. Finally, we prove a representation theorem for see-do forecasts that are both exchangeable in observations and functionally exchangeable in consequences.

\begin{definition}[Permutations and swaps]\label{def:permut_swap}
A \emph{finite permutation} $\rho'$ on $B\subseteq\mathbb{N}$ is a map $B\to B$ such that there is some finite $A\subset B$ for which $\rho'|_A:A\to A$ is an invertible function and $\rho'|_{B\setminus A} = \mathrm{Id}_{B\setminus A}$.

Given measureable space $(E,\sigalg{E})$ and a set of random variables $\{\RV{X}_i|i\in B\}$ the swap function $\rho^{\RV{X}}:E\to E$ associated with a finite permutation $\rho:B\to B$ and the random variables $\{\RV{X}_i\}_B$ is a $\sigalg{E}$ measurable function which has the property $\RV{X}_i\circ \rho^{\RV{X}}=\RV{X}_{\rho'(i)} $ for all $i\in B$, and for any $\RV{Y}:E\to Y$ with $\RV{Y}(\RV{X}^{-1}(A))=Y$ for any $A\in \sigalg{E}$, $Y\circ\rho^{\RV{X}}=\RV{Y}$. This swap function also has an associated Markov kernel $R:=\kernel{F}_{\rho^{\RV{X}}}$.

For example, if $E = Y\times X_1^{|B|}$ and $\RV{X}_i:E\to X_1$ projects the $i$-th ``x'' element of the space $(y,x_1,...,x_i,...)\mapsto x_i$, then for some finite permuation $\rho$ the associated swap is the the fuction $\rho^{\RV{X}}:(y,x_1,...,x_i,...)\mapsto (y,x_{\rho'(1)},...,x_{\rho'(i)},...)$.
\end{definition}

\begin{align}
    R \utimes_{i\in B} \kernel{F}_{\RV{X}_i} &= \utimes_{i\in B} R \kernel{F}_{\RV{X}_{i}}\label{eq:determ_commute}\\
                                               &= \utimes_{i\in B} \kernel{F}_{\RV{X}_{\rho(i)}}\label{eq:function_composition}
\end{align}

Where line \ref{eq:determ_commute} follows from the fact that deterministic kernels commute with the split map (\ref{eq:composition}), and line \ref{eq:function_composition} follows from the fact that for two functional kernels 

\begin{align}
    (\kernel{F}_{f}\kernel{F}_g)_x(A) &= \int_X (\kernel{F}_g)_y(A) d(\kernel{F}_f)_x(y)\\
                                      &= \int_X \delta_{g(y)}(A) d\delta_{f(x)}(y)\\
                                      &= \delta_{g(f(x))} (A)\\
                                      &= (\kernel{F}_{g\circ f})_x(A) 
\end{align}

\todo[inline]{lemmafy, move to chapter 2}

\begin{definition}[Partial frequencies]\label{def:partial_freq}
Given a standard measurable space $(E,\sigalg{E})$ along with random variables $\RV{X}_i:E\to X_1$ for each $i\in \mathbb{N}$, for $A\in \sigalg{X}_1$ and $m\leq n\in \mathbb{N}$ define the size $n$, $m$-tuple \emph{partial frequency of} $A$ with respect to $\RV{X}:=\utimes_{i\in\mathbb{N}}\RV{X}_i$ to be $\RV{Z}^{m,n}_A:=\frac{(n-m)!}{n!}\sum_{I\subset [n]} \prod_{i\in I} \mathds{1}_A\circ \RV{X}_i$ where $I$ ranges over all $m$-sized ordered subsets of $n$.

Define the $m$-tuple \emph{relative frequency of } $A$ with respect to $\RV{X}$ to be $\RV{Z}^{m,\infty}_A:= \lim_{n\to \infty}\frac{(n-m)!}{n!}\sum_{I\in [n]} \prod_{i\in I} \mathds{1}_A\circ \RV{X}_i$.

Given a countable set $\sigalg{G}$ generating $\sigalg{X}_1$ (i.e. $\sigma(\sigalg{G})=\sigalg{X}_1)$, define the $m$-tuple \emph{relative frequency of} $\RV{X}$ to be $\RV{Z}^{m,\infty}:=\utimes_{A\in G} \RV{Z}^{m,\infty}_A$, if such a limit exists for all $A$.

For the special case of $m=1$, let $\RV{Z}:=\RV{Z}^{1,\infty}$
\end{definition}

\begin{definition}[Exchangeable $\sigma$-algebra]\label{def:exchange_sig_alb}
Given a set of random variables $\RV{X}_i:E\to X_1$ for each $i\in \mathbb{N}$, with $X=X_1^{\mathbb{N}}$, a \emph{n-place symmetric function} $f:X\to W$ is a function for which $f = f\circ \rho$ for any permuation $\rho:\mathbb{N}\to\mathbb{N}$ such that $i>n\implies\rho(i)=i$. 

The \emph{n-place exchangeable $\sigma$-algebra} (with respect to the random variable $\RV{X}_i$), $\sigalg{H}^n$, is the $\sigma$-algebra generated by all n-place symmetric functions, and $\sigma{H}=\cap_{n\in \mathbb{N}} \sigalg{H}^n$. 

For standard measurable $(E,\sigalg{E})$ and $n\in \mathbb{N}$, a size $n$ swap function $\rho^{\RV{X}n}:E\to E$ is a swap function associated with a permutation $\rho^{\prime n}$ with the property $i>n\implies \rho^{\prime n}(i)=i$. An $n$-symmetric set $S\subset E$ has the property $\rho^{\RV{X}n}(S)=S$ for all size $n$ swap functions $\rho^{\RV{X}n}$. Define the symmetric sets $\sigalg{S}^n$ $n$-symmetric sets, and $\sigalg{S}=\cap_{n\in\mathbb{N}} \sigalg{S}^n$. Given random variables $\RV{X}_i:E\to X_1$ for each $i\in\mathbb{N}$, the size $n$ \emph{exchangeable $\sigma$-algebra} with respect to $\RV{X}:=\utimes_{i\in\mathbb{N}}\RV{X}_i$, denoted $\sigalg{H}^n\subset\sigma(\RV{X})$, is the $\sigma$-algebra generated by $\{\RV{X}^{-1}(A)|A\in\sigalg{X}\}\cap S^n$.

\end{definition}

\begin{lemma}
The size $n$ exchangeable sigma algebra $\sigalg{H}^n$ on $(E,\sigalg{E})$ with respect to $\RV{X}:=\utimes_{i\in\mathbb{N}}\RV{X}_i$ has the property $\rho^{\RV{X}n}(A)=A$ for all $A\in \sigalg{H}^n$, and all size $n$ swap functions $\rho^{\RV{X}n}$.
\end{lemma}

\begin{proof}
Let $W_f$ be the codomain of a function $f$, and $\sigalg{W}_f$ its $\sigma$-algebra. $\sigalg{H}^n$ is generated by $\sigalg{G}^n=\{f^{-1}(A)|A\in \sigalg{W}_f,f\text{ n-place symmetric}\}$. By the definition of n-place symmetric functions, any set of the form $f^{-1}(A)=(f\circ\rho^{\RV{X}})^{-1}(A) = (\rho^{\RV{X}}){-1}(f^{-1}(A))$. Because every $n$-place permutation $\rho$ has an inverse $\rho^{-1}$ that is also an $n$-place permutation, all sets in $B\in\sigalg{G}$ have the property $\rho^{\RV{X}}(B)=B$ for all $n$-place permuations $\rho$.

Define $\sigalg{S}^n$ to be all subsets of $E$ such that for $B\in \sigalg{S}^n$, $n$-place permuations $\rho$, $\rho^{\RV{X}}(B)=B$. $\sigalg{S}^n$ is a $\sigma$-algebra, and it contains $\sigalg{G}^n$, so it also contains $\sigalg{H}^n$.

Take $A\in\sigalg{S}^n$. By assumption, for any $\omega\in A$, $\rho^{\RV{X}}(\omega)\in A$ for all size $n$ swap functions $\rho^{\RV{X}}$. Consider $\omega\not\in A$, and suppose there is some swap function $\rho^{\RV{X}}$ such that $\rho^{\RV{X}}(\omega)\in A$. By definition, the permutation $\rho$ has an inverse $\rho^{-1}$ which is also a size $n$ permutation. By construction, $(\rho^{-1})^{\RV{X}}$ is also the inverse of $\rho^{\RV{X}}$. Thus $(\rho^{-1})^{\RV{X}}(\omega)\not\in A$ and so $\omega\not\in A$, a contradiction. Thus $E\setminus A\in \sigalg{G}^n$.

For any invertible function $f:E\to E$, $f(E)=E$. Thus $E\in \sigalg{G}^n$.

Finally, for a countable collection $A_1,A_2,...$ and any size $n$ swap function $\rho^{\RV{X}}$, $\rho^{\RV{X}}(\cup_{i=1}^{\infty} A_i) = \cup_{i=1}^\infty \rho^{\RV{X}}(A_i) = \cup_{i=1}^\infty A_i$. Thus $\sigalg{S}^n$ is a $\sigma$-algebra, and by the monotone class theorem it contains $\RV{H}^n$.
\end{proof}

\begin{definition}[Exchangeability]
Given a see-do forecast $(\kernel{T},\RV{D},\RV{X},\RV{Y})$ with the property that $\RV{X}:=\utimes_{i\in A} \RV{X}_i$ for some set of random variables $\{\RV{X}_i|i\in A\}$ all taking values in $X_1$ where $A\subseteq \mathbb{N}$. 

If for every finite $B\subset A$ and every permutation $\rho':B\to B$ of $B$ we have $\kernel{T}\rho=\kernel{T}$, where $\rho$ is the swap associated with $\rho'$ and $\{\RV{X}_i|i\in B\}$, then $(\kernel{T},\RV{D},\RV{X},\RV{Y})$ is \emph{exchangeable} with respect to $\{\RV{X}_i|i\in A\}$.

If $A$ is an infinite set then $\kernel{T}$ is \emph{infinitely exchangeable}, and if $\kernel{T}=\kernel{S}(\mathrm{Id}_{X}\otimes \stopper{0.15}\otimes \mathrm{Id}_Y)$ for some infinitely exchangeable $(\kernel{S},\RV{D},\RV{X}',\RV{Y}')$, then $\kernel{T}$ is infinitely exchangeably extendable.
\end{definition}

Note that $\kernel{T}R^{\RV{X}_i|\RV{D}} = \kernel{T}^{\RV{X}_{\rho(i)}|\RV{D}}$.

This implies the usual notion of exchangeability if we take $Y=\{*\}$ (that is, if we assume the consequences are trivial), as by assumption $\RV{X}$ is independent of $\RV{D}$.

\begin{lemma}[Infinitely exchangeably extendable forecasts]\label{lem:partial_representation}
Given a forecast $(\prob{P},\RV{X})$ where $\RV{X}=\utimes_{i\in A} \RV{X}_i$ for some $\{\RV{X}_i|i\in \mathbb{N}\}$ and $X$ is standard measurable, if $\prob{P}$ is exchangeable with respect to $\RV{X}$ then there exists a function $f:X\to H$ such that, defining $\RV{Z}:f\circ \RV{X}$:
    \begin{itemize}
        \item $\RV{X}_i\CI\RV{X}_{A\setminus\{i\}}|\RV{Z}$ for all $i\in A$
        \item $\prob{P}^{\RV{X}_i|\RV{Z}}=\prob{P}^{\RV{X}_j|\RV{Z}}$ for all $i,j\in A$
    \end{itemize}
\end{lemma}

\begin{proof}
Without loss of generality, assume $X_1=[0,1]$, $\sigalg{X}=\mathcal{B}([0,1])$ and $\RV{X}=[0,1]^{\mathbb{N}}$.

Let $\mathbb{Q}$ be the rationals between $[0,1]$ and define $\RV{Z}_q^n:D\times X\times Y \to [0,1]$ by $\omega \mapsto \frac{1}{n}\sum_{i}^n \mathds{1}_{[0,q)}(\RV{X}_i(\omega))$ Let $\sigalg{Z}^n_q=\sigma(\RV{Z}_q^n)$, i.e. $\RV{Z}^n$ is a 1-tuple partial frequency as in Definition \ref{def:partial_freq}.

$\RV{Z}^n\circ\rho^{\RV{X}n}=\RV{Z}^n$ for any size $n$ swap function $\rho^{\RV{X}n}$, so by Lemma \ref{lem:partialfreq_exchangeable} $\RV{Z}^n$ is $\sigalg{H}^n$-measurable.

Let $\rho'_{ij}:\mathbb{N}\to\mathbb{N}$ swaps indices $i$ and $j$ for some $i,j\in[n]$ and otherwise acts as the identity. $\rho_{ij}:D\times X\times Y\to \Delta(\sigalg{D}\times \sigalg{X}\times \sigalg{Y})$ is the swap kernel associated with $\rho'_{ij}$ and $\{\RV{X}_i|i\in \mathbb{N}\}$, and $\rho^{\RV{X}}_{ij}$ the function associated with $\rho_{ij}$. For any $m$, $n$, $A\in \sigalg{H}^n$, $d\in D$: 

\begin{align}
    \int_{A} \RV{Z}_q^{n} (\omega) d\prob{P}(\omega) &= \int_{A} \frac{1}{n}\sum_{i}^{n} (\mathds{1}_{[0,q)}\circ \RV{X}_i)(\omega)d\prob{P}(\omega)\\
    &= \frac{1}{n}\sum_{i}^{n} \int_{(\rho^{\RV{X}}_{ij})^{-1}\rho^{\RV{X}}_{ij}(A)} (\mathds{1}_{[0,q)}\circ \RV{X}_i)(\omega)d\prob{P}\rho_{ij}(\omega)\label{eq:permutation_invertible}\\
    &= \frac{1}{n}\sum_{i}^{n} \int_{\rho^{\RV{X}}_{ij}(A)} (\mathds{1}_{[0,q)}\circ \RV{X}_{i}\circ \rho_{ij}) (\omega)d\prob{P}(\omega)\label{eq:using_pushforward}\\
    &= \frac{1}{n}\sum_{i}^{n} \int_{A} (\mathds{1}_{[0,q)}\circ \RV{X}_{j})(\omega)d\prob{P}(\omega)\label{eq:closure_under_permutation}\\
    &= \int_{A} (\mathds{1}_{[0,q)} \circ \RV{X}_j)(\omega)d\prob{P}(\omega) \label{eq:cond_expectation_first}
\end{align}

Where line \ref{eq:permutation_invertible} follows from exchangeability of $\kernel{T}$ and invertibility of $\rho_{ij}$. the fact that $\rho$line \ref{eq:using_pushforward} follows from the fact that $\prob{P}\rho_{ij}$ is the pushforward measure of $\prob{P}$ with respect to $\rho^{\RV{X}}_{ij}$ and \ref{eq:closure_under_permutation} uses the fact that $\rho(A) = A$ for all $A\in \RV{Z}^n_q$ and all permutations $\rho$.

From Equation \ref{eq:cond_expectation_first}, we have for all $n$, $A\in \sigalg{H}^{n+1}$

\begin{align}
    \int_{A} \RV{Z}_q^{n+1} (\omega) d\prob{P}(\omega) &= \int_{A} \RV{Z}_q^{n} (\omega) d\prob{P}(\omega)
\end{align}

Because $\RV{Z}_q^{n+1}$ is $\sigalg{H}^{n+1}$ measurable, $\RV{Z}_q^{n+1} = \mathbb{E}[\RV{Z}_q^{n}|\sigalg{H}^{n+1}]$.

Thus the sequence $[\RV{Z}^1_q,\RV{Z}^2_q,...]$ is a backwards martingale with respect to the reversed filtration $\sigalg{H}^1\supset\sigalg{H}^2\supset...\supset \sigalg{H}^3$.

Furthermore, for all $n\in \mathbb{N}$, $\sup_\omega|\RV{Z}^n(\omega)|\leq 1$ so the sequence is also uniformly integrable. Thus it goes almost surely to the limit $\RV{Z}_q$, and for all $A\in \sigalg{H}$

\begin{align}
    \lim_{n\to\infty} \int_A \RV{Z}^n_q(\omega) d\prob{P}(\omega) &= \int_A \RV{Z}_q(\omega) d\prob{P}(\omega)
\end{align}

Finally, because for all $n\in \mathbb{N}$, all $j\in[n]$ and all $A\in \sigalg{H}^n$ we also have

\begin{align}
    \int_A \mathds{1}_{[0,q)}(\RV{X}_j(\omega))d\prob{P}(\omega) &= \int_A \RV{Z}_q^n(\omega) d\prob{P}(\omega)
\end{align}

it follows that for all $A\in \sigalg{H}$, $j\in \mathbb{N}$

\begin{align}
    \int_A \RV{Z}_q(\omega) d\prob{P}(\omega) = \int_A \mathds{1}_{[0,q)}(\RV{X}_j(\omega))d\prob{P}(\omega)\label{eq:cond_expectation}
\end{align}

[\citet{cinlar_probability_2011} Thm 4.7.]. Thus $\RV{Z}_q = \mathbb{E}[\mathds{1}_{[0,q)}\circ\RV{X}_j|\sigalg{H}]$ for all $j\in \mathbb{N}$. This implies $\RV{Z}_q$ is a version of $\mathbb{E}[\mathds{1}_{[0,q)}\circ \RV{X}_j|\sigalg{H}]$.

Define $\RV{Z} = \utimes_q\in \mathbb{Q} \RV{Z}_q$. As $\sigma(\RV{Z})\subset\sigalg{H}$, Equation \ref{eq:cond_expectation} establishes in addition that $\RV{Z}_q$ is a version of $\mathbb{E}[\mathds{1}_{[0,q)}\circ \RV{X}_j|\sigma(Z)]$. 

By the definition of conditional expectation, for any version of $\prob{P}_{\RV{Z}(\omega)}^{\RV{X}_j|\RV{Z}\RV{D}}([0,q))$ we have

\begin{align}
    \prob{P}_{\RV{Z}(\omega)}^{\RV{X}_j|\RV{Z}}([0,q)) &= \mathbb{E}[\mathds{1}_{[0,q)}\circ \RV{X}_j|\sigma(\RV{Z})](\omega)\\
\end{align}

$\prob{P}$-almost surely.

Furthermore, the measure $\prob{P}_{h}^{\RV{X}_j|\RV{Z}}$ is uniquely defined by its value on $[0,q)$ for all $q\in \mathbb{Q}$. Thus for all $i,j\in \mathbb{N}$ we have

\begin{align}
    \prob{P}^{\RV{X}_j|\RV{H}} = \prob{P}^{\RV{X}_i|\RV{H}}
\end{align}

Completing the proof of property 1.

Next, we will show $\RV{X}_i\CI_\kernel{T} \RV{X}_{\mathbb{N}\setminus \{i\}} | \RV{H}$.

Let $\RV{Z}^{m,n}_q$ be a partial frequency as in Definition \ref{def:partial_freq} where $q$ stands for the set $[0,q)$. $\RV{Z}^{m,n}_q\circ \rho^{\RV{X}n}=\RV{Z}^{m,n}_q$ for all size $n$ swap functions so by Lemma \ref{lem:partialfreq_exchangeable}, $\RV{Z}^{m,n}_q$  is $\sigalg{H}^n$ measurable.

Let $J\subset[n]$ be some set of $m$ elements from $n$ and $\rho'_{IJ}$ be a permutation that sends the elements of $I\subset[n]$ to $J$.

\begin{align}
    \int_{A} \RV{Z}_q^{m,n} (\omega) d\prob{P}(\omega) &= \int_{A} \frac{(n-m)!}{n!}\sum_{I\subset[n]}\prod_{i\in I} (\mathds{1}_{[0,q)}\circ \RV{X}_i)(\omega)d\prob{P}(\omega)\\
    &= \frac{(n-m)!}{n!}\sum_{I\subset[n]} \int_{(\rho^{\RV{X}}_{IJ})^{-1}\rho^{\RV{X}}_{IJ}(A)} \prod_{i\in I}(\mathds{1}_{[0,q)}\circ \RV{X}_i)(\omega)d\prob{P}\rho_{IJ}(\omega)\label{eq:permutation_invertible}\\
    &= \frac{(n-m)!}{n!}\sum_{I\subset[n]} \int_{\rho^{\RV{X}}_{IJ}(A)} \prod_{i\in I}(\mathds{1}_{[0,q)}\circ \RV{X}_{i}\circ \rho_{IJ}) (\omega)d\prob{P}(\omega)\label{eq:using_pushforward}\\
    &= \frac{(n-m)!}{n!}\sum_{I\subset[n]} \int_{A} \prod_{j\in J}(\mathds{1}_{[0,q)}\circ \RV{X}_{j})(\omega)d\prob{P}(\omega)\label{eq:closure_under_permutation}\\
    &= \int_{A} \prod_{j\in J}(\mathds{1}_{[0,q)} \circ \RV{X}_j)(\omega)d\prob{P}(\omega) \label{eq:cond_expectation_first}
\end{align}

From Equation \ref{eq:cond_expectation_first}, we have for all $n$, $m<n$, $A\in \sigalg{H}^{n+1}$

\begin{align}
    \int_{A} \RV{Z}_q^{m,n+1} (\omega) d\prob{P}(\omega) &= \int_{A} \RV{Z}_q^{m,n} (\omega) d\prob{P}(\omega)
\end{align}

Because $\RV{Z}_q^{m,n+1}$ is $\sigalg{H}^{n+1}$ measurable, $\RV{Z}_q^{m,n+1} = \mathbb{E}[\RV{Z}_q^{m,n}|\sigalg{H}^{n+1}]$.

Thus the sequence $[\RV{Z}^{m,1}_q,\RV{Z}^{m,2}_q,...]$ is a backwards martingale with respect to the reversed filtration $\sigalg{H}^1\supset\sigalg{H}^2\supset...\supset \sigalg{H}^3$.

Furthermore, for all $n\in \mathbb{N}$, $\sup_\omega|\RV{Z}^{m,n}_q(\omega)|\leq 1$ so the sequence is also uniformly integrable. Thus it goes almost surely to a limit $\RV{Z}^{m,\infty}_q$, and for all $A\in \sigalg{H}$

\begin{align}
    \lim_{n\to\infty} \int_A \RV{Z}^{m,n}_q(\omega) d\prob{P}(\omega) &= \int_A \RV{Z}^{m,\infty}_q(\omega) d\prob{P}(\omega)\\
    \implies \RV{Z}^{m,n}_q &= \mathbb{E}[\prod_{j\in J} \mathds{1}_{[0,q)}\circ \RV{X}_j|\sigalg{H}]
\end{align}

Let $[n]^m_\mathrm{rep}$ be the set of all $m$ length sequences of elements of $[n]$ with repeats. Note that $\lim_{n\to\infty}\frac{(n-m)!}{n!}|[n]^m_\mathrm{rep}| = \lim_{n\to\infty}\frac{n^m(n-m)!}{n!}-1 = 0$

\begin{align}
    \mathbb{E}[\prod_{j\in J} \mathds{1}_{[0,q)}\circ \RV{X}_j|\sigalg{H}](\omega) &= \lim_{n\to\infty}\frac{(n-m)!}{n!}\sum_{I\subset[n]}\prod_{i\in I} (\mathds{1}_{[0,q)}\circ \RV{X}_i)\\
                                                                      &= \lim_{n\to\infty}\frac{1}{n^m}\sum_{I\subset[n]}\prod_{i\in I}(\mathds{1}_{[0,q)}\circ \RV{X}_i)\\
                                                                      &= \lim_{n\to\infty}\frac{1}{n^m}\left[\sum_{i_1\in [n]}...\sum_{i_m\in [n]} (1_{[0,q)}\circ \RV{X}_{i_k})-\sum_{J\subset[n]^m_\mathrm{rep}}\prod_{i\in I} (\mathds{1}_{[0,q)}\circ \RV{X}_i)\right]\\ 
                                                                      &= \prod_{k\in [m]} \lim_{n\to\infty} \frac{1}{n}\sum_{i_k\in[n]}(1_{[0,q)}\circ\RV{X}_{ik})\\
                                                                      &= \prod_{k\in [m]} \mathbb{E}[\mathds{1}_{[0,q)}\circ \RV{X}_j|\sigalg{H}]\\
                                                                      &= \prod_{k\in J} \mathbb{E}[\mathds{1}_{[0,q)}\circ \RV{X}_j|\sigalg{H}]\\
                                                                      &= \prod_{k\in J} \RV{Z}_q\label{eq:h_measurable}
\end{align}

Because $\mathbb{E}[\prod_{j\in J} \mathds{1}_{[0,q)}\circ \RV{X}_j|\sigalg{H}]$ is $\RV{Z}$-measurable we also have

\begin{align}
    \mathbb{E}[\prod_{j\in J} \mathds{1}_{[0,q)}\circ \RV{X}_j|\sigma(\RV{Z})] &= \prod_{k\in J} \RV{Z}_q
\end{align}

By the definition of conditional expectation, for all $J\subset \mathbb{N}$

\begin{align}
    \prob{P}_{\RV{H}(\omega)}^{\RV{X}_J|\RV{H}}(\times_{j\in J}[0,q)) &= \mathbb{E}[\prod_{j\in J} \mathds{1}_{[0,q)}\circ \RV{X}_j|\sigalg{H}](\omega)\\
                                                                      &= \prod_{k\in J} \prob{P}_{\RV{H}(\omega)}^{\RV{X}_j|\RV{H}}([0,q))
\end{align}

And thus $\RV{X}_i\CI\RV{X}_{\mathbb{N}\setminus\{i\}}|\RV{Z}$ for all $i\in\mathbb{N}$.
\end{proof}

\begin{theorem}[Representation of infinitely exchangeably extendable see-do forecasts]\label{th:rep_seedo_obs}
Given a see-do forecast $(\kernel{T},\RV{D},\RV{X},\RV{Y})$ where $\RV{X}=\utimes_{i\in \mathbb{N}} \RV{X}_i$ for some $\{\RV{X}_i|i\in \mathbb{N}\}$ and $X\times Y$ is standard measurable, the following statements are equivalent:

\begin{enumerate}
    \item $(\kernel{T},\RV{D},\RV{X},\RV{Y})$ is infinitely exchangeably extendable with respect to $\{\RV{X}_i|i\in \mathbb{N}\}$
    \item There exists a function $f:X\to Z$ such that, defining $\RV{Z}:f\circ \RV{X}$,
    \begin{align}
        \kernel{T}^{\RV{Z}\RV{X}\RV{Y}|\RV{D}} = \begin{tikzpicture}
            \path (0,0) node[dist,inner sep=-2pt] (H) {$\kernel{T}^{\RV{Z}}$}
            + (0,-1) node (D) {$\RV{D}$}
            ++ (0.5,0) node[copymap] (copy0) {}
            ++ (0.6,0) node[copymap,label={$\mathbb{N}$}] (copy1) {}
            ++ (1.2,0.5) node[kernel] (XH) {$\kernel{T}^{\RV{X}_1|\RV{Z}}$}
            + (0,-0.5) node[kernel] (XH2) {$\kernel{T}^{\RV{X}_1|\RV{Z}}$}
            + (0,-1.25) node[kernel] (YD) {$\kernel{T}^{\RV{Y}|\RV{Z}\RV{D}}$}
            ++ (1.2,-0.25) node (X1) {$\RV{X}$}
            + (0,-1) node (Y) {$\RV{Y}$}
            + (0,0.9) node (Hb) {$\RV{Z}$};
            \draw (H) -- (copy1) (copy1) to [out=35,in=180] (XH) (copy1) to [out=-35,in=180] (XH2);
            \draw ($(XH.east)$) to [out=-15,in=180] ($(X1.west) + (-0.2,0.25)$) ($(XH2.east)$) to [out=15,in=180] ($(X1.west)+(-0.2,-0.25)$);
            \draw ($(X1.west) + (-0.2,0)$) to (X1);
            \draw (copy0) to [out=-90,in=180] ($(YD.west) + (0,0.15)$);
            \draw (D) to [out=0,in=180] ($(YD.west)+(0,-0.15)$) (YD) -- (Y);
            \draw ($(copy1.west)+(-0.1,0.8)$) rectangle ($(X1.west) + (-0.2,-0.55)$);
            \draw (copy0) to [out = 90, in=180] (Hb);
        \end{tikzpicture}
    \end{align}
    \item There exists a function $f:X\to Z$ such that, defining $\RV{Z}:f\circ \RV{X}$, for any $d\in D$, $L\in \sigalg{Z}$ $(\times_{i\in \mathbb{N}}J_i)\in \sigalg{X}$, $K\in \sigalg{Y}$
    \begin{align}
        \kernel{T}^{\RV{Z}\RV{X}\RV{Y}|\RV{D}}_d(L\times(\times_{i\in A}J_i)\times K) = \int_{L} \prod_{i\in \mathbb{N}} \kernel{T}^{\RV{X}_1|\RV{Z}}_z(J_i)\kernel{T}_{z,d}^{\RV{Y}|\RV{Z}\RV{D}}(K)d\kernel{T}^{\RV{Z}}(z)
    \end{align}
    \item There exists a function $f:X\to Z$ such that, defining $\RV{Z}:f\circ \RV{X}$
        \begin{itemize}
            \item $\RV{X}_i\CI\RV{X}_{\mathbb{N}\setminus\{i\}}|\RV{Z}$ for all $i\in A$
            \item $\kernel{T}^{\RV{X}_i|\RV{Z}}=\kernel{T}^{\RV{X}_j|\RV{Z}}$ for all $i,j\in A$
            \item $\RV{Y}\CI\RV{X}|\RV{Z}\utimes \RV{D}$
        \end{itemize}
\end{enumerate}
\end{theorem}

\begin{proof}

Without loss of generality, assume $X_1=Y=[0,1]$.

(2) $\iff$ (3) follows from the fact that (3) + the associated quantifications is the integral representation of the string diagram in (2).

We will show (3) $\implies$ (1), (1) $\implies$ (4) and (4) $\implies$ (3).

(3) $\implies$ (1):

we have

\begin{align}
    \kernel{T}_d((\times_{i\in A}J_i)\times K) = \int_{H_B} \prod_{i\in A} \kernel{T}^{\RV{X}_1|\RV{H}_B}_h(J_i)\kernel{T}_{h,d}^{\RV{Y}|\RV{H}_B\RV{D}}(K)d\kernel{T}^{\RV{H}_B}(h)
\end{align}

For any $(\times_{i\in \mathbb{N}}J_i)\in \sigalg{X_1}^{\mathbb{N}}$, define

\begin{align}
    \kernel{U}_d((\times_{i\in \mathbb{N}}J_i)\times K)) = \int_{H_B} \prod_{i\in C} \kernel{T}^{\RV{X}_1|\RV{H}_B}_h(J_i)\kernel{T}_{h,d}^{\RV{Y}|\RV{H}_B\RV{D}}(K)d\kernel{T}^{\RV{H}_B}(h)
\end{align}

We have

\begin{align}
    \kernel{U}_d(\mathrm{Id}_{X}\otimes \stopper{0.15}\otimes \mathrm{Id}_Y)(L\times (\times_{i\in A}J_i)\times K) &= \int_{L} \prod_{i\in A} \kernel{T}^{\RV{X}_1|\RV{H}_B}_h(J_i) \prod_{i\in \mathbb{N}\setminus A} (1) \kernel{T}_{h,d}^{\RV{Y}|\RV{H}_B\RV{D}}(K)d\kernel{T}^{\RV{H}_B}(h)\\
                                                                                                            &= \kernel{T}^{\RV{H}_B\RV{X}\RV{Y}|\RV{D}}_d(L\times(\times_{i\in A}J_i)\times K)
\end{align}

Define $\{\RV{X}'_i|i\in \mathbb{N}\}$ where $\RV{X}_i:H_B\times X_1^{\mathbb{N}}\times Y\to X_1$ takes $(h,x_1,...,x_i,...,y)\mapsto x_i$ for all $i$, and similarly $\RV{Y}':H_B\times X_1^{\mathbb{N}}\times Y\to Y$ takes $(h, x_1,...,y)\mapsto y$. Given any permutation $\rho':\mathbb{N}\to\mathbb{N}$

\begin{align}
    \kernel{U}_d\rho(H_b \times (\times_{i\in \mathbb{N}}J_{i})\times K)&= \kernel{U}_d(H_b \times (\times_{i\in \mathbb{N}}J_{\rho'(i)})\times K)\\
    &= \int_{H_B} \prod_{i\in \mathbb{N}} \kernel{T}^{\RV{X}_1|\RV{H}_B}_h(J_{\rho'(i)}) \kernel{T}_{h,d}^{\RV{Y}|\RV{H}_B\RV{D}}(K)d\kernel{T}^{\RV{H}_B}(h)\\
    &= \kernel{U}_d(H_b \times (\times_{i\in \mathbb{N}}J_{i})\times K)
\end{align}

Which shows $\kernel{U}_d$ is infinitely exchangeable, completing the proof that (3) $\implies$ (1)


(1) $\implies$ (4):

$\kernel{T}$ is a see-do forecast, so $\RV{X}\CI_{\kernel{T}}\RV{D}$. Thus there exists a marginal $\kernel{T}^{\RV{X}}$ independent of $\RV{D}$.

From Lemma \ref{lem:partial_representation}, $\kernel{T}^{\RV{X}_i|\RV{Z}}=\kernel{T}^{\RV{X}_j|\RV{Z}}$ for all $i,j\in \mathbb{N}$ and $\RV{X}_i\CI\RV{X}_{\mathbb{N}\setminus\{i\}} |\RV{Z}$.

We will show $\RV{Y}\CI\RV{X}|\RV{Z}\utimes\RV{D}$. 

For any swap function $\rho^{\RV{X}}$ there is, by definition, a permutation of indices $\rho'$ such that $\RV{X}\circ\rho^{\RV{X}}(\omega) = [\RV{X}_{\rho'(1)}(\omega),\RV{X}_{\rho'(2)}(\omega),...]$. Define $\rho'':[0,1]^{\mathbb{N}}\to[0,1]^{\mathbb{N}}$ to be the bijective map that performs the permutation in the codomain of $\RV{X}$, i.e. $\RV{X}\circ\rho^{\RV{X}} = \rho''\circ\RV{X}$.

Consider some $B\in \sigalg{B}([0,1])^\mathbb{N}$ and its preimage $\RV{X}^{-1}(B) = \{\omega|\RV{X}(\omega)\in B\}$, and some finite swap function $\rho^{\RV{X}}$. Then there exists $\rho''(B)\in [0,1]^{\mathbb{N}}$ such that $(\RV{X}\circ\rho^{\RV{X}})^{-1}(\rho''(B)) = \{\omega|\RV{X}(\rho^{\RV{X}}(\omega))\in \rho''(B)\} = \{\omega|\RV{X}\in B\}$. Thus $\sigma(\RV{X})=\sigma(\RV{X}\circ\rho^{\RV{X}})$ for any finite swap function $\rho^{\RV{X}}$.

Because $E=X\times Y$ and $\RV{X}$ forgets the $Y$-component of any $(x,y)\in E$, for any $A,B\in\sigalg{B}([0,1])$ there is some $C\subset X$ such that $\mathds{1}_{A}\circ\RV{Y}(\RV{X}^{-1}(B)) = \mathds{1}_{A}\circ\RV{Y} (C\times Y) = [0,1]$. Thus for any finite swap function $\rho^{\RV{X}}$, $\mathds{1}_{A}\circ\RV{Y}\circ \rho^{\RV{X}} = \mathds{1}_{A}\circ\RV{Y}$.

For $A\in \sigma(\RV{X})$:

\begin{align}
    \int_{A} \mathbb{E}[\mathds{1}_A\circ\RV{Y}|\sigma(\RV{X})] \circ\rho^{\RV{X}} d\kernel{T}_d &= \int_{\rho^{\RV{X}}(A)} \mathbb{E}[\mathds{1}_A\circ\RV{Y}|\sigma(\RV{X})] d(\kernel{T}R^{-1})_d\\
                                                                         &= \int_{\rho^{\RV{X}}(A)} \mathbb{E}[\mathds{1}_A\circ\RV{Y}|\sigma(\RV{X})] d\kernel{T}_d \\
                                                                         &= \int_{\rho^{\RV{X}}(A)} \mathds{1}_A\circ\RV{Y}d\kernel{T}_d\\
                                                                         &= \int_A \mathds{1}_A\circ\RV{Y} d\kernel{T}_d
\end{align}

It follows that $\mathbb{E}[\mathds{1}_A\circ\RV{Y}|\sigma(\RV{X})] \circ\rho^{\RV{X}}$ is a version of $\mathbb{E}[\mathds{1}_A\circ\RV{Y}|\sigma(\RV{X})]$. Because there are only a countable number of finite swap functions, it also follows that a version of $\mathbb{E}[\mathds{1}_A\circ\RV{Y}|\sigma(\RV{X})]$ that is $\sigalg{H}$-measurable exists (i.e. for which $\mathbb{E}[\mathds{1}_A\circ\RV{Y}|\sigma(\RV{X})]\circ\rho^{\RV{X}} = \mathbb{E}[\mathds{1}_A\circ\RV{Y}|\sigma(\RV{X})]$, for all $\rho^{\RV{X}}$).

Consider also for some swap function $\rho^{\RV{X}}$, $B\in \sigalg{B}([0,1])$:

\begin{align}
    \int_{\RV{X}_i^{-1}(B)} \mathbb{E}[\RV{V}|\sigma(\RV{X}_j)] \circ \rho^{\RV{X}}_{ji} d\kernel{T}_d &= \int_{\rho^{\RV{X}}_{ji}(\RV{X}_i^{-1}(B))} \mathbb{E}[\RV{V}|\sigma(\RV{X}_j)] d(\kernel{T}R_{ij})_d\\
                                                                               &= \int_{\RV{X}^{-1}_{j}(B)} \mathbb{E}[\RV{V}|\sigma(\RV{X}_j)] d\kernel{T}_d\\
                                                                               &= \int_{\RV{X}_j^{-1}(B)} \RV{V} d\kernel{T}_d\\
                                                                               &= \int_{\rho_{ji}^*(\RV{X}_j^{-1}(B))}\RV{V}\circ \rho^{\RV{X}}_{ji} d(\kernel{T}R_{ji})_d\\
                                                                               &= \int_{\RV{X}_i^{-1}(B)} \RV{V}d\kernel{T}_d
\end{align}

Thus $\mathbb{E}[\RV{V}|\sigma(\RV{X}_j)]\circ\rho^{\RV{X}}_{ji}$ is a version of $\mathbb{E}[\RV{V}|\sigma({\RV{X}_i})]$.

Define $\RV{W}^n_q := \frac{1}{n}\sum_{i\in [n]} \mathbb{E}[\mathds{1}_{[0,q]}\RV{Y}|\sigma(\RV{X}_i)]$. Note that for any size $n$ swap function $\rho^{\RV{X}}$, $\RV{W}^n_q\circ\rho^{\RV{X}}=\RV{W}_q^n$, therefore $\RV{W}_q^n$ is $\sigalg{H}^n$-measurable. 

Consider $\omega,\omega'$ such that $\RV{W}_q^n(\omega)\neq \RV{W}_q^n(\omega')$. Then there exists no size $n$ swap function $\rho^{\RV{X}}$ such that $\RV{X}_{[n]}(\omega)=\RV{X}_{[n]}(\rho^{\RV{X}}(\omega'))$. Without loss of generality, suppose $\RV{X}_1(\omega)\leq\RV{X}_2(\omega)\leq...\leq\RV{X}_n(\omega)$ and $\RV{X}_1(\omega')\leq\RV{X}_2(\omega')\leq...\leq\RV{X}_n(\omega')$. Then there is some first index $j$ for which $\RV{X}_{j}(\omega)> \RV{X}_{j}(\omega')$, and some and some rational $r$ such that $\RV{X}_j(\omega)>r>\RV{X}_j(\omega')$. Then $\sum_{i}^n \mathds{1}_{[0,q)}(\RV{X}_i(\omega)) > \sum_i^n \mathds{1}_{[0,q)}(\RV{X}_i(\omega'))$ and so $\RV{Z}^n(\omega)\neq\RV{Z}^n(\omega')$ also.

Thus $\RV{W}_q^n$ is $\sigma(\RV{Z}^n)$ measurable. Define $\sigalg{I}^n:=\vee_{n\to\infty} \sigma(\RV{Z}^n)$. Then $\sigalg{I}^1\supset\sigalg{I}^2\supset...\supset\sigma(\RV{Z})=\cap_{i}^\infty \sigalg{I}^i$. In addition, but the $\sigalg{H}^n$-measurability of $\RV{Z}^{>n}$, $\sigalg{I}^n\subset\sigalg{H}^n$ and $\sigalg{I}\subset\sigalg{H}$.

For $A\in\sigalg{I}^n$, $d\in D$:

\begin{align}
    \int_A \RV{W}_q^n d\kernel{T}_d &= \frac{1}{n}\sum_{i\in[n]} \int_A \mathbb{E}[\mathds{1}_{[0,q)}\circ\RV{Y}|\sigma(\RV{X}_i)] d\kernel{T}_d\\
                                  &= \frac{1}{n}\sum_{i\in[n]} \int_{\rho_{ij}^*(A)} \mathbb{E}[\mathds{1}_{[0,q)}\circ\RV{Y}|\sigma(\RV{X}_i)]\circ\rho_{ij}^*d(\kernel{T}R_{ij})_d\\
                                  &= \int_{A} \mathbb{E}[\mathds{1}_{[0,q)}\circ \RV{Y} \sigma(\RV{X}_j)]d\kernel{T}_d\\
                                  &= \int_{A} \mathds{1}_{[0,q)}\circ \RV{Y} d\kernel{T}_d
\end{align}

Where the last line follows from the fact that $\sigalg{I}^n\subset \sigma(\RV{X}_j)$. Therefore $\RV{W}^n_q$ is a version of $\mathbb{E}[\mathds{1}_{[0,q)}\circ|\sigalg{I}^n]$. Furthermore $\RV{W}^1_q,...,\RV{W}^n_q,..$ is a backwards martingale with respect to $\sigalg{I}^1,....,sigalg{I}^n,..\sigalg{I}$ and so it goes to a limit $\RV{W}_q=\mathbb{E}[\mathds{1}_{[0,q)}\circ \RV{V} |\sigma(\RV{Z})]$.

So $\RV{W}_q$ is a $\sigma(\RV{Z})$-measurable version of $\mathbb{E}[\mathds{1}_{[0,q]}\circ\RV{Y}|\sigalg{H}]$ which is itself a version of $\mathbb{E}[\mathds{1}_{[0,q]}\circ\RV{Y}|\sigma(\RV{X})]$. As before, for any $d\in D$, $\omega\in E$ we have

\begin{align}
    \kernel{T}_{\RV{D}(\omega),\RV{X}(\omega),\RV{Z}(\omega)}^{\RV{Y}|\RV{X}\RV{Z}\RV{D}}([0,q)) &= \mathbb{E}[\mathds{1}_{[0,q]}\circ\RV{Y}|\sigalg{H}](\omega)\\
                                                                                                 &= \kernel{T}_{\RV{D}(\omega),\RV{Z}(\omega)}^{\RV{Y}|\RV{Z}\RV{D}}([0,q))
\end{align}

 This shows that $\RV{Y}\CI_{\kernel{T}} \RV{X}|\RV{D}\RV{Z}$, as desired.


(4) $\implies$ (3):

$\RV{Z}$ is a function of $\RV{X}$ and $\RV{X}\CI\RV{D}$ so $\RV{Z}\CI\RV{D}$ also. 

By Lemma \ref{lem:representation_of_kernels}, and 
\begin{align}
    \kernel{T}^{\RV{Z}\RV{X}\RV{Y}|\RV{D}} &= \begin{tikzpicture}
        \path (0,0) node (D) {$\RV{D}$}
        ++ (0.5,0) node[copymap] (copy0) {}
        ++ (1,0) node[kernel] (H) {$\kernel{T}^{\RV{Z}|\RV{D}}$}
        ++ (0.9,0) node[copymap] (copy1) {}
        ++ (0.9,-0.5) node[kernel] (Xzd) {$\kernel{T}^{\RV{X}|\RV{ZD}}$}
        ++ (0.9,0) node[copymap] (copy2) {}
        ++ (0.9,-0.5) node[kernel] (Yzxd) {$\kernel{T}^{\RV{Y}|\RV{XZD}}$}
        ++ (1.5,0) node (Y) {$\RV{Y}$}
        +  (0,0.5) node (X) {$\RV{X}$}
        + (0,1) node (Z) {$\RV{Z}$};
        \draw (D) -- (H) -- (Z);
        \draw (copy0) to [out=-45,in=180] ($(Xzd.west) + (0,-0.15)$);
        \draw (copy1) to [out=-60,in=180] ($(Xzd.west) + (0,0.15)$);
        \draw (copy0) to [out=-90,in=180] ($(Yzxd.west) + (0,-0.15)$);
        \draw (copy1) to [out=-90,in=180] ($(Yzxd.west) + (0,0)$);
        \draw (copy2) to [out=-90,in=180] ($(Yzxd.west) + (0,0.15)$);
        \draw (Xzd) -- (X) (Yzxd) -- (Y);
    \end{tikzpicture}\\
     &= \begin{tikzpicture}
        \path (0,0) node (D) {$\RV{D}$}
        ++ (0.5,0) node[copymap] (copy0) {}
        ++ (1,0) node[dist] (H) {$\kernel{T}^{\RV{Z}}$}
        ++ (0.9,0) node[copymap] (copy1) {}
        ++ (0.9,-0.5) node[kernel] (Xzd) {$\kernel{T}^{\RV{X}|\RV{Z}}$}
        ++ (0.9,0) node[copymap] (copy2) {}
        ++ (0.9,-0.5) node[kernel] (Yzxd) {$\kernel{T}^{\RV{Y}|\RV{ZD}}$}
        ++ (1.5,0) node (Y) {$\RV{Y}$}
        +  (0,0.5) node (X) {$\RV{X}$}
        + (0,1) node (Z) {$\RV{Z}$};
        \draw[-{Rays[n=8]}] (D) -- ($(D.west) + (1,0)$);
        \draw (H) -- (Z);
        \draw[-{Rays[n=8]}] (copy0) to [out=-45,in=180] ($(Xzd.west) + (-0.7,-0.15)$);
        \draw (copy1) to [out=-60,in=180] ($(Xzd.west) + (0,0.15)$);
        \draw (copy0) to [out=-90,in=180] ($(Yzxd.west) + (0,-0.15)$);
        \draw (copy1) to [out=-90,in=180] ($(Yzxd.west) + (0,0)$);
        \draw[-{Rays[n=8]}] (copy2) to [out=-90,in=180] ($(Yzxd.west) + (-0.1,0.15)$);
        \draw (Xzd) -- (X) (Yzxd) -- (Y);
    \end{tikzpicture}\\
     &= \begin{tikzpicture}
        \path (0,0) node (D) {$\RV{D}$}
        ++ (0,1) node[dist] (H) {$\kernel{T}^{\RV{Z}}$}
        ++ (0.9,0) node[copymap] (copy1) {}
        ++ (0.9,-0.5) node[kernel] (Xzd) {$\kernel{T}^{\RV{X}|\RV{Z}}$}
        ++ (1.8,-0.5) node[kernel] (Yzxd) {$\kernel{T}^{\RV{Y}|\RV{ZD}}$}
        ++ (1.5,0) node (Y) {$\RV{Y}$}
        +  (0,0.5) node (X) {$\RV{X}$}
        + (0,1) node (Z) {$\RV{Z}$};
        \draw (H) -- (Z);
        \draw (copy1) to [out=-60,in=180] ($(Xzd.west) + (0,0.15)$);
        \draw (D) -- ($(Yzxd.west) + (0,-0.15)$);
        \draw (copy1) to [out=-90,in=180] ($(Yzxd.west) + (0,0)$);
        \draw (Xzd) -- (X) (Yzxd) -- (Y);
    \end{tikzpicture}
\end{align}

We still need to show

\begin{align}
    \begin{tikzpicture}
        \path (0,0) node[dist] (H) {$\kernel{T}^{\RV{Z}}$}
        ++ (0.9,0) node[copymap] (copy1) {}
        ++ (0.9,-0.5) node[kernel] (Xzd) {$\kernel{T}^{\RV{X}|\RV{Z}}$}
        ++ (1.5,0.) node (X) {$\RV{X}$}
        + (0,0.5) node (Z) {$\RV{Z}$};
        \draw (H) -- (Z);
        \draw (copy1) to [out=-60,in=180] ($(Xzd.west) + (0,0.15)$);
        \draw (Xzd) -- (X);
    \end{tikzpicture}
    \overset{?}{=} \begin{tikzpicture}
            \path (0,0) node[dist,inner sep=-2pt] (H) {$\kernel{T}^{\RV{Z}}$}
            ++ (0.5,0) node[copymap] (copy0) {}
            ++ (0.6,0) node[copymap,label={$A$}] (copy1) {}
            ++ (1.2,0.5) node[kernel] (XH) {$\kernel{T}^{\RV{X}_1|\RV{Z}}$}
            + (0,-0.5) node[kernel] (XH2) {$\kernel{T}^{\RV{X}_1|\RV{Z}}$}
            ++ (1.2,-0.25) node (X1) {$\RV{X}$}
            + (0,0.9) node (Hb) {$\RV{Z}$};
            \draw (H) -- (copy1) (copy1) to [out=35,in=180] (XH) (copy1) to [out=-35,in=180] (XH2);
            \draw ($(XH.east)$) to [out=-15,in=180] ($(X1.west) + (-0.2,0.25)$) ($(XH2.east)$) to [out=15,in=180] ($(X1.west)+(-0.2,-0.25)$);
            \draw ($(X1.west) + (-0.2,0)$) to (X1);
            \draw ($(copy1.west)+(-0.1,0.8)$) rectangle ($(X1.west) + (-0.2,-0.55)$);
            \draw (copy0) to [out = 90, in=180] (Hb);
        \end{tikzpicture}
\end{align}

Applying Lemma \ref{lem:representation_of_kernels} again, we have

\begin{align}
    \begin{tikzpicture}
        \path (0,0) node[dist] (H) {$\kernel{T}^{\RV{Z}}$}
        ++ (0.9,0) node[copymap] (copy1) {}
        ++ (0.9,-0.5) node[kernel] (Xzd) {$\kernel{T}^{\RV{X}|\RV{Z}}$}
        ++ (1.5,0.) node (X) {$\RV{X}$}
        + (0,0.5) node (Z) {$\RV{Z}$};
        \draw (H) -- (Z);
        \draw (copy1) to [out=-60,in=180] ($(Xzd.west) + (0,0.15)$);
        \draw (Xzd) -- (X);
    \end{tikzpicture} &= \begin{tikzpicture}
        \path (0,0) node[dist] (H) {$\kernel{T}^{\RV{Z}}$}
        ++ (0.9,0) node[copymap] (copy1) {}
        ++ (0.9,-0.5) node[kernel] (Xzd) {$\kernel{T}^{\RV{X}_1|\RV{Z}}$}
        + (0.7,0) node[copymap] (copy2) {}
        ++ (1.5,-0.5) node[kernel] (Xzd2) {$\kernel{T}^{\RV{X}_2|\RV{Z}\RV{X}_1}$}
        ++ (1.5,0.5) node (X) {$\RV{X}_1$}
        + (0,0.5) node (Z) {$\RV{Z}$}
        + (0,-0.5) node (X2) {$\RV{X}_2$}
        + (0,-1) node (X3) {$...$};
        \draw (H) -- (Z);
        \draw (copy1) to [out=-60,in=180] ($(Xzd.west) + (0,0.15)$);
        \draw (copy1) to [out=-90,in=180] ($(Xzd2.west) + (0,0)$);
        \draw (copy2) to [out=-90,in=180] ($(Xzd2.west) + (0,0.15)$);
        \draw (Xzd) -- (X) (Xzd2) -- (X2);
    \end{tikzpicture}\\
        &= \begin{tikzpicture}
        \path (0,0) node[dist] (H) {$\kernel{T}^{\RV{Z}}$}
        ++ (0.9,0) node[copymap] (copy1) {}
        ++ (0.9,-0.5) node[kernel] (Xzd) {$\kernel{T}^{\RV{X}_1|\RV{Z}}$}
        + (0.7,0) node[copymap] (copy2) {}
        ++ (1.5,-0.5) node[kernel] (Xzd2) {$\kernel{T}^{\RV{X}_2|\RV{Z}}$}
        ++ (1.5,0.5) node (X) {$\RV{X}_1$}
        + (0,0.5) node (Z) {$\RV{Z}$}
        + (0,-0.5) node (X2) {$\RV{X}_2$}
        + (0,-1) node (X3) {$...$};
        \draw (H) -- (Z);
        \draw (copy1) to [out=-60,in=180] ($(Xzd.west) + (0,0.15)$);
        \draw (copy1) to [out=-90,in=180] ($(Xzd2.west) + (0,0)$);
        \draw[-{Rays[n=8]}] (copy2) to [out=-90,in=180] ($(Xzd2.west) + (-0.1,0.15)$);
        \draw (Xzd) -- (X) (Xzd2) -- (X2);
    \end{tikzpicture}\\
     &= \begin{tikzpicture}
        \path (0,0) node[dist] (H) {$\kernel{T}^{\RV{Z}}$}
        ++ (0.9,0) node[copymap] (copy1) {}
        ++ (0.9,-0.5) node[kernel] (Xzd) {$\kernel{T}^{\RV{X}_1|\RV{Z}}$}
        ++ (0,-0.5) node[kernel] (Xzd2) {$\kernel{T}^{\RV{X}_1|\RV{Z}}$}
        ++ (1.5,0.5) node (X) {$\RV{X}_1$}
        + (0,0.5) node (Z) {$\RV{Z}$}
        + (0,-0.5) node (X2) {$\RV{X}_2$}
        + (0,-1) node (X3) {$...$};
        \draw (H) -- (Z);
        \draw (copy1) to [out=-60,in=180] ($(Xzd.west) + (0,0.15)$);
        \draw (copy1) to [out=-90,in=180] ($(Xzd2.west) + (0,0)$);
        \draw (Xzd) -- (X) (Xzd2) -- (X2);
    \end{tikzpicture}\\
    &=  \begin{tikzpicture}
            \path (0,0) node[dist,inner sep=-2pt] (H) {$\kernel{T}^{\RV{Z}}$}
            ++ (0.5,0) node[copymap] (copy0) {}
            ++ (0.6,0) node[copymap,label={$A$}] (copy1) {}
            ++ (1.2,0.5) node[kernel] (XH) {$\kernel{T}^{\RV{X}_1|\RV{Z}}$}
            + (0,-0.5) node[kernel] (XH2) {$\kernel{T}^{\RV{X}_1|\RV{Z}}$}
            ++ (1.2,-0.25) node (X1) {$\RV{X}$}
            + (0,0.9) node (Hb) {$\RV{Z}$};
            \draw (H) -- (copy1) (copy1) to [out=35,in=180] (XH) (copy1) to [out=-35,in=180] (XH2);
            \draw ($(XH.east)$) to [out=-15,in=180] ($(X1.west) + (-0.2,0.25)$) ($(XH2.east)$) to [out=15,in=180] ($(X1.west)+(-0.2,-0.25)$);
            \draw ($(X1.west) + (-0.2,0)$) to (X1);
            \draw ($(copy1.west)+(-0.1,0.8)$) rectangle ($(X1.west) + (-0.2,-0.55)$);
            \draw (copy0) to [out = 90, in=180] (Hb);
        \end{tikzpicture}
\end{align}

As desired.

\end{proof}

Theorem \ref{th:rep_seedo_obs} establishes that, given a see-do forecast with exchangeable observations, there exists a variable (namely $\RV{Z}$) that plays the role of the hypothesis with respect to the observations. That is, disintegrating a see-do forecast on the hypothesis gives a see-do model for which the observations are independent and identically distributed and which converge in limiting frequency. However, the consequences in the see-do model obtained by disintigrating such a see-do forecast need not be extentable to a kernel with any particular limiting frequency properties. This is a rather different type of model to a Causal Bayesian Network where consequences are given by expressions like $\prob{P}(\RV{Y}|do(\RV{W}=w))$ which seem to represent frequentist properties of a ``consequence generating mechanism''. As we will see in the next chapter, Causal Bayesian Networks are a type of see-do model, but they are one where both the observations and consequences are frequentist models. This type of model is associated with see-do forecasts for which the observations are exchangeable (as in Theorem \ref{th:rep_seedo_obs}) and the consequences are \emph{functionally exchangeable}.

Functional exchangeability is a generalisation of exchangeablility to Markov kernels. It captures the intuition that if we swap the order of the outputs (say, $\RV{Y}_1,\RV{Y}_2$ is swapped to $\RV{Y}_2, \RV{Y}_1$), we need to make analagous exchange of choices ($\RV{D}_1$, $\RV{D}_2$ becomes $\RV{D}_2$, $\RV{D}_1$) in order to maintain the correspondence of choices and outputs.

\begin{definition}[Functional Exchangeability]
Given a see-do forecast $(\kernel{T},\RV{D},\RV{X},\RV{Y})$ where $\RV{D}=\utimes_{i\in A}\RV{D}_i$ and $\RV{Y}=\utimes_{i\in A} \RV{X}_i$ for some random variables $\{\RV{D}_i\}_A, \{\RV{Y}_i\}_A$, for any perumtation $\rho:A\to A$ define the observation and choice swap function $\rho^{\RV{D}\RV{Y}}$ and the choice swap function $\rho^{\RV{D}}$ as in \ref{def:permut_swap}. Then $\kernel{T}$ is functionally exchangeable with respect to $\RV{D}$ and $\RV{X}$ if $\kernel{T}=R^{\RV{D}}\kernel{T}R^{\RV{D}\RV{Y}}$.

$\kernel{T}$ is infinitely functionally exchangeably extendable if here exists a do forecast $(\kernel{T}',\RV{D}',\RV{X}')$ non-interfering and functionally exchangeable with respect to $\RV{D}'=\utimes_{i\in\mathbb{N}}\RV{D}'_i$ and $\RV{Y}'=\utimes_{i\in\mathbb{N}}\RV{Y}'_i$ such that for all $B\subset A$

\begin{align}
    \kernel{T}^{\prime \RV{Y}'_B|\RV{D}'_B}=\kernel{T}^{\RV{Y}_B|\RV{D}_B} 
\end{align}

A see-do forecast that is infinitely exchangeably extendable with respect to $\RV{X}$ and infinitely functionally exchangeably extendable with respect to $\RV{D}$, $\RV{Y}$ is \emph{doubly exchangeable} (we omit ``infinitely extendable'' for the sake of brevity).

\end{definition}

The following lemma interprets an exchangeable probability measure as an exchangeable see-do forecast with observations and consequences interchanged. This is so we can re-use Theorem \ref{th:rep_seedo_obs} without separately proving an almost identical theorem for probability measures.

\begin{lemma}[Functionally exchangeable see-do models with exchangeable choices induce exchangeable see do models]\label{lem:f-ex2ex}
Given a see-do forecast $(\kernel{T},\RV{D},\RV{X},\RV{Y})$ functionally exchangeable with respect to $\RV{Y}=\utimes_{i\in A} \RV{Y}_i$ and $\RV{D}=\utimes_{i\in A}\RV{D}_i$ and some $\prob{P}^{\RV{D}}$ exchangeable with respect to $\RV{D}$, then $\prob{P}\kernel{T}\in \Delta(\sigalg{D}\otimes\sigalg{X}\otimes\sigalg{Y})$ is exchangeable with respect to $\RV{G}:=\utimes_i\in A \RV{Y}_i\utimes\RV{D}_i$. 

Furthermore, defining the trivial choice $\RV{C}:\{*\}\times X\times Y\to *$, $(\prob{P}\kernel{T},\RV{C},\RV{G},\RV{X})$ is a see-do forecast exchangeable with respect to $\RV{G}$.
\end{lemma}

\begin{proof}

For arbitrary $\rho:A\to A$, associated swap kernel $R^{\RV{D}}$ and $R^{\RV{X}}$:

\begin{align}
    \prob{P}\kernel{T} R^{\RV{D}\RV{X}} &= (\prob{P} R^{\RV{D}}) \kernel{T} R^{\RV{D}\RV{X}}\\
                                  &= \prob{P}(R^{\RV{D}}\kernel{T}R^{\RV{D}\RV{X}})\\
                                  &= \prob{P}\kernel{T}
\end{align}

This is sufficient for exchangeability of $(\prob{P}\kernel{T},\RV{C},\RV{G},\RV{O})$ with respect to $\RV{G}$.
\end{proof}

\begin{lemma}[Representation of functionally exchangeable do forecasts]\label{lem:rep_fex_sdf}
Given a see-do forecast $(\kernel{T},\RV{D},\RV{X},\RV{Y})$ with denumerable choice set $D$ and standard measurable $X$, $Y$, if $\kernel{T}$ is functionally exchangeable with respect to $\RV{D}=\utimes_{i\in \mathbb{N}}\RV{D}_i$ and $\RV{Y}=\utimes_{i\in \mathbb{N}} \RV{Y}_i$, then there exists a function $f:Y\times D\to W$ and a random variable $\RV{W}:=f\circ \RV{Y}$ such that for all $i\in \mathbb{N}$, $\RV{Y}_i\CI_{\kernel{T}}\RV{D}_{\mathbb{N}\setminus\{i\}}\RV{Y}_{\mathbb{N}\setminus \{i\}}|\RV{W}\RV{D}_i$ and $\kernel{T}^{\RV{Y}_i|\RV{D}_i}=\kernel{T}^{\RV{Y}_j|\RV{D}_j}$ for all $i,j\in \mathbb{N}$ and $\RV{X}\CI\RV{Y}|\RV{W}\RV{D}$.

\end{lemma}

\begin{proof}
By Lemma \ref{lem:f-ex2ex} we have some $\prob{P}$ such that $\prob{P}\kernel{T}$ is exchangeable with respect to $\RV{G}:=\utimes_{i\in\mathbb{N}} \RV{Y}_i\utimes\RV{D}_i$. Furthermore, by denumerability of $D$, $\prob{P}$ can be chosen such that $\prob{P}(\RV{D}_[n]\in B)>0$ for all $n\in\mathbb{N}$, $B\in \sigalg{D}_1^n$, $B\neq \emptyset$. By Theorem \ref{th:rep_seedo_obs} we therefore have some $\RV{W}$ (defined as a function of $\RV{Y}\utimes \RV{D}$) such that $(\prob{P}\kernel{T}')^{\RV{Y}_i\RV{D}_i|\RV{W}}=(\prob{P}\kernel{T}')^{\RV{Y}_j\RV{D}_j|\RV{W}}$ for all $i,j\in \mathbb{N}$ and $\RV{Y}_i\utimes\RV{D}_i\CI_{\prob{P}\kernel{T}'} \utimes_{j\in\mathbb{N}\setminus\{i\}} \RV{Y}_j \utimes \RV{D}_j | \RV{W}$ and $\RV{X}\CI\RV{G}|\RV{W}\RV{D}$. \todo[inline]{semigraphoid axioms} By weak union, $\RV{Y}_i\CI_{\prob{P}\kernel{T}'} \utimes_{j\in\mathbb{N}\setminus\{i\}} \RV{Y}_j \utimes \RV{D}_j | \RV{W} \RV{D}_i$ and $\RV{X}\CI\RV{Y}|\RV{W}$.

Because $\prob{P}$ is strictly positive on $\sigalg{D}$, we can apply Lemma \ref{lem:agree_disint} for
\begin{align}
    \kernel{T}^{\RV{X}\RV{Y}|\RV{W}\RV{D}} \overset{a.s.}{=} (\prob{P}\kernel{T})^{\RV{X}\RV{Y}'_A|\RV{W}\RV{D}'_A}
\end{align}

Thus $\RV{Y}_i\CI_{\kernel{T}} \utimes_{j\in \mathbb{N} \setminus\{i\}} \RV{Y}_j \utimes \RV{D}_j | \RV{W} \RV{D}_i$ and $\RV{X}\CI\RV{Y}|\RV{W}$. Marginalising and applying this independence, we finally have for all $i, j\in A$

\begin{align}
    \kernel{T}^{\RV{Y}_i|\RV{D}_i}&\text{ and }\kernel{T}^{\RV{Y}_j|\RV{D}_j}\text{ exist and }\\
    \kernel{T}^{\RV{Y}_i|\RV{D}_i} &= \kernel{T}^{\RV{Y}_j|\RV{D}_j}
\end{align}
\end{proof}

\begin{theorem}[Representation of doubly exchangeable see-do forecasts]\label{th:rep_dex_sdf}
Given a see-do forecast $(\kernel{T},\RV{D},\RV{X},\RV{Y})$ with denumerable $D$ and standard measurable $X$, $Y$, the following statements are equivalent (given finite $A\subset\mathbb{N}$, $B\subset\mathbb{N}$ not necessarily finite):

\begin{enumerate}
    \item $(\kernel{T},\RV{D},\RV{X},\RV{Y})$ is doubly exchangeable with respect to $\RV{D}=\utimes_{i\in A}\RV{D}_i$, $\RV{X}=\utimes_{i\in B} \RV{X}_i$, $\RV{Y}=\utimes_{i\in A} \RV{Y}_i$
    \item There exists a set $H$, $\kernel{T}^{\RV{H}}\in \Delta(\sigalg{H})$ and Markov kernels $\kernel{T}^{\RV{X}_1|\RV{H}}$ and $\kernel{T}^{\RV{Y}_1|\RV{H}\RV{D}_1}$ such that
    \begin{align}
        \kernel{T}^{\RV{Z}\RV{X}\RV{Y}|\RV{D}} = \begin{tikzpicture}
            \path (0,0) node[dist,inner sep=-2pt] (H) {$\kernel{T}^{\RV{H}}$}
            + (0,-1) node (D) {$\RV{D}$}
            ++ (0.5,0) node[copymap] (copy0) {}
            ++ (0.6,0) node[copymap,label={$B$}] (copy1) {}
            ++ (1.2,0.5) node[kernel] (XH) {$\kernel{T}^{\RV{X}_1|\RV{H}}$}
            + (0,-0.5) node[kernel] (XH2) {$\kernel{T}^{\RV{X}_1|\RV{H}}$}
            + (0,-1.75) node[kernel] (YD) {$\kernel{T}^{\RV{Y}_1|\RV{H}\RV{D}_1}$}
            + (-.9,-1.6) node[copymap,label={$A$}] (copy2) {}
            + (-.9,-2.2) node[copymap, label={$A$}] (copy3) {}
            + (0,-2.25) node[kernel] (YD2) {$\kernel{T}^{\RV{Y}_1|\RV{H}\RV{D}_1}$}
            ++ (1.2,-0.25) node (X1) {$\RV{X}$}
            + (0,-1.5) node (Y) {$\RV{Y}$};
            \draw (H) -- (copy1) (copy1) to [out=35,in=180] (XH) (copy1) to [out=-35,in=180] (XH2);
            \draw ($(XH.east)$) to [out=-15,in=180] ($(X1.west) + (-0.2,0.25)$) ($(XH2.east)$) to [out=15,in=180] ($(X1.west)+(-0.2,-0.25)$);
            \draw ($(X1.west) + (-0.2,0)$) to (X1);
            \draw (copy0) to [out=-90,in=180] (copy2) -- ($(YD.west) + (0,0.15)$);
            \draw (D) to [out=0,in=180] (copy3) -- ($(YD.west)+(0,-0.15)$) (YD);
            \draw (copy2) to [out=-45,in=180] ($(YD2.west) + (0,0.15)$);
            \draw (copy3) to [out=-45,in=180] ($(YD2.west) + (0,-0.15)$);
            \draw ($(copy1.west)+(-0.1,0.8)$) rectangle ($(X1.west) + (-0.2,-0.55)$);
            \draw ($(copy2.west)+(-0.1,0.5)$) rectangle ($(Y.west) + (-0.2,-0.85)$);
            \draw ($(Y.west) + (-0.2,0)$) -- (Y);
        \end{tikzpicture}
    \end{align}
    \item There exists a set $H$, $\mu\in \Delta(\sigalg{H})$ and Markov kernels $\kernel{T}^{\RV{H}}$, $\kernel{T}^{\RV{X}_1|\RV{H}}$ and $\kernel{T}^{\RV{Y}_1|\RV{H}\RV{D}_1}$ such that for all $\mathbf{d}_A\in D$, $\{J_i\in \sigalg{X}_1|i\in B\}$, $\{K_i\in \sigalg{Y}_1|i\in A\}$:
    \begin{align}
        \kernel{T}^{\RV{X}\RV{Y}|\RV{D}}_{\mathbf{d}_A}((\times_{i\in B}J_i)\times (\times_{j\in A} K_j)) = \int_{H} \prod_{i\in B} \kernel{T}^{\RV{X}_1|\RV{H}}_h(J_i)\prod_{i\in A}\kernel{T}_{h,d_i}^{\RV{Y}_1|\RV{H}\RV{D}_1}(K_i)d\kernel{T}^{\RV{H}}(h)
    \end{align}

\end{enumerate}
\end{theorem}

\begin{proof}
$(2)$ and $(3)$ are string and integral notation for the same statement. 

$(1)\implies (2)$:

By infinite extendability of $\kernel{T}$, there is a doubly exchangeable see-do forecast $(\kernel{T}',\RV{D}',\RV{X}',\RV{Y}')$ with $\RV{X}'=\utimes{i\in\mathbb{N}}\RV{X}'_i$, $\RV{Y}'=\utimes{i\in\mathbb{N}}\RV{Y}'_i$, $\RV{D}'=\utimes{i\in\mathbb{N}}\RV{D}'_i$ that agrees with $(\kernel{T},\RV{D},\RV{X},\RV{Y})$ on the appropriate marginals. Note that, because the codomain of $\RV{D}'$ is in any nontrivial case uncountably infinite, we don't necessarily have the existence of arbitrary disintegrations of $(\kernel{T}',\RV{D}',\RV{X}',\RV{Y}')$ (see Theorem \ref{th:disintegration_exist} and related discussion).

By Theorem \ref{th:rep_seedo_obs} we have $\RV{Z}'$ such that 
\begin{itemize}
    \item $\RV{X}'_i\CI_{\kernel{T}'}\RV{X}'_{\mathbb{N}\setminus\{i\}}|\RV{Z}'$ for all $i\in A$
    \item $\kernel{T}^{\prime \RV{X}'_i|\RV{Z}''}=\kernel{T}^{\prime \RV{X}'_j|\RV{Z}''}$ for all $i,j\in A$
    \item $\RV{Y}'\CI_{\kernel{T}'}\RV{X}'|\RV{Z}'\utimes \RV{D}'$
\end{itemize}

Because $\RV{W}'$ is a function of $\RV{Y}'$, we also have $\RV{X}'\CI_{\kernel{T}'} \RV{W}'|\RV{Z}'\utimes\RV{D}'$. Furthermore, because $\RV{Z}'$ is a function of $\RV{X}'$ we have $\RV{Z}'\CI_{\kernel{T}'} \RV{D}'$ and so $\RV{X}'\CI_{\kernel{T}'}\RV{D}'\utimes\RV{Z}'$ by contraction and $\RV{X}'\CI_{\kernel{T}'}\RV{D}'|\RV{Z}'$ by weak union. 

Finally, this implies $\RV{X}'\CI_{\kernel{T}'} \RV{W}'\utimes\RV{D}'|\RV{Z}'$ by contraction which in turn implies $\RV{X}'\CI_{\kernel{T}'}\RV{W}'|\RV{Z}'$ by weak union.

By Lemma \ref{lem:rep_fex_sdf} we have $\RV{W}'$ such that

\begin{itemize}
    \item for all $i\in \mathbb{N}$, $\RV{Y}'_i\CI_{\kernel{T}''}\RV{D}'_{\mathbb{N}\setminus\{i\}}\RV{Y}'_{\mathbb{N}\setminus \{i\}}|\RV{W}'\utimes \RV{D}'_i$
    \item $\kernel{T}^{\prime \RV{Y}'_i|\RV{D}'_i}=\kernel{T}^{\RV{Y}'_j|\RV{D}'_j}$ for all $i,j\in \mathbb{N}$ 
    \item $\RV{X}'\CI_{\kernel{T}'}\RV{Y}'|\RV{W}'\utimes \RV{D}'$
\end{itemize}

Because $\RV{Z}'$ is a function of $\RV{X}'$, and thus $\RV{Z}'\utimes\RV{X}'$ is a function of $\RV{X}'$, we also have $\RV{Y}'\CI_{\kernel{T}'}\RV{Z}'\utimes\RV{X}'|\RV{W}'\utimes \RV{D}'$.

By Lemma \ref{lem:representation_of_kernels}, and 
\begin{align}
    \kernel{T}^{\prime \RV{Z}'\RV{W}'\RV{X}'\RV{Y}'|\RV{D}'} &= \begin{tikzpicture}
        \path (0,0) node (D) {$\RV{D}$}
        ++ (0.5,0) node[copymap] (copy0) {}
        ++ (1,0) node[kernel] (H) {$\kernel{T}^{\RV{Z}|\RV{D}}$}
        ++ (0.9,0) node[copymap] (copy1) {}
        ++ (0.9,-0.5) node[kernel] (Xzd) {$\kernel{T}^{\RV{X}|\RV{ZD}}$}
        ++ (0.9,0) node[copymap] (copy2) {}
        ++ (0.9,-0.5) node[kernel] (Yzxd) {$\kernel{T}^{\RV{Y}|\RV{XZD}}$}
        ++ (1.5,0) node (Y) {$\RV{Y}$}
        +  (0,0.5) node (X) {$\RV{X}$}
        + (0,1) node (Z) {$\RV{Z}$};
        \draw (D) -- (H) -- (Z);
        \draw (copy0) to [out=-45,in=180] ($(Xzd.west) + (0,-0.15)$);
        \draw (copy1) to [out=-60,in=180] ($(Xzd.west) + (0,0.15)$);
        \draw (copy0) to [out=-90,in=180] ($(Yzxd.west) + (0,-0.15)$);
        \draw (copy1) to [out=-90,in=180] ($(Yzxd.west) + (0,0)$);
        \draw (copy2) to [out=-90,in=180] ($(Yzxd.west) + (0,0.15)$);
        \draw (Xzd) -- (X) (Yzxd) -- (Y);
    \end{tikzpicture}\\
     &= \begin{tikzpicture}
        \path (0,0) node (D) {$\RV{D}$}
        ++ (0.5,0) node[copymap] (copy0) {}
        ++ (1,0) node[dist] (H) {$\kernel{T}^{\RV{Z}}$}
        ++ (0.9,0) node[copymap] (copy1) {}
        ++ (0.9,-0.5) node[kernel] (Xzd) {$\kernel{T}^{\RV{X}|\RV{Z}}$}
        ++ (0.9,0) node[copymap] (copy2) {}
        ++ (0.9,-0.5) node[kernel] (Yzxd) {$\kernel{T}^{\RV{Y}|\RV{ZD}}$}
        ++ (1.5,0) node (Y) {$\RV{Y}$}
        +  (0,0.5) node (X) {$\RV{X}$}
        + (0,1) node (Z) {$\RV{Z}$};
        \draw[-{Rays[n=8]}] (D) -- ($(D.west) + (1,0)$);
        \draw (H) -- (Z);
        \draw[-{Rays[n=8]}] (copy0) to [out=-45,in=180] ($(Xzd.west) + (-0.7,-0.15)$);
        \draw (copy1) to [out=-60,in=180] ($(Xzd.west) + (0,0.15)$);
        \draw (copy0) to [out=-90,in=180] ($(Yzxd.west) + (0,-0.15)$);
        \draw (copy1) to [out=-90,in=180] ($(Yzxd.west) + (0,0)$);
        \draw[-{Rays[n=8]}] (copy2) to [out=-90,in=180] ($(Yzxd.west) + (-0.1,0.15)$);
        \draw (Xzd) -- (X) (Yzxd) -- (Y);
    \end{tikzpicture}\\
     &= \begin{tikzpicture}
        \path (0,0) node (D) {$\RV{D}$}
        ++ (0,1) node[dist] (H) {$\kernel{T}^{\RV{Z}}$}
        ++ (0.9,0) node[copymap] (copy1) {}
        ++ (0.9,-0.5) node[kernel] (Xzd) {$\kernel{T}^{\RV{X}|\RV{Z}}$}
        ++ (1.8,-0.5) node[kernel] (Yzxd) {$\kernel{T}^{\RV{Y}|\RV{ZD}}$}
        ++ (1.5,0) node (Y) {$\RV{Y}$}
        +  (0,0.5) node (X) {$\RV{X}$}
        + (0,1) node (Z) {$\RV{Z}$};
        \draw (H) -- (Z);
        \draw (copy1) to [out=-60,in=180] ($(Xzd.west) + (0,0.15)$);
        \draw (D) -- ($(Yzxd.west) + (0,-0.15)$);
        \draw (copy1) to [out=-90,in=180] ($(Yzxd.west) + (0,0)$);
        \draw (Xzd) -- (X) (Yzxd) -- (Y);
    \end{tikzpicture}
\end{align}

\end{proof}