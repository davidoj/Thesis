
%!TEX root = main.tex

\chapter{Statistical models with consequences}\label{ch:2p_statmodels}

\section{Summary}

Statistical models are ubiquitous in the analysis of inference problems. A statistical model features a set of \emph{states}, and each state is mapped to a probability distribution over \emph{outcomes}. If we want to model problems involving \emph{decisions} and \emph{consequences}, we need to consider different kinds of statistical models. We introduce two types of model for this purpose: \emph{two player statistical models} which differ from classical statistical model in that the state is assumed to consist of a decision and a \emph{hypothesis} (the two players are the decision maker ``player D'' and the hypothesis selector ``player H''). They model the consequences of decisions under various hypotheses. A \emph{see-do model} is a special case of two player statistical model that can be used in situtations where some \emph{observations} are available for review prior to selecting a decision. See-do models are the main focus of work here and problems involving observations, decisions and consequences will be discussed at length in Chapter \ref{ch:evaluating_decisions}.

A common simplifying assumption made when using classical statistical models is that they are \emph{conditionally independent and identically distributed} (conditinally IID); this means that the model maps each state to an independent and identically distributed (IID) sequence of observations. This just a common choice, it is not a strict requirement. A similar assumption is likely to be useful for see-do models. If consequences depend on choices, then it does not make sense to assert that observations and consequences together form a single IID sequence of random variables, so we need to consider alternatives. We propose that models where observervations are an IID sequence and choices and consequences together are \emph{independent and functionally identical} (IFI; defined later in this chapter) are similar to conditionally IID statistical models. 

Instead of directly assuming that a conditionally IID model is appropriate, \emph{De Finetti's representation theorem} shows that probability models where the sequence of observations is \emph{exchangeable} induce conditionally IID statistical models. The assumption of exchangeabile observations is preferable to the assumption of IID observations if a probability model is being used to represent subjective uncertainty. We investigate whether there is an analogous result relating ``exchangeability-like'' assumptions for see-do models to ``IID-like'' assumptions. We show that there is: in particular, a see-do ``forecast'' with exchangeable observations and \emph{functionally exchangeable} decision to consequence maps induces a see-do model with IID observations and IFI consequences.

The assumption of functional exchangeability will appear again in Chapter \ref{ch:ints_counterfactuals} as part of the definition of \emph{counterfactual models}, and the joint assumptions of exchangeable observations and functionally exchangeable consequences to motivate the assumption of \emph{imitability} in Chapter \ref{ch:inferring_causes}, an assumption that in combination with a number of other assumptions allows for inference of consequences from data.

\section{Modelling observations, choices and consequences}

\subsection{Modelling observations with statistical models}

Statistical models are a ubiquitous type of model in statistics and machine learning. They consist of a set of \emph{states} $(S,\sigalg{S})$, and for each state the model prescribes a single probability distribution on a given measurable set of \emph{outcomes} $(O,\sigalg{O})$.

\begin{definition}[Statistical model]\label{def:statistical model}
A statistical model is a set of states $(S,\sigalg{S})$, a set of outcomes $(O,\sigalg{O})$ and a stochastic map $\kernel{T}:S\to \Delta(\sigalg{O})$.
\end{definition}

\begin{definition}[State and outcome variables]\label{def:state_outcome}
Given a statistical model $(\kernel{T},(O,\sigalg{O}),(S,\sigalg{S}))$, define the \emph{state variable} $\RV{S}:S\times O\to S$ as the projection from $S\times O\to S$ and define the \emph{outcome variable} $\RV{O}:S\times O\to O$ as the projection onto $O$.
\end{definition}

The common example of a potentially biased coin is modelled with a statistical model. We suppose our coin has some rate of heads $\theta\in [0,1]$, and we furthermore suppose that for each $\theta$ the result of flipping the coin can be modeled (in some sense) by the probability distribution $\text{Bernoulli}(\theta)$. The statistical model here is the set of states $S=[0,1]$ (corresponding to \emph{rates of heads}), the observation space $O=\{0,1\}^n$ with the discrete sigma-algebra (where $n$ is the number of flips observed) and the stochastic map $\kernel{B}:[0,1]\to \Delta(\mathscr{P}(0,1))$ which is given by $\kernel{B}:\theta\to \text{Bernoulli}(\theta)$.

Almost any theoretical treatment of statistics or machine learning will at some point make use of statistical models to describe the problem they are addressing -- for a collection of examples from the last 100 years, see \cite{Goodfellow-et-al-2016,vapnik_nature_2013,bishop_pattern_2006,le_cam_comparison_1996,freedman_asymptotic_1963,wald_statistical_1950,de_finetti_foresight_1992,fisher_statistical_1992}. They are often simply assumed without a great deal of discussion of why this type of model is chosen, or what role they play.

If we want to reason about how well some learning algorithm performs in some context, we typically require a reasonable model of the context in which the learning algorithm operates. The algorithms themselves may not give us such a model. Because learning almost always operates in a context with noise an uncertainty, we need models that can handle noise and uncertainty. Probability models are a very common choice for this. In addition, it is often assumed that we do not know with certainty the exact probability model that should be used to model a context. A statistical model assumes a certain number of states may prevail -- reflecting uncertainty in the ``mechanics of the world'' -- and given any state it gives us a probability distribution -- reflecting uncertainty remaining after the mechanics of the world are well-understood.

Learning algorithms don't necessarily implement reasonable models of the world. For example, consider a linear regressor that takes a set of predictors $x\in X$ and targets $y\in Y$ and returns some $\beta\in B$ such that $(y-x^T\beta)^2$ is as small as possible. It is possible to interpret $B$ as a set of states, and consider the learner to be implementing the statistical model $(\kernel{T},B,\mathcal{L}_{X\to Y})$ where $\mathcal{L}_{X\to Y}$ is the set of liner function $X\to Y$ given by $\{x\mapsto x^T\beta|\beta\in B\}$ and $\kernel{T}$ maps a state $\beta\in B$ deterministically to the function $x\mapsto x^T\beta$. This is, formally, a statistical model, but it is not one that would typically be considered a good model of the world in the kinds of problems that a linear regressor is used to solve. One problem with this model is that it is deterministic - the outcome for any $\beta\in B$ will be a particular function $X\to Y$. However, it will almost never be the case that some set of targets $y$ will be an exact function of some set of predictors $x$, and insisting on an exact functional relationship will typically give very poor generalisation results if this demand can be satisfied at all.

Suppose we want to ask whether the function $f$ given by a linear regressor is useful for some purpose. In order to address this question, we want to consider a more appropriate model of the world than the crude statistical model given above, and consider what behaviour we will see from the regressor under different assumptions imposed on this model. Statistical models typically are used to serve the purpose of a ``more appropriate model of the world''. In this example, we might consider a statistical model $(\kernel{T},H,O)$ where for each $h\in H$, $\kernel{T}_h\in \Delta(\sigalg{Y}\otimes\sigalg{X})$ such that $\kernel{T}_h^{\RV{Y}|\RV{X}}=\text{Normal}(\RV{X}^T\beta_h,\sigma_h)$. If we assume the data generating process is described by such a probability distribution for some $h$, we can ask questions like ``does the linear regressor output a $\beta$ such that $\RV{Y}-\RV{X}^T\beta<\epsilon$ with high probability for all $h\in H$?''

\subsection{Modelling choices and consequences with two-player statistical models}

The states in a statistical model are usually considered to be ``under the control of nature''. In the possibly biased coin example above, if we were to consider some ``player D'' acutally flipping the coin and trying to infer the bias, we would typically assume that their opinion about the coin's bias does not affect the coin's actual bias; they could decide it is biased towards heads when in fact it is completely unbiased. In some cases player D can make choices that affect the outcomes. Suppose player D has the option to choose how high to toss the coin -- perhaps they can aim for a toss height anywhere from 10 to 50cm. This plausibly affects the outcomes of their coin toss and, unlike the coin's bias, they gets to choose the height they intend to toss it. If they decide to toss it to a height of 15cm then 15cm is the height they have chosen to toss it to. Unlike the state, which can differ from whatever player D ultimately decides on, the choice made by player D is the same thing as whatever they ultimately decide on. We call features of the state that are not under player D's control \emph{hypotheses} and features that are under player D's control \emph{decisions}, and statistical models in which the state is the Cartesian product of a set of hypotheses and a set of decisions ``two player statistical models'' (the two players being nature or ``player H'' and the decision maker or ``player D'').

\begin{definition}[Two player statistical model]\label{def:2p_stat}
A \emph{two-player statistical model} is a tuple $(\kernel{T},\RV{H},\RV{D},\RV{O})$ where $(\kernel{T},(H\times D,\sigalg{H}\otimes\sigalg{D}), (O,\sigalg{O}))$ is a statistical model and $\RV{H}:H\times D\times O\to H$, $\RV{D}:H\times D\times O\to D$ and $\RV{O}:H\times D\times O\to O$ are measurable functions that project elements of $H\times D\times O$ to their respective codomains. $\RV{H}$ is called the \emph{hypothesis}, $\RV{D}$ the \emph{decision} and $\RV{O}$ the \emph{outcome}.
\end{definition}

Whenever we propose a two player statistical model, we will also assume for any random variables $\RV{X}: H\times D\times O\to X$ and $\RV{Y}:H\times D\times O\to Y$, a disintegration $\kernel{K}^{\RV{Y}|\RV{XDH}}:X\times D\times H\to \Delta(\sigalg{Y})$ exists (see Theorem \ref{th:existence_continous} for a sufficient condition).

The problems that we will mostly study in this work, in addition to having a second player (``player D''), will often involve some data $\RV{X}$ that is observed before the second player is able to make a choice. Two player statistical models with \emph{observations} are called \emph{see-do models}.

\begin{definition}[See-Do model]\label{def:seedo}
A \emph{see-do model} $(\kernel{T},\RV{H},\RV{D},\RV{X},\RV{Y})$ is a two-player statistical model along with two additional random variables: the \emph{observation} $\RV{X}: H\times D\times O\to X$ and the \emph{consequence} $\RV{Y}:H\times D\times O\to Y$. The outcome variable is defined to be the coupled product of the observation and the consequence $\RV{O}=(\RV{X},\RV{Y})$, and we will leave this implicit when specifying a see-do model. A see-do model must observe the conditional independence:
\begin{align}
\RV{X}\CI_\kernel{T} \RV{D}|\RV{H} \label{eq:see_do_independence_requirement}
\end{align}
\end{definition}

We can informally read the independence requirement as saying ``the observations are independent of the decision given the hypothesis''. This does not imply that probability models we construct from $\kernel{T}$ will necessarily have the property that $\RV{D}$ and $\RV{X}$ will be independent conditional on $\RV{H}$, and in fact this will often not be the case. In Chapter \ref{ch:ints_counterfactuals} we will argue that this requirement captures the intuition that observations are not ``affected'' by decisions. For now, we will observe that this independence requirement means that $\kernel{T}$ can be drawn with no path from $\RV{D}$ to $\RV{X}$.

Explicitly, the independence on line \ref{eq:see_do_independence_requirement} implies that the kernel $\kernel{T}$ can be drawn as follows:

\begin{align}
\kernel{T}:= \begin{tikzpicture}
                 \path (0,0) node (H) {$\RV{H}$}
                 + (0,-1) node (D) {$\RV{D}$}
                 ++ (0.5,0) node[copymap] (copy0) {}
                 ++ (0.9,0) node[kernel] (XH) {$\kernel{T}^{\RV{X}|\RV{H}}$}
                 + (0,-0.85)  node[kernel] (YHD) {$\kernel{T}^{\RV{Y}|\RV{HD}}$}
                 ++ (1,0) node (X) {$\RV{X}$}
                 + (0,-0.85) node (Y) {$\RV{Y}$};
                 \draw (H) -- (XH) -- (X);
                 \draw (copy0) to [out=-45,in=180] ($(YHD.west) + (0,0.15)$);
                 \draw (D) to [out=0,in=180] ($(YHD.west) + (0,-0.15)$) (YHD) -- (Y);
             \end{tikzpicture}
\end{align}

In this picture, again informally, $\RV{Y}$ takes input from $\RV{D}$ but $\RV{X}$ does not.    

\section{Repeatable experiments and actions}\label{sec:repeatable_experiments}

It is very common that statistical problems feature data from repeatable experiments. These are typically modelled with statistical models of \emph{conditionally independent and identically distributed} (conditionally IID) sequences. Given a statistical model $(\kernel{T}, (O,\sigalg{O}), (S,\sigalg{S}))$ with state variable $\RV{S}$, we say that a sequence of random variables $(\RV{X}_i)_{i\in A}$, where $\RV{X}_i:O\to X_0$ for all $i$, is conditionally independent and identically distributed if $\RV{X}_i\CI_{\kernel{T}}\RV{X}_{A\setminus \{i\}}|\RV{S}$ for all $i\in A$. Given a particular state $s\in S$, this implies $(\RV{X}_i)_{i\in A}$ are independent and identically distributed with respect to $\kernel{T}_s$; $\RV{X}_i\CI_{\kernel{T}_s} \RV{X}_{A\setminus \{i\}}$ for all $i\in A$.

This assumption is very common and sometimes made implicitly. Statements like ``assume we have an independent and identically distributed sequence'' can be understood to imply a conditionally IID statistical model -- each state can be thought of as a possible marginal distribution of one observation, and the statistical model maps all such states to a sequence of mutually independent observations that all share this distribution.

Given how common the conditional IID assumption is, and given how similar see-do models are to statistical models, it is reasonable to suppose that see-do models will often be used with assumptions like conditional IID. One way to apply a conditional IID assumption to a see-do model is to forget the distinction between observations and consequences and assume that that observations and consequences are together IID conditional on $(\RV{H},\RV{D})$. This is usually an inappropriate assumption, so the question arises: what assumptions similar to the conditional IID assumption are appropriate for see-do models?

Let's first consider the assumption that observations and consequences are jointly IID conditional on $(\RV{H},\RV{D})$. From an informal point of view, if $\RV{X}$ and $\RV{Y}$ form a single IID sequence, and $\RV{X}$ is independent of $\RV{D}$, then it seems that $\RV{D}$ cannot affect the consequences $\RV{Y}$. More formally, suppose we have a see-do model $(\kernel{T},\RV{H},\RV{D},\RV{X},\RV{Y})$ and the observations and consequences are both sequences of random variables $\RV{X}=(\RV{X}_i)_{i\in A}$, $\RV{Y}=(\RV{Y}_i)_{i\in B}$. See-do models assume that $\RV{X}\CI\RV{D}|\RV{H}$ which implies for all $i\in A$ $\RV{X}_i\CI\RV{D}|\RV{H}$. If we assume that the sequence $(\RV{X}_i,\RV{Y}_i)_{i\in A}$ is condtionally IID given $(\RV{H},\RV{D})$ (recall that this pair is the state variable for see-do models), then $\kernel{T}^{\RV{Y}_i|\RV{HD}}=\kernel{T}^{\RV{X}_j|\RV{HD}}$ for all $i\in B$, $j\in A$. Because $\RV{Y}=(\RV{Y}_i)_{i\in A}$, $\RV{Y}\CI \RV{D}|\RV{H}$ also.

There is some subtlety in understanding what this means. Just as $\RV{X}\CI_{\kernel{T}}\RV{D}|\RV{H}$ does not necessarily mean $\RV{X}$ and $\RV{D}$ are probabilistically independent in any probability model we construct from $\kernel{T}$, $\RV{Y}\CI_{\kernel{T}}\RV{D}|\RV{H}$ does not rule out constructing probability models from $\kernel{T}$ where $\RV{Y}$ and $\RV{D}$ are not independent. However, such constructions rely on non-standard usage of see-do models where ``decisions follow from consequences'' (this is discussed further in Chapter \ref{ch:ints_counterfactuals}). Thus for most normal usage see-do models should avoid the property $\RV{Y}\CI_{\kernel{T}}\RV{D}|\RV{H}$.

Because we want observations to be independent of decisions $\RV{X}\CI_{\kernel{T}}\RV{D}|\RV{H}$ but we also want consequences to be non-independent $\RV{Y}\not\CI_{\kernel{T}}\RV{D}|\RV{H}$, this suggests that a ``conditional IID-like'' assumption for see-do models must treat $\RV{X}$ and $\RV{Y}$ as different sequences. For the observations, the obvious choice is to assume that $(\RV{X}_i)_{i\in A}$ are IID conditional on $\RV{H}$. Because $\RV{X}\CI\RV{D}|\RV{H}$ and any subsequence of observations is a function of $\RV{X}$, this implies that observations are IID conditional on $(\RV{H},\RV{D})$ also. If this assumption holds, then the see-do model induces a conditionally IID statistical model if we forget about the consequences. This aligns with a common practice in the construction of causal models: assuming that a conditionally IID statistical model describes the observations. This assumption is usually being made, for example, when it is assumed that there is a fixed but unknown ``observational probability distribution'' that could in principle be determined precisely given infinite data.

There are at least two different ``IID-like'' assumptions we could make pertaining to a sequence of consequences $\RV{Y}=(\RV{Y}_i)_{i\in A}$. In the first case, we could assume that it is only possible to influence all consequences at once; in the second case, we assume that all consequences can be individually influenced. The first case could be used to model ``high-level policy'' decisions -- for example, a teacher deciding which instruction to give to an entire class at once. The second could be used to model ``repeated individual'' decisions, for example a teacher offering feedback on individual students' work.

If the entire sequence must be controlled at once, we can assume that $(\RV{Y}_i)_{i\in B}$ is conditionally IID given $(\RV{H},\RV{D})$. Combined with a conditional IID assumption for observations, this gives us a see-do model with two sequences -- $\RV{X}$ and $\RV{Y}$ -- both of which are conditionally IID given $(\RV{H},\RV{D})$, but each may in general have different distributions. We will call such models \emph{double IID}.

To model independent control of each element $\RV{Y}_i$ of the sequence of consequences, we must also have a sequence of ``sub-decisions'' $D=(\RV{D}_i)_{i\in B}$, where each $\RV{D}_i$ controls one consequence $\RV{Y}_i$. In this case, it doesn't make sense to assume that the sequence of consequences is IID, as different choices for $\RV{D}_i$ and $\RV{D}_j$ may yield different distributions for $\RV{Y}_i$ and $\RV{Y}_j$. Rather than assuming that each $\RV{Y}_i$ is identically distributed, we can assume that the function $\kernel{T}^{\RV{Y}_i|\RV{H}\RV{D}_i}:D_1\to \Delta(\RV{Y}_1)$ is identical for each $(\RV{D}_i,\RV{Y}_i)$ pair. If this is true, we will say that $\RV{D}$ and $\RV{Y}$ together are conditionally \emph{independent and functionally identical} (IFI). As with IID sequences, IFI sequences are mutually independent. Rather than each consequence having the same distribution, the assumption of functional identity holds that the stochastic maps from sub-decisions to sub-consequences are identical for all $(\RV{D}_i,\RV{Y}_i)$.

\begin{definition}[Conditionally independent and functionally identical]\label{def:ciid_cifi}
Given a see-do model $(\kernel{T},\RV{H},\RV{D}_B,\RV{X}_A,\RV{Y}_B)$ where $\RV{D}_B=(\RV{D}_i)_{i\in B}$, $\RV{Y}_B=(\RV{Y}_i)_{i\in B}$, $\RV{D}_B$ and $\RV{Y}_B$ are conditionally independent and functionally identical if both of the following hold:
\begin{enumerate}
    \item $\RV{Y}_i\CI_{\kernel{T}} (\RV{X},\RV{Y}_{B\setminus \{i\}},\RV{D}_{B\setminus \{i\}})|\RV{H}\RV{D}_i$ for all $i\in A$
    \item $\kernel{T}^{\RV{Y}_i|\RV{H}\RV{D}_i} = \kernel{T}^{\RV{Y}_j|\RV{H}\RV{D}_j}$ for all $i,j\in A$
\end{enumerate}
\end{definition}

\subsection{Justifications for ``independent and identically distributed'' type assumptions}

The assumption of independent and identically distributed sequences is a very common one, but this doesn't mean that it's always a justified one. In this section, we revisit a classic result that shows that \emph{exchangeability} of observations can justify the use of conditionally independent and identically distributed statistical models, and show that this result can be extended to see-do models in the form of an assumption of \emph{functional exchangeability}. 

To begin with, we will consider what it is probability is being used to model in statistical models and see-do models. It is well-known that probability can be used to model different things (see, for some examples, \citet{hajek_interpretations_2019}). Two common uses for probability are:

\begin{enumerate}
    \item To model systems which produce repeatable observations whose long run frequencies converge to a probability distribution
    \item To make forecasts of things which we do not yet know for sure
\end{enumerate}

In the first case, the probability seems to be a property of the system, so we'll call this ``objective probability'', while in the second case probability is used to quantify someone's personal best guess of something about which they are uncertain. We can call this ``subjective probability''.

It is very common to view probability in classical statistical models as corresponding to something like objective probability. In this view, ``states'' are properties of the world, and for each state the system being described can produce repeatable results that converge in relative frequency to the probability associated with that state. Incidentally, it is hard to choose names for elements of statistical mdoels without suggesting a particular interpretation -- to my ear, the name ``states'' is suggestive of objective probability, but it is not my intention to suggest that we must interpret probability in statistical models as objective probability.

With see-do models, there is an extra reason why subjective view of probability can be appealing. A classical statistical model is often modelling observations, and the kinds of problems in which we use classical statistical models are problems in which we see observations and do something with them in order to make some kind of inference. The kinds of problems in which we use see-do models also involve receiving observations and doing something with them, but they further ask us to select a good decision on the basis of the consequences we anticipate seeing for that decision. Both the observations and the consequences are modelled with probability, and the problem asks us to make a forecast of consequences, something we do not yet know for certain.

Because see-do models are motivated by classical statistical models, and reduce to classical statistical models if we ignore the consequences, it is reasonable to think that they might be used as models of objective probability. Because the problems in which we might use see-do models call for forecasts, there is also reason to think they might be used as models of subjective probability.

One practical consequence of this arises from the fact that, while the assumption of independent and identically distributed sequences is very common in objective probability models, it is much less attractive in models of subjective probability. If a probability distribution is describing a tendency for a tossed coin to land on heads or tails, it seems reasonable to assume that the coin's tendency to land one way or the other is fixed, and its tendency to land one way or the other on the 100th toss has nothing to do with the way it landed on tosses 1 through 99. However, a subjective probability model that describes beliefs about this coin which are updated upon tossing the coin and viewing the results should definitely have the property that beliefs about the outcome of the 100th toss depend in some way on the outcomes of tosses 1 through 99. If one observes 98 heads and 1 tail, it would be very reasonable to increase the strength of one's belief that one will also observe heads on the 100th toss. In order to allow for such updating of beliefs, a subjective probability model must allow for dependence between observations.

De Finetti's well-known representation theorem shows that probability models with \emph{exchangeable} sequences of random variables can always be represented as mixtures of models with IID sequences of random variables. Exchangeable models are more attractive than IID models for subjective probability because they do not rule out updating expectations on the basis of observations of subsequences. Without aiming to be prescriptive about how see-do models ought to be used, and noting that there are reasons to think they might be used to model either objective or subjective probability, we are motivated to ask whether there is a version of the exchangeability assumption for see-do models that can justify the use of objective probability models given an initial goal of representing subjective uncertainty.

We will find additional motivation for considering these assumptions in forthcoming chapters; in Chapter \ref{ch:ints_counterfactuals} we will discuss how exchangeability-like assumptions seem to be a key feature of models of counterfactual propositions (such as potential outcomes), and in Chapter \ref{ch:inferring_causes} we will consider how assumptions of symmetry that hold within each sequence $(\RV{X}_i)_{i\in A}$ and $(\RV{Y}_i)_{i\in B}$ could be extended to some kind of symmetry that holds across both sequences.

We have acknowledged some ambiguity in whether see-do models will be used to represent subjective or objective uncertainty. We find a similar ambiguity in \citet[Chapter 1]{pearl_causality:_2009}; Introducing probability, Pearl writes:

\begin{quote}
We will adhere to the Bayesian interpretation of probability, according to which probabilities encode degrees of belief about events in the world and data are used to strengthen, update, or weaken those degrees of belief. In this formalism, degrees of belief are assigned to propositions (sentences that take on true or false values) in some language, and those degrees of belief are combined and manipulated according to the rules of probability calculus.
\end{quote}

Which endorses the view that, in the work that follows, probability will be used to model subjective uncertainty. However, later in the same chapter he writes

\begin{quote}
[...] causal relationships are ontological, describing objective physical constraints in our world, whereas probabilistic relationships are epistemic, reflecting what we know or believe about the world.
\end{quote}

If causes are objective physical constraints, then if Causal Bayesian Networks described by Pearl model causes it appears that they must be modelling such objective constraints, not merely subjective uncertainty about consequences.

In the following discussion we will consider modelling subjective uncertainty with a single probability distribution. This is a common choice, but the aim of modelling subjective uncertainty does not force us to use a single probability distribution. \citet{walley_statistical_1991} has developed a kind of model for subjective uncertainty he calls a \emph{prevision}, and each prevision is equivalent to a class of probability measures, and a class of probability measures can be represented as a statistical model. If the motivating arguments for previsions can be extended to address joint observations, decisions and consequences, and whether any resulting ``see-do prevision'' can be represented by a see-do model is an open question.

\subsection{A representation theorem for see-do models}

De Finetti's representation theorem shows that every exchangeable probability measure is the product of a conditionally IID statistical model and a \emph{prior}, which is a probability measure over states. Exchangeability is a property of a probability measure and a sequence of random variables - a probability measure $\prob{P}$ is exchangeable with respect to a sequence $(\RV{X}_i)_{i\in A}$ if for all finite permutations $\rho:A\to A$, $\prob{P}^{(\RV{X}_i)_{i\in A}} = \prob{P}^{(\RV{X}_{\rho{i}})_{i\in A}}$.

We present a version of De Finetti's theorem and go on to extend it to see-do models. To formally define exchangeability, we will make use of swap functions. Swap functions are functions that swap random variables in a sequence according to some permutation of the sequence indices. We define swap functions with respect to the sequence of random variables to be swapped and a second random variable that should be left untouched. This second part of the definition is to facilitate exchangeability of two different sequences.

\begin{definition}[Permutations]\label{def:permutation}
A \emph{finite permutation} $\rho'$ on $B\subseteq\mathbb{N}$ is a map $B\to B$ such that there is some finite $A\subset B$ for which $\rho'|_A:A\to A$ is an invertible function and $\rho'|_{B\setminus A} = \mathrm{Id}_{B\setminus A}$.
\end{definition}

\begin{definition}[Swap function]\label{def:swap}
Given measureable space $(E,\sigalg{E})$, a sequence of random variables $\RV{X}:=(\RV{X}_i)_{i\in A}$, a random variable $\RV{Y}$ with $\sigma(\RV{X})\cap \sigma(\RV{Y})=\{\emptyset,E\}$ and a finite permutation $\rho:A\to A$, the swap function $\rho^{\RV{X}}_{\RV{Y}}:E\to E$ that holds $\RV{Y}$ fixed is a $\sigalg{E}$-measurable function for which $\RV{X}_i\circ \rho^{\RV{X}}_{\RV{Y}}=\RV{X}_{\rho(i)} $ for all $i\in A$, and $\RV{Y}\circ\rho^{\RV{X}}_{\RV{Y}}=\RV{Y}$.
\end{definition}

For example, and as a proof of existence, if $E = Y\times X_0^{|A|}$, $\RV{X}_i:E\to X_0$ projects the $i$-th ``x'' element of the sequence $(y,x_1,...,x_i,...)\mapsto x_i$, $\RV{X}=(\RV{X}_i)_{i\in A}$ then for any finite permuation $\rho:A\to A$ the swap function is the the fuction $\rho^{\RV{X}}_{\RV{Y}}:(y,x_1,...,x_i,...)\mapsto (y,x_{\rho(1)},...,x_{\rho(i)},...)$.

If we are only interested in permuting the sequence $\RV{X}$ then we can choose $\RV{Y}=\mathds{1}_E$, for which the second condition will be satisfied automatically. In this case, we will use the notation $\rho^{\RV{X}}$.

Note that a swap function $\rho^{\RV{X}}_{\RV{Y}}$ also has an associated Markov kernel $\kernel{F}_{\rho^{\RV{X}}_{\RV{Y}}}$. The product of the swap kernel with the kernel associated with the original random variable $\RV{X}$ is the permuted version of $\RV{X}$:

\begin{align}
    \kernel{F}_{\rho^{\RV{X}}_{\RV{Y}}} \kernel{F}_{(\RV{X},\RV{Y})} &= \kernel{F}_{\rho^{\RV{X}}_{\RV{Y}}} (\utimes_{i\in B} \kernel{F}_{\RV{X}_i})\utimes \kernel{F}_{\RV{Y}} \\
    &= (\utimes_{i\in B} \kernel{F}_{\rho^{\RV{X}}_{\RV{Y}}} \kernel{F}_{\RV{X}_{i}})\utimes \kernel{F}_{\rho^{\RV{X}}_{\RV{Y}}} \kernel{F}_{\RV{Y}} \label{eq:determ_commute}\\
                                               &= (\utimes_{i\in B} \kernel{F}_{\RV{X}_{\rho(i)}})\utimes \kernel{F}_{\RV{Y}}\label{eq:function_composition}
\end{align}

Where line \ref{eq:determ_commute} follows from the fact that deterministic kernels commute with the split map (\ref{eq:composition}), and line \ref{eq:function_composition} follows from Lemma \ref{lem:func_kern_product}.

As with swap functions, we will define exchangeability with respect to a sequence and an additional random variable that should remain untouched.

\begin{definition}[Exchangeability]\label{def:exchangeability}
Given a probability measure $(\kernel{T},(O,\sigalg{O}))$ with a sequence of random variables $\RV{X}:O\to X_0^{|A|}$, $\RV{X}:=(\RV{X}_i)_{i\in A}$ for $A\subseteq \mathbb{N}$ and some $\RV{Y}:O\to Y$ with $\sigma(\RV{X})\cap\sigma(\RV{Y})=\{\emptyset,O\}$, if for every finite permutation $\rho:A\to A$ we have $\kernel{T}\kernel{F}_{\rho^{\RV{X}}}=\kernel{T}$ then $\kernel{T}$ is \emph{exchangeable} with respect to $\RV{X}$.
\end{definition}

The following lemma shows how this definition relates to the first definition of exchangeability given above.

\begin{lemma}
Given a statistical model $(\kernel{T},(O,\sigalg{O}))$ with random variables $\RV{X}:=(\RV{X}_i)_{i\in A}$ and $\RV{Y}:O\to Y$ such that $\mathrm{Id}_O = (\RV{Y},\RV{X})$, then for every finite permutation $\rho:A\to A$, $\kernel{T}^{\RV{Y}(\RV{X}_{i})_{i\in A}}=\kernel{T}^{\RV{Y}(\RV{X}_{\rho(i)})_{i\in A}}$ if and only if $(\kernel{T},S,O)$ is exchangeable with respect to $(\RV{X}_i)_{i\in A}$ and $\RV{Y}$.
\end{lemma}

\begin{proof}
$\implies$
For arbitrary $\rho:A\to A$
\begin{align}
    \kernel{T}^{\RV{Y}(\RV{X}_i)_{i\in A}} &= \kernel{T}[\kernel{F}_{\RV{Y}}\utimes(\utimes_{i\in A}\kernel{F}_{\RV{X}_i})] \\
                                     &= \kernel{T}\kernel{F}_{\rho^{\RV{X}}}[\kernel{F}_{\RV{Y}}\utimes(\utimes_{i\in A}\kernel{F}_{\RV{X}_i})] \qquad\text{by exchangeability}\\
                                     &= \kernel{T}[\kernel{F}_{\RV{Y}}\utimes(\utimes_{i\in A}\kernel{F}_{\RV{X}_{\rho(i)}})]\\
                                     &= \kernel{T}^{\RV{Y}(\RV{X}_{\rho(i)})_{i\in A}}
\end{align}
$\Leftarrow$
For arbitrary $\rho:A\to A$, by assumption
\begin{align}
\kernel{T}^{\RV{Y}(\RV{X}_i)_{i\in A}} &= \kernel{T}[\kernel{F}_{\RV{Y}}(\utimes_{i\in A}\kernel{F}_{\RV{X}_i})]\\
                                     &= \kernel{T}^{\RV{Y}(\RV{X}_{\rho(i)})_{i\in A}}\\
                                     &= \kernel{T}[\kernel{F}_{\RV{Y}}\utimes(\utimes_{i\in A}\kernel{F}_{\RV{X}_{\rho(i)}})]\\
                                     &= \kernel{T}[\kernel{F}_{\rho^{\RV{X}}}\kernel{F}_{\RV{Y}}\utimes(\utimes_{i\in A}\kernel{F}_{\rho^{\RV{X}}}\kernel{F}_{\RV{X}_{i}})]\\
                                     &= \kernel{T}\kernel{F}_{\rho^{\RV{X}}}[\kernel{F}_{\RV{Y}}\utimes(\utimes_{i\in A} \kernel{F}_{\RV{X}_i})]\\
                                     &= \kernel{F}\kernel{F}_{\rho^{\RV{X}}} \kernel{F}_{\mathrm{Id}_O} \qquad \text{by assumption }\mathrm{Id}_O=(\RV{X},\RV{Y})\\
                                     &= \kernel{F}\kernel{F}_{\rho^{\RV{X}}}
\end{align}
\end{proof}

A statement of De Finetti's representation theorem follows. The theorem is quite familiar, but we will include our own proof in Section \ref{sec:de_finetti_rep_thm} because we want the right form to extend to see-do models.

\begin{theorem}[Representation]\label{th:de_finetti_rep_thm}
Given a probability space $(\kernel{P},(O,\sigalg{O}))$ exchangeable with respect to $\RV{X}:=(\RV{X}_i)_{i\in \mathbb{N}}$ (where $\RV{X}_i:O\to X_0$ for all $i\in \mathbb{N}$) and $\RV{Y}:O\to Y$, where $O$ is standard measurable and $A$ is countably infinite, then there exists some $\RV{Z}:O\to Z$ such that $\sigma(\RV{Z})\subset \sigma(\RV{X})$ and both of the following hold:
\begin{enumerate}
    \item $\RV{X}_i\CI_{\kernel{P}}\RV{X}_{A\setminus\{i\}}|\RV{Z}$ for all $i\in A$
    \item $\prob{P}^{\RV{X}_i|\RV{Z}}=\prob{P}^{\RV{X}_j|\RV{Z}}$ for all $i,j\in A$
\end{enumerate}
\end{theorem}

\begin{proof}
While this is a well-known result (see \citet{de_finetti_foresight_1992,hewitt_symmetric_1955}, a proof is given in Section \ref{sec:de_finetti_rep_thm} for the particular version we use in subsequent results.
\end{proof}

The Markov kernel $\prob{P}^{\RV{X}|\RV{Z}}$ along with $Z$ and $X_0^{|A|}$ is a conditionally IID statistical model. Furthermore, by Lemma \ref{lem:representation_of_kernels} we have

\begin{align}
    \kernel{P}^{\RV{X}}=\kernel{P}^{\RV{Z}}\kernel{P}^{\RV{X}|\RV{Z}}
\end{align}

Thus $\kernel{P}^{\RV{X}}$ is the product of the ``prior'' $\kernel{P}^{\RV{Z}}$ and the conditionally IID kernel $\kernel{P}^{\RV{X}|\RV{Z}}$.

If we choose $\RV{Y}=\mathds{1}_{O}$, then the reverse also holds: the prodcut of a prior $\prob{Q}^{\RV{Z}}\in \Delta(\sigalg{Z})$ and a conditionally IID statistical model $(\prob{Q}^{\RV{X}|\RV{Z}},(Z,\sigalg{Z}),(X,\sigalg{X}))$ is exchangeable with respect to $(\RV{X}_i)_{i\in A}$ and $\RV{Y}$. We won't prove this here; this choice of $\RV{Y}$ means the second part of our exchangeability condition is trivial and so it precisely matches the usual definition of exchangeability. The reverse implication is a standard result given in, for example, \citet{de_finetti_foresight_1992}.


\subsubsection{See-do models with priors}

De Finetti's representation theorem assumes an exchangeable probability distribution and shows how a conditionally IID statistical model is induced. To get from a statistical model to a probability distribution, we take the product of the model with a prior, which is a probability distribution over states. To prove a similar result to see-do models, we need to define the kind of thing one gets when we take the product of a prior distribution over \emph{hypotheses} and a see-do model. We will call this a \emph{see-do forecast}.

\begin{definition}[Forecasts, see-do forecast]\label{def:do_forecast}
A \emph{see-do forecast} is a tuple $(\kernel{F},\RV{D},\RV{X},\RV{Y})$ where the decisions $\RV{D}$ take values in $(D,\sigalg{D})$, observations $\RV{X}$ takes values in $(X,\sigalg{X})$, consequences $\RV{Y}$ takes values in $(Y,\sigalg{Y})$, $\kernel{F}:D\to \Delta(\sigalg{X}\otimes \sigalg{Y})$ and $\RV{X}\CI_{\kernel{F}}\RV{D}$.
\end{definition}

The following theorem shows that see-do forecasts actually are the kind of thing one gets by taking the product of a prior and a see-do model.

\begin{theorem}
Given a see-do model $(\kernel{K},\RV{H},\RV{D},\RV{X},\RV{Y})$ and a prior $\mu\in \Delta(\sigalg{H})$, defining $\kernel{L}:=(\mu\otimes \mathrm{Id}_D)\kernel{K}$, $(\kernel{L},\RV{D},\RV{X},\RV{Y})$ is a see-do forecast. 
\end{theorem}

\begin{proof}
$(\mu\otimes \mathrm{Id}_D)\kernel{K}$ is a Markov kernel $D\to \Delta(\sigalg{X}\otimes\sigalg{Y})$. We require $\RV{X}\CI_{\kernel{L}}\RV{D}$. By Theorem \ref{th:iterated_disint}

\begin{align}
    \kernel{K} &= \kernel{K}^{\RV{X}\RV{Y}|\RV{HD}}\\
    &= \begin{tikzpicture}
        \path (0,0) node (H) {$\RV{H}$}
        + (0,-1.15) node (D) {$\RV{D}$}
        ++ (0.5,0) node[copymap] (copy0) {}
        ++ (0.7,0) node[kernel] (X) {$\kernel{K}^{\RV{X}|\RV{H}}$}
        ++ (0.5,0) node[copymap] (copy1) {}
        +  (0.8,-1) node[kernel] (Y) {$\kernel{K}^{\RV{Y}|\RV{XHD}}$}
        ++ (2,0) node (Xr) {$\RV{X}^{sd}$}
        +  (0,-1) node (Yr) {$\RV{Y}^{sd}$};
        \draw (H) -- (X) -- (Xr);
        \draw (copy0) to [out=-90,in=180] ($(Y.west) + (0,0)$) 
              (copy1) to [out=-90,in=180] ($(Y.west) + (0,0.15)$)
              (D) -- ($(Y.west) + (0,-0.15)$);
        \draw (Y) -- (Yr);
    \end{tikzpicture}
\end{align}

It follows that

\begin{align}
    \kernel{L}^{\RV{X}\RV{Y}|\RV{D}} &= \kernel{L}\\
    &= (\mu\otimes \mathrm{Id}_D)\kernel{K}\\
    &=  \begin{tikzpicture}
        \path (0,0) node[dist] (H) {$\mu$}
        + (0,-1.15) node (D) {$\RV{D}$}
        ++ (0.5,0) node[copymap] (copy0) {}
        ++ (0.7,0) node[kernel] (X) {$\kernel{K}^{\RV{X}|\RV{H}}$}
        ++ (0.5,0) node[copymap] (copy1) {}
        +  (0.8,-1) node[kernel] (Y) {$\kernel{K}^{\RV{Y}|\RV{XHD}}$}
        ++ (2,0) node (Xr) {$\RV{X}$}
        +  (0,-1) node (Yr) {$\RV{Y}$};
        \draw (H) -- (X) -- (Xr);
        \draw (copy0) to [out=-90,in=180] ($(Y.west) + (0,0)$) 
              (copy1) to [out=-90,in=180] ($(Y.west) + (0,0.15)$)
              (D) -- ($(Y.west) + (0,-0.15)$);
        \draw (Y) -- (Yr);
    \end{tikzpicture}
\end{align}

Then

\begin{align}
    \kernel{L}^{\RV{X}|\RV{D}} &= \begin{tikzpicture}
        \path (0,0) node[dist] (H) {$\mu$}
        + (0,-1.15) node (D) {$\RV{D}$}
        ++ (0.5,0) node[copymap] (copy0) {}
        ++ (0.7,0) node[kernel] (X) {$\kernel{K}^{\RV{X}|\RV{H}}$}
        ++ (0.5,0) node[copymap] (copy1) {}
        +  (0.8,-1) node[kernel] (Y) {$\kernel{K}^{\RV{Y}|\RV{XHD}}$}
        ++ (2,0) node (Xr) {$\RV{X}$}
        +  (0,-1) node (Yr) {};
        \draw (H) -- (X) -- (Xr);
        \draw (copy0) to [out=-90,in=180] ($(Y.west) + (0,0)$) 
              (copy1) to [out=-90,in=180] ($(Y.west) + (0,0.15)$)
              (D) -- ($(Y.west) + (0,-0.15)$);
        \draw[-{Rays[n=8]}] (Y) -- (Yr);
    \end{tikzpicture}\\
    &= \begin{tikzpicture}
        \path (0,0) node[dist] (H) {$\mu$}
        + (0,-1.) node (D) {$\RV{D}$}
        ++ (1.2,0) node[kernel] (X) {$\kernel{K}^{\RV{X}|\RV{H}}$}
        ++ (2.5,0) node (Xr) {$\RV{X}$}
        +  (0,-1) node (Yr) {};
        \draw (H) -- (X) -- (Xr);
        \draw[-{Rays[n=8]}] (D) -- (Yr);
    \end{tikzpicture}
\end{align}

And so $\RV{X}\CI_{\kernel{L}}\RV{D}$.
\end{proof}

\subsubsection{Functional exchangeability}

With the definition of see-do forecasts in hand, we require a notion analogous to exchangeability for probability distributions. Motivated by the discussion at the start of Section \ref{sec:repeatable_experiments}, we will look for two exchangeability assumptions, the first for the observations $(\RV{X}_i)_{i\in A}$ and the second for the consequences $(\RV{Y}_i)_{i\in B}$. As suggested by that discussion, we will consider an assumption analogous to ordinary exchangeability for the observations. We will consider models that allow for all elements of the consequence sequence $(\RV{Y}_i)_{i\in B}$ to be controlled individually, which demands the introduction of \emph{functional exchangeability}:

\begin{definition}[Exchangeable observations]
A see-do forecast $(\kernel{T},\RV{D},\RV{X},\RV{Y})$ where $\RV{X}=(\RV{X}_i)_{i\in A}$ for which $(\kernel{T}^{\RV{X}}_d,(X\times Y,\sigalg{X}\otimes\sigalg{Y}))$ is exchangeable with respect to $\RV{X}$ holding $\RV{Y}$ fixed (Definition \ref{def:exchangeability}) is said to have exchangeable observations.
\end{definition}

\begin{definition}[Functional exchangeability]
Given a see-do forecast $(\kernel{T},\RV{D},\RV{X},\RV{Y})$ where $\RV{D}=(\RV{D}_i)_{i\in B}$ and $\RV{Y}=(\RV{Y}_i)_{i\in B}$, define the interleaved consequence and decision variable $\RV{DY}:=(\RV{D}_i,\RV{Y}_i)_{i\in B}$. For any permutation $\rho:B\to B$ we have the decision and consequence swap function $\rho^{\RV{DY}}:D\times X\times Y\to D\times X\times Y$ (which holds observations $\RV{X}$ fixed) and the decision swap function $\rho^{\RV{D}}:D\to D$ (which holds $\mathds{1}_D$ fixed; see Definition \ref{def:swap}). Then $\kernel{T}$ is functionally exchangeable with respect to $\RV{D}$ and $\RV{X}$ if $\kernel{T}=\kernel{F}_{\rho^{\RV{D}}}\kernel{T}\kernel{F}_{rho^{\RV{D}\RV{Y}}}$.
\end{definition}

Functional exchangeability captures the intuition that if we swap the order of the outputs (say, $\RV{Y}_1,\RV{Y}_2$ is swapped to $\RV{Y}_2, \RV{Y}_1$), we need to make analagous exchange of choices ($\RV{D}_1$, $\RV{D}_2$ becomes $\RV{D}_2$, $\RV{D}_1$) in order to maintain the correspondence of choices and outputs.

The product of a functionally exchangeable see-do forecast and an exchangeable distribution over decisions is an exchangeable distribution. This allows us to apply the representation theorem (Theorem \ref{th:de_finetti_rep_thm}) to the product of a see-do forecast and an exchangeable distribution over decisions. To show the representation theorem, we will choose an exchangeable distribution over decisions with full support so that the independence properties for the product distribution hold for the see-do forecast also. 

\begin{lemma}[Functionally exchangeable see-do models with exchangeable choices induce exchangeable probability distributions]\label{lem:f-ex2ex}
Given a see-do forecast $(\kernel{T},\RV{D},\RV{X},\RV{Y})$ functionally exchangeable with respect to $\RV{Y}=(\RV{Y}_i)_{i\in A}$ and $\RV{D}=(\RV{D}_i)_{i\in A}$ holding $\RV{X}$ fixed, some $\prob{P}^{\RV{D}}\in \Delta(\sigalg{D})$ exchangeable with respect to $\RV{D}$, then $\prob{P}^{\RV{D}}\kernel{T}\in \Delta(\sigalg{D}\otimes\sigalg{X}\otimes\sigalg{Y})$ is exchangeable with respect to $\RV{DY}:=(\RV{D}_i,\RV{Y}_i)_{i\in A}$ holding $\RV{X}$ fixed.
\end{lemma}

\begin{proof}

For arbitrary $\rho:A\to A$, associated swap kernels $\kernel{F}_{\rho^{\RV{D}}}$ and $\kernel{F}_{\rho^{\RV{D}\RV{X}}}$:

\begin{align}
    \prob{P}^{\RV{D}}\kernel{T} \kernel{F}_{\rho^{\RV{D}\RV{X}}} &= (\prob{P} \kernel{F}_{\rho^{\RV{D}}}) \kernel{T} \kernel{F}_{\rho^{\RV{D}\RV{X}}}\\
                                  &= \prob{P}(\kernel{F}_{\rho^{\RV{D}}}\kernel{T}\kernel{F}_{\rho^{\RV{DX}}})\\
                                  &= \prob{P}\kernel{T}
\end{align}

This is sufficient for exchangeability of $(\prob{P}^{\RV{D}}\kernel{T},\RV{C},\RV{G},\RV{O})$ with respect to $\RV{G}$.
\end{proof}

Using Lemma \ref{lem:rep_seedo_obs} and Lemma \ref{lem:f-ex2ex} we show in Theorem \ref{th:rep_dex_sdf} that a see-do forecast exchangeable in observations and functionally exchangeable in decisions and consequences is the prodcut of a prior and a statistical model with independent and identically distributed observations and independent and functionally identical consequences.

In order to prove this theorem, we also require the idea of an \emph{extension} of a see-do forecast. This is because the theorem holds for finite sequences of decisions and consequences, but the proof requires an infinite extension of this sequence (for finite sequences, approximate versions of De Finetti's representation theorem hold, see \citet{diaconis_finite_1980,kerns_definettis_2006}). One kernel is an extension of another if the original kernel can be obtained by marginalising the extension.

\begin{definition}[Extension]\label{def:extension}
Given a see-do forecast $(\kernel{T}, \RV{D}, \RV{X}, \RV{Y})$ where $\RV{X}$ takes values in $(X,\sigalg{X})$ and $\RV{Y}$ takes values in $(Y,\sigalg{Y})$, let $O=X\times Y$. $\kernel{T}':D'\to \Delta(\sigalg{X}'\otimes\sigalg{Y}')$ is an \emph{extension} of $\kernel{T}$ if there exists some $O^*$, $D^*$ such that $\kernel{T}\otimes\mathrm{Id}_{D^*} = \kernel{T}'(\mathrm{Id}_O\otimes\stopper{0.2}_{O^*})$.
\end{definition}

\begin{theorem}[Representation of doubly exchangeable see-do forecasts]\label{th:rep_dex_sdf}
Given a see-do forecast $(\kernel{T},\RV{D},\RV{X},\RV{Y})$ with denumerable $D:=D_0^B$ and standard measurable $X:=X_0^A$, $Y:=Y_0^B$, and where $\RV{X}=(\RV{X}_i)_{i\in A}$, $\RV{Y}=(\RV{Y}_i)_{i\in B}$ adn $\RV{D}=(\RV{D}_i)_{i\in B}$ for finite $B\subset \mathbb{N}$, $A\subseteq \mathbb{N}$ not necessarily finite, the following statements are equivalent:

\begin{enumerate}
    \item There exists $(\kernel{T}',\RV{D}',\RV{X}',\RV{Y}')$ where $\kernel{T}'$ is an extension of $\kernel{T}$, $\RV{X}'=(\RV{X}'_i)_{i\in \mathbb{N}}$, $\RV{Y}'=(\RV{Y}'_i)_{i\in \mathbb{N}}$, $\RV{D}'=(\RV{D}'_i)_{i\in\mathbb{N}}$ and $(\kernel{T}',\RV{D}',\RV{X}',\RV{Y}')$ has exchangeable observations (holding $\RV{Y}'$ and $\RV{D}'$ fixed) and exchangeable consequences (holding $\RV{X}'$ fixed).
    \item There exists a a set $H$, a prior $\kernel{T}^{\RV{H}}\in\Delta(\sigalg{H})$ and Markov kernels $\kernel{T}^{\RV{X}_0|\RV{H}}$ and $\kernel{T}^{\RV{Y}_0|\RV{H}\RV{D}_0}$ such that for all $d\in D$, $\{J_i\in \sigalg{X}_0|i\in A\}$, $\{K_i\in \sigalg{Y}_0|i\in B\}$:
    \begin{align}
        \kernel{T}^{\RV{X}\RV{Y}|\RV{D}}_{d}((\bigtimes_{i\in A}J_i)\times (\bigtimes_{j\in B} K_j)) = \int_{H} \prod_{i\in A} \kernel{T}^{\RV{X}_0|\RV{H}}_h(J_i)\prod_{i\in B}\kernel{T}_{h,d_i}^{\RV{Y}_0|\RV{H}\RV{D}_0}(K_i)d\kernel{T}^{\RV{H}}(h)
    \end{align}
\end{enumerate}
\end{theorem}

\begin{proof}
A proof is given in Section \ref{sec:rep_dex_sdf}.
\end{proof}

\subsubsection{Proof of Theorem \ref{th:de_finetti_rep_thm}}\label{sec:de_finetti_rep_thm}

We start with a definition of partial relative frequencies and exchangeable $\sigma$-algebras.

\begin{definition}[Partial relative frequencies]\label{def:partial_freq}
Given a standard measurable space $(E,\sigalg{E})$ along with random variables $\RV{X}_i:E\to X_1$ for each $i\in \mathbb{N}$, for $A\in \sigalg{X}_1$ and $m\leq n\in \mathbb{N}$ define the size $n$, $m$-tuple \emph{partial frequency of} $A$ with respect to $\RV{X}:=\utimes_{i\in\mathbb{N}}\RV{X}_i$ to be $\RV{Z}^{m,n}_A:=\frac{(n-m)!}{n!}\sum_{I\subset [n]} \prod_{i\in I} \mathds{1}_A\circ \RV{X}_i$ where $I$ ranges over all $m$-sized ordered subsets of $n$.

Define the $m$-tuple \emph{relative frequency of } $A$ with respect to $\RV{X}$ to be $\RV{Z}^{m,\infty}_A:= \lim_{n\to \infty}\frac{(n-m)!}{n!}\sum_{I\in [n]} \prod_{i\in I} \mathds{1}_A\circ \RV{X}_i$.

Given a countable set $\sigalg{G}$ generating $\sigalg{X}_1$ (i.e. $\sigma(\sigalg{G})=\sigalg{X}_1)$, define the $m$-tuple \emph{relative frequency of} $\RV{X}$ to be $\RV{Z}^{m,\infty}:=\utimes_{A\in G} \RV{Z}^{m,\infty}_A$, if such a limit exists for all $A$.

For the special case of $m=1$, let $\RV{Z}:=\RV{Z}^{1,\infty}$
\end{definition}

\begin{definition}[Exchangeable $\sigma$-algebra]\label{def:exchange_sig_alb}
Given a set of random variables $\RV{X}_i:E\to X_1$ for each $i\in \mathbb{N}$, with $X=X_1^{\mathbb{N}}$, a \emph{n-place symmetric function} $f:X\to W$ is a function for which $f = f\circ \rho$ for any permuation $\rho:\mathbb{N}\to\mathbb{N}$ such that $i>n\implies\rho(i)=i$. 

The \emph{n-place exchangeable $\sigma$-algebra} (with respect to the random variable $\RV{X}_i$), $\sigalg{H}^n$, is the $\sigma$-algebra generated by all n-place symmetric functions, and $\sigma{H}=\cap_{n\in \mathbb{N}} \sigalg{H}^n$. 

For standard measurable $(E,\sigalg{E})$ and $n\in \mathbb{N}$, a size $n$ swap function $\rho^{\RV{X}n}:E\to E$ is a swap function associated with a permutation $\rho^{\prime n}$ with the property $i>n\implies \rho^{\prime n}(i)=i$. An $n$-symmetric set $S\subset E$ has the property $\rho^{\RV{X}n}(S)=S$ for all size $n$ swap functions $\rho^{\RV{X}n}$. Define the symmetric sets $\sigalg{S}^n$ $n$-symmetric sets, and $\sigalg{S}=\cap_{n\in\mathbb{N}} \sigalg{S}^n$. Given random variables $\RV{X}_i:E\to X_1$ for each $i\in\mathbb{N}$, the size $n$ \emph{exchangeable $\sigma$-algebra} with respect to $\RV{X}:=\utimes_{i\in\mathbb{N}}\RV{X}_i$, denoted $\sigalg{H}^n\subset\sigma(\RV{X})$, is the $\sigma$-algebra generated by $\{\RV{X}^{-1}(A)|A\in\sigalg{X}\}\cap S^n$.

\end{definition}

\begin{lemma}
The size $n$ exchangeable sigma algebra $\sigalg{H}^n$ on $(E,\sigalg{E})$ with respect to $\RV{X}:=\utimes_{i\in\mathbb{N}}\RV{X}_i$ has the property $\rho^{\RV{X}n}(A)=A$ for all $A\in \sigalg{H}^n$, and all size $n$ swap functions $\rho^{\RV{X}n}$.
\end{lemma}

\begin{proof}
Let $W_f$ be the codomain of a function $f$, and $\sigalg{W}_f$ its $\sigma$-algebra. $\sigalg{H}^n$ is generated by $\sigalg{G}^n=\{f^{-1}(A)|A\in \sigalg{W}_f,f\text{ n-place symmetric}\}$. By the definition of n-place symmetric functions, any set of the form $f^{-1}(A)=(f\circ\rho^{\RV{X}})^{-1}(A) = (\rho^{\RV{X}}){-1}(f^{-1}(A))$. Because every $n$-place permutation $\rho$ has an inverse $\rho^{-1}$ that is also an $n$-place permutation, all sets in $B\in\sigalg{G}$ have the property $\rho^{\RV{X}}(B)=B$ for all $n$-place permuations $\rho$.

Define $\sigalg{S}^n$ to be all subsets of $E$ such that for $B\in \sigalg{S}^n$, $n$-place permuations $\rho$, $\rho^{\RV{X}}(B)=B$. $\sigalg{S}^n$ is a $\sigma$-algebra, and it contains $\sigalg{G}^n$, so it also contains $\sigalg{H}^n$.

Take $A\in\sigalg{S}^n$. By assumption, for any $\omega\in A$, $\rho^{\RV{X}}(\omega)\in A$ for all size $n$ swap functions $\rho^{\RV{X}}$. Consider $\omega\not\in A$, and suppose there is some swap function $\rho^{\RV{X}}$ such that $\rho^{\RV{X}}(\omega)\in A$. By definition, the permutation $\rho$ has an inverse $\rho^{-1}$ which is also a size $n$ permutation. By construction, $(\rho^{-1})^{\RV{X}}$ is also the inverse of $\rho^{\RV{X}}$. Thus $(\rho^{-1})^{\RV{X}}(\omega)\not\in A$ and so $\omega\not\in A$, a contradiction. Thus $E\setminus A\in \sigalg{G}^n$.

For any invertible function $f:E\to E$, $f(E)=E$. Thus $E\in \sigalg{G}^n$.

Finally, for a countable collection $A_1,A_2,...$ and any size $n$ swap function $\rho^{\RV{X}}$, $\rho^{\RV{X}}(\cup_{i=1}^{\infty} A_i) = \cup_{i=1}^\infty \rho^{\RV{X}}(A_i) = \cup_{i=1}^\infty A_i$. Thus $\sigalg{S}^n$ is a $\sigma$-algebra, and by the monotone class theorem it contains $\RV{H}^n$.
\end{proof}

The proof of the main theorem follows.

\begin{reptheorem}{th:de_finetti_rep_thm}[Representation]
Given a probability space $(\kernel{P},(O,\sigalg{O}))$ exchangeable with respect to $\RV{X}:=(\RV{X}_i)_{i\in \mathbb{N}}$ (where $\RV{X}_i:O\to X_0$ for all $i\in \mathbb{N}$) and $\RV{Y}:O\to Y$, where $O$ is standard measurable and $A$ is countably infinite, then there exists some $\RV{Z}:O\to Z$ such that $\sigma(\RV{Z})\subset \sigma(\RV{X})$ and both of the following hold:
\begin{enumerate}
    \item $\RV{X}_i\CI_{\kernel{P}}\RV{X}_{A\setminus\{i\}}|\RV{Z}$ for all $i\in A$
    \item $\prob{P}^{\RV{X}_i|\RV{Z}}=\prob{P}^{\RV{X}_j|\RV{Z}}$ for all $i,j\in A$
\end{enumerate}
\end{reptheorem}

\begin{proof}
Because all standard measurable spaces are isomorphic to $([0,1],\mathcal{B}([0,1]))$, without loss of generality we can assume $X_0=[0,1]$, $\sigalg{X}=\mathcal{B}([0,1])$ and $\RV{X}=[0,1]^{\mathbb{N}}$.

Let $\mathbb{Q}$ be the rationals between $[0,1]$ and define $\RV{Z}_q^n:D\times X\times Y \to [0,1]$ by $\omega \mapsto \frac{1}{n}\sum_{i}^n \mathds{1}_{[0,q)}(\RV{X}_i(\omega))$ Let $\sigalg{Z}^n_q=\sigma(\RV{Z}_q^n)$, i.e. $\RV{Z}^n$ is a 1-tuple partial relative frequency as in Definition \ref{def:partial_freq}.

$\RV{Z}^n\circ\rho^{\RV{X}n}=\RV{Z}^n$ for any size $n$ swap function $\rho^{\RV{X}n}$, so $\RV{Z}^n$ is $\sigalg{H}^n$-measurable.

Let $\rho_{ij}:\mathbb{N}\to\mathbb{N}$ swaps indices $i$ and $j$ for some $i,j\in[n]$ and otherwise acts as the identity. $\rho_{ij}:D\times X\times Y\to \Delta(\sigalg{D}\times \sigalg{X}\times \sigalg{Y})$ is the swap kernel associated with $\rho'_{ij}$ and $\{\RV{X}_i|i\in \mathbb{N}\}$, and $\rho^{\RV{X}}_{ij}$ the function associated with $\rho_{ij}$. For any $m$, $n$, $A\in \sigalg{H}^n$, $d\in D$: 

\begin{align}
    \int_{A} \RV{Z}_q^{n} (\omega) d\prob{P}(\omega) &= \int_{A} \frac{1}{n}\sum_{i}^{n} (\mathds{1}_{[0,q)}\circ \RV{X}_i)(\omega)d\prob{P}(\omega)\\
    &= \frac{1}{n}\sum_{i}^{n} \int_{(\rho^{\RV{X}}_{ij})^{-1}\rho^{\RV{X}}_{ij}(A)} (\mathds{1}_{[0,q)}\circ \RV{X}_i)(\omega)d\prob{P}\rho_{ij}(\omega)\label{eq:permutation_invertible}\\
    &= \frac{1}{n}\sum_{i}^{n} \int_{\rho^{\RV{X}}_{ij}(A)} (\mathds{1}_{[0,q)}\circ \RV{X}_{i}\circ \rho_{ij}) (\omega)d\prob{P}(\omega)\label{eq:using_push2}\\
    &= \frac{1}{n}\sum_{i}^{n} \int_{A} (\mathds{1}_{[0,q)}\circ \RV{X}_{j})(\omega)d\prob{P}(\omega)\label{eq:closure_under_permutation}\\
    &= \int_{A} (\mathds{1}_{[0,q)} \circ \RV{X}_j)(\omega)d\prob{P}(\omega) \label{eq:cond_expectation_first}
\end{align}

Where line \ref{eq:permutation_invertible} follows from exchangeability of $\kernel{T}$ and invertibility of $\rho_{ij}$. Line \ref{eq:using_push2} follows from the fact that $\prob{P}\rho_{ij}$ is the pushforward measure of $\prob{P}$ with respect to $\rho^{\RV{X}}_{ij}$ and \ref{eq:closure_under_permutation} uses the fact that $\rho(A) = A$ for all $A\in \RV{Z}^n_q$ and all permutations $\rho$.

From Equation \ref{eq:cond_expectation_first}, we have for all $n$, $A\in \sigalg{H}^{n+1}$

\begin{align}
    \int_{A} \RV{Z}_q^{n+1} (\omega) d\prob{P}(\omega) &= \int_{A} \RV{Z}_q^{n} (\omega) d\prob{P}(\omega)
\end{align}

Because $\RV{Z}_q^{n+1}$ is $\sigalg{H}^{n+1}$ measurable, $\RV{Z}_q^{n+1} = \mathbb{E}[\RV{Z}_q^{n}|\sigalg{H}^{n+1}]$.

Thus the sequence $[\RV{Z}^1_q,\RV{Z}^2_q,...]$ is a backwards martingale with respect to the reversed filtration $\sigalg{H}^1\supset\sigalg{H}^2\supset...\supset \sigalg{H}^3$.

Furthermore, for all $n\in \mathbb{N}$, $\sup_\omega|\RV{Z}^n(\omega)|\leq 1$ so the sequence is also uniformly integrable. Thus it goes almost surely to the limit $\RV{Z}_q$, and for all $A\in \sigalg{H}$

\begin{align}
    \lim_{n\to\infty} \int_A \RV{Z}^n_q(\omega) d\prob{P}(\omega) &= \int_A \RV{Z}_q(\omega) d\prob{P}(\omega)
\end{align}

Finally, because for all $n\in \mathbb{N}$, all $j\in[n]$ and all $A\in \sigalg{H}^n$ we also have

\begin{align}
    \int_A \mathds{1}_{[0,q)}(\RV{X}_j(\omega))d\prob{P}(\omega) &= \int_A \RV{Z}_q^n(\omega) d\prob{P}(\omega)
\end{align}

it follows that for all $A\in \sigalg{H}$, $j\in \mathbb{N}$

\begin{align}
    \int_A \RV{Z}_q(\omega) d\prob{P}(\omega) = \int_A \mathds{1}_{[0,q)}(\RV{X}_j(\omega))d\prob{P}(\omega)\label{eq:cond_expectation}
\end{align}

\citep[Thm ~4.7]{cinlar_probability_2011}. Thus $\RV{Z}_q = \mathbb{E}[\mathds{1}_{[0,q)}\circ\RV{X}_j|\sigalg{H}]$ for all $j\in \mathbb{N}$. This implies $\RV{Z}_q$ is a version of $\mathbb{E}[\mathds{1}_{[0,q)}\circ \RV{X}_j|\sigalg{H}]$.

Define $\RV{Z} = \utimes_q\in \mathbb{Q} \RV{Z}_q$. As $\sigma(\RV{Z})\subset\sigalg{H}$, Equation \ref{eq:cond_expectation} establishes in addition that $\RV{Z}_q$ is a version of $\mathbb{E}[\mathds{1}_{[0,q)}\circ \RV{X}_j|\sigma(Z)]$. 

By the definition of conditional expectation, for any version of $\prob{P}_{\RV{Z}(\omega)}^{\RV{X}_j|\RV{Z}\RV{D}}([0,q))$ we have

\begin{align}
    \prob{P}_{\RV{Z}(\omega)}^{\RV{X}_j|\RV{Z}}([0,q)) &= \mathbb{E}[\mathds{1}_{[0,q)}\circ \RV{X}_j|\sigma(\RV{Z})](\omega)\\
\end{align}

$\prob{P}$-almost surely.

Furthermore, the measure $\prob{P}_{h}^{\RV{X}_j|\RV{Z}}$ is uniquely defined by its value on $[0,q)$ for all $q\in \mathbb{Q}$. Thus for all $i,j\in \mathbb{N}$ we have

\begin{align}
    \prob{P}^{\RV{X}_j|\RV{H}} = \prob{P}^{\RV{X}_i|\RV{H}}
\end{align}

Completing the proof of property 1: the $\RV{X}_i$'s are mutually independent.

Next, we will show $\RV{X}_i\CI_\kernel{T} \RV{X}_{\mathbb{N}\setminus \{i\}} | \RV{H}$.

Let $\RV{Z}^{m,n}_q$ be a partial frequency as in Definition \ref{def:partial_freq} where $q$ stands for the set $[0,q)$. $\RV{Z}^{m,n}_q\circ \rho^{\RV{X}n}=\RV{Z}^{m,n}_q$ for all size $n$ swap functions so $\RV{Z}^{m,n}_q$  is $\sigalg{H}^n$ measurable.

Let $J\subset[n]$ be some set of $m$ elements from $n$ and $\rho'_{IJ}$ be a permutation that sends the elements of $I\subset[n]$ to $J$.

\begin{align}
    \int_{A} \RV{Z}_q^{m,n} (\omega) d\prob{P}(\omega) &= \int_{A} \frac{(n-m)!}{n!}\sum_{I\subset[n]}\prod_{i\in I} (\mathds{1}_{[0,q)}\circ \RV{X}_i)(\omega)d\prob{P}(\omega)\\
    &= \frac{(n-m)!}{n!}\sum_{I\subset[n]} \int_{(\rho^{\RV{X}}_{IJ})^{-1}\rho^{\RV{X}}_{IJ}(A)} \prod_{i\in I}(\mathds{1}_{[0,q)}\circ \RV{X}_i)(\omega)d\prob{P}\rho_{IJ}(\omega)\\
    &= \frac{(n-m)!}{n!}\sum_{I\subset[n]} \int_{\rho^{\RV{X}}_{IJ}(A)} \prod_{i\in I}(\mathds{1}_{[0,q)}\circ \RV{X}_{i}\circ \rho_{IJ}) (\omega)d\prob{P}(\omega)\label{eq:using_pushforward}\\
    &= \frac{(n-m)!}{n!}\sum_{I\subset[n]} \int_{A} \prod_{j\in J}(\mathds{1}_{[0,q)}\circ \RV{X}_{j})(\omega)d\prob{P}(\omega)\label{eq:closure_under_permutation2}\\
    &= \int_{A} \prod_{j\in J}(\mathds{1}_{[0,q)} \circ \RV{X}_j)(\omega)d\prob{P}(\omega) \label{eq:cond_expectation_first2}
\end{align}

From Equation \ref{eq:cond_expectation_first2}, we have for all $n$, $m<n$, $A\in \sigalg{H}^{n+1}$

\begin{align}
    \int_{A} \RV{Z}_q^{m,n+1} (\omega) d\prob{P}(\omega) &= \int_{A} \RV{Z}_q^{m,n} (\omega) d\prob{P}(\omega)
\end{align}

Because $\RV{Z}_q^{m,n+1}$ is $\sigalg{H}^{n+1}$ measurable, $\RV{Z}_q^{m,n+1} = \mathbb{E}[\RV{Z}_q^{m,n}|\sigalg{H}^{n+1}]$.

Thus the sequence $[\RV{Z}^{m,1}_q,\RV{Z}^{m,2}_q,...]$ is a backwards martingale with respect to the reversed filtration $\sigalg{H}^1\supset\sigalg{H}^2\supset...\supset \sigalg{H}^3$.

Furthermore, for all $n\in \mathbb{N}$, $\sup_\omega|\RV{Z}^{m,n}_q(\omega)|\leq 1$ so the sequence is also uniformly integrable. Thus it goes almost surely to a limit $\RV{Z}^{m,\infty}_q$, and for all $A\in \sigalg{H}$

\begin{align}
    \lim_{n\to\infty} \int_A \RV{Z}^{m,n}_q(\omega) d\prob{P}(\omega) &= \int_A \RV{Z}^{m,\infty}_q(\omega) d\prob{P}(\omega)\\
    \implies \RV{Z}^{m,n}_q &= \mathbb{E}[\prod_{j\in J} \mathds{1}_{[0,q)}\circ \RV{X}_j|\sigalg{H}]
\end{align}

Let $[n]^m_\mathrm{rep}$ be the set of all $m$ length sequences of elements of $[n]$ with repeats. Note that $\lim_{n\to\infty}\frac{(n-m)!}{n!}|[n]^m_\mathrm{rep}| = \lim_{n\to\infty}\frac{n^m(n-m)!}{n!}-1 = 0$

\begin{align}
    \mathbb{E}[\prod_{j\in J} \mathds{1}_{[0,q)}\circ \RV{X}_j|\sigalg{H}](\omega) &= \lim_{n\to\infty}\frac{(n-m)!}{n!}\sum_{I\subset[n]}\prod_{i\in I} (\mathds{1}_{[0,q)}\circ \RV{X}_i)\\
                                                                      &= \lim_{n\to\infty}\frac{1}{n^m}\sum_{I\subset[n]}\prod_{i\in I}(\mathds{1}_{[0,q)}\circ \RV{X}_i)\\
                                                                      &= \lim_{n\to\infty}\frac{1}{n^m}\left[\sum_{i_1\in [n]}...\sum_{i_m\in [n]} (1_{[0,q)}\circ \RV{X}_{i_k})-\sum_{J\subset[n]^m_\mathrm{rep}}\prod_{i\in I} (\mathds{1}_{[0,q)}\circ \RV{X}_i)\right]\\ 
                                                                      &= \prod_{k\in [m]} \lim_{n\to\infty} \frac{1}{n}\sum_{i_k\in[n]}(1_{[0,q)}\circ\RV{X}_{ik})\\
                                                                      &= \prod_{k\in [m]} \mathbb{E}[\mathds{1}_{[0,q)}\circ \RV{X}_j|\sigalg{H}]\\
                                                                      &= \prod_{k\in J} \mathbb{E}[\mathds{1}_{[0,q)}\circ \RV{X}_j|\sigalg{H}]\\
                                                                      &= \prod_{k\in J} \RV{Z}_q\label{eq:h_measurable}
\end{align}

Because $\mathbb{E}[\prod_{j\in J} \mathds{1}_{[0,q)}\circ \RV{X}_j|\sigalg{H}]$ is $\RV{Z}$-measurable we also have

\begin{align}
    \mathbb{E}[\prod_{j\in J} \mathds{1}_{[0,q)}\circ \RV{X}_j|\sigma(\RV{Z})] &= \prod_{k\in J} \RV{Z}_q
\end{align}

By the definition of conditional expectation, for all $J\subset \mathbb{N}$

\begin{align}
    \prob{P}_{\RV{H}(\omega)}^{\RV{X}_J|\RV{H}}(\times_{j\in J}[0,q)) &= \mathbb{E}[\prod_{j\in J} \mathds{1}_{[0,q)}\circ \RV{X}_j|\sigalg{H}](\omega)\\
                                                                      &= \prod_{k\in J} \prob{P}_{\RV{H}(\omega)}^{\RV{X}_j|\RV{H}}([0,q))
\end{align}

And thus $\RV{X}_i\CI\RV{X}_{\mathbb{N}\setminus\{i\}}|\RV{Z}$ for all $i\in\mathbb{N}$.
\end{proof}


\subsubsection{Proof of Theorem \ref{th:rep_dex_sdf}}\label{sec:rep_dex_sdf}


% \begin{lemma}[Independent and identically distributed random variables]\label{lem:iid_rvs}
% Suppose we have a forecast $(\prob{P},\RV{X})$ where $\RV{X}= (\RV{X}_i)_{i\in\mathbb{N}}$, $X=X_0^\mathbb{N}$ is standard measurable, and $\prob{P}$ is exchangeable with respect to $\RV{X}$. Furthermore, suppose we have $\RV{V}=(f\circ \RV{X}_i)_{i\in\mathbb{N}}$ for some measurable $f:X_0\to V_0$ such that $\RV{V}$ is independent and identically distributed - that is, $\prob{P}^{\RV{V}} = \otimes_{i\in\mathbb{N}} \prob{P}^{\RV{V}_0}$. Then, letting $\RV{Z}$ be the hypothesis from Theorem \ref{th:de_finetti_rep_thm}, $\RV{V}\CI\RV{Z}$.
% \end{lemma}

% \begin{proof}

% We have by Theorem \ref{th:de_finetti_rep_thm} that for any measurable $g:Z_1\to \mathbb{R}$,
% \begin{align}
%     \mathbb{E}[g(\RV{V}_1)|\sigma(\RV{Z})] = \lim_{n\to\infty} \frac{1}{n}\sum_i^n g(\RV{Z}_i)
% \end{align}

% However, by the strong law of large numbers (\citet{cinlar_probability_2011}, pg 119), because the $\RV{V}_i$ are independent and identically distributed

% \begin{align}
%     \mathbb{E}[g(\RV{V}_1)] &\overset{a.s.}{=} \lim_{n\to\infty}\frac{1}{n}\sum_i^n g(\RV{Z}_i)\\
%                             &= \mathbb{E}[g(\RV{V}_1)|\sigma(\RV{Z})]
% \end{align}

% Thus $\RV{V}\CI\RV{Z}$.

% \end{proof}

For the main theorem, we require a slight extension of Theorem \ref{th:de_finetti_rep_thm} - we need to show that if a see-do forecast is exchangeable with respect to $\RV{X}$ holding $\RV{Y}$ fixed then we have $\RV{X}\CI\RV{Y}|(\RV{Z,D})$ where $\RV{Z}$ is the hypothesis variable conditioning on which makes $\RV{X}$ IID and $\RV{D}$ is the decision variable.

\begin{lemma}[Representation of infinitely exchangeably extendable see-do forecasts]\label{lem:rep_seedo_obs}
Given a see-do forecast $(\kernel{T},\RV{D},\RV{X},\RV{Y})$ where $\RV{X}=(\RV{X}_i)_{i\in\mathbb{N}}$ and $X\times Y$ is standard measurable, if $(\kernel{T},\RV{D},\RV{X},\RV{Y})$ is infinitely exchangeable with respect to $\{\RV{X}_i|i\in \mathbb{N}\}$ then there exists a function $f:X\to Z$ such that, defining $\RV{Z}:f\circ \RV{X}$:
\begin{itemize}
    \item $\RV{X}_i\CI\RV{X}_{\mathbb{N}\setminus\{i\}}|\RV{Z}$ for all $i\in A$
    \item $\kernel{T}^{\RV{X}_i|\RV{Z}}=\kernel{T}^{\RV{X}_j|\RV{Z}}$ for all $i,j\in A$
    \item $\RV{Y}\CI\RV{X}|\RV{Z}\utimes \RV{D}$
\end{itemize}
\end{lemma}

\begin{proof}
Without loss of generality, assume $X_1=Y=[0,1]$.

$\kernel{T}$ is a see-do forecast, so $\RV{X}\CI_{\kernel{T}}\RV{D}$. Thus there exists a marginal $\kernel{T}^{\RV{X}}$ independent of $\RV{D}$.

From Theorem \ref{th:de_finetti_rep_thm}, $\kernel{T}^{\RV{X}_i|\RV{Z}}=\kernel{T}^{\RV{X}_j|\RV{Z}}$ for all $i,j\in \mathbb{N}$ and $\RV{X}_i\CI\RV{X}_{\mathbb{N}\setminus\{i\}} |\RV{Z}$.

We will show $\RV{Y}\CI\RV{X}|(\RV{Z},\RV{D})$. 

By the definition of $\RV{X}$ and $\RV{Y}$, $\sigma(\RV{X})\cap\sigma(\RV{Y})=\{\emptyset,E\}$. Thus we can define swap functions on $\RV{X}$ that hold $\RV{Y}$ fixed.

For any swap function $\rho^{\RV{X}}_{\RV{Y}}$ there is, by definition, a permutation of indices $\rho$ such that $\RV{X}\circ\rho^{\RV{X}}_{\RV{Y}}(\omega) = [\RV{X}_{\rho(1)}(\omega),\RV{X}_{\rho(2)}(\omega),...]$. Define $\rho':[0,1]^{\mathbb{N}}\to[0,1]^{\mathbb{N}}$ to be the bijective map that performs the permutation in the codomain of $\RV{X}$, i.e. $\RV{X}\circ\rho^{\RV{X}}_{\RV{Y}} = \rho'\circ\RV{X}$.

Consider some $B\in \sigalg{B}([0,1])^\mathbb{N}$ and its preimage $\RV{X}^{-1}(B) = \{\omega|\RV{X}(\omega)\in B\}$, and some finite swap function $\rho^{\RV{X}}_{\RV{Y}}$. Then there exists $\rho'(B)\in [0,1]^{\mathbb{N}}$ such that $(\RV{X}\circ\rho^{\RV{X}}_{\RV{Y}})^{-1}(\rho'(B)) = \{\omega|\RV{X}(\rho^{\RV{X}}_{\RV{Y}}(\omega))\in \rho'(B)\} = \{\omega|\RV{X}\in B\}$. Thus $\sigma(\RV{X})=\sigma(\RV{X}\circ\rho^{\RV{X}}_{\RV{Y}})$ for any finite swap function $\rho^{\RV{X}}_{\RV{Y}}$.

Because $E=X\times Y$ and $\RV{X}$ forgets the $Y$-component of any $(x,y)\in E$, for any $A,B\in\sigalg{B}([0,1])$ there is some $C\subset X$ such that $\mathds{1}_{A}\circ\RV{Y}(\RV{X}^{-1}(B)) = \mathds{1}_{A}\circ\RV{Y} (C\times Y) = [0,1]$. Thus for any finite swap function $\rho^{\RV{X}}_{\RV{Y}}$, $\mathds{1}_{A}\circ\RV{Y}\circ \rho^{\RV{X}}_{\RV{Y}} = \mathds{1}_{A}\circ\RV{Y}$.

For $A\in \sigma(\RV{X})$:

\begin{align}
    \int_{A} \mathbb{E}[\mathds{1}_A\circ\RV{Y}|\sigma(\RV{X})] \circ\rho^{\RV{X}}_{\RV{Y}} d\kernel{T}_d &= \int_{\rho^{\RV{X}}_{\RV{Y}}(A)} \mathbb{E}[\mathds{1}_A\circ\RV{Y}|\sigma(\RV{X})] d(\kernel{T}R^{-1})_d\\
                                                                         &= \int_{\rho^{\RV{X}}_{\RV{Y}}(A)} \mathbb{E}[\mathds{1}_A\circ\RV{Y}|\sigma(\RV{X})] d\kernel{T}_d \\
                                                                         &= \int_{\rho^{\RV{X}}_{\RV{Y}}(A)} \mathds{1}_A\circ\RV{Y}d\kernel{T}_d\\
                                                                         &= \int_A \mathds{1}_A\circ\RV{Y} d\kernel{T}_d
\end{align}

It follows that $\mathbb{E}[\mathds{1}_A\circ\RV{Y}|\sigma(\RV{X})] \circ\rho^{\RV{X}}$ is a version of $\mathbb{E}[\mathds{1}_A\circ\RV{Y}|\sigma(\RV{X})]$. Because there are only a countable number of finite swap functions, it also follows that a version of $\mathbb{E}[\mathds{1}_A\circ\RV{Y}|\sigma(\RV{X})]$ that is $\sigalg{H}$-measurable exists (i.e. for which $\mathbb{E}[\mathds{1}_A\circ\RV{Y}|\sigma(\RV{X})]\circ\rho^{\RV{X}}_{\RV{Y}} = \mathbb{E}[\mathds{1}_A\circ\RV{Y}|\sigma(\RV{X})]$, for all $\rho^{\RV{X}}_{\RV{Y}}$).

Consider also for some swap function $\rho^{\RV{X}}_{ji,\RV{Y}}$ that swaps only the $i$ and $j$ indices, $B\in \sigalg{B}([0,1])$:

\begin{align}
    \int_{\RV{X}_i^{-1}(B)} \mathbb{E}[\RV{V}|\sigma(\RV{X}_j)] \circ \rho^{\RV{X}}_{ji,\RV{Y}} d\kernel{T}_d &= \int_{\rho^{\RV{X}}_{ji,\RV{Y}}(\RV{X}_i^{-1}(B))} \mathbb{E}[\RV{V}|\sigma(\RV{X}_j)] d(\kernel{T}R_{ij})_d\\
                                                                               &= \int_{\RV{X}^{-1}_{j}(B)} \mathbb{E}[\RV{V}|\sigma(\RV{X}_j)] d\kernel{T}_d\\
                                                                               &= \int_{\RV{X}_j^{-1}(B)} \RV{V} d\kernel{T}_d\\
                                                                               &= \int_{\rho_{ji}^*(\RV{X}_j^{-1}(B))}\RV{V}\circ \rho^{\RV{X}}_{ji,\RV{Y}} d(\kernel{T}R_{ji})_d\\
                                                                               &= \int_{\RV{X}_i^{-1}(B)} \RV{V}d\kernel{T}_d
\end{align}

Thus $\mathbb{E}[\RV{V}|\sigma(\RV{X}_j)]\circ\rho^{\RV{X}}_{ji,\RV{Y}}$ is a version of $\mathbb{E}[\RV{V}|\sigma({\RV{X}_i})]$.

Define $\RV{W}^n_q := \frac{1}{n}\sum_{i\in [n]} \mathbb{E}[\mathds{1}_{[0,q]}\RV{Y}|\sigma(\RV{X}_i)]$. Note that for any size $n$ swap function $\rho^{\RV{X}}$, $\RV{W}^n_q\circ\rho^{\RV{X}}=\RV{W}_q^n$, therefore $\RV{W}_q^n$ is $\sigalg{H}^n$-measurable. 

Consider $\omega,\omega'$ such that $\RV{W}_q^n(\omega)\neq \RV{W}_q^n(\omega')$. Then there exists no size $n$ swap function $\rho^{\RV{X}}_{\RV{Y}}$ such that $\RV{X}_{[n]}(\omega)=\RV{X}_{[n]}(\rho^{\RV{X}}_{\RV{Y}}(\omega'))$. Without loss of generality, suppose $\RV{X}_1(\omega)\leq\RV{X}_2(\omega)\leq...\leq\RV{X}_n(\omega)$ and $\RV{X}_1(\omega')\leq\RV{X}_2(\omega')\leq...\leq\RV{X}_n(\omega')$. Then there is some first index $j$ for which $\RV{X}_{j}(\omega)> \RV{X}_{j}(\omega')$, and some and some rational $r$ such that $\RV{X}_j(\omega)>r>\RV{X}_j(\omega')$. Then $\sum_{i}^n \mathds{1}_{[0,q)}(\RV{X}_i(\omega)) > \sum_i^n \mathds{1}_{[0,q)}(\RV{X}_i(\omega'))$ and so $\RV{Z}^n(\omega)\neq\RV{Z}^n(\omega')$ also.

Thus $\RV{W}_q^n$ is $\sigma(\RV{Z}^n)$ measurable. Define $\sigalg{I}^n:=\vee_{n\to\infty} \sigma(\RV{Z}^n)$. Then $\sigalg{I}^1\supset\sigalg{I}^2\supset...\supset\sigma(\RV{Z})=\cap_{i}^\infty \sigalg{I}^i$. In addition, by the $\sigalg{H}^n$-measurability of $\RV{Z}^{>n}$, $\sigalg{I}^n\subset\sigalg{H}^n$ and $\sigalg{I}\subset\sigalg{H}$.

Noting that $\rho^{\RV{X}}_{ji,\RV{Y}}$ is its own inverse, for $A\in\sigalg{I}^n$, $d\in D$:

\begin{align}
    \int_A \RV{W}_q^n d\kernel{T}_d &= \frac{1}{n}\sum_{i\in[n]} \int_A \mathbb{E}[\mathds{1}_{[0,q)}\circ\RV{Y}|\sigma(\RV{X}_i)] d\kernel{T}_d\\
                                  &= \frac{1}{n}\sum_{i\in[n]} \int_{\rho^{\RV{X}}_{ji,\RV{Y}}(A)} \mathbb{E}[\mathds{1}_{[0,q)}\circ\RV{Y}|\sigma(\RV{X}_i)]\circ\rho^{\RV{X}}_{ji,\RV{Y}}d(\kernel{T}\kernel{F}_{\rho^{\RV{X}}_{ji,\RV{Y}}})_d\\
                                  &= \int_{A} \mathbb{E}[\mathds{1}_{[0,q)}\circ \RV{Y} |\sigma(\RV{X}_j)]d\kernel{T}_d\\
                                  &= \int_{A} \mathds{1}_{[0,q)}\circ \RV{Y} d\kernel{T}_d
\end{align}

Where the last line follows from the fact that $\sigalg{I}^n\subset \sigma(\RV{X}_j)$. Therefore $\RV{W}^n_q$ is a version of $\mathbb{E}[\mathds{1}_{[0,q)}\circ|\sigalg{I}^n]$. Furthermore $\RV{W}^1_q,...,\RV{W}^n_q,..$ is a backwards martingale with respect to $\sigalg{I}^1,....,sigalg{I}^n,..\sigalg{I}$ and so it goes to a limit $\RV{W}_q=\mathbb{E}[\mathds{1}_{[0,q)}\circ \RV{V} |\sigma(\RV{Z})]$.

So $\RV{W}_q$ is a $\sigma(\RV{Z})$-measurable version of $\mathbb{E}[\mathds{1}_{[0,q]}\circ\RV{Y}|\sigalg{H}]$ which is itself a version of $\mathbb{E}[\mathds{1}_{[0,q]}\circ\RV{Y}|\sigma(\RV{X})]$. As before, for any $d\in D$, $\omega\in E$ we have

\begin{align}
    \kernel{T}_{\RV{D}(\omega),\RV{X}(\omega),\RV{Z}(\omega)}^{\RV{Y}|\RV{X}\RV{Z}\RV{D}}([0,q)) &= \mathbb{E}[\mathds{1}_{[0,q]}\circ\RV{Y}|\sigalg{H}](\omega)\\
                                                                                                 &= \kernel{T}_{\RV{D}(\omega),\RV{Z}(\omega)}^{\RV{Y}|\RV{Z}\RV{D}}([0,q))
\end{align}

This shows that $\RV{Y}\CI_{\kernel{T}} \RV{X}|(\RV{D},\RV{Z})$, as desired.

\end{proof}

The proof of the main theorem follows.

\begin{reptheorem}{th:rep_dex_sdf}[Representation of doubly exchangeable see-do forecasts]
Given a see-do forecast $(\kernel{T},\RV{D},\RV{X},\RV{Y})$ with denumerable $D:=D_0^B$ and standard measurable $X:=X_0^A$, $Y:=Y_0^B$, and where $\RV{X}=(\RV{X}_i)_{i\in A}$, $\RV{Y}=(\RV{Y}_i)_{i\in B}$ and $\RV{D}=(\RV{D}_i)_{i\in B}$ for finite $B\subset \mathbb{N}$, $A\subseteq \mathbb{N}$ not necessarily finite, the following statements are equivalent:

\begin{enumerate}
    \item There exists $(\kernel{T}',\RV{D}',\RV{X}',\RV{Y}')$ where $\kernel{T}'$ is an extension of $\kernel{T}$, $\RV{X}'=(\RV{X}'_i)_{i\in \mathbb{N}}$, $\RV{Y}'=(\RV{Y}'_i)_{i\in \mathbb{N}}$, $\RV{D}'=(\RV{D}'_i)_{i\in\mathbb{N}}$ and $(\kernel{T}',\RV{D}',\RV{X}',\RV{Y}')$ has exchangeable observations (holding $\RV{Y}'$ and $\RV{D}'$ fixed) and exchangeable consequences (holding $\RV{X}'$ fixed).
    \item There exists a a set $H$, a prior $\kernel{T}^{\RV{H}}\in\Delta(\sigalg{H})$ and Markov kernels $\kernel{T}^{\RV{X}_0|\RV{H}}$ and $\kernel{T}^{\RV{Y}_0|\RV{H}\RV{D}_0}$ such that for all $d\in D$, $\{J_i\in \sigalg{X}_0|i\in A\}$, $\{K_i\in \sigalg{Y}_0|i\in B\}$:
    \begin{align}
        \kernel{T}^{\RV{X}\RV{Y}|\RV{D}}_{d}((\bigtimes_{i\in A}J_i)\times (\bigtimes_{j\in B} K_j)) = \int_{H} \prod_{i\in A} \kernel{T}^{\RV{X}_0|\RV{H}}_h(J_i)\prod_{i\in B}\kernel{T}_{h,d_i}^{\RV{Y}_0|\RV{H}\RV{D}_0}(K_i)d\kernel{T}^{\RV{H}}(h)
    \end{align}
\end{enumerate}
\end{reptheorem}

\begin{proof}
$(1)\implies (2)$:

Define $\prob{P}\in \Delta(\sigalg{D_0}^{\mathbb{N}})$ such that $\prob{P}=\otimes_{i\in\mathbb{N}} \prob{P}^{\RV{D}_0}$ for some strictly positive $\prob{P}^{\RV{D}_0}$ (recall that $D$ and hence $D_0$ is denumerable). $\prob{P}^{\RV{D}_0}$ is exchangeable and independent and identically distributed. Consider some infinite doubly exchangeable extension $\kernel{T}'$ of $\kernel{T}$. Then $\prob{P}\kernel{T}'$ is exchangeable with respect to $\RV{DY}':=(\RV{D}'_i,\RV{Y}'_i)_{i\in\mathbb{N}}$ (Lemma \ref{lem:f-ex2ex}) and exchangeable with respect to $\RV{X}'=\utimes_{i\in\mathbb{N}} \RV{X}'_i$ as $\RV{X}$ is independent of $\RV{D}$.


By Lemma \ref{lem:rep_seedo_obs} we have $\RV{Z}':= f\circ\RV{X}'$ for some $f$ such that 
\begin{enumerate}
    \item $\RV{X}'_i\CI_{\prob{P}\kernel{T}'}\RV{X}'_{\mathbb{N}\setminus\{i\}}|\RV{Z}'$ for all $i\in A$
    \item $(\prob{P}\kernel{T}')^{\RV{X}'_i|\RV{Z}'}=(\prob{P}\kernel{T}'')^{\RV{X}'_j|\RV{Z}'}$ for all $i,j\in A$
    \item $\RV{D}\utimes\RV{Y}'\CI_{\prob{P}\kernel{T}'}\RV{X}'|\RV{Z}'$
\end{enumerate}

Applying Lemma  \ref{lem:rep_seedo_obs} to $\RV{DY}'$, and noting that $(\RV{D}',\RV{Y}')$ is an invertible function of $\RV{DY}'$, we have $\RV{W}'=g\circ\RV{DY}'$ for some $g$ such that

\begin{enumerate}
    \setcounter{enumi}{3}
    \item for all $i\in \mathbb{N}$, $\RV{D}_i\utimes\RV{Y}'_i\CI_{\prob{P}\kernel{T}'}\RV{D}'_{\mathbb{N}\setminus\{i\}}\RV{Y}'_{\mathbb{N}\setminus\{i\}}|\RV{W}'$
    \item $(\prob{P}\kernel{T}')^{\RV{Y}'_i|\RV{D}'_i}=(\prob{P}\kernel{T}')^{\RV{Y}'_j|\RV{D}'_j}$ for all $i,j\in \mathbb{N}$ 
    \item $\RV{X}'\CI_{\prob{P}\kernel{T}'}\RV{D}'\utimes\RV{Y}'|\RV{W}'$
\end{enumerate}

Because $(\RV{W}',\RV{D}')$ is a function of $\RV{DY}'$, we also have $\RV{X}'\CI_{\prob{P}\kernel{T}'} \RV{W}'\utimes\RV{D}'|\RV{Z}'$ by property 6.

$\RV{D}'$ is also a function of $\RV{DY}'$ and $\RV{Z}'$ is a function of $\RV{X}'$ so $\RV{D}'\CI_{\prob{P}\kernel{T}'}\RV{Z}'|\RV{W}'$, also by property 6. Because $\prob{P}$ is independent and identically distributed, $\RV{D}'\CI_{\prob{P}\kernel{T}'} \RV{W}'$, so $\RV{D}'\CI_{\prob{P}\kernel{T}'}(\RV{Z}', \RV{W}')$

Becuase $\RV{Z}'$ is a function of $\RV{X}'$, $\RV{Y}'\CI_{\prob{P}\kernel{T}'}\RV{Z}'|(\RV{D}',\RV{X}',\RV{W}')$. 

Applying weak union and symmetry to property 6 we have $\RV{Y}'\CI \RV{X}'|(\RV{W}',\RV{D}')$, which combined with the above gives $\RV{Y}'\CI (\RV{X}', \RV{Z}') |(\RV{W}',\RV{D}')$ by contraction.

In summary, we will use the following conditional independences:
\begin{itemize}
    \item $\RV{X}'\CI_{\prob{P}\kernel{T}'} (\RV{W}' \RV{D}')|\RV{Z}'$
    \item $\RV{D}'\CI_{\prob{P}\kernel{T}'}(\RV{Z}' \RV{W}')$
    \item $\RV{Y}'\CI_{\prob{P}\kernel{T}'}\RV{Z}'|(\RV{D}',\RV{X}',\RV{W}')$
    \item $\RV{Y}'\CI (\RV{X}', \RV{Z}')|(\RV{W}', \RV{D}')$
\end{itemize}

Let $\prob{U}:=\prob{P}\kernel{T}'$. By Lemma \ref{lem:representation_of_kernels}, and 
\begin{align}
    (\prob{P}\kernel{T}')^{\RV{X}'\RV{Y}'\RV{D}'} &= \begin{tikzpicture}
        \path (0,0) node[dist,inner sep=-2pt] (Z) {$\prob{U}^{\RV{Z}'}$}
        ++ (0.5,0) node[copymap] (copy0) {}
        ++ (0.7,0) node[kernel] (W) {$\prob{U}^{\RV{W}'|\RV{Z}'}$}
        ++ (0.9,0) node[copymap] (copy1) {}
        ++ (1.0,0.) node[kernel] (D) {$\prob{U}^{\RV{D'}|\RV{W'Z'}}$}
        ++ (0.9,0) node[copymap] (copy2) {}
        ++ (1.3,0.5)  node[kernel] (X) {$\prob{U}^{\RV{X}'|\RV{Z'W'D'}}$}
        ++ (0.9,0) node[copymap] (copy3) {}
        ++ (1.3,-0.5) node[kernel] (Y) {$\prob{U}^{\RV{Y'}|\RV{X'W'Z'D'}}$}
        ++ (1.5,0) node (Yrv) {$\RV{Y}'$}
        +  (0,0.5) node (Xrv) {$\RV{X}'$}
        +  (0,-0.5) node (Drv) {$\RV{D}'$};
        \draw (Z) -- (W);
        \draw (copy0) to [out=-65,in=180] ($(D.west) + (0,-0.15)$);
        \draw (copy0) to [out=65,in=180] ($(X.west) + (0,-0.15)$);
        \draw (copy1) to [out=65,in=180] ($(X.west) + (0,0)$);
        \draw (copy1) to [out=25,in=180] ($(D.west) + (0,0.15)$);
        \draw (copy2) to [out=25,in=180] ($(X.west) + (0,0.15)$);
        \draw (copy0) to [out=-65,in=180] ($(Y.west) + (0,-0.18)$);
        \draw (copy1) to [out=-65,in=180] ($(Y.west) + (0,-0.09)$);
        \draw (copy2) to [out=-65,in=180] ($(Y.west) + (0,0.09)$);
        \draw (copy3) to [out=-65,in=180] ($(Y.west) + (0,0.18)$);
        \draw (Z) -- (copy0) (W) -- (copy1) (D) -- (copy2);
        \draw (X) -- (Xrv) (Y) -- (Yrv);
        \draw (copy2) to [out=-90,in=180] (Drv);
    \end{tikzpicture}\\
     &= \begin{tikzpicture}
        \path (0,0) node[dist,inner sep=-2pt] (Z) {$\prob{U}^{\RV{Z}'}$}
        ++ (0.5,0) node[copymap] (copy0) {}
        ++ (0.7,0) node[kernel] (W) {$\prob{U}^{\RV{W}'|\RV{Z}'}$}
        ++ (0.9,0) node[copymap] (copy1) {}
        ++ (1.0,0.) node[dist,inner sep=-2pt] (D) {$\prob{U}^{\RV{D'}}$}
        ++ (0.9,0) node[copymap] (copy2) {}
        ++ (1.3,0.5)  node[kernel] (X) {$\prob{U}^{\RV{X}'|\RV{Z'}}$}
        ++ (0.9,0) node[copymap] (copy3) {}
        ++ (1.3,-0.5) node[kernel] (Y) {$\prob{U}^{\RV{Y'}|\RV{W'D'}}$}
        ++ (1.5,0) node (Yrv) {$\RV{Y}'$}
        +  (0,0.5) node (Xrv) {$\RV{X}'$}
        +  (0,-0.5) node (Drv) {$\RV{D}'$};
        \draw (Z) -- (W);
        \draw[-{Rays[n=8]}] (copy0) to [out=-65,in=180] ($(D.west) + (-0.1,-0.15)$);
        \draw (copy0) to [out=65,in=180] ($(X.west) + (0,-0.15)$);
        \draw[-{Rays[n=8]}] (copy1) to [out=65,in=180] ($(X.west) + (-0.4,0)$);
        \draw[-{Rays[n=8]}] (copy1) to [out=25,in=180] ($(D.west) + (-0.1,0.15)$);
        \draw[-{Rays[n=8]}] (copy2) to [out=25,in=180] ($(X.west) + (-0.4,0.15)$);
        \draw[-{Rays[n=8]}] (copy0) to [out=-65,in=180] ($(Y.west) + (-0.4,-0.18)$);
        \draw (copy1) to [out=-65,in=180] ($(Y.west) + (0,-0.09)$);
        \draw (copy2) to [out=-65,in=180] ($(Y.west) + (0,0.09)$);
        \draw[-{Rays[n=8]}] (copy3) to [out=-65,in=180] ($(Y.west) + (-0.4,0.18)$);
        \draw (Z) -- (copy0) (W) -- (copy1) (D) -- (copy2);
        \draw (X) -- (Xrv) (Y) -- (Yrv);
        \draw (copy2) to [out=-90,in=180] (Drv);
    \end{tikzpicture}\\
     &= \begin{tikzpicture}
        \path (0,0) node[dist,inner sep=-2pt] (Z) {$\prob{U}^{\RV{Z}'}$}
        + (0,-1) node[dist,inner sep=-2pt] (D) {$\prob{U}^{\RV{D'}}$}
        + (0.9,-1) node[copymap] (copy2) {}
        ++ (0.5,0) node[copymap] (copy0) {}
        ++ (0.7,-0.5) node[kernel] (W) {$\prob{U}^{\RV{W}'|\RV{Z}'}$}
        ++ (1.7,0.5)  node[kernel] (X) {$\prob{U}^{\RV{X}'|\RV{Z'}}$}
        +  (0,-1) node[kernel] (Y) {$\prob{U}^{\RV{Y'}|\RV{W'D'}}$}
        ++ (1.5,-1) node (Yrv) {$\RV{Y}'$}
        +  (0,1) node (Xrv) {$\RV{X}'$}
        +  (0,-0.5) node (Drv) {$\RV{D}'$};
        \draw (copy0) to [out=-60,in=180] ($(W.west) + (0,0)$);
        \draw (copy0) to [out=0,in=180] ($(X.west) + (0,0)$);
        \draw (W) to [out=0,in=180] ($(Y.west) + (0,0.15)$);
        \draw (copy2) to [out=0,in=180] ($(Y.west) + (0,-0.15)$);
        \draw (Z) -- (copy0) (D) -- (copy2);
        \draw (X) -- (Xrv) (Y) -- (Yrv);
        \draw (copy2) to [out=-60,in=180] (Drv);
    \end{tikzpicture}
\end{align}

By mutual indepenence of $\RV{Y}'_i\utimes\RV{D}'_i$'s', we have in particular $\RV{X}'_A\utimes \RV{Y}'_B\utimes \RV{D}'_B\CI_{\prob{U}} \RV{X}'_{A^C}\utimes \RV{Y}'_{B^C}\utimes \RV{D}'_{B^C}$. Therefore
\begin{align}
 \prob{U}^{\RV{X}'_A\RV{Y}'_B\RV{D}'_B}&=\prob{P}^{\RV{D}}(\kernel{T}'\otimes\stopper{0.2}_{D^{B^C}})(\mathrm{Id}_{X^A}\otimes \stopper{0.2}_{X_{A^C}}\otimes \mathrm{Id}_{Y^B}\otimes \stopper{0.2}_{Y^{B^C}})\\
                                       &=\prob{P}^{\RV{D}_B}\kernel{T}
\end{align}
Furthermore, $\prob{P}^{\RV{D}_A}$ is positive by assumption, so by Lemma \ref{lem:agree_disint}:

\begin{align}
    \kernel{T}^{\RV{XY}|\RV{D}} &= \prob{U}^{\RV{X}'_A\RV{Y}'_B|\RV{D}'_B}\\
               &= \begin{tikzpicture}
        \path (0,0) node[dist,inner sep=-2pt] (Z) {$\prob{U}^{\RV{Z}'}$}
        + (0,-1) node (D) {$\RV{D}$}
        + (0.9,-1) node[copymap] (copy2) {}
        ++ (0.5,0) node[copymap] (copy0) {}
        ++ (0.7,-0.5) node[kernel] (W) {$\prob{U}^{\RV{W}'|\RV{Z}'}$}
        ++ (1.7,0.5)  node[kernel] (X) {$\prob{U}^{\RV{X}'_A|\RV{Z'}}$}
        +  (0,-1) node[kernel] (Y) {$\prob{U}^{\RV{Y'}_B|\RV{W'D'_B}}$}
        ++ (1.5,-1) node (Yrv) {$\RV{Y}$}
        +  (0,1) node (Xrv) {$\RV{X}$};
        \draw (copy0) to [out=-60,in=180] ($(W.west) + (0,0)$);
        \draw (copy0) to [out=0,in=180] ($(X.west) + (0,0)$);
        \draw (W) to [out=0,in=180] ($(Y.west) + (0,0.15)$);
        \draw (copy2) to [out=0,in=180] ($(Y.west) + (0,-0.15)$);
        \draw (Z) -- (copy0) (D) -- (copy2);
        \draw (X) -- (Xrv) (Y) -- (Yrv);
    \end{tikzpicture}\\
    &\overset{def}{=} \begin{tikzpicture}
        \path (0,0) node[dist,inner sep=-2pt] (Z) {$\prob{T}^{\RV{H}}$}
        + (0,-1) node (D) {$\RV{D}$}
        ++ (0.5,0) node[copymap] (copy0) {}
        ++ (1.7,0)  node[kernel] (X) {$\kernel{T}^{\RV{X}|\RV{H}}$}
        +  (0,-1) node[kernel] (Y) {$\kernel{T}^{\RV{Y}|\RV{H D}}$}
        ++ (1.5,-1) node (Yrv) {$\RV{Y}$}
        +  (0,1) node (Xrv) {$\RV{X}$};
        \draw (copy0) to [out=0,in=180] ($(X.west) + (0,0)$);
        \draw (copy0) to [out=-25,in=180] ($(Y.west) + (0,0.15)$);
        \draw (D) to [out=0,in=180] ($(Y.west) + (0,-0.15)$);
        \draw (Z) -- (copy0);
        \draw (X) -- (Xrv) (Y) -- (Yrv);
    \end{tikzpicture}
\end{align}

In integral notation, we have shown, for all $d\in D$, $\{J_i\in \sigalg{X}_0|i\in A\}$, $\{K_i\in \sigalg{Y}_0|i\in B\}$:

\begin{align}
    \kernel{T}^{\RV{X}\RV{Y}|\RV{D}}_{d}((\bigtimes_{i\in A}J_i)\times (\bigtimes_{j\in B} K_j)) = \int_{H} \kernel{T}^{\RV{X}|\RV{H}}_h(\bigtimes_{i\in A}J_i) \kernel{T}_{h,d_i}^{\RV{Y}|\RV{H}\RV{D}}((\bigtimes_{j\in B} K_j))d\kernel{T}^{\RV{H}}(h)
\end{align}

We still need to show

\begin{align}
   \kernel{T}^{\RV{X}|\RV{H}}_h(\bigtimes_{i\in A}J_i) = \prod_{i\in A} \kernel{T}^{\RV{X}_0|\RV{H}}_h(J_i) \label{eq:IID}
\end{align}

and

\begin{align}
    \kernel{T}_{h,d}^{\RV{Y}|\RV{H}\RV{D}}((\bigtimes_{j\in B} K_j)) = \prod_{i\in B}\kernel{T}_{h,d_i}^{\RV{Y}_0|\RV{H}\RV{D}_0}(K_i)\label{eq:IFI}
\end{align}

We will show \ref{eq:IFI} holds first.

Let $\RV{D}_{<n}:=(\RV{D}_i)_{i\in A \cap\{n-1\}}$. Let $N:=|B|$, and without loss of generality, assume $B=[N]$. Consider some $n>1\in N$. From Lemma \ref{lem:representation_of_kernels}

\begin{align}
    \kernel{T}_{h,d_{[n]}}^{\RV{Y}_{[n]}|\RV{H}\RV{D}_{[n]}}((\bigtimes_{j\in [n]} K_j)) &= \int_{\bigtimes_{j<n} K_{i}}\kernel{T}_{h,d,y_{<n}}^{\RV{Y}_n|\RV{H}\RV{D}\RV{Y}_{<n}}(K_n)d\kernel{T}_{h,d}^{\RV{Y}_n|\RV{H}\RV{D}_n}(y_{<n}) \\
     &= \kernel{T}_{h,d_n}^{\RV{Y}_n|\RV{H}\RV{D}_n}(K_n)\kernel{T}_{h,d_{[n-1]}}^{\RV{Y}_n|\RV{H}\RV{D}_n}(\bigtimes_{j<n} K_{i})
\end{align}
Where the second line follows from mutual independence of $(\RV{Y}_i,\RV{D}_i)$s. For $n=1$ we simply have $\kernel{T}_{h,d,y_{1}}^{\RV{Y}_1|\RV{H}\RV{D}}(K_n)$. Thus

\begin{align}
    \kernel{T}_{h,d}^{\RV{Y}|\RV{H}\RV{D}}((\bigtimes_{j\in [N]} K_j)) &= \prod_{i\in [N]}\kernel{T}_{h,d_n}^{\RV{Y}_i|\RV{H}\RV{D}_i}(K_i)
\end{align}

Equation \ref{eq:IID} follows by an identical argument.

This completes the proof that $(1)\implies (2)$.

$(2)\implies (1)$:

For all sets of events $\{J_i\in \sigalg{X}_0\}_A$, $\{K_i\in \sigalg{Y}_0\}_B$, $\mathbf{d}_B\in D$:

\begin{align}
  \kernel{T}^{\RV{X}\RV{Y}|\RV{D}}_{\mathbf{d}_B}((\times_{i\in A}J_i)\times (\times_{j\in B} K_j)) = \int_{H} \prod_{i\in A} \kernel{T}^{\RV{X}_0|\RV{H}}_h(J_i)\prod_{i\in B}\kernel{T}_{h,d_i}^{\RV{Y}_0|\RV{H}\RV{D}_0}(K_i)d\kernel{T}^{\RV{H}}(h)
\end{align}

For all $\{J_i\in \sigalg{X}_1\}_\mathbb{N}$, $\{K_i\in \sigalg{Y}_0\}_\mathbb{N}$, $\mathbf{d} \in D^{\mathbb{N}}$ define

\begin{align}
  \kernel{T}^{\prime \RV{X}'\RV{Y}'|\RV{D}'}_{\mathbf{d}}((\times_{i\in \mathbb{N}}J_i)\times (\times_{j\in \mathbb{N}} K_j)) = \int_{H} \prod_{i\in \mathbb{N}} \kernel{T}^{\RV{X}_1|\RV{H}}_h(J_i)\prod_{i\in \mathbb{N}}\kernel{T}_{h,d_i}^{\RV{Y}_1|\RV{H}\RV{D}_1}(K_i)d\kernel{T}^{\RV{H}}(h)
\end{align}

Marginalising over $\mathbb{N}\setminus B$ and $\mathbb{N}\setminus A$ is equivalent to choosing $K_i=Y_0$ for $i\not\in A$ and $J_i=X_0$ for $i\not\in B$. Then

\begin{align}
  \kernel{T}^{\prime \RV{X}'_A\RV{Y}'_B|\RV{D}'}_{\mathbf{d}}((\times_{i\in A}J_i\times X_0^{\mathbb{N}\setminus A})\times (\times_{j\in B} K_j\times Y_0^{\mathbb{N}\setminus B})) &= \int_{H} \prod_{i\in A} \kernel{T}^{\RV{X}_0|\RV{H}}_h(J_i) [\kernel{T}^{\RV{X}_0|\RV{H}}_h (X_0)]^{\mathbb{N}\setminus A} \prod_{i\in B}\kernel{T}_{h,d_i}^{\RV{Y}_0|\RV{H}\RV{D}_0}(K_i)[\kernel{T}^{\RV{Y}_0|\RV{H}}_h (Y_0)]^{\mathbb{N}\setminus B}d\kernel{T}^{\RV{H}}(h)\\
  &= \int_{H} \prod_{i\in A} \kernel{T}^{\RV{X}_0|\RV{H}}_h(J_i) \prod_{i\in B}\kernel{T}_{h,d_i}^{\RV{Y}_0|\RV{H}\RV{D}_0}(K_i)d\kernel{T}^{\RV{H}}(h)\\
  &= \kernel{T}^{\RV{X}\RV{Y}|\RV{D}}_{\mathbf{d}_A}((\times_{i\in A}J_i)\times (\times_{j\in B} K_j))
\end{align}

Thus $\kernel{T}'$ is an extension of $\kernel{T}$. Furthermore, for any swaps $\rho_r$, $\rho_s$ with associated kernels $R^{\RV{X}'}$, $S^{\RV{D}'}$,$S^{\RV{Y'}}$:

\begin{align}
  (S^{\RV{D'}\RV{Y'}}\kernel{T}'(R^{\RV{X'}}\otimes S^{\RV{Y}'}))^{\RV{X}'\RV{Y}'|\RV{D}'}_{\mathbf{d}}((\times_{i\in \mathbb{N}}J_i)\times (\times_{j\in \mathbb{N}} K_j)) &= \int_{H} \prod_{i\in \mathbb{N}} \kernel{T}^{\RV{X}_0|\RV{H}}_h(J_{\rho_r(i)})\prod_{i\in \mathbb{N}}\kernel{T}_{h,d_{\rho_s(i)}}^{\RV{Y}_0|\RV{H}\RV{D}_0}(K_{\rho_s(i)})d\kernel{T}^{\RV{H}}(h) \\
                            &= \int_{H} \prod_{i\in \rho_{r}(\mathbb{N})} \kernel{T}^{\RV{X}_0|\RV{H}}_h(J_{i})\prod_{i\in \rho_s(\mathbb{N})}\kernel{T}_{h,d_{i}}^{\RV{Y}_0|\RV{H}\RV{D}_0}(K_{i})d\kernel{T}^{\RV{H}}(h) \\
                            &= \kernel{T}^{\prime \RV{X}'\RV{Y}'|\RV{D}'}_{\mathbf{d}}((\times_{i\in \mathbb{N}}J_i)\times (\times_{j\in \mathbb{N}} K_j))
\end{align}

Therefore $\kernel{T}'$ exchangeable in observations and functionally exchangeable in consequences.
\end{proof}