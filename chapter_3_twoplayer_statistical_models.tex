
%!TEX root = main.tex

\chapter{Statistical models with consequences}\label{ch:2p_statmodels}

\section{Summary}

Statistical models are ubiquitous in the analysis of inference problems. A statistical model features a set of hypotheses, and each hypothesis is mapped to a probability distribution over \emph{outcomes}. We need to consider variants of statistical models that model consequences of decisions. We do this with two types of model: the first model type is the \emph{two player statistical model}, which are a special kind of statistical model that differ from classical statistical model by incorporating dependence on decisions in addition to dependence on hypotheses. The ``two players'' are player $H$, who chooses a hypothesis, and player $D$ who chooses a decision. The second model type is the \emph{see-do model}, which is a two-player statistical model where we also draw a distinction among outcomes between \emph{observations} and \emph{consequences}, and require that the observations are independent of the decisions. A see-do model can be used in problems where we are given some data and wish to make a decision with good consequences on the basis of this, a type of problem that is discussed at length in Chapter \ref{ch:evaluating_decisions}.

See-do models are an extension of classical statistical models. A common simplifying assumption made in classical statistical models is that they are \emph{conditionally independent and identically distributed} (conditinally IID); this means that the model maps each hypothesis to an independent and identically distributed (IID) sequence of observations (though this just a common choice, not a strict requirement). The question arises of what a similar assumption relevant to see-do models is. If consequences depend on choices, then it does not make sense to assert that observations and consequences together form a single IID sequence of random variables, so we need to consider alternatives. We propose that models where observervations are an IID sequence and choices and consequences together are \emph{independent and functionally identical} (IFI; defined later in this chapter) are similar to conditionally IID statistical models. 

Assuming that the appropriate statistical model is conditionally IID may be a basic assumption for a given problem, or it might not be immediately obvious whether this assumption is sound for the problem. \emph{De Finetti's representation theorem} shows that conditionally IID models are also induced by ``Bayesian forecast'' models for which the sequences of observations is assumed to be \emph{exchangeable}, which offers another way of justifying conditionally IID models: one can either assume it directly, or assume exchangeability and study the induced conditionally IID model. A further question arises: is there an analogous representation theorem for see-do models for which observations are IID and consequences are IFI? We show that there is: a see-do ``forecast'' with exchangeable observations and \emph{functionally exchangeable} consequences induces a see-do model with IID observations and IFI consequences.

The assumption of functional exchangeability will appear again in Chapter \ref{ch:ints_counterfactuals} in our discussion of counterfactual random variables, and we will consider the joint assumptions of exchangeable observations and functionally exchangeable consequences to motivate the assumption of \emph{imitability} in Chapter \ref{ch:inferring_causes}, an assumption that in combination with a number of other assumptions allows for inference of consequences from data.

\section{Modelling observations, choices and consequences}

\subsection{Modelling observations with statistical models}

Statistical models are a ubiquitous in statistics and machine learning. It consists of a set of \emph{states} $(S,\sigalg{S})$, and for each state the model prescribes a single probability distribution on a given measurable set of \emph{outcomes} $(O,\sigalg{O})$.

\begin{definition}[Statistical model]\label{def:statistical model}
A statistical model is a set of states $(S,\sigalg{S})$, a set of outcomes $(O,\sigalg{O})$ and a stochastic map $\kernel{T}:S\to \Delta(\sigalg{O})$.
\end{definition}

\begin{definition}[State and outcome variables]\label{def:state_outcome}
Given a statistical model $(\kernel{T},(O,\sigalg{O}),(S,\sigalg{S}))$, define the \emph{state variable} $\RV{S}:S\times O\to S$ as the projeciton from $S\times O\to S$ and define the \emph{outcome variable} $\RV{O}:S\times O\to O$ as the projection onto $O$.
\end{definition}

The often cited example of a potentially biased coin is modelled with a statistical model. We suppose our coin has some rate of heads $\theta\in [0,1]$, and we furthermore suppose that for each $\theta$ the result of flipping the coin can be modeled (in some sense) by the probability distribution $\text{Bernoulli}(\theta)$. The statistical model here is the set of hypotheses $H=[0,1]$ (corresponding to \emph{rates of heads}), the observation space $O=\{0,1\}$ (with the discrete sigma-algebra) and the stochastic map $\kernel{B}:[0,1]\to \Delta(\mathscr{P}(0,1))$ which is given by $\kernel{B}:\theta\to \text{Bernoulli}(\theta)$.

Almost any theoretical treatment of statistics or machine learning will at some point make use of statistical models to describe the problem they are addressing -- for a collection of examples from the last 100 years, see \cite{Goodfellow-et-al-2016,vapnik_nature_2013,bishop_pattern_2006,le_cam_comparison_1996,freedman_asymptotic_1963,wald_statistical_1950,de_finetti_foresight_1992,fisher_statistical_1992}. 

Statistical models don't necessarily play a prominent role in the execution of learning algorithms, but they are important for understanding the results. For example, consider a linear regressor that takes a set of predictors $x\in X$ and targets $y\in Y$ and returns some $\beta\in B$ such that $(y-x^T\beta)^2$ is as small as possible. If we interpret $B$ as a set of hypotheses, then we can put together a crude statistical model as follows: identify $B$ as a set of hypotheses and the set of linear functions $X\to Y$ as the set of observations, which we will denote $Y^X$ and assume we also have some sigma-algebra $\sigalg{Y}^X$. Then the function $\kernel{T}:B\to \Delta(\sigalg{Y}^X)$ that sends $\beta$ deterministically to the function $x\mapsto x^T\beta$ is the stochastic map. $(\kernel{T},B,Y^X)$ is a statistical model, but it would be unreasonable to expect it to be a good model of the world.

Suppose we want to ask whether the function $f$ given by a linear regressor is useful for some purpose. It will almost never be helpful to ask whether predictors $x$ and targets $y$ will always be related by $f$, because they are almost always sampled with noise and so the answer will almost always be ``no''. It is also unhelpful, in the face of such uncertainty, to declare all possible functions equally fit for our purpose. Rather, we need some way of describing the relationship between $x$ and $y$ with allowance for uncertainty. To do this, we usually use probability. In this example, we might consider a statistical model $(\kernel{T},H,O)$ where for each $h\in H$, $\kernel{T}_h\in \Delta(\sigalg{Y}\otimes\sigalg{X})$ such that $\kernel{T}_h^{\RV{Y}|\RV{X}}=\text{Normal}(\RV{X}^T\beta_h,\sigma_h)$. If we assume the data generating process is described by such a probability distribution for some $h$, we can ask questions like ``does the linear regressor output a $\beta$ such that $\RV{Y}-\RV{X}^T\beta<\epsilon$ with high probability for all $h\in H$?''

One will not necessarily construct an explicit statistical model in the course of developing a learning algorithm for a problem of interest, but if we want to reason about how well the results of the algorithm fit the world then we often need a model of the world that is more credible than the model ``embedded in the algorithm'', and statistical models are very often the kinds of models chosen for this purpose.

\subsection{Modelling choices and consequences with two-player statistical models}

Typically the states in a statistical model are considered to be ``under the control of nature''. In the possibly biased coin example above, if we were to consider someone, call her ``player D'', acutally flipping the coin and trying to infer the bias, we would typically assume that their opinion about the coin's bias does not affect the coin's actual bias. However, in some cases player D can make choices that might affect the outcomes. Suppose player D has the option to choose how high to toss the coin, perhaps she can aim for a toss height anywhere from 10 to 50cm. This plausibly affects the outcomes of her coin toss and, unlike the coin's bias, she gets to choose the height she tosses it to (or at least the height she intends to toss it to). We call features of the state that are not under player D's control \emph{hypotheses} and features that are under player D's control \emph{decisions}, and statistical models in which the state is the Cartesian product of a set of hypotheses and a set of decisions ``two player statistical models'' (the two players being nature or ``player H'' and the decision maker or ``player D'').

\begin{definition}[Two player statistical model]\label{def:2p_stat}
A \emph{two-player statistical model} is a tuple $(\kernel{T},\RV{H},\RV{D},\RV{O})$ where $(\kernel{T},(H\times D,\sigalg{H}\otimes\sigalg{D}), (O,\sigalg{O}))$ is a statistical model and $\RV{H}:H\times D\times O\to H$, $\RV{D}:H\times D\times O\to D$ and $\RV{O}:H\times D\times O\to O$ are measurable functions that project elements of $H\times D\times O$ to their respective codomains. $\RV{H}$ is called the \emph{hypothesis}, $\RV{D}$ the \emph{decision} and $\RV{O}$ the \emph{outcome}.
\end{definition}

Whenever we propose a two player statistical model, we will also assume for any random variables $\RV{X}: H\times D\times O\to X$ and $\RV{Y}:H\times D\times O\to Y$, a disintegration $\kernel{K}^{\RV{Y}|\RV{XDH}}:X\times D\times H\to \Delta(\sigalg{Y})$ exists (see Theorem \ref{th:existence_continous} for a sufficient condition).

The problems that we will mostly study in this work, in addition to having a second player (``player D''), will often involve some data $\RV{X}$ that is observed before the second player is able to make a choice. Two player statistical models with observations that cannot be affected by the second player's choice are called \emph{see-do models}.

\begin{definition}[See-Do model]\label{def:seedo}
A \emph{see-do model} $(\kernel{T},\RV{H},\RV{D},\RV{X},\RV{Y})$ is a two-player statistical model along with two additional random variables: the \emph{observation} $\RV{X}: H\times D\times O\to X$ and the \emph{consequence} $\RV{Y}:H\times D\times O\to Y$. The outcome variable is defined to be the coupled product of the observation and the consequence $\RV{O}=(\RV{X},\RV{Y})$, and we will leave this implicit when specifying a see-do model. A see-do model must observe the conditional independence:
\begin{align}
\RV{X}\CI_\kernel{T} \RV{D}|\RV{H} \label{eq:see_do_independence_requirement}
\end{align}
\end{definition}

We can informally read the independence requirement as saying ``the observations are independent of the decision given the hypothesis''. This does not imply that in probability models we construct from $\kernel{T}$ that $\RV{D}$ and $\RV{X}$ will necessarily be probabilistically independent conditional on $\RV{H}$ and in fact this will often not be the case. In Chapter \ref{ch:ints_counterfactuals} we will argue that this requirement captures the intuition that observations are not ``affected'' by decisions. For now, we will observe that this independence requirement means that $\kernel{T}$ can be drawn with no path from $\RV{D}$ to $\RV{X}$, but we can also construct probability models by drawing a path from $\RV{X}$ to $\RV{D}$.

Explicitly, the indepence on line \ref{eq:see_do_independence_requirement} implies that the kernel $\kernel{T}$ can be drawn as follows:

\begin{align}
\kernel{T}:= \begin{tikzpicture}
                 \path (0,0) node (H) {$\RV{H}$}
                 + (0,-1) node (D) {$\RV{D}$}
                 ++ (0.5,0) node[copymap] (copy0) {}
                 ++ (0.9,0) node[kernel] (XH) {$\kernel{T}^{\RV{X}|\RV{H}}$}
                 + (0,-0.85)  node[kernel] (YHD) {$\kernel{T}^{\RV{Y}|\RV{HD}}$}
                 ++ (1,0) node (X) {$\RV{X}$}
                 + (0,-0.85) node (Y) {$\RV{Y}$};
                 \draw (H) -- (XH) -- (X);
                 \draw (copy0) to [out=-45,in=180] ($(YHD.west) + (0,0.15)$);
                 \draw (D) to [out=0,in=180] ($(YHD.west) + (0,-0.15)$) (YHD) -- (Y);
             \end{tikzpicture}
\end{align}

\section{Repeatable experiments and actions}

It is very common that statistical problems feature data from repeatable experiments. These are typically modelled with statistical models of \emph{conditionally independent and identically distributed} (conditionally IID) sequences. Given a statistical model $(\kernel{T}, (O,\sigalg{O}), (S,\sigalg{S}))$ with state variable $\RV{S}$, we say that a sequence of random variables $(\RV{X}_i)_{i\in A}$, where $\RV{X}_i:O\to X_0$ for all $i$, is conditionally independent and identically distributed if $\RV{X}_i\CI_{\kernel{T}}\RV{X}_{A\setminus \{i\}}|\RV{S}$ for all $i\in A$. Given a particular state $s\in S$, we have $(\RV{X}_i)_{i\in A}$ are independent and identically distributed with respect to $\kernel{T}_s$; $\RV{X}_i\CI_{\kernel{T}_s} \RV{X}_{A\setminus \{i\}}$ for all $i\in A$.

This assumption is so common that it is often made implicitly. Statements like ``assume we have an independent and identically distributed sequence'' are usually invocations of the conditional IID assumption, as they typically mean to assume that the sequence is IID with some unknown distribution, in which case the set of possible distributions can be identified with the set of states. 

See-do models are much less well-known than classical statistical models, and as we shall see the assumption that observations and consequences together are conditionally independent and identically distributed is one that we usually want to avoid. Nonetheless, given how common the conditional IID assumption is in classical statistical models it is likely that we will also often want to make an assumption like the conditional IID assumption for see-do models. Thus the question arises: what assumption is the appropriate analogue?

The conditional IID assumption itself, applied to see-do models, has implications that we think are usually undesirable. From an informal point of view, if both $\RV{X}$ and $\RV{Y}$ form a single IID sequence then conditional on both $\RV{H}$ and $\RV{D}$ then it seems the decision $\RV{D}$ cannot have any impact on the consequence $\RV{Y}$. Suppose we have a see-do model $(\kernel{T},\RV{H},\RV{D},\RV{X},\RV{Y})$ and the observations and consequences are both sequences of random variables $\RV{X}=(\RV{X}_i)_{i\in A}$, $\RV{Y}=(\RV{Y}_i)_{i\in B}$. See-do models assume that $\RV{X}\CI\RV{D}|\RV{H}$ which implies for all $i\in A$ $\RV{X}_i\CI\RV{D}|\RV{H}$. If we assume that the sequence $(\RV{X}_i,\RV{Y}_i)_{i\in A}$ is condtionally IID given $(\RV{H},\RV{D})$ (recall that this pair is the state variable for see-do models), then $\kernel{T}^{\RV{Y}_i|\RV{HD}}=\kernel{T}^{\RV{X}_j|\RV{HD}}$ for all $i\in B$, $j\in A$. Because $\RV{Y}=(\RV{Y}_i)_{i\in A}$, $\RV{Y}\CI \RV{D}|\RV{H}$ also.

There is some subtlety here. The statement $\RV{Y}_i\CI\RV{D}|\RV{H}$ seems like it could mean something like ``decisions cannot affect consequences''. We will see in Chapter \ref{ch:ints_counterfactuals}, it doesn't necessarily mean that we expect decisions and consequences to be probabilistically independent. However, when using a see-do model ``in the usual way'' (which, again, we will spell out in Chapter \ref{ch:ints_counterfactuals}), it does imply that we expect decisions and consequences to be probabilistically independent, and we take this as enough to say that in usual cases we want to use see-do models where $\RV{Y}$ is not independent of $\RV{D}$ given $\RV{H}$.

The previous discussion suggests that generalising the conditional IID assumption to see-do models requires treating the observations the observations $(\RV{X}_i)_{i\in A}$ and the consequences $(\RV{Y}_i)_{i\in B}$ separately. For the observations, the obvious choice is to assume that $(\RV{X}_i)_{i\in A}$ are IID conditional on $\RV{H}$. Because $\RV{X}\CI\RV{D}|\RV{H}$ and any subsequence of observations is a function of $\RV{X}$, this implies that observations are IID conditional on $(\RV{H},\RV{D})$ also. Furthermore, if we consider a see-do model $(\kernel{T},\RV{H},\RV{D},\RV{X},\RV{Y})$ where $\RV{X}=(\RV{X}_i)_{i\in A}$ are IID conditional on $\RV{H}$, then marginalising over $\RV{Y}$ induces the statistical model $(\kernel{T}^{\RV{X}|\RV{H}},\RV{H},\RV{X})$ where $(\RV{X}_i)_{i\in A}$ are conditionally IID by assumption. It is common practice with causal models to assume that conditionally IID statistical models; wherever it is assumed that we have an ``observational probability distribution'' that can be determined precisely in the limit of infinite data, there typically appears to be an implicit assumption that the observations are modeled by a conditionally IID statistical model.

There are at least two different ``IID-like'' assumptions we could make pertaining to a sequence of consequences $\RV{Y}=(\RV{Y}_i)_{i\in A}$. In the first case, we could assume that it is only possible to influence all consequences at once; in the second case, we assume that all consequences can be individually influenced. The first case could be used to model ``high-level policy'' decisions -- for example, a teacher deciding which instruction to give to an entire class at once. The second could be used to model ``repeated individual'' decisions, for example a teacher offering feedback on individual students' work.

If the entire sequence must be controlled at once, we can assume that $(\RV{Y}_i_{i\in B}$ is conditionally IID given $(\RV{H},\RV{D})$. Combined with a conditional IID assumption for observations, this gives us a see-do model with two sequences -- $\RV{X}$ and $\RV{Y}$ -- both of which are conditionally IID given $(\RV{H},\RV{D})$, but each may in general have different distributions. We will call such models \emph{double IID}.

To model independent control of each element $\RV{Y}_i$ of the sequence of consequences, we must also have a sequence of ``sub-decisions'' $D=(\RV{D}_i)_{i\in B}$, where each $\RV{D}_i$ controls one consequence $\RV{Y}_i$. In this case, it doesn't make sense to assume that the sequence of consequences is IID, as different choices for $\RV{D}_i$ and $\RV{D}_j$ may yield different distributions for $\RV{Y}_i$ and $\RV{Y}_j$. Rather than assuming that each $\RV{Y}_i$ is identically distributed, we can assume that the function $\kernel{T}^{\RV{Y}_i|\RV{H}\RV{D}_i}:D_1\to \Delta(\RV{Y}_1)$ is identical for each $(\RV{D}_i,\RV{Y}_i)$ pair. If this is true, we will say that $\RV{D}$ and $\RV{Y}$ together are conditionally \emph{independent and functionally identical} (IFI). As with IID sequences, IFI sequences are mutually independent. Rather than each consequence having the same distribution, the assumption of functional identity holds that the stochastic maps from sub-decisions to sub-consequences are identical for all $(\RV{D}_i,\RV{Y}_i)$.

\begin{definition}[Conditionally independent and functionally identical]
Given a see-do model $(\kernel{T},\RV{H},\RV{D}_B,\RV{X}_A,\RV{Y}_B)$ where $\RV{D}_B=(\RV{D}_i)_{i\in B}$, $\RV{Y}_B=(\RV{Y}_i)_{i\in B}$, $\RV{D}_B$ and $\RV{Y}_B$ are conditionally independent and functionally identical if both of the following hold:
\begin{enumerate}
    \item $\RV{Y}_i\CI_{\kernel{T}} (\RV{X},\RV{Y}_{B\setminus \{i\}},\RV{D}_{B\setminus \{i\}})|\RV{H}\RV{D}_i$ for all $i\in A$
    \item $\kernel{T}^{\RV{Y}_i|\RV{H}\RV{D}_i} = \kernel{T}^{\RV{Y}_j|\RV{H}\RV{D}_j}$ for all $i,j\in A$
\end{enumerate}
\end{definition}

\subsection{Justifications for ``independent and identically distributed'' type assumptions}

The assumption of independent and identically distributed sequences is a very common one, but this doesn't mean that it's always a justified one. In this section, we revisit a classic result that shows that \emph{exchangeability} of observations can justify the use of conditionally independent and identically distributed statistical models, and show that this result can be extended to see-do models in the form of an assumption of \emph{functional exchangeability}. 

To begin with, we will consider what it is probability is being used to model in statistical models and see-do models. It is well-known that probability can be used to model different things (see, for some examples, \citet{hajek_interpretations_2019}). Two common uses for probability are:

\begin{enumerate}
    \item To model systems which produce repeatable observations whose long run frequencies converge to a probability distribution
    \item To make forecasts of things which we do not yet know for sure
\end{enumerate}

In the first case, the probability seems to be a property of the system, so we'll call this ``objective probability'', while in the second case probability is used to quantify someone's personal best guess of something about which they are uncertain. We can call this ``subjective probability''.

It is very common to view probability in classical statistical models as corresponding to something like objective probability. In this view, ``states'' are properties of the world, and for each state the system being described can produce repeatable results that converge in relative frequency to the probability associated with that state. Incidentally, it is hard to choose names for elements of statistical mdoels without suggesting a particular interpretation -- to my ear, the name ``states'' is suggestive of objective probability, but it is not my intention to suggest that we must interpret probability in statistical models as objective probability.

With see-do models, there is an extra reason why subjective view of probability can be appealing. A classical statistical model is often modelling observations, and the kinds of problems in which we use classical statistical models are problems in which we see observations and do something with them in order to make some kind of inference. The kinds of problems in which we use see-do models also involve receiving observations and doing something with them, but they further ask us to select a good decision on the basis of the consequences we anticipate seeing for that decision. Both the observations and the consequences are modelled with probability, and the problem asks us to make a forecast of consequences, something we do not yet know for certain.

Because see-do models are motivated by classical statistical models, and reduce to classical statistical models if we ignore the consequences, it is reasonable to think that they might be used as models of objective probability. Because the problems in which we might use see-do models call for forecasts, there is also reason to think they might be used as models of subjective probability.

One practical consequence of this arises from the fact that, while the assumption of independent and identically distributed sequences is very common in objective probability models, it is much less attractive in models of subjective probability. If a probability distribution is describing a tendency for a tossed coin to land on heads or tails, it seems reasonable to assume that the coin's tendency to land one way or the other is fixed, and its tendency to land one way or the other on the 100th toss has nothing to do with the way it landed on tosses 1 through 99. However, a subjective probability model that describes beliefs about this coin which are updated upon tossing the coin and viewing the results should definitely have the property that beliefs about the outcome of the 100th toss depend in some way on the outcomes of tosses 1 through 99. If one observes 98 heads and 1 tail, it would be very reasonable to increase the strength of one's belief that one will also observe heads on the 100th toss. In order to allow for such updating of beliefs, a subjective probability model must allow for dependence between observations.

De Finetti's well-known representation theorem shows that probability models with \emph{exchangeable} sequences of random variables can always be represented as mixtures of models with IID sequences of random variables. Exchangeable models are more attractive than IID models for subjective probability because they do not rule out updating expectations on the basis of observations of subsequences. Without aiming to be prescriptive about how see-do models ought to be used, and noting that there are reasons to think they might be used to model either objective or subjective probability, we are motivated to ask whether there is a version of the exchangeability assumption for see-do models that can justify the use of objective probability models given an initial goal of representing subjective uncertainty.

We will find additional motivation for considering these assumptions in forthcoming chapters; in Chapter \ref{ch:ints_counterfactuals} we will discuss how exchangeability-like assumptions seem to be a key feature of models of counterfactual propositions (such as potential outcomes), and in Chapter \ref{ch:inferring_causes} we will consider how assumptions of symmetry that hold within each sequence $(\RV{X}_i)_{i\in A}$ and $(\RV{Y}_i)_{i\in B}$ could be extended to some kind of symmetry that holds across both sequences.

In the following discussion we will assume that subjective uncertainty is modeled with a single probability distribution. This is a common view, but the aim of modelling subjective uncertainty does not force us to use a single probability distribution. \citet{walley_statistical_1991} has developed a kind of model for subjective uncertainty he calls a \emph{prevision}, and each prevision is equivalent to a class of probability measures, and a class of probability measures can be represented as a statistical model. If the motivating arguments for previsions can be extended to address joint observations, decisions and consequences, and whether any resulting ``see-do prevision'' can be represented by a see-do model is an open question.

We have acknowledged some ambiguity in whether see-do models will be used to represent subjective or objective uncertainty. We find a similar ambiguity in \citet[Chapter 1]{pearl_causality:_2009}; Introducing probability, Pearl writes:

\begin{quote}
We will adhere to the Bayesian interpretation of probability, according to which probabilities encode degrees of belief about events in the world and data are used to strengthen, update, or weaken those degrees of belief. In this formalism, degrees of belief are assigned to propositions (sentences that take on true or false values) in some language, and those degrees of belief are combined and manipulated according to the rules of probability calculus.
\end{quote}

Which endorses the view that, in the work that follows, probability will be used to model subjective uncertainty. However, later in the same chapter he writes

\begin{quote}
[...] causal relationships are ontological, describing objective physical constraints in our world, whereas probabilistic relationships are epistemic, reflecting what we know or believe about the world.
\end{quote}

If causes are objective physical constraints, then if Causal Bayesian Networks described by Pearl model causes it appears that they must be modelling such objective constraints, not merely subjective uncertainty about consequences.

In the world of ``ordinary'' one-player statistical models, De Finetti's Representation Theorem establishes that if we are using probabilities to represent forecasts and we are willing to assume that these forecasts are \emph{exchangeable} -- that is, the probability distribution we choose to represent our forecast of future events is exactly the same as the probability distribution we would choose to represent our forecasts of any permutation of these events -- then it is possible to define a random variable representing the \emph{hypothesis} such that the probability distribution representing our forecast can be written as the product of a \emph{prior distribution} over hypotheses and a statistical model that maps hypotheses to independent and identically distributed sequences of observations, which is a sequence of precisely the type that converges in the frequentist sense.

Thus, one way that statistical models could describe objective facts is if forecasts can be assumed to be exchangeable, in which case the forecast can be factored into a prior and a statistical model describing  independent and identically distributed sequences. As \citet{walley_statistical_1991} cautions, exchangeability is quite a strong assumption -- for example, it entails that one will never believe, no matter how much evidence is apparent, that a sequence is cyclic rather than converging to a stable relative frequency. Acknowledging there is a lot more to explore in this area, in Theorem \ref{th:rep_dex_sdf} we extend De Finetti's representation theorem to see-do models and show that, considering \emph{see-do forecasts}, if both observations and consequences satisfy a kind of exchangeability then the forecast is the product of a prior and a see-do model which, given a hypothesis, describes a sequence of independent and identical consequence maps.

\begin{definition}[Forecasts, see-do forecast]\label{def:do_forecast}
A \emph{do forecast} $(\kernel{F},\RV{D},\RV{O})$ is a Markov kernel $\kernel{F}:D\to \Delta(\sigalg{O})$ for some set of choices $D$ and outcomes $O$. The choice variable $\RV{D}:D\times O\to D$ is the map $(d,e)\mapsto d$ that forgest the outcome and and the outcome variable $\RV{O}:D\times O\to O$ is the map $(d,o)\to o$ that forgets the choice.

A \emph{see-do forecast} is a forecast $(\kernel{F},\RV{D},\RV{X},\RV{Y})$ with an \emph{observation variable} $\RV{X}:D\times O\to X$ and a \emph{consequence variable} $\RV{Y}:D\times O\to Y$ such that $\RV{O}=\RV{X}\utimes\RV{Y}$ and $\RV{X}\CI\RV{D}$.

A \emph{forecast} $(\kernel{F},\RV{O})$ is a probability measure $\prob{F}\in\Delta(\sigalg{O})$ and an outcome variable $\RV{O}:O\to O$.
\end{definition}

We can go from a see-do model to a see-do forecast by adding a prior to the model.

\begin{theorem}
Given a two player statistical model $(\kernel{K},\RV{H},\RV{D},\RV{O})$ and any prior $\mu\in \Delta(\sigalg{H})$, defining $\kernel{L}:=(\mu\otimes \mathrm{Id}_D)\kernel{K}$ we have $(\kernel{L},\RV{D},\RV{O})$ is a do-forecast. 

If $(\kernel{K},\RV{H},\RV{D},\RV{X},\RV{Y})$ is a see-do model then, defining $\kernel{L}$ as before, $(\kernel{L},\RV{D},\RV{X},\RV{Y})$ is a see-do forecast.
\end{theorem}


\begin{proof}
The first part is trivial: $(\mu\otimes \mathrm{Id}_D)\kernel{K}$ is a Markov kernel $D\to \Delta(\sigalg{O})$ by construction, and $\RV{D}$ and $\RV{O}$ are choice and outcome variables by definition of the original two player statistical model.

For the second part $\RV{X}\CI_{\kernel{L}}\RV{D}$ is required. By Theorem \ref{th:iterated_disint} we have

\begin{align}
    \kernel{K} &= \kernel{K}^{\RV{X}\RV{Y}|\RV{HD}}\\
    &= \begin{tikzpicture}
        \path (0,0) node (H) {$\RV{H}$}
        + (0,-1.15) node (D) {$\RV{D}$}
        ++ (0.5,0) node[copymap] (copy0) {}
        ++ (0.7,0) node[kernel] (X) {$\kernel{K}^{\RV{X}|\RV{H}}$}
        ++ (0.5,0) node[copymap] (copy1) {}
        +  (0.8,-1) node[kernel] (Y) {$\kernel{K}^{\RV{Y}|\RV{XHD}}$}
        ++ (2,0) node (Xr) {$\RV{X}^{sd}$}
        +  (0,-1) node (Yr) {$\RV{Y}^{sd}$};
        \draw (H) -- (X) -- (Xr);
        \draw (copy0) to [out=-90,in=180] ($(Y.west) + (0,0)$) 
              (copy1) to [out=-90,in=180] ($(Y.west) + (0,0.15)$)
              (D) -- ($(Y.west) + (0,-0.15)$);
        \draw (Y) -- (Yr);
    \end{tikzpicture}
\end{align}

It follows that

\begin{align}
    \kernel{L}^{\RV{X}\RV{Y}|\RV{D}} &= \kernel{L}\\
    &= (\mu\otimes \mathrm{Id}_D)\kernel{K}\\
    &=  \begin{tikzpicture}
        \path (0,0) node[dist] (H) {$\mu$}
        + (0,-1.15) node (D) {$\RV{D}$}
        ++ (0.5,0) node[copymap] (copy0) {}
        ++ (0.7,0) node[kernel] (X) {$\kernel{K}^{\RV{X}|\RV{H}}$}
        ++ (0.5,0) node[copymap] (copy1) {}
        +  (0.8,-1) node[kernel] (Y) {$\kernel{K}^{\RV{Y}|\RV{XHD}}$}
        ++ (2,0) node (Xr) {$\RV{X}$}
        +  (0,-1) node (Yr) {$\RV{Y}$};
        \draw (H) -- (X) -- (Xr);
        \draw (copy0) to [out=-90,in=180] ($(Y.west) + (0,0)$) 
              (copy1) to [out=-90,in=180] ($(Y.west) + (0,0.15)$)
              (D) -- ($(Y.west) + (0,-0.15)$);
        \draw (Y) -- (Yr);
    \end{tikzpicture}
\end{align}

Then

\begin{align}
    \kernel{L}^{\RV{X}|\RV{D}} &= \begin{tikzpicture}
        \path (0,0) node[dist] (H) {$\mu$}
        + (0,-1.15) node (D) {$\RV{D}$}
        ++ (0.5,0) node[copymap] (copy0) {}
        ++ (0.7,0) node[kernel] (X) {$\kernel{K}^{\RV{X}|\RV{H}}$}
        ++ (0.5,0) node[copymap] (copy1) {}
        +  (0.8,-1) node[kernel] (Y) {$\kernel{K}^{\RV{Y}|\RV{XHD}}$}
        ++ (2,0) node (Xr) {$\RV{X}$}
        +  (0,-1) node (Yr) {};
        \draw (H) -- (X) -- (Xr);
        \draw (copy0) to [out=-90,in=180] ($(Y.west) + (0,0)$) 
              (copy1) to [out=-90,in=180] ($(Y.west) + (0,0.15)$)
              (D) -- ($(Y.west) + (0,-0.15)$);
        \draw[-{Rays[n=8]}] (Y) -- (Yr);
    \end{tikzpicture}\\
    &= \begin{tikzpicture}
        \path (0,0) node[dist] (H) {$\mu$}
        + (0,-1.) node (D) {$\RV{D}$}
        ++ (1.2,0) node[kernel] (X) {$\kernel{K}^{\RV{X}|\RV{H}}$}
        ++ (2.5,0) node (Xr) {$\RV{X}$}
        +  (0,-1) node (Yr) {};
        \draw (H) -- (X) -- (Xr);
        \draw[-{Rays[n=8]}] (D) -- (Yr);
    \end{tikzpicture}
\end{align}

And so $\RV{X}\CI_{\kernel{L}}\RV{D}$.
\end{proof}

In addition, any see-do forecast can be interpreted as a see-do model with a single hypothesis. Recalling the discussion of the indiscrete space $\{*\}$ in \ref{sec:string_diagram_elements}, we can identify a Markov kernel $\kernel{F}:D\to \Delta(\sigalg{E})$ with a Markov kernel $\kernel{T}:\{*\}\times D\to \Delta(\sigalg{E})$ where $\kernel{T}_{*,d}=\kernel{F}_d$ for all $d\in D$. Defining the hypothesis $\RV{H}:\{*\}\times D\times E\to H$ given by the constant fuction $(*,d,e)\mapsto *$, we can create from any see-do forecast $(\kernel{F},\RV{D},\RV{X},\RV{Y})$ a see-do model $(\RV{T},\RV{H},\RV{D},\RV{X},\RV{Y})$ (the required conditional independence is observed by construction in the single hypothesis $*$). However, this single hypothesis model is typically not a \emph{frequentist model}.

\citet{de_finetti_foresight_1992} has shown how frequentist models in particular can be recovered from exchangeable forecasts. Informally speaking, if and only if a forecast $(\prob{P},\RV{O})$ has the property that distribution of a sequence of random variables $\prob{P}^{\RV{X}_1\RV{X}_2\RV{X}_3}$ is identical to the distribution of any permutation of the sequence $\prob{P}^{\RV{X}_2\RV{X}_1\RV{X}_3}$ (an assumption known as \emph{exchangeability}), and this sequence can be extended infinitely, then there exists a hypothesis class $(H,\sigalg{H})$, a Markov kernel $\kernel{Q}:H\to \Delta(\sigalg{O})$ and a \emph{prior} $\mu\in \Delta(\sigalg{H})$ such that

\begin{align}
    \prob{P}^{\RV{X}_1\RV{X}_2\RV{X}_3} = \begin{tikzpicture}
        \path (0,0) node[dist] (P) {$\mu$}
        ++ (0.7,0) node[copymap] (copy0) {}
        ++ (0.5,0.5) node[kernel] (Q1) {$\kernel{Q}$}
        +  (0,-0.5) node[kernel] (Q2) {$\kernel{Q}$}
        +  (0,-1) node[kernel] (Q3) {$\kernel{Q}$}
        ++ (1,0) node (X1) {$\RV{X}_1$}
        + (0,-0.5) node (X2) {$\RV{X}_2$}
        + (0,-1) node (X3) {$\RV{X}_3$};
        \draw (P) -- (copy0);
        \draw (copy0) to [out=45,in=180] (Q1) (copy0) to [out=0, in=180] (Q2) (copy0) to [out=-45,in=180] (Q3);
        \draw (Q1) -- (X1) (Q2) -- (X2) (Q3) -- (X3);
    \end{tikzpicture}
\end{align}

Defining the hypothesis $\RV{H}:O\mapsto H$ such that $\prob{P}^{\RV{H}}=\mu$ and $\prob{P}^{\RV{O}|\RV{H}}=\kernel{Q}$, $(\kernel{Q},\RV{H},\RV{O})$ is a statistical model.

In the following section, we extend this result to the case of see-do forecasts. We first consider a model that is exchangeable in the observations only, and then introduce the notion of functional exchangeability which is a generalisation of exchangeability to Markov kernels. Finally, we prove a representation theorem for see-do forecasts that are both exchangeable in observations and functionally exchangeable in consequences.

\begin{definition}[Permutations and swaps]\label{def:permut_swap}
A \emph{finite permutation} $\rho'$ on $B\subseteq\mathbb{N}$ is a map $B\to B$ such that there is some finite $A\subset B$ for which $\rho'|_A:A\to A$ is an invertible function and $\rho'|_{B\setminus A} = \mathrm{Id}_{B\setminus A}$.

Given measureable space $(E,\sigalg{E})$ and a set of random variables $\{\RV{X}_i|i\in B\}$ the swap function $\rho^{\RV{X}}:E\to E$ associated with a finite permutation $\rho:B\to B$ and the random variables $\{\RV{X}_i\}_B$ is a $\sigalg{E}$ measurable function which has the property $\RV{X}_i\circ \rho^{\RV{X}}=\RV{X}_{\rho'(i)} $ for all $i\in B$, and for any $\RV{Y}:E\to Y$ with $\RV{Y}(\RV{X}^{-1}(A))=Y$ for any $A\in \sigalg{E}$, $Y\circ\rho^{\RV{X}}=\RV{Y}$. This swap function also has an associated Markov kernel $R:=\kernel{F}_{\rho^{\RV{X}}}$.

For example, if $E = Y\times X_1^{|B|}$ and $\RV{X}_i:E\to X_1$ projects the $i$-th ``x'' element of the space $(y,x_1,...,x_i,...)\mapsto x_i$, then for some finite permuation $\rho$ the associated swap is the the fuction $\rho^{\RV{X}}:(y,x_1,...,x_i,...)\mapsto (y,x_{\rho'(1)},...,x_{\rho'(i)},...)$.
\end{definition}

\begin{align}
    R \utimes_{i\in B} \kernel{F}_{\RV{X}_i} &= \utimes_{i\in B} R \kernel{F}_{\RV{X}_{i}}\label{eq:determ_commute}\\
                                               &= \utimes_{i\in B} \kernel{F}_{\RV{X}_{\rho(i)}}\label{eq:function_composition}
\end{align}

Where line \ref{eq:determ_commute} follows from the fact that deterministic kernels commute with the split map (\ref{eq:composition}), and line \ref{eq:function_composition} follows from the fact that for two functional kernels 

\begin{align}
    (\kernel{F}_{f}\kernel{F}_g)_x(A) &= \int_X (\kernel{F}_g)_y(A) d(\kernel{F}_f)_x(y)\\
                                      &= \int_X \delta_{g(y)}(A) d\delta_{f(x)}(y)\\
                                      &= \delta_{g(f(x))} (A)\\
                                      &= (\kernel{F}_{g\circ f})_x(A) 
\end{align}

\todo[inline]{lemmafy, move to chapter 2}

\begin{definition}[Partial frequencies]\label{def:partial_freq}
Given a standard measurable space $(E,\sigalg{E})$ along with random variables $\RV{X}_i:E\to X_1$ for each $i\in \mathbb{N}$, for $A\in \sigalg{X}_1$ and $m\leq n\in \mathbb{N}$ define the size $n$, $m$-tuple \emph{partial frequency of} $A$ with respect to $\RV{X}:=\utimes_{i\in\mathbb{N}}\RV{X}_i$ to be $\RV{Z}^{m,n}_A:=\frac{(n-m)!}{n!}\sum_{I\subset [n]} \prod_{i\in I} \mathds{1}_A\circ \RV{X}_i$ where $I$ ranges over all $m$-sized ordered subsets of $n$.

Define the $m$-tuple \emph{relative frequency of } $A$ with respect to $\RV{X}$ to be $\RV{Z}^{m,\infty}_A:= \lim_{n\to \infty}\frac{(n-m)!}{n!}\sum_{I\in [n]} \prod_{i\in I} \mathds{1}_A\circ \RV{X}_i$.

Given a countable set $\sigalg{G}$ generating $\sigalg{X}_1$ (i.e. $\sigma(\sigalg{G})=\sigalg{X}_1)$, define the $m$-tuple \emph{relative frequency of} $\RV{X}$ to be $\RV{Z}^{m,\infty}:=\utimes_{A\in G} \RV{Z}^{m,\infty}_A$, if such a limit exists for all $A$.

For the special case of $m=1$, let $\RV{Z}:=\RV{Z}^{1,\infty}$
\end{definition}

\begin{definition}[Exchangeable $\sigma$-algebra]\label{def:exchange_sig_alb}
Given a set of random variables $\RV{X}_i:E\to X_1$ for each $i\in \mathbb{N}$, with $X=X_1^{\mathbb{N}}$, a \emph{n-place symmetric function} $f:X\to W$ is a function for which $f = f\circ \rho$ for any permuation $\rho:\mathbb{N}\to\mathbb{N}$ such that $i>n\implies\rho(i)=i$. 

The \emph{n-place exchangeable $\sigma$-algebra} (with respect to the random variable $\RV{X}_i$), $\sigalg{H}^n$, is the $\sigma$-algebra generated by all n-place symmetric functions, and $\sigma{H}=\cap_{n\in \mathbb{N}} \sigalg{H}^n$. 

For standard measurable $(E,\sigalg{E})$ and $n\in \mathbb{N}$, a size $n$ swap function $\rho^{\RV{X}n}:E\to E$ is a swap function associated with a permutation $\rho^{\prime n}$ with the property $i>n\implies \rho^{\prime n}(i)=i$. An $n$-symmetric set $S\subset E$ has the property $\rho^{\RV{X}n}(S)=S$ for all size $n$ swap functions $\rho^{\RV{X}n}$. Define the symmetric sets $\sigalg{S}^n$ $n$-symmetric sets, and $\sigalg{S}=\cap_{n\in\mathbb{N}} \sigalg{S}^n$. Given random variables $\RV{X}_i:E\to X_1$ for each $i\in\mathbb{N}$, the size $n$ \emph{exchangeable $\sigma$-algebra} with respect to $\RV{X}:=\utimes_{i\in\mathbb{N}}\RV{X}_i$, denoted $\sigalg{H}^n\subset\sigma(\RV{X})$, is the $\sigma$-algebra generated by $\{\RV{X}^{-1}(A)|A\in\sigalg{X}\}\cap S^n$.

\end{definition}

\begin{lemma}
The size $n$ exchangeable sigma algebra $\sigalg{H}^n$ on $(E,\sigalg{E})$ with respect to $\RV{X}:=\utimes_{i\in\mathbb{N}}\RV{X}_i$ has the property $\rho^{\RV{X}n}(A)=A$ for all $A\in \sigalg{H}^n$, and all size $n$ swap functions $\rho^{\RV{X}n}$.
\end{lemma}

\begin{proof}
Let $W_f$ be the codomain of a function $f$, and $\sigalg{W}_f$ its $\sigma$-algebra. $\sigalg{H}^n$ is generated by $\sigalg{G}^n=\{f^{-1}(A)|A\in \sigalg{W}_f,f\text{ n-place symmetric}\}$. By the definition of n-place symmetric functions, any set of the form $f^{-1}(A)=(f\circ\rho^{\RV{X}})^{-1}(A) = (\rho^{\RV{X}}){-1}(f^{-1}(A))$. Because every $n$-place permutation $\rho$ has an inverse $\rho^{-1}$ that is also an $n$-place permutation, all sets in $B\in\sigalg{G}$ have the property $\rho^{\RV{X}}(B)=B$ for all $n$-place permuations $\rho$.

Define $\sigalg{S}^n$ to be all subsets of $E$ such that for $B\in \sigalg{S}^n$, $n$-place permuations $\rho$, $\rho^{\RV{X}}(B)=B$. $\sigalg{S}^n$ is a $\sigma$-algebra, and it contains $\sigalg{G}^n$, so it also contains $\sigalg{H}^n$.

Take $A\in\sigalg{S}^n$. By assumption, for any $\omega\in A$, $\rho^{\RV{X}}(\omega)\in A$ for all size $n$ swap functions $\rho^{\RV{X}}$. Consider $\omega\not\in A$, and suppose there is some swap function $\rho^{\RV{X}}$ such that $\rho^{\RV{X}}(\omega)\in A$. By definition, the permutation $\rho$ has an inverse $\rho^{-1}$ which is also a size $n$ permutation. By construction, $(\rho^{-1})^{\RV{X}}$ is also the inverse of $\rho^{\RV{X}}$. Thus $(\rho^{-1})^{\RV{X}}(\omega)\not\in A$ and so $\omega\not\in A$, a contradiction. Thus $E\setminus A\in \sigalg{G}^n$.

For any invertible function $f:E\to E$, $f(E)=E$. Thus $E\in \sigalg{G}^n$.

Finally, for a countable collection $A_1,A_2,...$ and any size $n$ swap function $\rho^{\RV{X}}$, $\rho^{\RV{X}}(\cup_{i=1}^{\infty} A_i) = \cup_{i=1}^\infty \rho^{\RV{X}}(A_i) = \cup_{i=1}^\infty A_i$. Thus $\sigalg{S}^n$ is a $\sigma$-algebra, and by the monotone class theorem it contains $\RV{H}^n$.
\end{proof}

\begin{definition}[Exchangeability]
Given a see-do forecast $(\kernel{T},\RV{D},\RV{X},\RV{Y})$ with the property that $\RV{X}:=\utimes_{i\in A} \RV{X}_i$ for some set of random variables $\{\RV{X}_i|i\in A\}$ all taking values in $X_1$ where $A\subseteq \mathbb{N}$. 

If for every finite $B\subset A$ and every permutation $\rho':B\to B$ of $B$ we have $\kernel{T}\rho=\kernel{T}$, where $\rho$ is the swap associated with $\rho'$ and $\{\RV{X}_i|i\in B\}$, then $(\kernel{T},\RV{D},\RV{X},\RV{Y})$ is \emph{exchangeable} with respect to $\{\RV{X}_i|i\in A\}$.

If $A$ is an infinite set then $\kernel{T}$ is \emph{infinitely exchangeable}, and if $\kernel{T}=\kernel{S}(\mathrm{Id}_{X}\otimes \stopper{0.15}\otimes \mathrm{Id}_Y)$ for some infinitely exchangeable $(\kernel{S},\RV{D},\RV{X}',\RV{Y}')$, then $\kernel{T}$ is infinitely exchangeably extendable.
\end{definition}

Note that $\kernel{T}R^{\RV{X}_i|\RV{D}} = \kernel{T}^{\RV{X}_{\rho(i)}|\RV{D}}$.

This implies the usual notion of exchangeability if we take $Y=\{*\}$ (that is, if we assume the consequences are trivial), as by assumption $\RV{X}$ is independent of $\RV{D}$.

\begin{lemma}[Infinitely exchangeably extendable forecasts]\label{lem:partial_representation}
Given a forecast $(\prob{P},\RV{X})$ where $\RV{X}=\utimes_{i\in A} \RV{X}_i$ for some $\{\RV{X}_i|i\in \mathbb{N}\}$ and $X$ is standard measurable, if $\prob{P}$ is exchangeable with respect to $\RV{X}$ then there exists a function $f:X\to H$ such that, defining $\RV{Z}:f\circ \RV{X}$:
    \begin{itemize}
        \item $\RV{X}_i\CI\RV{X}_{A\setminus\{i\}}|\RV{Z}$ for all $i\in A$
        \item $\prob{P}^{\RV{X}_i|\RV{Z}}=\prob{P}^{\RV{X}_j|\RV{Z}}$ for all $i,j\in A$
    \end{itemize}
Call $\RV{Z}$ the \emph{hypothesis}.
\end{lemma}

\begin{proof}
Without loss of generality, assume $X_1=[0,1]$, $\sigalg{X}=\mathcal{B}([0,1])$ and $\RV{X}=[0,1]^{\mathbb{N}}$.

Let $\mathbb{Q}$ be the rationals between $[0,1]$ and define $\RV{Z}_q^n:D\times X\times Y \to [0,1]$ by $\omega \mapsto \frac{1}{n}\sum_{i}^n \mathds{1}_{[0,q)}(\RV{X}_i(\omega))$ Let $\sigalg{Z}^n_q=\sigma(\RV{Z}_q^n)$, i.e. $\RV{Z}^n$ is a 1-tuple partial frequency as in Definition \ref{def:partial_freq}.

$\RV{Z}^n\circ\rho^{\RV{X}n}=\RV{Z}^n$ for any size $n$ swap function $\rho^{\RV{X}n}$, so $\RV{Z}^n$ is $\sigalg{H}^n$-measurable.

Let $\rho'_{ij}:\mathbb{N}\to\mathbb{N}$ swaps indices $i$ and $j$ for some $i,j\in[n]$ and otherwise acts as the identity. $\rho_{ij}:D\times X\times Y\to \Delta(\sigalg{D}\times \sigalg{X}\times \sigalg{Y})$ is the swap kernel associated with $\rho'_{ij}$ and $\{\RV{X}_i|i\in \mathbb{N}\}$, and $\rho^{\RV{X}}_{ij}$ the function associated with $\rho_{ij}$. For any $m$, $n$, $A\in \sigalg{H}^n$, $d\in D$: 

\begin{align}
    \int_{A} \RV{Z}_q^{n} (\omega) d\prob{P}(\omega) &= \int_{A} \frac{1}{n}\sum_{i}^{n} (\mathds{1}_{[0,q)}\circ \RV{X}_i)(\omega)d\prob{P}(\omega)\\
    &= \frac{1}{n}\sum_{i}^{n} \int_{(\rho^{\RV{X}}_{ij})^{-1}\rho^{\RV{X}}_{ij}(A)} (\mathds{1}_{[0,q)}\circ \RV{X}_i)(\omega)d\prob{P}\rho_{ij}(\omega)\label{eq:permutation_invertible}\\
    &= \frac{1}{n}\sum_{i}^{n} \int_{\rho^{\RV{X}}_{ij}(A)} (\mathds{1}_{[0,q)}\circ \RV{X}_{i}\circ \rho_{ij}) (\omega)d\prob{P}(\omega)\label{eq:using_push2}\\
    &= \frac{1}{n}\sum_{i}^{n} \int_{A} (\mathds{1}_{[0,q)}\circ \RV{X}_{j})(\omega)d\prob{P}(\omega)\label{eq:closure_under_permutation}\\
    &= \int_{A} (\mathds{1}_{[0,q)} \circ \RV{X}_j)(\omega)d\prob{P}(\omega) \label{eq:cond_expectation_first}
\end{align}

Where line \ref{eq:permutation_invertible} follows from exchangeability of $\kernel{T}$ and invertibility of $\rho_{ij}$. Line \ref{eq:using_push2} follows from the fact that $\prob{P}\rho_{ij}$ is the pushforward measure of $\prob{P}$ with respect to $\rho^{\RV{X}}_{ij}$ and \ref{eq:closure_under_permutation} uses the fact that $\rho(A) = A$ for all $A\in \RV{Z}^n_q$ and all permutations $\rho$.

From Equation \ref{eq:cond_expectation_first}, we have for all $n$, $A\in \sigalg{H}^{n+1}$

\begin{align}
    \int_{A} \RV{Z}_q^{n+1} (\omega) d\prob{P}(\omega) &= \int_{A} \RV{Z}_q^{n} (\omega) d\prob{P}(\omega)
\end{align}

Because $\RV{Z}_q^{n+1}$ is $\sigalg{H}^{n+1}$ measurable, $\RV{Z}_q^{n+1} = \mathbb{E}[\RV{Z}_q^{n}|\sigalg{H}^{n+1}]$.

Thus the sequence $[\RV{Z}^1_q,\RV{Z}^2_q,...]$ is a backwards martingale with respect to the reversed filtration $\sigalg{H}^1\supset\sigalg{H}^2\supset...\supset \sigalg{H}^3$.

Furthermore, for all $n\in \mathbb{N}$, $\sup_\omega|\RV{Z}^n(\omega)|\leq 1$ so the sequence is also uniformly integrable. Thus it goes almost surely to the limit $\RV{Z}_q$, and for all $A\in \sigalg{H}$

\begin{align}
    \lim_{n\to\infty} \int_A \RV{Z}^n_q(\omega) d\prob{P}(\omega) &= \int_A \RV{Z}_q(\omega) d\prob{P}(\omega)
\end{align}

Finally, because for all $n\in \mathbb{N}$, all $j\in[n]$ and all $A\in \sigalg{H}^n$ we also have

\begin{align}
    \int_A \mathds{1}_{[0,q)}(\RV{X}_j(\omega))d\prob{P}(\omega) &= \int_A \RV{Z}_q^n(\omega) d\prob{P}(\omega)
\end{align}

it follows that for all $A\in \sigalg{H}$, $j\in \mathbb{N}$

\begin{align}
    \int_A \RV{Z}_q(\omega) d\prob{P}(\omega) = \int_A \mathds{1}_{[0,q)}(\RV{X}_j(\omega))d\prob{P}(\omega)\label{eq:cond_expectation}
\end{align}

[\citet{cinlar_probability_2011} Thm 4.7.]. Thus $\RV{Z}_q = \mathbb{E}[\mathds{1}_{[0,q)}\circ\RV{X}_j|\sigalg{H}]$ for all $j\in \mathbb{N}$. This implies $\RV{Z}_q$ is a version of $\mathbb{E}[\mathds{1}_{[0,q)}\circ \RV{X}_j|\sigalg{H}]$.

Define $\RV{Z} = \utimes_q\in \mathbb{Q} \RV{Z}_q$. As $\sigma(\RV{Z})\subset\sigalg{H}$, Equation \ref{eq:cond_expectation} establishes in addition that $\RV{Z}_q$ is a version of $\mathbb{E}[\mathds{1}_{[0,q)}\circ \RV{X}_j|\sigma(Z)]$. 

By the definition of conditional expectation, for any version of $\prob{P}_{\RV{Z}(\omega)}^{\RV{X}_j|\RV{Z}\RV{D}}([0,q))$ we have

\begin{align}
    \prob{P}_{\RV{Z}(\omega)}^{\RV{X}_j|\RV{Z}}([0,q)) &= \mathbb{E}[\mathds{1}_{[0,q)}\circ \RV{X}_j|\sigma(\RV{Z})](\omega)\\
\end{align}

$\prob{P}$-almost surely.

Furthermore, the measure $\prob{P}_{h}^{\RV{X}_j|\RV{Z}}$ is uniquely defined by its value on $[0,q)$ for all $q\in \mathbb{Q}$. Thus for all $i,j\in \mathbb{N}$ we have

\begin{align}
    \prob{P}^{\RV{X}_j|\RV{H}} = \prob{P}^{\RV{X}_i|\RV{H}}
\end{align}

Completing the proof of property 1.

Next, we will show $\RV{X}_i\CI_\kernel{T} \RV{X}_{\mathbb{N}\setminus \{i\}} | \RV{H}$.

Let $\RV{Z}^{m,n}_q$ be a partial frequency as in Definition \ref{def:partial_freq} where $q$ stands for the set $[0,q)$. $\RV{Z}^{m,n}_q\circ \rho^{\RV{X}n}=\RV{Z}^{m,n}_q$ for all size $n$ swap functions so $\RV{Z}^{m,n}_q$  is $\sigalg{H}^n$ measurable.

Let $J\subset[n]$ be some set of $m$ elements from $n$ and $\rho'_{IJ}$ be a permutation that sends the elements of $I\subset[n]$ to $J$.

\begin{align}
    \int_{A} \RV{Z}_q^{m,n} (\omega) d\prob{P}(\omega) &= \int_{A} \frac{(n-m)!}{n!}\sum_{I\subset[n]}\prod_{i\in I} (\mathds{1}_{[0,q)}\circ \RV{X}_i)(\omega)d\prob{P}(\omega)\\
    &= \frac{(n-m)!}{n!}\sum_{I\subset[n]} \int_{(\rho^{\RV{X}}_{IJ})^{-1}\rho^{\RV{X}}_{IJ}(A)} \prod_{i\in I}(\mathds{1}_{[0,q)}\circ \RV{X}_i)(\omega)d\prob{P}\rho_{IJ}(\omega)\\
    &= \frac{(n-m)!}{n!}\sum_{I\subset[n]} \int_{\rho^{\RV{X}}_{IJ}(A)} \prod_{i\in I}(\mathds{1}_{[0,q)}\circ \RV{X}_{i}\circ \rho_{IJ}) (\omega)d\prob{P}(\omega)\label{eq:using_pushforward}\\
    &= \frac{(n-m)!}{n!}\sum_{I\subset[n]} \int_{A} \prod_{j\in J}(\mathds{1}_{[0,q)}\circ \RV{X}_{j})(\omega)d\prob{P}(\omega)\label{eq:closure_under_permutation2}\\
    &= \int_{A} \prod_{j\in J}(\mathds{1}_{[0,q)} \circ \RV{X}_j)(\omega)d\prob{P}(\omega) \label{eq:cond_expectation_first2}
\end{align}

From Equation \ref{eq:cond_expectation_first2}, we have for all $n$, $m<n$, $A\in \sigalg{H}^{n+1}$

\begin{align}
    \int_{A} \RV{Z}_q^{m,n+1} (\omega) d\prob{P}(\omega) &= \int_{A} \RV{Z}_q^{m,n} (\omega) d\prob{P}(\omega)
\end{align}

Because $\RV{Z}_q^{m,n+1}$ is $\sigalg{H}^{n+1}$ measurable, $\RV{Z}_q^{m,n+1} = \mathbb{E}[\RV{Z}_q^{m,n}|\sigalg{H}^{n+1}]$.

Thus the sequence $[\RV{Z}^{m,1}_q,\RV{Z}^{m,2}_q,...]$ is a backwards martingale with respect to the reversed filtration $\sigalg{H}^1\supset\sigalg{H}^2\supset...\supset \sigalg{H}^3$.

Furthermore, for all $n\in \mathbb{N}$, $\sup_\omega|\RV{Z}^{m,n}_q(\omega)|\leq 1$ so the sequence is also uniformly integrable. Thus it goes almost surely to a limit $\RV{Z}^{m,\infty}_q$, and for all $A\in \sigalg{H}$

\begin{align}
    \lim_{n\to\infty} \int_A \RV{Z}^{m,n}_q(\omega) d\prob{P}(\omega) &= \int_A \RV{Z}^{m,\infty}_q(\omega) d\prob{P}(\omega)\\
    \implies \RV{Z}^{m,n}_q &= \mathbb{E}[\prod_{j\in J} \mathds{1}_{[0,q)}\circ \RV{X}_j|\sigalg{H}]
\end{align}

Let $[n]^m_\mathrm{rep}$ be the set of all $m$ length sequences of elements of $[n]$ with repeats. Note that $\lim_{n\to\infty}\frac{(n-m)!}{n!}|[n]^m_\mathrm{rep}| = \lim_{n\to\infty}\frac{n^m(n-m)!}{n!}-1 = 0$

\begin{align}
    \mathbb{E}[\prod_{j\in J} \mathds{1}_{[0,q)}\circ \RV{X}_j|\sigalg{H}](\omega) &= \lim_{n\to\infty}\frac{(n-m)!}{n!}\sum_{I\subset[n]}\prod_{i\in I} (\mathds{1}_{[0,q)}\circ \RV{X}_i)\\
                                                                      &= \lim_{n\to\infty}\frac{1}{n^m}\sum_{I\subset[n]}\prod_{i\in I}(\mathds{1}_{[0,q)}\circ \RV{X}_i)\\
                                                                      &= \lim_{n\to\infty}\frac{1}{n^m}\left[\sum_{i_1\in [n]}...\sum_{i_m\in [n]} (1_{[0,q)}\circ \RV{X}_{i_k})-\sum_{J\subset[n]^m_\mathrm{rep}}\prod_{i\in I} (\mathds{1}_{[0,q)}\circ \RV{X}_i)\right]\\ 
                                                                      &= \prod_{k\in [m]} \lim_{n\to\infty} \frac{1}{n}\sum_{i_k\in[n]}(1_{[0,q)}\circ\RV{X}_{ik})\\
                                                                      &= \prod_{k\in [m]} \mathbb{E}[\mathds{1}_{[0,q)}\circ \RV{X}_j|\sigalg{H}]\\
                                                                      &= \prod_{k\in J} \mathbb{E}[\mathds{1}_{[0,q)}\circ \RV{X}_j|\sigalg{H}]\\
                                                                      &= \prod_{k\in J} \RV{Z}_q\label{eq:h_measurable}
\end{align}

Because $\mathbb{E}[\prod_{j\in J} \mathds{1}_{[0,q)}\circ \RV{X}_j|\sigalg{H}]$ is $\RV{Z}$-measurable we also have

\begin{align}
    \mathbb{E}[\prod_{j\in J} \mathds{1}_{[0,q)}\circ \RV{X}_j|\sigma(\RV{Z})] &= \prod_{k\in J} \RV{Z}_q
\end{align}

By the definition of conditional expectation, for all $J\subset \mathbb{N}$

\begin{align}
    \prob{P}_{\RV{H}(\omega)}^{\RV{X}_J|\RV{H}}(\times_{j\in J}[0,q)) &= \mathbb{E}[\prod_{j\in J} \mathds{1}_{[0,q)}\circ \RV{X}_j|\sigalg{H}](\omega)\\
                                                                      &= \prod_{k\in J} \prob{P}_{\RV{H}(\omega)}^{\RV{X}_j|\RV{H}}([0,q))
\end{align}

And thus $\RV{X}_i\CI\RV{X}_{\mathbb{N}\setminus\{i\}}|\RV{Z}$ for all $i\in\mathbb{N}$.
\end{proof}

\begin{lemma}[Independent and identically distributed random variables]\label{lem:iid_rvs}
Suppose we have a forecast $(\prob{P},\RV{X})$ where $\RV{X}=\utimes_{i\in A} \RV{X}_i$ for some $\{\RV{X}_i|i\in \mathbb{N}\}$ and $X$ is standard measurable, and $\prob{P}$ is exchangeable with respect to $\RV{X}$. Furthermore, suppose we have $\RV{V}=\utimes_{i\in\mathbb{N}} f\circ \RV{X}_i$ for some measurable $f:X_1\to V_1$ such that $\RV{V}$ is independent and identically distributes - that is, $\prob{P}^{\RV{V}} = \otimes_{i\in\mathbb{N}} \prob{P}^{\RV{V}_1}$. Then, letting $\RV{Z}$ be the hypothesis from Lemma \ref{lem:partial_representation}, $\RV{V}\CI\RV{Z}$.
\end{lemma}

\begin{proof}

We have by Lemma \ref{lem:partial_representation} that fo rany measurable $g:Z_1\to \mathbb{R}$,
\begin{align}
    \mathbb{E}[g(\RV{V}_1)|\sigma(\RV{Z})] = \lim_{n\to\infty} \frac{1}{n}\sum_i^n g(\RV{Z}_i)
\end{align}

However, by the strong law of large numbers (\citet{cinlar_probability_2011}, pg 119), because the $\RV{V}_i$ are independent and identically distributed

\begin{align}
    \mathbb{E}[g(\RV{V}_1)] &\overset{a.s.}{=} \lim_{n\to\infty}\frac{1}{n}\sum_i^n g(\RV{Z}_i)\\
                            &= \mathbb{E}[g(\RV{V}_1)|\sigma(\RV{Z})]
\end{align}

Thus $\RV{V}\CI\RV{Z}$.

\end{proof}

\begin{lemma}[Representation of infinitely exchangeably extendable see-do forecasts]\label{lem:rep_seedo_obs}
Given a see-do forecast $(\kernel{T},\RV{D},\RV{X},\RV{Y})$ where $\RV{X}=\utimes_{i\in \mathbb{N}} \RV{X}_i$ for some $\{\RV{X}_i|i\in \mathbb{N}\}$ and $X\times Y$ is standard measurable, if $(\kernel{T},\RV{D},\RV{X},\RV{Y})$ is infinitely exchangeable with respect to $\{\RV{X}_i|i\in \mathbb{N}\}$ then there exists a function $f:X\to Z$ such that, defining $\RV{Z}:f\circ \RV{X}$:
\begin{itemize}
    \item $\RV{X}_i\CI\RV{X}_{\mathbb{N}\setminus\{i\}}|\RV{Z}$ for all $i\in A$
    \item $\kernel{T}^{\RV{X}_i|\RV{Z}}=\kernel{T}^{\RV{X}_j|\RV{Z}}$ for all $i,j\in A$
    \item $\RV{Y}\CI\RV{X}|\RV{Z}\utimes \RV{D}$
\end{itemize}
\end{lemma}

\begin{proof}

Without loss of generality, assume $X_1=Y=[0,1]$.

$\kernel{T}$ is a see-do forecast, so $\RV{X}\CI_{\kernel{T}}\RV{D}$. Thus there exists a marginal $\kernel{T}^{\RV{X}}$ independent of $\RV{D}$.

From Lemma \ref{lem:partial_representation}, $\kernel{T}^{\RV{X}_i|\RV{Z}}=\kernel{T}^{\RV{X}_j|\RV{Z}}$ for all $i,j\in \mathbb{N}$ and $\RV{X}_i\CI\RV{X}_{\mathbb{N}\setminus\{i\}} |\RV{Z}$.

We will show $\RV{Y}\CI\RV{X}|\RV{Z}\utimes\RV{D}$. 

For any swap function $\rho^{\RV{X}}$ there is, by definition, a permutation of indices $\rho'$ such that $\RV{X}\circ\rho^{\RV{X}}(\omega) = [\RV{X}_{\rho'(1)}(\omega),\RV{X}_{\rho'(2)}(\omega),...]$. Define $\rho'':[0,1]^{\mathbb{N}}\to[0,1]^{\mathbb{N}}$ to be the bijective map that performs the permutation in the codomain of $\RV{X}$, i.e. $\RV{X}\circ\rho^{\RV{X}} = \rho''\circ\RV{X}$.

Consider some $B\in \sigalg{B}([0,1])^\mathbb{N}$ and its preimage $\RV{X}^{-1}(B) = \{\omega|\RV{X}(\omega)\in B\}$, and some finite swap function $\rho^{\RV{X}}$. Then there exists $\rho''(B)\in [0,1]^{\mathbb{N}}$ such that $(\RV{X}\circ\rho^{\RV{X}})^{-1}(\rho''(B)) = \{\omega|\RV{X}(\rho^{\RV{X}}(\omega))\in \rho''(B)\} = \{\omega|\RV{X}\in B\}$. Thus $\sigma(\RV{X})=\sigma(\RV{X}\circ\rho^{\RV{X}})$ for any finite swap function $\rho^{\RV{X}}$.

Because $E=X\times Y$ and $\RV{X}$ forgets the $Y$-component of any $(x,y)\in E$, for any $A,B\in\sigalg{B}([0,1])$ there is some $C\subset X$ such that $\mathds{1}_{A}\circ\RV{Y}(\RV{X}^{-1}(B)) = \mathds{1}_{A}\circ\RV{Y} (C\times Y) = [0,1]$. Thus for any finite swap function $\rho^{\RV{X}}$, $\mathds{1}_{A}\circ\RV{Y}\circ \rho^{\RV{X}} = \mathds{1}_{A}\circ\RV{Y}$.

For $A\in \sigma(\RV{X})$:

\begin{align}
    \int_{A} \mathbb{E}[\mathds{1}_A\circ\RV{Y}|\sigma(\RV{X})] \circ\rho^{\RV{X}} d\kernel{T}_d &= \int_{\rho^{\RV{X}}(A)} \mathbb{E}[\mathds{1}_A\circ\RV{Y}|\sigma(\RV{X})] d(\kernel{T}R^{-1})_d\\
                                                                         &= \int_{\rho^{\RV{X}}(A)} \mathbb{E}[\mathds{1}_A\circ\RV{Y}|\sigma(\RV{X})] d\kernel{T}_d \\
                                                                         &= \int_{\rho^{\RV{X}}(A)} \mathds{1}_A\circ\RV{Y}d\kernel{T}_d\\
                                                                         &= \int_A \mathds{1}_A\circ\RV{Y} d\kernel{T}_d
\end{align}

It follows that $\mathbb{E}[\mathds{1}_A\circ\RV{Y}|\sigma(\RV{X})] \circ\rho^{\RV{X}}$ is a version of $\mathbb{E}[\mathds{1}_A\circ\RV{Y}|\sigma(\RV{X})]$. Because there are only a countable number of finite swap functions, it also follows that a version of $\mathbb{E}[\mathds{1}_A\circ\RV{Y}|\sigma(\RV{X})]$ that is $\sigalg{H}$-measurable exists (i.e. for which $\mathbb{E}[\mathds{1}_A\circ\RV{Y}|\sigma(\RV{X})]\circ\rho^{\RV{X}} = \mathbb{E}[\mathds{1}_A\circ\RV{Y}|\sigma(\RV{X})]$, for all $\rho^{\RV{X}}$).

Consider also for some swap function $\rho^{\RV{X}}$, $B\in \sigalg{B}([0,1])$:

\begin{align}
    \int_{\RV{X}_i^{-1}(B)} \mathbb{E}[\RV{V}|\sigma(\RV{X}_j)] \circ \rho^{\RV{X}}_{ji} d\kernel{T}_d &= \int_{\rho^{\RV{X}}_{ji}(\RV{X}_i^{-1}(B))} \mathbb{E}[\RV{V}|\sigma(\RV{X}_j)] d(\kernel{T}R_{ij})_d\\
                                                                               &= \int_{\RV{X}^{-1}_{j}(B)} \mathbb{E}[\RV{V}|\sigma(\RV{X}_j)] d\kernel{T}_d\\
                                                                               &= \int_{\RV{X}_j^{-1}(B)} \RV{V} d\kernel{T}_d\\
                                                                               &= \int_{\rho_{ji}^*(\RV{X}_j^{-1}(B))}\RV{V}\circ \rho^{\RV{X}}_{ji} d(\kernel{T}R_{ji})_d\\
                                                                               &= \int_{\RV{X}_i^{-1}(B)} \RV{V}d\kernel{T}_d
\end{align}

Thus $\mathbb{E}[\RV{V}|\sigma(\RV{X}_j)]\circ\rho^{\RV{X}}_{ji}$ is a version of $\mathbb{E}[\RV{V}|\sigma({\RV{X}_i})]$.

Define $\RV{W}^n_q := \frac{1}{n}\sum_{i\in [n]} \mathbb{E}[\mathds{1}_{[0,q]}\RV{Y}|\sigma(\RV{X}_i)]$. Note that for any size $n$ swap function $\rho^{\RV{X}}$, $\RV{W}^n_q\circ\rho^{\RV{X}}=\RV{W}_q^n$, therefore $\RV{W}_q^n$ is $\sigalg{H}^n$-measurable. 

Consider $\omega,\omega'$ such that $\RV{W}_q^n(\omega)\neq \RV{W}_q^n(\omega')$. Then there exists no size $n$ swap function $\rho^{\RV{X}}$ such that $\RV{X}_{[n]}(\omega)=\RV{X}_{[n]}(\rho^{\RV{X}}(\omega'))$. Without loss of generality, suppose $\RV{X}_1(\omega)\leq\RV{X}_2(\omega)\leq...\leq\RV{X}_n(\omega)$ and $\RV{X}_1(\omega')\leq\RV{X}_2(\omega')\leq...\leq\RV{X}_n(\omega')$. Then there is some first index $j$ for which $\RV{X}_{j}(\omega)> \RV{X}_{j}(\omega')$, and some and some rational $r$ such that $\RV{X}_j(\omega)>r>\RV{X}_j(\omega')$. Then $\sum_{i}^n \mathds{1}_{[0,q)}(\RV{X}_i(\omega)) > \sum_i^n \mathds{1}_{[0,q)}(\RV{X}_i(\omega'))$ and so $\RV{Z}^n(\omega)\neq\RV{Z}^n(\omega')$ also.

Thus $\RV{W}_q^n$ is $\sigma(\RV{Z}^n)$ measurable. Define $\sigalg{I}^n:=\vee_{n\to\infty} \sigma(\RV{Z}^n)$. Then $\sigalg{I}^1\supset\sigalg{I}^2\supset...\supset\sigma(\RV{Z})=\cap_{i}^\infty \sigalg{I}^i$. In addition, but the $\sigalg{H}^n$-measurability of $\RV{Z}^{>n}$, $\sigalg{I}^n\subset\sigalg{H}^n$ and $\sigalg{I}\subset\sigalg{H}$.

For $A\in\sigalg{I}^n$, $d\in D$:

\begin{align}
    \int_A \RV{W}_q^n d\kernel{T}_d &= \frac{1}{n}\sum_{i\in[n]} \int_A \mathbb{E}[\mathds{1}_{[0,q)}\circ\RV{Y}|\sigma(\RV{X}_i)] d\kernel{T}_d\\
                                  &= \frac{1}{n}\sum_{i\in[n]} \int_{\rho_{ij}^*(A)} \mathbb{E}[\mathds{1}_{[0,q)}\circ\RV{Y}|\sigma(\RV{X}_i)]\circ\rho_{ij}^*d(\kernel{T}R_{ij})_d\\
                                  &= \int_{A} \mathbb{E}[\mathds{1}_{[0,q)}\circ \RV{Y} \sigma(\RV{X}_j)]d\kernel{T}_d\\
                                  &= \int_{A} \mathds{1}_{[0,q)}\circ \RV{Y} d\kernel{T}_d
\end{align}

Where the last line follows from the fact that $\sigalg{I}^n\subset \sigma(\RV{X}_j)$. Therefore $\RV{W}^n_q$ is a version of $\mathbb{E}[\mathds{1}_{[0,q)}\circ|\sigalg{I}^n]$. Furthermore $\RV{W}^1_q,...,\RV{W}^n_q,..$ is a backwards martingale with respect to $\sigalg{I}^1,....,sigalg{I}^n,..\sigalg{I}$ and so it goes to a limit $\RV{W}_q=\mathbb{E}[\mathds{1}_{[0,q)}\circ \RV{V} |\sigma(\RV{Z})]$.

So $\RV{W}_q$ is a $\sigma(\RV{Z})$-measurable version of $\mathbb{E}[\mathds{1}_{[0,q]}\circ\RV{Y}|\sigalg{H}]$ which is itself a version of $\mathbb{E}[\mathds{1}_{[0,q]}\circ\RV{Y}|\sigma(\RV{X})]$. As before, for any $d\in D$, $\omega\in E$ we have

\begin{align}
    \kernel{T}_{\RV{D}(\omega),\RV{X}(\omega),\RV{Z}(\omega)}^{\RV{Y}|\RV{X}\RV{Z}\RV{D}}([0,q)) &= \mathbb{E}[\mathds{1}_{[0,q]}\circ\RV{Y}|\sigalg{H}](\omega)\\
                                                                                                 &= \kernel{T}_{\RV{D}(\omega),\RV{Z}(\omega)}^{\RV{Y}|\RV{Z}\RV{D}}([0,q))
\end{align}

This shows that $\RV{Y}\CI_{\kernel{T}} \RV{X}|\RV{D}\RV{Z}$, as desired.

\end{proof}

Using Lemma \ref{lem:rep_seedo_obs} it is possible to prove a version of De Finetti's representation theorem, which is a well known result \citep{de_finetti_foresight_1992,hewitt_symmetric_1955}. We are interested in establishing an analogous result for consequence maps. This requires the assumption of \emph{functional exchangeability}.

Functional exchangeability is a generalisation of exchangeablility to Markov kernels. It captures the intuition that if we swap the order of the outputs (say, $\RV{Y}_1,\RV{Y}_2$ is swapped to $\RV{Y}_2, \RV{Y}_1$), we need to make analagous exchange of choices ($\RV{D}_1$, $\RV{D}_2$ becomes $\RV{D}_2$, $\RV{D}_1$) in order to maintain the correspondence of choices and outputs.

\begin{definition}[Functional Exchangeability]
\todo[inline]{Maybe include a diagram here? I think the pictorial representation helps with intuition, though it's hard to state as rigorously with pictures}
Given a see-do forecast $(\kernel{T},\RV{D},\RV{X},\RV{Y})$ where $\RV{D}=\utimes_{i\in A}\RV{D}_i$ and $\RV{Y}=\utimes_{i\in A} \RV{X}_i$ for some random variables $\{\RV{D}_i\}_A, \{\RV{Y}_i\}_A$, for any perumtation $\rho:A\to A$ define the observation and choice swap function $\rho^{\RV{D}\RV{Y}}$ and the choice swap function $\rho^{\RV{D}}$ as in \ref{def:permut_swap}. Then $\kernel{T}$ is functionally exchangeable with respect to $\RV{D}$ and $\RV{X}$ if $\kernel{T}=R^{\RV{D}}\kernel{T}R^{\RV{D}\RV{Y}}$.

$\kernel{T}$ is infinitely functionally exchangeably extendable if here exists a do forecast $(\kernel{T}',\RV{D}',\RV{X}')$ non-interfering and functionally exchangeable with respect to $\RV{D}'=\utimes_{i\in\mathbb{N}}\RV{D}'_i$ and $\RV{Y}'=\utimes_{i\in\mathbb{N}}\RV{Y}'_i$ such that for all $B\subset A$

\begin{align}
    \kernel{T}^{\prime \RV{Y}'_B|\RV{D}'_B}=\kernel{T}^{\RV{Y}_B|\RV{D}_B} 
\end{align}

A see-do forecast that is infinitely exchangeably extendable with respect to $\RV{X}$ and infinitely functionally exchangeably extendable with respect to $\RV{D}$, $\RV{Y}$ is \emph{doubly exchangeable} (we omit ``infinitely extendable'' for the sake of brevity).

\end{definition}

The following lemma interprets an exchangeable probability measure as an exchangeable see-do forecast with observations and consequences interchanged. This is so we can re-use Lemma \ref{lem:rep_seedo_obs} without separately proving an almost identical result for probability measures.

\begin{lemma}[Functionally exchangeable see-do models with exchangeable choices induce exchangeable see do models]\label{lem:f-ex2ex}
Given a see-do forecast $(\kernel{T},\RV{D},\RV{X},\RV{Y})$ functionally exchangeable with respect to $\RV{Y}=\utimes_{i\in A} \RV{Y}_i$ and $\RV{D}=\utimes_{i\in A}\RV{D}_i$ and some $\prob{P}^{\RV{D}}$ exchangeable with respect to $\RV{D}$, then $\prob{P}\kernel{T}\in \Delta(\sigalg{D}\otimes\sigalg{X}\otimes\sigalg{Y})$ is exchangeable with respect to $\RV{G}:=\utimes_i\in A \RV{Y}_i\utimes\RV{D}_i$. 

Furthermore, defining the trivial choice $\RV{C}:\{*\}\times X\times Y\to *$, $(\prob{P}\kernel{T},\RV{C},\RV{G},\RV{X})$ is a see-do forecast exchangeable with respect to $\RV{G}$.
\end{lemma}

\begin{proof}

For arbitrary $\rho:A\to A$, associated swap kernel $R^{\RV{D}}$ and $R^{\RV{X}}$:

\begin{align}
    \prob{P}\kernel{T} R^{\RV{D}\RV{X}} &= (\prob{P} R^{\RV{D}}) \kernel{T} R^{\RV{D}\RV{X}}\\
                                  &= \prob{P}(R^{\RV{D}}\kernel{T}R^{\RV{D}\RV{X}})\\
                                  &= \prob{P}\kernel{T}
\end{align}

This is sufficient for exchangeability of $(\prob{P}\kernel{T},\RV{C},\RV{G},\RV{O})$ with respect to $\RV{G}$.
\end{proof}

\begin{theorem}[Representation of doubly exchangeable see-do forecasts]\label{th:rep_dex_sdf}
Given a see-do forecast $(\kernel{T},\RV{D},\RV{X},\RV{Y})$ with denumerable $D$ and standard measurable $X$, $Y$, the following statements are equivalent (given finite $A\subset\mathbb{N}$, $B\subset\mathbb{N}$ not necessarily finite):

\begin{enumerate}
    \item $(\kernel{T},\RV{D},\RV{X},\RV{Y})$ is doubly exchangeable with respect to $\RV{D}=\utimes_{i\in A}\RV{D}_i$, $\RV{X}=\utimes_{i\in B} \RV{X}_i$, $\RV{Y}=\utimes_{i\in A} \RV{Y}_i$
    \item There exists a set $H$, $\kernel{T}^{\RV{H}}\in \Delta(\sigalg{H})$ and Markov kernels $\kernel{T}^{\RV{X}_1|\RV{H}}$ and $\kernel{T}^{\RV{Y}_1|\RV{H}\RV{D}_1}$ such that
    \begin{align}
        \kernel{T}^{\RV{X}\RV{Y}|\RV{D}} = \begin{tikzpicture}
            \path (0,0) node[dist,inner sep=-2pt] (H) {$\kernel{T}^{\RV{H}}$}
            + (0,-1.4) node (D) {$\RV{D}_1$}
            + (0,-1.9) node (D2) {$\RV{D}_2$}
            + (0,-2.4) node (D3) {$...$}
            + (0,-2.7) node (D4) {$\RV{D}_{|A|}$}
            ++ (0.5,0) node[copymap] (copy0) {}
            ++ (0.6,0) node[copymap,label={$B$}] (copy1) {}
            ++ (1.2,0.5) node[kernel] (XH) {$\kernel{T}^{\RV{X}_1|\RV{H}}$}
            + (0,-0.5) node[kernel] (XH2) {$\kernel{T}^{\RV{X}_1|\RV{H}}$}
            + (0,-1.75) node[kernel] (YD) {$\kernel{T}^{\RV{Y}_1|\RV{H}\RV{D}_1}$}
            + (-.9,-1.6) node[copymap,label={$A$}] (copy2) {}
            + (0,-2.25) node[kernel] (YD2) {$\kernel{T}^{\RV{Y}_1|\RV{H}\RV{D}_1}$}
            + (0,-2.95) node[kernel] (YD3) {$\kernel{T}^{\RV{Y}_1|\RV{H}\RV{D}_1}$}
            ++ (1.2,-0.25) node (X1) {$\RV{X}$}
            + (0,-1.5) node (Y) {$\RV{Y}$};
            \draw (H) -- (copy1) (copy1) to [out=35,in=180] (XH) (copy1) to [out=-35,in=180] (XH2);
            \draw ($(XH.east)$) to [out=-15,in=180] ($(X1.west) + (-0.2,0.25)$) ($(XH2.east)$) to [out=15,in=180] ($(X1.west)+(-0.2,-0.25)$);
            \draw ($(X1.west) + (-0.2,0)$) to (X1);
            \draw (copy0) to [out=-90,in=180] (copy2) -- ($(YD.west) + (0,0.15)$);
            \draw (D) to [out=0,in=180] ($(YD.west)+(0,-0.15)$);
            \draw (D2) to [out=0,in=180] ($(YD2.west)+(0,-0.15)$);
            \draw (D4) to [out=0,in=180] ($(YD3.west)+(0,-0.15)$);
            \draw (copy2) to [out=-45,in=180] ($(YD2.west) + (0,0.15)$);
            \draw (copy2) to [out=-65,in=180] ($(YD3.west) + (0,0.15)$);
            \draw ($(copy1.west)+(-0.1,0.8)$) rectangle ($(X1.west) + (-0.2,-0.55)$);
            \draw ($(copy2.west)+(-0.1,0.5)$) rectangle ($(Y.west) + (-0.2,-1.75)$);
            \draw ($(Y.west) + (-0.2,0)$) -- (Y);
        \end{tikzpicture}
    \end{align}
    \item There exists a set $H$, $\mu\in \Delta(\sigalg{H})$ and Markov kernels $\kernel{T}^{\RV{H}}$, $\kernel{T}^{\RV{X}_1|\RV{H}}$ and $\kernel{T}^{\RV{Y}_1|\RV{H}\RV{D}_1}$ such that for all $\mathbf{d}_A\in D$, $\{J_i\in \sigalg{X}_1|i\in B\}$, $\{K_i\in \sigalg{Y}_1|i\in A\}$:
    \begin{align}
        \kernel{T}^{\RV{X}\RV{Y}|\RV{D}}_{\mathbf{d}_A}((\times_{i\in B}J_i)\times (\times_{j\in A} K_j)) = \int_{H} \prod_{i\in B} \kernel{T}^{\RV{X}_1|\RV{H}}_h(J_i)\prod_{i\in A}\kernel{T}_{h,d_i}^{\RV{Y}_1|\RV{H}\RV{D}_1}(K_i)d\kernel{T}^{\RV{H}}(h)
    \end{align}

\end{enumerate}
\end{theorem}

\begin{proof}
$(2)$ and $(3)$ are string and integral notation for the same statement. 

$(1)\implies (2)$:

Define $\prob{P}\in \Delta(\sigalg{D_1}^{\mathbb{N}})$ such that $\prob{P}=\otimes_{i\in\mathbb{N}} \prob{P}^{\RV{D}_1}$ for some strictly positive $\prob{P}^{\RV{D}_1}$ (recall that $D$ and hence $D_1$ is denumerable). $\prob{P}$ is exchangeable and independent and identically distributed. Consider some infinite doubly exchangeable extension $\kernel{T}'$ of $\kernel{T}$. Then $\prob{P}\kernel{T}'$ is exchangeable with respect to $\RV{DY}':=\utimes_{i\in \mathbb{N}}\RV{D}'_i\utimes\RV{Y}'_i$ (Lemma \ref{lem:f-ex2ex}) and exchangeable with respect to $\RV{X}'=\utimes_{i\in\mathbb{N}} \RV{X}'_i$ as $\RV{X}$ is independent of $\RV{D}$.


By Lemma \ref{lem:rep_seedo_obs} we have $\RV{Z}':= f\circ\RV{X}'$ for some $f$ such that 
\begin{enumerate}
    \item $\RV{X}'_i\CI_{\prob{P}\kernel{T}'}\RV{X}'_{\mathbb{N}\setminus\{i\}}|\RV{Z}'$ for all $i\in A$
    \item $(\prob{P}\kernel{T}')^{\RV{X}'_i|\RV{Z}''}=(\prob{P}\kernel{T}'')^{\RV{X}'_j|\RV{Z}''}$ for all $i,j\in A$
    \item $\RV{D}\utimes\RV{Y}'\CI_{\prob{P}\kernel{T}'}\RV{X}'|\RV{Z}'$
\end{enumerate}

Applying Lemma  \ref{lem:rep_seedo_obs} to $\RV{DY}'$, and noting that $\RV{D}'\utimes\RV{Y}'$ is an invertible function of $\RV{DY}'$, we have $\RV{W}'=g\circ\RV{DY}'$ for some $g$ such that

\begin{enumerate}
    \setcounter{enumi}{3}
    \item for all $i\in \mathbb{N}$, $\RV{D}_i\utimes\RV{Y}'_i\CI_{\prob{P}\kernel{T}'}\RV{D}'_{\mathbb{N}\setminus\{i\}}\RV{Y}'_{\mathbb{N}\setminus\{i\}}|\RV{W}'$
    \item $(\prob{P}\kernel{T}')^{\RV{Y}'_i|\RV{D}'_i}=(\prob{P}\kernel{T}')^{\RV{Y}'_j|\RV{D}'_j}$ for all $i,j\in \mathbb{N}$ 
    \item $\RV{X}'\CI_{\prob{P}\kernel{T}'}\RV{D}'\utimes\RV{Y}'|\RV{W}'$
\end{enumerate}

Because $\RV{W}'\utimes\RV{D}'$ is a function of $\RV{DY}'$, we also have $\RV{X}'\CI_{\prob{P}\kernel{T}'} \RV{W}'\utimes\RV{D}'|\RV{Z}'$ by property 6.

$\RV{D}'$ is also a function of $\RV{DY}'$ and $\RV{Z}'$ is a function of $\RV{X}'$ so $\RV{D}'\CI_{\prob{P}\kernel{T}'}\RV{Z}'|\RV{W}'$, also by property 6. Because $\prob{P}$ is independent and identically distributed, $\RV{D}'\CI_{\prob{P}\kernel{T}'} \RV{W}'$, so $\RV{D}'\CI_{\prob{P}\kernel{T}'}\RV{Z}'\utimes \RV{W}'$

Becuase $\RV{Z}'$ is a function of $\RV{X}'$, $\RV{Y}'\CI_{\prob{P}\kernel{T}'}\RV{Z}'|\RV{D}'\utimes\RV{X}'\utimes\RV{W}'$. 

Applying weak union and symmetry to property 6 we have $\RV{Y}'\CI \RV{X}'|\RV{W}'\utimes\RV{D}'$, which combined with the above gives $\RV{Y}'\CI \RV{X}'\utimes \RV{Z}' |\RV{W}'\utimes\RV{D}'$ by contraction.

In summary, we will use the following conditional independences:
\begin{itemize}
    \item $\RV{X}'\CI_{\prob{P}\kernel{T}'} \RV{W}'\utimes \RV{D}'|\RV{Z}'$
    \item $\RV{D}'\CI_{\prob{P}\kernel{T}'}\RV{Z}'\utimes \RV{W}'$
    \item $\RV{Y}'\CI_{\prob{P}\kernel{T}'}\RV{Z}'|\RV{D}'\utimes\RV{X}'\utimes\RV{W}'$
    \item $\RV{Y}'\CI \RV{X}'\utimes \RV{Z}'|\RV{W}'\utimes\RV{D}'$
\end{itemize}

Let $\prob{U}:=\prob{P}\kernel{T}'$. By Lemma \ref{lem:representation_of_kernels}, and 
\begin{align}
    (\prob{P}\kernel{T}')^{\RV{X}'\RV{Y}'\RV{D}'} &= \begin{tikzpicture}
        \path (0,0) node[dist,inner sep=-2pt] (Z) {$\prob{U}^{\RV{Z}'}$}
        ++ (0.5,0) node[copymap] (copy0) {}
        ++ (0.7,0) node[kernel] (W) {$\prob{U}^{\RV{W}'|\RV{Z}'}$}
        ++ (0.9,0) node[copymap] (copy1) {}
        ++ (1.0,0.) node[kernel] (D) {$\prob{U}^{\RV{D'}|\RV{W'Z'}}$}
        ++ (0.9,0) node[copymap] (copy2) {}
        ++ (1.3,0.5)  node[kernel] (X) {$\prob{U}^{\RV{X}'|\RV{Z'W'D'}}$}
        ++ (0.9,0) node[copymap] (copy3) {}
        ++ (1.3,-0.5) node[kernel] (Y) {$\prob{U}^{\RV{Y'}|\RV{X'W'Z'D'}}$}
        ++ (1.5,0) node (Yrv) {$\RV{Y}'$}
        +  (0,0.5) node (Xrv) {$\RV{X}'$}
        +  (0,-0.5) node (Drv) {$\RV{D}'$};
        \draw (Z) -- (W);
        \draw (copy0) to [out=-65,in=180] ($(D.west) + (0,-0.15)$);
        \draw (copy0) to [out=65,in=180] ($(X.west) + (0,-0.15)$);
        \draw (copy1) to [out=65,in=180] ($(X.west) + (0,0)$);
        \draw (copy1) to [out=25,in=180] ($(D.west) + (0,0.15)$);
        \draw (copy2) to [out=25,in=180] ($(X.west) + (0,0.15)$);
        \draw (copy0) to [out=-65,in=180] ($(Y.west) + (0,-0.18)$);
        \draw (copy1) to [out=-65,in=180] ($(Y.west) + (0,-0.09)$);
        \draw (copy2) to [out=-65,in=180] ($(Y.west) + (0,0.09)$);
        \draw (copy3) to [out=-65,in=180] ($(Y.west) + (0,0.18)$);
        \draw (Z) -- (copy0) (W) -- (copy1) (D) -- (copy2);
        \draw (X) -- (Xrv) (Y) -- (Yrv);
        \draw (copy2) to [out=-90,in=180] (Drv);
    \end{tikzpicture}\\
     &= \begin{tikzpicture}
        \path (0,0) node[dist,inner sep=-2pt] (Z) {$\prob{U}^{\RV{Z}'}$}
        ++ (0.5,0) node[copymap] (copy0) {}
        ++ (0.7,0) node[kernel] (W) {$\prob{U}^{\RV{W}'|\RV{Z}'}$}
        ++ (0.9,0) node[copymap] (copy1) {}
        ++ (1.0,0.) node[dist,inner sep=-2pt] (D) {$\prob{U}^{\RV{D'}}$}
        ++ (0.9,0) node[copymap] (copy2) {}
        ++ (1.3,0.5)  node[kernel] (X) {$\prob{U}^{\RV{X}'|\RV{Z'}}$}
        ++ (0.9,0) node[copymap] (copy3) {}
        ++ (1.3,-0.5) node[kernel] (Y) {$\prob{U}^{\RV{Y'}|\RV{W'D'}}$}
        ++ (1.5,0) node (Yrv) {$\RV{Y}'$}
        +  (0,0.5) node (Xrv) {$\RV{X}'$}
        +  (0,-0.5) node (Drv) {$\RV{D}'$};
        \draw (Z) -- (W);
        \draw[-{Rays[n=8]}] (copy0) to [out=-65,in=180] ($(D.west) + (-0.1,-0.15)$);
        \draw (copy0) to [out=65,in=180] ($(X.west) + (0,-0.15)$);
        \draw[-{Rays[n=8]}] (copy1) to [out=65,in=180] ($(X.west) + (-0.4,0)$);
        \draw[-{Rays[n=8]}] (copy1) to [out=25,in=180] ($(D.west) + (-0.1,0.15)$);
        \draw[-{Rays[n=8]}] (copy2) to [out=25,in=180] ($(X.west) + (-0.4,0.15)$);
        \draw[-{Rays[n=8]}] (copy0) to [out=-65,in=180] ($(Y.west) + (-0.4,-0.18)$);
        \draw (copy1) to [out=-65,in=180] ($(Y.west) + (0,-0.09)$);
        \draw (copy2) to [out=-65,in=180] ($(Y.west) + (0,0.09)$);
        \draw[-{Rays[n=8]}] (copy3) to [out=-65,in=180] ($(Y.west) + (-0.4,0.18)$);
        \draw (Z) -- (copy0) (W) -- (copy1) (D) -- (copy2);
        \draw (X) -- (Xrv) (Y) -- (Yrv);
        \draw (copy2) to [out=-90,in=180] (Drv);
    \end{tikzpicture}\\
     &= \begin{tikzpicture}
        \path (0,0) node[dist,inner sep=-2pt] (Z) {$\prob{U}^{\RV{Z}'}$}
        + (0,-1) node[dist,inner sep=-2pt] (D) {$\prob{U}^{\RV{D'}}$}
        + (0.9,-1) node[copymap] (copy2) {}
        ++ (0.5,0) node[copymap] (copy0) {}
        ++ (0.7,-0.5) node[kernel] (W) {$\prob{U}^{\RV{W}'|\RV{Z}'}$}
        ++ (1.7,0.5)  node[kernel] (X) {$\prob{U}^{\RV{X}'|\RV{Z'}}$}
        +  (0,-1) node[kernel] (Y) {$\prob{U}^{\RV{Y'}|\RV{W'D'}}$}
        ++ (1.5,-1) node (Yrv) {$\RV{Y}'$}
        +  (0,1) node (Xrv) {$\RV{X}'$}
        +  (0,-0.5) node (Drv) {$\RV{D}'$};
        \draw (copy0) to [out=-60,in=180] ($(W.west) + (0,0)$);
        \draw (copy0) to [out=0,in=180] ($(X.west) + (0,0)$);
        \draw (W) to [out=0,in=180] ($(Y.west) + (0,0.15)$);
        \draw (copy2) to [out=0,in=180] ($(Y.west) + (0,-0.15)$);
        \draw (Z) -- (copy0) (D) -- (copy2);
        \draw (X) -- (Xrv) (Y) -- (Yrv);
        \draw (copy2) to [out=-60,in=180] (Drv);
    \end{tikzpicture}
\end{align}

By mutual indepenence of $\RV{Y}'_i\utimes\RV{D}'_i$'s', we have in particular $\RV{X}'_A\utimes \RV{Y}'_A\utimes \RV{D}'_A\CI_{\prob{U}} \RV{X}'_{A^C}\utimes \RV{Y}'_{A^C}\utimes \RV{D}'_{A^C}$. Therefore $\prob{U}^{\RV{X}'_A\RV{Y}'_A\RV{D}'_A}=(\prob{P}^{\RV{D}_A}\kernel{T})$. Furthermore, $\prob{P}^{\RV{D}_A}$ is positive by assumption, so by Lemma \ref{lem:agree_disint}:

\begin{align}
    \kernel{T} &= \prob{U}^{\RV{X}'_A\RV{Y}'_A|\RV{D}'_A}\\
               &= \begin{tikzpicture}
        \path (0,0) node[dist,inner sep=-2pt] (Z) {$\prob{U}^{\RV{Z}'}$}
        + (0,-1) node (D) {$\RV{D}$}
        + (0.9,-1) node[copymap] (copy2) {}
        ++ (0.5,0) node[copymap] (copy0) {}
        ++ (0.7,-0.5) node[kernel] (W) {$\prob{U}^{\RV{W}'|\RV{Z}'}$}
        ++ (1.7,0.5)  node[kernel] (X) {$\prob{U}^{\RV{X}'|\RV{Z'}}$}
        +  (0,-1) node[kernel] (Y) {$\prob{U}^{\RV{Y'}|\RV{W'D'}}$}
        ++ (1.5,-1) node (Yrv) {$\RV{Y}$}
        +  (0,1) node (Xrv) {$\RV{X}$};
        \draw (copy0) to [out=-60,in=180] ($(W.west) + (0,0)$);
        \draw (copy0) to [out=0,in=180] ($(X.west) + (0,0)$);
        \draw (W) to [out=0,in=180] ($(Y.west) + (0,0.15)$);
        \draw (copy2) to [out=0,in=180] ($(Y.west) + (0,-0.15)$);
        \draw (Z) -- (copy0) (D) -- (copy2);
        \draw (X) -- (Xrv) (Y) -- (Yrv);
    \end{tikzpicture}\\
    &\overset{def}{=} \begin{tikzpicture}
        \path (0,0) node[dist,inner sep=-2pt] (Z) {$\prob{T}^{\RV{H}}$}
        + (0,-1) node (D) {$\RV{D}$}
        ++ (0.5,0) node[copymap] (copy0) {}
        ++ (1.7,0)  node[kernel] (X) {$\kernel{T}^{\RV{X}|\RV{H}}$}
        +  (0,-1) node[kernel] (Y) {$\kernel{T}^{\RV{Y}|\RV{H D}}$}
        ++ (1.5,-1) node (Yrv) {$\RV{Y}$}
        +  (0,1) node (Xrv) {$\RV{X}$};
        \draw (copy0) to [out=0,in=180] ($(X.west) + (0,0)$);
        \draw (copy0) to [out=-25,in=180] ($(Y.west) + (0,0.15)$);
        \draw (D) to [out=0,in=180] ($(Y.west) + (0,-0.15)$);
        \draw (Z) -- (copy0);
        \draw (X) -- (Xrv) (Y) -- (Yrv);
    \end{tikzpicture}
\end{align}

We still need to show

\begin{align}
    \kernel{T}^{\RV{X}} &= \prob{U}^{\RV{X}'_A}\\
    &\overset{?}{=} \begin{tikzpicture}
            \path (0,0) node[dist,inner sep=-2pt] (H) {$\prob{U}^{\RV{H}'}$}
            ++ (0.5,0) node[copymap] (copy0) {}
            ++ (0.6,0) node[copymap,label={$A$}] (copy1) {}
            ++ (1.2,0.5) node[kernel] (XH) {$\prob{U}^{\RV{X}'_1|\RV{H}''}$}
            + (0,-0.5) node[kernel] (XH2) {$\prob{U}^{\RV{X}'_1|\RV{H}''}$}
            ++ (1.2,-0.25) node (X1) {$\RV{X}$};
            \draw (H) -- (copy1) (copy1) to [out=35,in=180] (XH) (copy1) to [out=-35,in=180] (XH2);
            \draw ($(XH.east)$) to [out=-15,in=180] ($(X1.west) + (-0.2,0.25)$) ($(XH2.east)$) to [out=15,in=180] ($(X1.west)+(-0.2,-0.25)$);
            \draw ($(X1.west) + (-0.2,0)$) to (X1);
            \draw ($(copy1.west)+(-0.1,0.8)$) rectangle ($(X1.west) + (-0.2,-0.55)$);
        \end{tikzpicture}
\end{align}

and

\begin{align}
    \kernel{T}^{\RV{Y}|\RV{D}} &= \prob{U}^{\RV{Y}'_A|\RV{D'}_A} \\
    &\overset{?}{=}
    \begin{tikzpicture}
            \path (0,0) node[dist,inner sep=-2pt] (H) {$\prob{U}^{\RV{H}'}$}
            + (0,-0.7) node (D) {$\RV{D}_1$}
            + (0,-1.2) node (D2) {$\RV{D}_2$}
            + (0,-1.6) node (D3) {$...$}
            + (0,-1.9) node (D4) {$\RV{D}_{|A|}$}
            ++ (0.5,0) coordinate (copy0)
            ++ (2,0.5) coordinate (placeholder)
            + (0,-0.95) node[kernel] (YD) {$\prob{U}^{\RV{Y'}_1|\RV{H'}\RV{D'}_1}$}
            + (-1.5,-0.8) node[copymap,label={$A$}] (copy2) {}
            + (0,-1.45) node[kernel] (YD2) {$\prob{U}^{\RV{Y'}_1|\RV{H'}\RV{D'}_1}$}
            + (0,-2.35) node[kernel] (YD3) {$\prob{U}^{\RV{Y'}_1|\RV{H'}\RV{D'}_1}$}
            + (1.9,-1.5) node (Y) {$\RV{Y'}$};
            \draw (H) to [out=0,in=180] (copy2) -- ($(YD.west) + (0,0.15)$);
            \draw (D) to [out=0,in=180] ($(YD.west)+(0,-0.15)$);
            \draw (D2) to [out=0,in=180] ($(YD2.west)+(0,-0.15)$);
            \draw (D4) to [out=0,in=180] ($(YD3.west)+(0,-0.15)$);
            \draw (copy2) to [out=-45,in=180] ($(YD2.west) + (0,0.15)$);
            \draw (copy2) to [out=-65,in=180] ($(YD3.west) + (0,0.15)$);
            \draw ($(copy2.west)+(-0.1,0.5)$) rectangle ($(Y) + (-0.5,-1.25)$);
            \draw ($(Y) + (-0.5,0)$) -- (Y);
            \draw (YD) -- ($(YD) + (1.4,0)$) (YD2) -- ($(YD2) + (1.4,0)$) (YD3) -- ($(YD3) + (1.4,0)$);
        \end{tikzpicture}\label{eq:fex_copyrep}
\end{align}

Where $\RV{H}'=\RV{Z}'\utimes\RV{W}'$.

The first follows straightforwardly from the mutual independence of the $\RV{X}'_i$'s. The following diagram uses ``$...$'' informally to indicate a missing section of diagram that continues ``as you'd expect''. Applying Lemma \ref{lem:representation_of_kernels}, we have

\begin{align}
    \begin{tikzpicture}
        \path (0,0) node[dist,inner sep=-2pt] (H) {$\prob{U}^{\RV{H}'}$}
        ++ (1,0) node[kernel] (Xzd) {$\prob{U}^{\RV{X}'_A|\RV{H}'}$}
        ++ (1.5,0.) node (X) {$\RV{X}'_A$};
        \draw (H) to [out=0,in=180] ($(Xzd.west) + (0,0)$);
        \draw (Xzd) -- (X);
    \end{tikzpicture} &=  \begin{tikzpicture}
        \path (0,0) node[dist,inner sep=-2pt] (H) {$\prob{U}^{\RV{H}'}$}
        ++ (0.9,0) node[copymap] (copy1) {}
        ++ (0.9,0) node[kernel] (Xzd) {$\prob{U}^{\RV{X}_1|\RV{Z}}$}
        + (0.7,0) node[copymap] (copy2) {}
        ++ (1.5,-0.5) node[kernel] (Xzd2) {$\prob{U}^{\RV{X}_2|\RV{Z}\RV{X}_1}$}
        +  (0.8,0) node[copymap] (copy3) {}
        ++ (1.5,0.5) node (X) {$\RV{X}_1$}
        + (0,-0.5) node (X2) {$\RV{X}_2$}
        + (-2.5,-1) node (X3) {$...$}
        + (0,-1.5) node (X4) {$\RV{X}_{|A|}$};
        \draw (H) -- (Xzd);
        \draw (copy1) to [out=-90,in=180] ($(Xzd2.west) + (0,0)$);
        \draw (copy2) to [out=-90,in=180] ($(Xzd2.west) + (0,0.15)$);
        \draw (Xzd) -- (X) (Xzd2) -- (X2);
        \draw (copy1) to [out=-90,in=115] ($(copy1.south) + (0.1,-0.8)$);
        \draw (copy2) to [out=-90,in=115] ($(copy2.south) + (0.1,-0.8)$);
        \draw (copy3) to [out=-90,in=115] ($(copy3.south) + (0.1,-0.4)$);
        \draw ($(X4.west) + (-0.2,0.2)$) to [out=-45,in=180] (X4.west);
    \end{tikzpicture}\\
    &= \begin{tikzpicture}
        \path (0,0) node[dist,inner sep=-2pt] (H) {$\prob{U}^{\RV{H}'}$}
        ++ (0.9,0) node[copymap] (copy1) {}
        ++ (0.9,0) node[kernel] (Xzd) {$\prob{U}^{\RV{X}_1|\RV{Z}}$}
        + (0.7,0) node[copymap] (copy2) {}
        ++ (1.5,-0.5) node[kernel] (Xzd2) {$\prob{U}^{\RV{X}_2|\RV{Z}}$}
        +  (0.8,0) node[copymap] (copy3) {}
        ++ (1.5,0.5) node (X) {$\RV{X}_1$}
        + (0,-0.5) node (X2) {$\RV{X}_2$}
        + (-2.5,-1) node (X3) {$...$}
        + (0,-1.5) node (X4) {$\RV{X}_{|A|}$};
        \draw (H) -- (Xzd);
        \draw (copy1) to [out=-90,in=180] ($(Xzd2.west) + (0,0)$);
        \draw[-{Rays[n=8]}] (copy2) to [out=-90,in=180] ($(Xzd2.west) + (0,0.15)$);
        \draw (copy1) to [out=-90,in=115] ($(copy1.south) + (0.1,-0.8)$);
        \draw[-{Rays[n=8]}] (copy2) to [out=-90,in=115] ($(copy2.south) + (0.1,-0.8)$);
        \draw[-{Rays[n=8]}] (copy3) to [out=-90,in=115] ($(copy3.south) + (0.1,-0.4)$);
        \draw (Xzd) -- (X) (Xzd2) -- (X2);
        \draw ($(X4.west) + (-0.2,0.2)$) to [out=-45,in=180] (X4.west);
    \end{tikzpicture}\\
      &= \begin{tikzpicture}
        \path (0,0) node[dist,inner sep=-2pt] (H) {$\prob{U}^{\RV{H}'}$}
        ++ (0.9,0) node[copymap] (copy1) {}
        ++ (0.9,0) node[kernel] (Xzd) {$\prob{U}^{\RV{X}_1|\RV{Z}}$}
        ++ (0,-0.5) node[kernel] (Xzd2) {$\prob{U}^{\RV{X}_1|\RV{Z}}$}
        ++ (1.5,0.5) node (X) {$\RV{X}_1$}
        + (0,-0.5) node (X2) {$\RV{X}_2$}
        + (-1.5,-1) node (X3) {$...$}
        + (0,-1.5) node (X4) {$\RV{X}_{|A|}$};
        \draw (H) -- (Xzd);
        \draw (copy1) to [out=-90,in=180] ($(Xzd2.west) + (0,0)$);
        \draw (copy1) to [out=-90,in=115] ($(copy1.south) + (0.1,-0.8)$);
        \draw (Xzd) -- (X) (Xzd2) -- (X2);
        \draw ($(X4.west) + (-0.2,0.2)$) to [out=-45,in=180] (X4.west);
    \end{tikzpicture}\\
    &=  \begin{tikzpicture}
            \path (0,0) node[dist,inner sep=-2pt] (H) {$\prob{U}^{\RV{H}}$}
            ++ (0.8,0) node[copymap,label={$A$}] (copy1) {}
            ++ (1.2,0.5) node[kernel] (XH) {$\prob{U}^{\RV{X}_1|\RV{Z}}$}
            + (0,-0.5) node[kernel] (XH2) {$\prob{U}^{\RV{X}_1|\RV{Z}}$}
            ++ (1.2,-0.25) node (X1) {$\RV{X}'_A$};
            \draw (H) -- (copy1) (copy1) to [out=35,in=180] (XH) (copy1) to [out=-35,in=180] (XH2);
            \draw ($(XH.east)$) to [out=-15,in=180] ($(X1.west) + (-0.2,0.25)$) ($(XH2.east)$) to [out=15,in=180] ($(X1.west)+(-0.2,-0.25)$);
            \draw ($(X1.west) + (-0.2,0)$) to (X1);
            \draw ($(copy1.west)+(-0.1,0.8)$) rectangle ($(X1.west) + (-0.2,-0.55)$);
        \end{tikzpicture}
\end{align}

As desired.

For $\prob{U}^{\RV{Y}'_A|\RV{D}'_A}$, define the ``interleaved'' random variable $\overset{~}{\RV{YD}'}_A=\utimes_{i\in A} \RV{Y}'_i\utimes\RV{D}'_i$. Note that this implies that $\prob{U}^{\RV{Y}'_A\RV{D}'_A}=\prob{U}^{\overset{~}{\RV{YD}'}_A}\rho$ where $\rho$ is a swap kernel that moves all the $\RV{D}'_i$s below the $\RV{Y}'_i$s. Then, applying what we just showed but substituting $\overset{~}{\RV{YD}'}_A$ for $\RV{X}'_A$,

\begin{align}
      \prob{U}^{\overset{~}{\RV{YD}'}_A} &= \begin{tikzpicture}
        \path (0,0) node[dist,inner sep=-2pt] (H) {$\prob{U}^{\RV{H}'}$}
        ++ (0.9,0) node[copymap] (copy1) {}
        ++ (0.9,0) node[kernel] (Xzd) {$\prob{U}^{\RV{Y}'_1\RV{D}'_1|\RV{Z}}$}
        ++ (0,-0.5) node[kernel] (Xzd2) {$\prob{U}^{\RV{Y}'_1\RV{D}'_1|\RV{Z}}$}
        ++ (2,0.5) node (X) {$\RV{Y'}_1\utimes\RV{D'}_1$}
        + (0,-0.5) node (X2) {$\RV{Y'}_2\utimes\RV{D'}_2$}
        + (-1,-1) node (X3) {$...$}
        + (0,-1.5) node (X4) {$\RV{Y'}_{|A|}\utimes\RV{D'}_{|A|}$};
        \draw (H) -- (Xzd);
        \draw (copy1) to [out=-90,in=180] ($(Xzd2.west) + (0,0)$);
        \draw (copy1) to [out=-90,in=115] ($(copy1.south) + (0.1,-0.8)$);
        \draw (Xzd) -- (X) (Xzd2) -- (X2);
        \draw ($(X4.west) + (-0.2,0.2)$) to [out=-45,in=180] (X4.west);
    \end{tikzpicture}\\
    &= \begin{tikzpicture}
        \path (0,0) node[dist,inner sep=-2pt] (H) {$\prob{U}^{\RV{H}'}$}
        + (0,-1.5) node[dist,inner sep=-3pt] (D) {$\prob{U}^{\RV{D}'_A}$}
        + (0.6,-1.35) node[copymap] (copy00) {}
        + (0.6,-1.5) node[copymap] (copy01) {}
        ++ (0.6,0) node[copymap] (copy1) {}
        ++ (1.2,0) node[kernel] (Xzd) {$\prob{U}^{\RV{Y}'_1|\RV{Z}}$}
        ++ (0,-1) node[kernel] (Xzd2) {$\prob{U}^{\RV{Y}'_1|\RV{Z}}$}
        ++ (1.5,1) node (X) {$\RV{Y'}_1$}
        + (0,-0.5) node (D1) {$\RV{D}'_1$}
        + (0,-1) node (X2) {$\RV{Y'}_2$}
        + (0,-1.5) node (D2) {$\RV{D'}_2$}
        + (-2,-2) node (X3) {$...$}
        + (0,-2.5) node (X4) {$\RV{Y'}_{|A|}$}
        + (0,-3) node (D4) {$\RV{D'}_{|A|}$};
        \draw (H) -- (Xzd);
        \draw (copy1) to [out=-65,in=180] ($(Xzd2.west) + (0,0)$);
        \draw (copy1) to [out=-90,in=115] ($(copy1.south) + (0.6,-1.8)$);
        \draw (Xzd) -- (X) (Xzd2) -- (X2);
        \draw ($(D.east) + (0,0.15)$) to [out=0,in=180] (copy00) to [out=65,in=180] ($(Xzd.west) + (0,-0.15)$);
        \draw ($(D.east) + (0,0.)$) to [out=0,in=180] (copy01) to [out=0,in=180] ($(Xzd2.west) + (0,-0.15)$);
        \draw (copy00) to [out=45,in=180] ($(copy00)+(0.7,0.85)$) -- (D1) (copy01) to [out=0,in=180] (D2);
        \draw (copy00) to [out=-45,in=115] ($(copy00.south) + (0.4,-0.5)$);
        \draw (copy01) to [out=-45,in=115] ($(copy01.south) + (0.2,-0.3)$);
        \draw ($(X4.west) + (-0.2,0.2)$) to [out=-45,in=180] (X4.west);
        \draw ($(D4.west) + (-0.2,0.2)$) to [out=-45,in=180] (D4.west);
    \end{tikzpicture}\\
    \implies \prob{U}^{\RV{Y}'_A\RV{D}'_A} &= \prob{U}^{\overset{~}{\RV{YD}'}_A}\rho\\
    &= \begin{tikzpicture}
        \path (0,0) node[dist,inner sep=-2pt] (H) {$\prob{U}^{\RV{H}'}$}
        + (0,-2.6) node[dist,inner sep=-3pt] (D) {$\prob{U}^{\RV{D}'_A}$}
        + (0.6,-2.45) node[copymap] (copy00) {}
        + (0.6,-2.7) node[copymap] (copy01) {}
        ++ (0.6,0) node[copymap] (copy1) {}
        ++ (1.2,0) node[kernel] (Xzd) {$\prob{U}^{\RV{Y}'_1|\RV{Z}}$}
        ++ (0,-0.5) node[kernel] (Xzd2) {$\prob{U}^{\RV{Y}'_1|\RV{Z}}$}
        ++ (2,0.5) node (X) {$\RV{Y'}_1$}
        + (0,-0.5) node (X2) {$\RV{Y'}_2$}
        + (-1.5,-1) node (X3) {$...$}
        + (0,-1.5) node (X4) {$\RV{Y'}_{|A|}$}
        + (0,-2.3) node (D1) {$\RV{D}'_1$}
        + (0,-2.8) node (D2) {$\RV{D'}_2$}
        + (-1,-3.3) node (D3) {$...$}
        + (0,-3.8) node (D4) {$\RV{D'}_{|A|}$};
        \draw (H) -- (Xzd);
        \draw (copy1) to [out=-65,in=180] ($(Xzd2.west) + (0,0)$);
        \draw (Xzd) -- (X) (Xzd2) to [out=0,in=180] (X2);
        \draw ($(D.east) + (0,0.15)$) to [out=0,in=180] (copy00) to [out=65,in=180] ($(Xzd.west) + (0,-0.15)$);
        \draw (copy1) to [out=-90,in=180] ($(copy1.south) + (0.6,-1.)$);
        \draw (copy00) to [out=35,in=180] ($(copy1.south) + (0.6,-1.1)$);
        \draw (copy01) to [out=35,in=180] ($(copy1.south) + (0.6,-1.2)$);
        \draw (copy00) to [out=-45,in=115] ($(copy00.south) + (0.3,-0.5)$);
        \draw (copy01) to [out=-45,in=105] ($(copy01.south) + (0.2,-0.35)$);
        \draw ($(D.east) + (0,0.)$) to [out=0,in=180] (copy01) to [out=0,in=180] ($(Xzd2.west) + (0,-0.15)$);
        \draw (copy00) to [out=0,in=180]  (D1) (copy01) to [out=-35,in=180] (D2);
        \draw ($(X4.west) + (-0.2,0.2)$) to [out=-45,in=180] (X4.west);
        \draw ($(D4.west) + (-0.2,0.2)$) to [out=-45,in=180] (D4.west);
        \draw[red] ($(H.west) + (-0.1,0.55)$) rectangle ($(X4.east) + (0.1,-0.4)$);
    \end{tikzpicture}
\end{align}

The red boxed kernel is the disintegration $\kernel{U}^{\RV{Y}'_A|\RV{D}'_A}$ and (notational difficulties aside) is equal to the kernel in Equation \ref{eq:fex_copyrep}.

This completes the proof that $(1)\implies (2)$.

$(2)\implies (1)$:

We will use integral notation. For all sets of events $\{J_i\in \sigalg{X}_1\}_B$, $\{K_i\in \sigalg{Y}_1\}_A$, $\mathbf{d}_A\in D$:

\begin{align}
  \kernel{T}^{\RV{X}\RV{Y}|\RV{D}}_{\mathbf{d}_A}((\times_{i\in B}J_i)\times (\times_{j\in A} K_j)) = \int_{H} \prod_{i\in B} \kernel{T}^{\RV{X}_1|\RV{H}}_h(J_i)\prod_{i\in A}\kernel{T}_{h,d_i}^{\RV{Y}_1|\RV{H}\RV{D}_1}(K_i)d\kernel{T}^{\RV{H}}(h)
\end{align}

For all $\{J_i\in \sigalg{X}_1\}_\mathbb{N}$, $\{K_i\in \sigalg{Y}_1\}_\mathbb{N}$, $\mathbf{d} \in D^{\mathbb{N}}$ define

\begin{align}
  \kernel{T}^{\prime \RV{X}'\RV{Y}'|\RV{D}'}_{\mathbf{d}}((\times_{i\in \mathbb{N}}J_i)\times (\times_{j\in \mathbb{N}} K_j)) = \int_{H} \prod_{i\in \mathbb{N}} \kernel{T}^{\RV{X}_1|\RV{H}}_h(J_i)\prod_{i\in \mathbb{N}}\kernel{T}_{h,d_i}^{\RV{Y}_1|\RV{H}\RV{D}_1}(K_i)d\kernel{T}^{\RV{H}}(h)
\end{align}

Marginalising over $\mathbb{N}\setminus A$ and $\mathbb{N}\setminus B$ is equivalent to choosing $K_i=Y_1$ for $i\not\in A$ and $J_i=X_1$ for $i\not\in B$. Then

\begin{align}
  \kernel{T}^{\prime \RV{X}'_B\RV{Y}'_A|\RV{D}'}_{\mathbf{d}}((\times_{i\in B}J_i\times X_1^{\mathbb{N}\setminus B})\times (\times_{j\in A} K_j\times Y_1^{\mathbb{N}\setminus A})) &= \int_{H} \prod_{i\in B} \kernel{T}^{\RV{X}_1|\RV{H}}_h(J_i) [\kernel{T}^{\RV{X}_1|\RV{H}}_h (X_1)]^{\mathbb{N}\setminus B} \prod_{i\in A}\kernel{T}_{h,d_i}^{\RV{Y}_1|\RV{H}\RV{D}_1}(K_i)[\kernel{T}^{\RV{Y}_1|\RV{H}}_h (Y_1)]^{\mathbb{N}\setminus A}d\kernel{T}^{\RV{H}}(h)\\
  &= \int_{H} \prod_{i\in B} \kernel{T}^{\RV{X}_1|\RV{H}}_h(J_i) \prod_{i\in A}\kernel{T}_{h,d_i}^{\RV{Y}_1|\RV{H}\RV{D}_1}(K_i)d\kernel{T}^{\RV{H}}(h)\\
  &= \kernel{T}^{\RV{X}\RV{Y}|\RV{D}}_{\mathbf{d}_A}((\times_{i\in B}J_i)\times (\times_{j\in A} K_j))
\end{align}

Thus $\kernel{T}'$ is an extension of $\kernel{T}$. Furthermore, for any swaps $\rho_r$, $\rho_s$ with associated kernels $R^{\RV{X}'}$, $S^{\RV{D}'}$,$S^{\RV{Y'}}$:

\begin{align}
  (S^{\RV{D'}\RV{Y'}}\kernel{T}'(R^{\RV{X'}}\otimes S^{\RV{Y}'}))^{\RV{X}'\RV{Y}'|\RV{D}'}_{\mathbf{d}}((\times_{i\in \mathbb{N}}J_i)\times (\times_{j\in \mathbb{N}} K_j)) &= \int_{H} \prod_{i\in \mathbb{N}} \kernel{T}^{\RV{X}_1|\RV{H}}_h(J_{\rho_r(i)})\prod_{i\in \mathbb{N}}\kernel{T}_{h,d_{\rho_s(i)}}^{\RV{Y}_1|\RV{H}\RV{D}_1}(K_{\rho_s(i)})d\kernel{T}^{\RV{H}}(h) \\
                            &= \int_{H} \prod_{i\in \rho_{r}(\mathbb{N})} \kernel{T}^{\RV{X}_1|\RV{H}}_h(J_{i})\prod_{i\in \rho_s(\mathbb{N})}\kernel{T}_{h,d_{i}}^{\RV{Y}_1|\RV{H}\RV{D}_1}(K_{i})d\kernel{T}^{\RV{H}}(h) \\
                            &= \kernel{T}^{\prime \RV{X}'\RV{Y}'|\RV{D}'}_{\mathbf{d}}((\times_{i\in \mathbb{N}}J_i)\times (\times_{j\in \mathbb{N}} K_j))
\end{align}

Therefore $\kernel{T}$ is infinitely doubly exchangeably extendable.
\end{proof}